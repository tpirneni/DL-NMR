{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize CNN architecture and hyperparameters on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epochs used later in training (this will have to be redefined later......)\n",
    "num_epochs = 5000\n",
    "\n",
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"CNN_Opt_Dist5_RAE_44Met\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1 \n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('Dataset44_Dist5_Spec.npy')\n",
    "conc1 = np.load('Dataset44_Dist5_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load('Dataset44_Dist5_Val_Spec.npy')\n",
    "concVal = np.load('Dataset44_Dist5_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "ValSpectra = np.load(\"Dataset44_Dist5_RepresentativeExamples_Spectra.npy\")\n",
    "ValConc = np.load(\"Dataset44_Dist5_RepresentativeExamples_Concentrations.npy\")\n",
    "ValSpecNames = np.load(\"Dataset44_Dist5_RepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "Val_datasets = torch.utils.data.TensorDataset(spectraVal, concVal)\n",
    "#train_iter = torch.utils.data.DataLoader(datasets, batch_size = 128, shuffle=True)\n",
    "#test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del spectraVal\n",
    "del concVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae\n",
    "    \n",
    "    \n",
    "    \n",
    "# MAPE loss function for directly comparing models despite loss function used\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "        return loss * 100  # To get percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 12:28:22,959] A new study created in RDB with name: CNN44_Dist5_RAE_Opt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 | Train Loss: 15261.125 | Test Loss: 3659.900 | Test Loss [MAPE]: 901713.773 | Test Loss [MSE]: 185991.707 --time-- 22.40710973739624\n"
     ]
    }
   ],
   "source": [
    "## Optimization function\n",
    "\n",
    "# Switch to directory for saving weights\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelOptimizations/BestWeights')\n",
    "\n",
    "# Define file name for best model weights\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "\n",
    "# Define the objective function to be minimized.\n",
    "def objective(trial):\n",
    "    \n",
    "\n",
    "    # Suggest values of the hyperparameters using a trial object.\n",
    "    n_conv_layers = trial.suggest_int('num_conv_layers', 1, 4)\n",
    "\n",
    "    # Make some empty variables for use in model building\n",
    "    layers = []\n",
    "    \n",
    "    # Define other hyperparameters\n",
    "    kernel_size = trial.suggest_int('kernel_size', 3, 9)\n",
    "    num_channels = trial.suggest_int('num_channels', 5, 50)\n",
    "    pooling_type = trial.suggest_categorical('pooling_type', ['none', 'max', 'avg'])\n",
    "    pool_stride = trial.suggest_int('pool_stride', 1, 2)\n",
    "    conv_stride = 1\n",
    "    pool_kernel = 2\n",
    "    \n",
    "    n_fc_layers = 2  # Number of fully connected layers\n",
    "\n",
    "    \n",
    "    # Build the CNN architecture\n",
    "    in_channels = 1  # Assuming 1 input channel\n",
    "    in_features = 46000  # Number of input features\n",
    "    dilation = 1\n",
    "    padding = 1\n",
    "    for i in range(n_conv_layers):\n",
    "        out_channels = num_channels\n",
    "        layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation, padding=padding))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_features = ((in_features + 2*padding - dilation*(kernel_size - 1) - 1)/conv_stride) + 1\n",
    "        in_features = int(in_features)\n",
    "        \n",
    "        in_channels = out_channels\n",
    "        \n",
    "        if pooling_type == 'none':\n",
    "            pass\n",
    "        elif pooling_type == 'max':\n",
    "            layers.append(nn.MaxPool1d(pool_kernel, stride=pool_stride, dilation=dilation, padding=padding))\n",
    "            in_features = ((in_features + 2*padding - dilation*(pool_kernel - 1) - 1)/pool_stride) + 1\n",
    "            in_features = int(in_features)\n",
    "        else:\n",
    "            layers.append(nn.AvgPool1d(pool_kernel, stride=pool_stride, padding=padding))\n",
    "            in_features = ((in_features + 2*padding - pool_kernel)/pool_stride) + 1\n",
    "            in_features = int(in_features)\n",
    "                \n",
    "    # Flatten the output for fully connected layers\n",
    "    layers.append(nn.Flatten())\n",
    "    \n",
    "    # Add fully connected layers\n",
    "    for _ in range(n_fc_layers):\n",
    "        out_features = 200\n",
    "        layers.append(nn.Linear(in_features*in_channels, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_features = out_features\n",
    "        in_channels = 1\n",
    "    \n",
    "    # Add the final fully connected layer\n",
    "    layers.append(nn.Linear(in_features, 44))   # For quantifying 44 metabolites\n",
    "    \n",
    "    model = nn.Sequential(*layers)\n",
    "    model.to(device)  # Move the model to the GPU    \n",
    "\n",
    "    \n",
    "     # Train and evaluate the model to obtain the loss\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "                     \n",
    "    '''\n",
    "    # Add (or don't add) L2 regularization\n",
    "    reg_type = trial.suggest_categorical('regularization', ['none', 'l2'])\n",
    "    reg_strength = trial.suggest_float('reg_strength', 1e-6, 1e-3)\n",
    "    \n",
    "    if optimizer_name == \"Adam\":\n",
    "        if reg_type == 'l2':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg_strength)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        if reg_type == 'l2':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=reg_strength)\n",
    "        else:\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)    \n",
    "    else:\n",
    "        optimizer_name == \"SGD\"\n",
    "        if reg_type == 'l2':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=reg_strength)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "    '''\n",
    "    \n",
    "        \n",
    "    ## Split training data appropriately, selecting the batch size as a hyperparameter\n",
    "    bs = 16\n",
    "    train_loader = torch.utils.data.DataLoader(datasets, batch_size=bs, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(Test_datasets, batch_size=bs, shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(Val_datasets, batch_size=bs, shuffle=False)\n",
    "    \n",
    "    num_epochs = 500\n",
    "    \n",
    "    criterion = RelativeAbsoluteError()\n",
    "    criterion2 = MAPELoss()\n",
    "    criterion3 = nn.MSELoss()  #This just for direct comparison with past MSE trained models\n",
    "    best_test_loss = float('inf')\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        start2 = time.time()\n",
    "        running_train_loss = 0.\n",
    "        running_test_loss = 0.\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move input data to the GPU\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)  # Make sure labels are properly loaded and passed here\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()* inputs.size(0)\n",
    "        \n",
    "        # Testing phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move input data to the GPU\n",
    "                inputs = inputs.unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_test_loss += loss.item()* inputs.size(0)\n",
    "                \n",
    "                \n",
    "        ## Also compute just MAPE loss for model optimization\n",
    "        running_test_loss2 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "                loss2 = criterion2(outputs, targets)\n",
    "                running_test_loss2 += loss2.item() * inputs.size(0)\n",
    "                \n",
    "                \n",
    "        ## Also compute just MSE loss for more direct comparison with previous models\n",
    "        running_test_loss3 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "                loss3 = criterion3(outputs, targets)\n",
    "                running_test_loss3 += loss3.item()\n",
    "\n",
    "                \n",
    "        if running_test_loss2 < best_test_loss:\n",
    "            best_test_loss = running_test_loss2\n",
    "  \n",
    "\n",
    "\n",
    "        # Print loss every ___ epochs\n",
    "        if epoch % 1 == 0:\n",
    "            end_time2 = time.time()\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} | Train Loss: {running_train_loss:.3f} | Test Loss: {running_test_loss:.3f} | Test Loss [MAPE]: {running_test_loss2:.3f} | Test Loss [MSE]: {running_test_loss3:.3f}', \"--time--\", end_time2-start2)\n",
    "\n",
    "            \n",
    "        # Prune considered after a certain number of epochs)\n",
    "        if epoch > 20 and trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "            \n",
    "    end_time = time.time()\n",
    "    print(end_time - start)   \n",
    "    \n",
    "    return best_test_loss\n",
    "\n",
    "\n",
    "## Create study and optimize\n",
    "study_name = \"CNN44_Dist5_RAE_Opt\"\n",
    "storage = \"sqlite:///CNN44_Dist5_RAE_Opt.db\"  # SQLite database as storage\n",
    "study = optuna.create_study(direction='minimize',  sampler = optuna.samplers.TPESampler(seed = 1), \n",
    "                            study_name=study_name, storage=storage,\n",
    "                            pruner = optuna.pruners.MedianPruner(n_warmup_steps=30))\n",
    "\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to directory for saving weights\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelOptimizations/BestWeights/')\n",
    "\n",
    "loaded_study = optuna.load_study(study_name=\"CNN44_Opt\", storage=\"sqlite:///CNN44_Opt_30trials.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Hyperparameter     Value\n",
      "0         n_conv_layers         3\n",
      "1           kernel_size         9\n",
      "2          pooling_type      none\n",
      "3           pool_stride         2\n",
      "4   regularization_type      none\n",
      "5       batch_norm_used     False\n",
      "6           n_fc_layers         1\n",
      "7   conv_0_out_channels        24\n",
      "8   conv_1_out_channels        55\n",
      "9   conv_2_out_channels        27\n",
      "10      fc_out_features       252\n",
      "11        learning_rate  0.000117\n",
      "12           batch_size        64\n",
      "13            optimizer   RMSprop\n",
      "14            Best Loss  69.48823\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters and best value from the study\n",
    "best_params = loaded_study.best_params\n",
    "best_value = loaded_study.best_value\n",
    "\n",
    "# Create a dictionary to store hyperparameters and best loss\n",
    "data = {\n",
    "    'Hyperparameter': list(best_params.keys()) + ['Best Loss'],\n",
    "    'Value': list(best_params.values()) + [best_value]\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_optimization_history(study)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_contour(study)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_rank(study)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "optuna.visualization.plot_rank(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_slice(study)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_terminator_improvement(study)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "optuna.visualization.plot_terminator_improvement(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_timeline(study)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "optuna.visualization.plot_timeline(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_edf(study)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "optuna.visualization.plot_edf(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file name for best model weights\n",
    "save_path = ModelName + '_Params_30trials.pt'\n",
    "\n",
    "def get_best_model(study):\n",
    "    # Get the best trial\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    # Extract the best parameters\n",
    "    best_params = best_trial.params\n",
    "    \n",
    "    # Load the entire dictionary from the saved file\n",
    "    checkpoint = torch.load(save_path)\n",
    "    \n",
    "    # Load the model architecture from the checkpoint\n",
    "    model_architecture = checkpoint['model_architecture']\n",
    "    \n",
    "    # Initialize the model and move it to the appropriate device\n",
    "    model = model_architecture.to(device)\n",
    "    \n",
    "    # Load the model's state dictionary from the loaded dictionary\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    return model\n",
    "\n",
    "best_model = get_best_model(loaded_study).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.56735616922379"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(datasets, batch_size=64, shuffle=False)\n",
    "TheLoss = 0\n",
    "\n",
    "# Testing phase\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move input data to the GPU\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        TheLoss += loss.item()\n",
    "        \n",
    "TheLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16000, 46000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 46000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FirstEx = X_train.unsqueeze(1)[0]\n",
    "FirstEx.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0532, 49.2963, 37.4884, 10.4130, 22.6152, 13.7744, 16.9744, 15.2754,\n",
       "         33.8414, 39.4097, 36.6177, 16.6846, 23.7907, 18.3062, 30.6628, 30.9816,\n",
       "         26.7715, 14.7555,  2.8625, 41.7423, 37.2032, 46.3311,  1.9252, 14.5158,\n",
       "         26.6019, 32.6411, 39.1162, 36.9748, 35.2184, 33.2565, 31.9444, 36.7302,\n",
       "         35.4330, 49.6390,  7.5264,  7.8743,  9.8146, 17.3302, 29.6859,  1.5771,\n",
       "         35.7887, 34.5430, 44.2107, 41.5333]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = FirstEx.to(device)\n",
    "inputs = inputs.unsqueeze(1)\n",
    "outputs = best_model(inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.0717, 49.5768, 38.3677, 11.3346, 22.3017, 12.9476, 16.9491, 16.3040,\n",
       "        34.2957, 40.2190, 36.4923, 17.5898, 24.9011, 18.9798, 31.6682, 30.6385,\n",
       "        27.1793, 15.0093,  3.1209, 42.3842, 37.3571, 47.2943,  1.7990, 14.4903,\n",
       "        27.5353, 31.2422, 38.8733, 37.4527, 35.7897, 33.2167, 32.0472, 36.9620,\n",
       "        35.3223, 49.3332,  7.2213,  8.1352,  9.6507, 18.3274, 29.5438,  2.0174,\n",
       "        36.3401, 34.8497, 44.4357, 42.3643], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Switch to directory for saving model metrics\n",
    "\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ValConc[i]\n",
    "    Prediction = best_model(ValSpectra[i].unsqueeze(1))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.09  -  AllAq1\n",
      "7.34  -  AllAq5\n",
      "1.63  -  AllAq25\n",
      "11.71  -  AllAq50\n",
      "3.01  -  ThreeAddedSinglets\n",
      "8.0  -  ThirtyAddedSinglets\n",
      "84.95  -  ShiftedSpec\n",
      "17.5  -  SineBase\n",
      "469.07  -  HighDynamicRange\n",
      "inf  -  HalfZeros\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - \",ValSpecNames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
