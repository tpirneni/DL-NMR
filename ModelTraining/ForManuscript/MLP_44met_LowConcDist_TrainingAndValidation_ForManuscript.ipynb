{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 5000\n",
    "\n",
    "# Identification part of the filenames\n",
    "base_name = 'LowConc'\n",
    "base_dir = '/path/to/base/directory'   # Set base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"MLP_44met_{base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load(f'Dataset44_{base_name}_ForManuscript_Spec.npy')\n",
    "conc1 = np.load(f'Dataset44_{base_name}_ForManuscript_Conc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 169, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 169, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "size_hidden1 = 200\n",
    "size_hidden2 = 44\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(46000, size_hidden1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "    def forward(self, input):\n",
    "        return (self.lin2(self.relu1(self.lin1(input))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = RelativeAbsoluteError()\n",
    "    optimizer = optim.AdamW(model.parameters(), weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in best validation loss constitutes early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "            \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/5000], Train Loss: 14818.4830, Test Loss: 3315.7588\n",
      "Epoch [2/5000], Train Loss: 12267.5950, Test Loss: 2979.6366\n",
      "Epoch [3/5000], Train Loss: 10736.8965, Test Loss: 2698.8948\n",
      "Epoch [4/5000], Train Loss: 9426.7006, Test Loss: 2380.8268\n",
      "Epoch [5/5000], Train Loss: 8421.6052, Test Loss: 2181.9501\n",
      "Epoch [6/5000], Train Loss: 7493.1410, Test Loss: 2091.4819\n",
      "Epoch [7/5000], Train Loss: 6963.5400, Test Loss: 1897.3524\n",
      "Epoch [8/5000], Train Loss: 6525.6094, Test Loss: 1879.8510\n",
      "Epoch [9/5000], Train Loss: 6180.2221, Test Loss: 1785.2538\n",
      "Epoch [10/5000], Train Loss: 5804.4290, Test Loss: 1683.3210\n",
      "Epoch [11/5000], Train Loss: 5627.7847, Test Loss: 1656.2056\n",
      "Epoch [12/5000], Train Loss: 5381.8875, Test Loss: 1606.6188\n",
      "Epoch [13/5000], Train Loss: 5292.8734, Test Loss: 1582.8596\n",
      "Epoch [14/5000], Train Loss: 5172.2064, Test Loss: 1556.8457\n",
      "Epoch [15/5000], Train Loss: 4921.0807, Test Loss: 1485.4967\n",
      "Epoch [16/5000], Train Loss: 4832.6279, Test Loss: 1545.6947\n",
      "Epoch [17/5000], Train Loss: 4774.1382, Test Loss: 1544.5071\n",
      "Epoch [18/5000], Train Loss: 4702.8758, Test Loss: 1491.4515\n",
      "Epoch [19/5000], Train Loss: 4602.9547, Test Loss: 1442.5313\n",
      "Epoch [20/5000], Train Loss: 4505.2950, Test Loss: 1452.0812\n",
      "Epoch [21/5000], Train Loss: 4500.6384, Test Loss: 1414.2526\n",
      "Epoch [22/5000], Train Loss: 4372.6314, Test Loss: 1372.9891\n",
      "Epoch [23/5000], Train Loss: 4292.5732, Test Loss: 1358.6698\n",
      "Epoch [24/5000], Train Loss: 4320.3942, Test Loss: 1386.5187\n",
      "Epoch [25/5000], Train Loss: 4270.0608, Test Loss: 1347.6419\n",
      "Epoch [26/5000], Train Loss: 4218.3080, Test Loss: 1370.0964\n",
      "Epoch [27/5000], Train Loss: 4166.1481, Test Loss: 1357.9009\n",
      "Epoch [28/5000], Train Loss: 4073.4041, Test Loss: 1322.8432\n",
      "Epoch [29/5000], Train Loss: 4072.2163, Test Loss: 1411.8801\n",
      "Epoch [30/5000], Train Loss: 4035.1896, Test Loss: 1381.2032\n",
      "Epoch [31/5000], Train Loss: 3996.3123, Test Loss: 1315.5493\n",
      "Epoch [32/5000], Train Loss: 3934.3661, Test Loss: 1323.1077\n",
      "Epoch [33/5000], Train Loss: 3929.0983, Test Loss: 1347.6591\n",
      "Epoch [34/5000], Train Loss: 3946.1355, Test Loss: 1259.9849\n",
      "Epoch [35/5000], Train Loss: 3853.0453, Test Loss: 1296.1081\n",
      "Epoch [36/5000], Train Loss: 3766.4054, Test Loss: 1295.6825\n",
      "Epoch [37/5000], Train Loss: 3803.6818, Test Loss: 1268.4135\n",
      "Epoch [38/5000], Train Loss: 3698.0568, Test Loss: 1298.1132\n",
      "Epoch [39/5000], Train Loss: 3718.6045, Test Loss: 1250.7957\n",
      "Epoch [40/5000], Train Loss: 3751.5646, Test Loss: 1265.3320\n",
      "Epoch [41/5000], Train Loss: 3670.6783, Test Loss: 1276.0826\n",
      "Epoch [42/5000], Train Loss: 3678.2787, Test Loss: 1278.6588\n",
      "Epoch [43/5000], Train Loss: 3674.1107, Test Loss: 1257.6544\n",
      "Epoch [44/5000], Train Loss: 3676.6828, Test Loss: 1255.1195\n",
      "Epoch [45/5000], Train Loss: 3611.6523, Test Loss: 1242.9902\n",
      "Epoch [46/5000], Train Loss: 3553.8217, Test Loss: 1242.6738\n",
      "Epoch [47/5000], Train Loss: 3675.6390, Test Loss: 1203.2398\n",
      "Epoch [48/5000], Train Loss: 3520.9745, Test Loss: 1196.0868\n",
      "Epoch [49/5000], Train Loss: 3551.8645, Test Loss: 1206.2614\n",
      "Epoch [50/5000], Train Loss: 3549.8767, Test Loss: 1219.2607\n",
      "Epoch [51/5000], Train Loss: 3513.5409, Test Loss: 1253.2847\n",
      "Epoch [52/5000], Train Loss: 3517.0349, Test Loss: 1205.0119\n",
      "Epoch [53/5000], Train Loss: 3490.7816, Test Loss: 1231.2355\n",
      "Epoch [54/5000], Train Loss: 3500.3786, Test Loss: 1225.3289\n",
      "Epoch [55/5000], Train Loss: 3440.1153, Test Loss: 1218.7625\n",
      "Epoch [56/5000], Train Loss: 3496.4708, Test Loss: 1228.8388\n",
      "Epoch [57/5000], Train Loss: 3423.1979, Test Loss: 1188.1975\n",
      "Epoch [58/5000], Train Loss: 3414.1903, Test Loss: 1191.7795\n",
      "Epoch [59/5000], Train Loss: 3426.5778, Test Loss: 1247.9220\n",
      "Epoch [60/5000], Train Loss: 3375.7784, Test Loss: 1206.5927\n",
      "Epoch [61/5000], Train Loss: 3353.6556, Test Loss: 1190.9074\n",
      "Epoch [62/5000], Train Loss: 3353.6494, Test Loss: 1186.1054\n",
      "Epoch [63/5000], Train Loss: 3322.5865, Test Loss: 1184.5370\n",
      "Epoch [64/5000], Train Loss: 3334.7580, Test Loss: 1206.5325\n",
      "Epoch [65/5000], Train Loss: 3314.4590, Test Loss: 1162.5644\n",
      "Epoch [66/5000], Train Loss: 3268.1025, Test Loss: 1197.1736\n",
      "Epoch [67/5000], Train Loss: 3386.9298, Test Loss: 1196.9656\n",
      "Epoch [68/5000], Train Loss: 3376.0225, Test Loss: 1208.6046\n",
      "Epoch [69/5000], Train Loss: 3292.1789, Test Loss: 1157.8679\n",
      "Epoch [70/5000], Train Loss: 3297.3594, Test Loss: 1187.8934\n",
      "Epoch [71/5000], Train Loss: 3343.8567, Test Loss: 1160.2157\n",
      "Epoch [72/5000], Train Loss: 3257.2764, Test Loss: 1161.0380\n",
      "Epoch [73/5000], Train Loss: 3207.2095, Test Loss: 1193.8713\n",
      "Epoch [74/5000], Train Loss: 3245.4524, Test Loss: 1200.7508\n",
      "Epoch [75/5000], Train Loss: 3212.2799, Test Loss: 1170.5826\n",
      "Epoch [76/5000], Train Loss: 3181.9991, Test Loss: 1192.8524\n",
      "Epoch [77/5000], Train Loss: 3272.9687, Test Loss: 1197.0525\n",
      "Epoch [78/5000], Train Loss: 3289.1401, Test Loss: 1184.9811\n",
      "Epoch [79/5000], Train Loss: 3143.3326, Test Loss: 1128.1457\n",
      "Epoch [80/5000], Train Loss: 3194.0798, Test Loss: 1193.8505\n",
      "Epoch [81/5000], Train Loss: 3198.5862, Test Loss: 1152.6545\n",
      "Epoch [82/5000], Train Loss: 3161.9012, Test Loss: 1119.9138\n",
      "Epoch [83/5000], Train Loss: 3163.4285, Test Loss: 1202.8028\n",
      "Epoch [84/5000], Train Loss: 3177.3108, Test Loss: 1146.9155\n",
      "Epoch [85/5000], Train Loss: 3104.6979, Test Loss: 1122.6996\n",
      "Epoch [86/5000], Train Loss: 3080.3226, Test Loss: 1180.3768\n",
      "Epoch [87/5000], Train Loss: 3140.3840, Test Loss: 1130.9354\n",
      "Epoch [88/5000], Train Loss: 3124.8864, Test Loss: 1152.9255\n",
      "Epoch [89/5000], Train Loss: 3105.3861, Test Loss: 1135.5140\n",
      "Epoch [90/5000], Train Loss: 3149.6715, Test Loss: 1139.6145\n",
      "Epoch [91/5000], Train Loss: 3111.4819, Test Loss: 1212.7310\n",
      "Epoch [92/5000], Train Loss: 3127.4294, Test Loss: 1112.7816\n",
      "Epoch [93/5000], Train Loss: 3091.6978, Test Loss: 1141.2096\n",
      "Epoch [94/5000], Train Loss: 3097.1829, Test Loss: 1141.2194\n",
      "Epoch [95/5000], Train Loss: 3104.9699, Test Loss: 1117.2723\n",
      "Epoch [96/5000], Train Loss: 3072.9336, Test Loss: 1126.6573\n",
      "Epoch [97/5000], Train Loss: 3064.6936, Test Loss: 1149.7876\n",
      "Epoch [98/5000], Train Loss: 3113.8660, Test Loss: 1133.1573\n",
      "Epoch [99/5000], Train Loss: 3039.3165, Test Loss: 1143.1048\n",
      "Epoch [100/5000], Train Loss: 3041.6601, Test Loss: 1158.3894\n",
      "Epoch [101/5000], Train Loss: 2995.6475, Test Loss: 1162.8517\n",
      "Epoch [102/5000], Train Loss: 3037.2258, Test Loss: 1115.7149\n",
      "Epoch [103/5000], Train Loss: 3005.4523, Test Loss: 1164.1469\n",
      "Epoch [104/5000], Train Loss: 3051.2211, Test Loss: 1106.9524\n",
      "Epoch [105/5000], Train Loss: 3015.5154, Test Loss: 1120.2669\n",
      "Epoch [106/5000], Train Loss: 3034.6958, Test Loss: 1133.6831\n",
      "Epoch [107/5000], Train Loss: 3079.4366, Test Loss: 1096.2991\n",
      "Epoch [108/5000], Train Loss: 3032.3750, Test Loss: 1170.8659\n",
      "Epoch [109/5000], Train Loss: 2978.6839, Test Loss: 1121.6217\n",
      "Epoch [110/5000], Train Loss: 2979.8657, Test Loss: 1119.6631\n",
      "Epoch [111/5000], Train Loss: 2993.3325, Test Loss: 1131.3120\n",
      "Epoch [112/5000], Train Loss: 2897.8788, Test Loss: 1136.2089\n",
      "Epoch [113/5000], Train Loss: 3040.4522, Test Loss: 1171.4224\n",
      "Epoch [114/5000], Train Loss: 3034.0878, Test Loss: 1144.8764\n",
      "Epoch [115/5000], Train Loss: 2967.8233, Test Loss: 1125.7558\n",
      "Epoch [116/5000], Train Loss: 2931.6678, Test Loss: 1132.1587\n",
      "Epoch [117/5000], Train Loss: 2868.6470, Test Loss: 1186.6819\n",
      "Epoch [118/5000], Train Loss: 3021.5000, Test Loss: 1104.7788\n",
      "Epoch [119/5000], Train Loss: 2939.2063, Test Loss: 1086.4618\n",
      "Epoch [120/5000], Train Loss: 2898.6430, Test Loss: 1098.2406\n",
      "Epoch [121/5000], Train Loss: 2944.6187, Test Loss: 1141.4514\n",
      "Epoch [122/5000], Train Loss: 2912.9980, Test Loss: 1110.9137\n",
      "Epoch [123/5000], Train Loss: 2859.2572, Test Loss: 1120.4833\n",
      "Epoch [124/5000], Train Loss: 2866.4102, Test Loss: 1079.5056\n",
      "Epoch [125/5000], Train Loss: 2927.1238, Test Loss: 1083.4962\n",
      "Epoch [126/5000], Train Loss: 3001.3688, Test Loss: 1117.2442\n",
      "Epoch [127/5000], Train Loss: 2807.7035, Test Loss: 1093.4664\n",
      "Epoch [128/5000], Train Loss: 2880.4555, Test Loss: 1110.7255\n",
      "Epoch [129/5000], Train Loss: 2937.3461, Test Loss: 1110.4606\n",
      "Epoch [130/5000], Train Loss: 2844.8658, Test Loss: 1098.2616\n",
      "Epoch [131/5000], Train Loss: 2874.6013, Test Loss: 1090.9677\n",
      "Epoch [132/5000], Train Loss: 2962.8518, Test Loss: 1091.4810\n",
      "Epoch [133/5000], Train Loss: 2854.1077, Test Loss: 1104.2925\n",
      "Epoch [134/5000], Train Loss: 2822.9033, Test Loss: 1102.2759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [135/5000], Train Loss: 2867.3240, Test Loss: 1099.6152\n",
      "Epoch [136/5000], Train Loss: 2834.2579, Test Loss: 1055.8749\n",
      "Epoch [137/5000], Train Loss: 2826.7050, Test Loss: 1075.1624\n",
      "Epoch [138/5000], Train Loss: 2872.3454, Test Loss: 1089.9833\n",
      "Epoch [139/5000], Train Loss: 2816.1177, Test Loss: 1083.6001\n",
      "Epoch [140/5000], Train Loss: 2749.0633, Test Loss: 1045.0588\n",
      "Epoch [141/5000], Train Loss: 2868.0340, Test Loss: 1095.5091\n",
      "Epoch [142/5000], Train Loss: 2841.4722, Test Loss: 1123.5461\n",
      "Epoch [143/5000], Train Loss: 2825.6296, Test Loss: 1062.6604\n",
      "Epoch [144/5000], Train Loss: 2800.5789, Test Loss: 1129.2703\n",
      "Epoch [145/5000], Train Loss: 2827.9360, Test Loss: 1080.0384\n",
      "Epoch [146/5000], Train Loss: 2830.4379, Test Loss: 1088.5091\n",
      "Epoch [147/5000], Train Loss: 2812.4529, Test Loss: 1036.9229\n",
      "Epoch [148/5000], Train Loss: 2836.2404, Test Loss: 1073.8260\n",
      "Epoch [149/5000], Train Loss: 2814.1215, Test Loss: 1021.4041\n",
      "Epoch [150/5000], Train Loss: 2774.9767, Test Loss: 1109.9319\n",
      "Epoch [151/5000], Train Loss: 2763.0837, Test Loss: 1075.1205\n",
      "Epoch [152/5000], Train Loss: 2856.9779, Test Loss: 1038.5658\n",
      "Epoch [153/5000], Train Loss: 2762.6349, Test Loss: 1080.1900\n",
      "Epoch [154/5000], Train Loss: 2783.1422, Test Loss: 1085.4558\n",
      "Epoch [155/5000], Train Loss: 2780.1267, Test Loss: 1039.3517\n",
      "Epoch [156/5000], Train Loss: 2769.5018, Test Loss: 1110.1820\n",
      "Epoch [157/5000], Train Loss: 2802.7700, Test Loss: 1058.3865\n",
      "Epoch [158/5000], Train Loss: 2758.4512, Test Loss: 1062.9606\n",
      "Epoch [159/5000], Train Loss: 2730.0252, Test Loss: 1110.3440\n",
      "Epoch [160/5000], Train Loss: 2701.9380, Test Loss: 1037.4783\n",
      "Epoch [161/5000], Train Loss: 2729.8803, Test Loss: 1060.5229\n",
      "Epoch [162/5000], Train Loss: 2736.9526, Test Loss: 1109.9861\n",
      "Epoch [163/5000], Train Loss: 2821.0220, Test Loss: 1084.9715\n",
      "Epoch [164/5000], Train Loss: 2770.3980, Test Loss: 1084.9553\n",
      "Epoch [165/5000], Train Loss: 2772.1581, Test Loss: 1087.8543\n",
      "Epoch [166/5000], Train Loss: 2826.6703, Test Loss: 1091.8594\n",
      "Epoch [167/5000], Train Loss: 2672.7825, Test Loss: 1030.8834\n",
      "Epoch [168/5000], Train Loss: 2646.9410, Test Loss: 1067.1799\n",
      "Epoch [169/5000], Train Loss: 2697.8563, Test Loss: 1038.2435\n",
      "Epoch [170/5000], Train Loss: 2746.2605, Test Loss: 1069.6941\n",
      "Epoch [171/5000], Train Loss: 2620.8381, Test Loss: 1080.1971\n",
      "Epoch [172/5000], Train Loss: 2770.7221, Test Loss: 1077.1259\n",
      "Epoch [173/5000], Train Loss: 2695.4362, Test Loss: 1052.9400\n",
      "Epoch [174/5000], Train Loss: 2695.8142, Test Loss: 1084.3186\n",
      "Epoch [175/5000], Train Loss: 2721.0454, Test Loss: 1051.1709\n",
      "Epoch [176/5000], Train Loss: 2698.7062, Test Loss: 1040.8175\n",
      "Epoch [177/5000], Train Loss: 2598.6831, Test Loss: 1062.8049\n",
      "Epoch [178/5000], Train Loss: 2683.7856, Test Loss: 1071.6590\n",
      "Epoch [179/5000], Train Loss: 2657.2561, Test Loss: 1121.1012\n",
      "Epoch [180/5000], Train Loss: 2714.5565, Test Loss: 1067.7980\n",
      "Epoch [181/5000], Train Loss: 2772.1063, Test Loss: 1032.7915\n",
      "Epoch [182/5000], Train Loss: 2600.4553, Test Loss: 1070.7881\n",
      "Epoch [183/5000], Train Loss: 2680.8566, Test Loss: 1069.4709\n",
      "Epoch [184/5000], Train Loss: 2657.8321, Test Loss: 1057.0884\n",
      "Epoch [185/5000], Train Loss: 2573.7838, Test Loss: 1055.0416\n",
      "Epoch [186/5000], Train Loss: 2640.7861, Test Loss: 1034.4056\n",
      "Epoch [187/5000], Train Loss: 2584.7514, Test Loss: 1059.5908\n",
      "Epoch [188/5000], Train Loss: 2665.7246, Test Loss: 1043.1837\n",
      "Epoch [189/5000], Train Loss: 2773.6372, Test Loss: 1031.1658\n",
      "Epoch [190/5000], Train Loss: 2616.0459, Test Loss: 1059.9805\n",
      "Epoch [191/5000], Train Loss: 2637.9939, Test Loss: 1067.3182\n",
      "Epoch [192/5000], Train Loss: 2653.9858, Test Loss: 1057.5690\n",
      "Epoch [193/5000], Train Loss: 2666.2951, Test Loss: 1073.5641\n",
      "Epoch [194/5000], Train Loss: 2676.7120, Test Loss: 1076.0211\n",
      "Epoch [195/5000], Train Loss: 2627.3020, Test Loss: 1044.1351\n",
      "Epoch [196/5000], Train Loss: 2613.3488, Test Loss: 1031.1341\n",
      "Epoch [197/5000], Train Loss: 2585.5045, Test Loss: 1091.7071\n",
      "Epoch [198/5000], Train Loss: 2631.3661, Test Loss: 1021.2102\n",
      "Epoch [199/5000], Train Loss: 2590.2204, Test Loss: 1041.6136\n",
      "Epoch [200/5000], Train Loss: 2609.9526, Test Loss: 1050.5168\n",
      "Epoch [201/5000], Train Loss: 2637.6838, Test Loss: 1044.3473\n",
      "Epoch [202/5000], Train Loss: 2624.2848, Test Loss: 999.4600\n",
      "Epoch [203/5000], Train Loss: 2600.5408, Test Loss: 1036.6810\n",
      "Epoch [204/5000], Train Loss: 2649.3239, Test Loss: 1054.0085\n",
      "Epoch [205/5000], Train Loss: 2607.5475, Test Loss: 1062.2122\n",
      "Epoch [206/5000], Train Loss: 2600.9390, Test Loss: 1081.1127\n",
      "Epoch [207/5000], Train Loss: 2624.9724, Test Loss: 1055.7124\n",
      "Epoch [208/5000], Train Loss: 2590.3597, Test Loss: 1056.1272\n",
      "Epoch [209/5000], Train Loss: 2572.8720, Test Loss: 1025.4552\n",
      "Epoch [210/5000], Train Loss: 2539.2258, Test Loss: 1048.2012\n",
      "Epoch [211/5000], Train Loss: 2577.6584, Test Loss: 1051.2516\n",
      "Epoch [212/5000], Train Loss: 2595.4702, Test Loss: 1016.5443\n",
      "Epoch [213/5000], Train Loss: 2545.7914, Test Loss: 1022.8633\n",
      "Epoch [214/5000], Train Loss: 2585.1769, Test Loss: 1051.0121\n",
      "Epoch [215/5000], Train Loss: 2533.6204, Test Loss: 1051.4986\n",
      "Epoch [216/5000], Train Loss: 2617.3561, Test Loss: 1031.6384\n",
      "Epoch [217/5000], Train Loss: 2573.3743, Test Loss: 1030.5545\n",
      "Epoch [218/5000], Train Loss: 2623.1232, Test Loss: 1054.3840\n",
      "Epoch [219/5000], Train Loss: 2509.6686, Test Loss: 1056.1924\n",
      "Epoch [220/5000], Train Loss: 2585.1595, Test Loss: 1079.9388\n",
      "Epoch [221/5000], Train Loss: 2643.4818, Test Loss: 1039.1347\n",
      "Epoch [222/5000], Train Loss: 2557.2341, Test Loss: 1056.0845\n",
      "Epoch [223/5000], Train Loss: 2608.3719, Test Loss: 996.0109\n",
      "Epoch [224/5000], Train Loss: 2535.6225, Test Loss: 1010.6305\n",
      "Epoch [225/5000], Train Loss: 2541.2161, Test Loss: 1068.4405\n",
      "Epoch [226/5000], Train Loss: 2536.0931, Test Loss: 1043.1781\n",
      "Epoch [227/5000], Train Loss: 2513.4419, Test Loss: 1046.9674\n",
      "Epoch [228/5000], Train Loss: 2560.7261, Test Loss: 1023.4280\n",
      "Epoch [229/5000], Train Loss: 2527.7846, Test Loss: 1015.7083\n",
      "Epoch [230/5000], Train Loss: 2537.3823, Test Loss: 1000.5784\n",
      "Epoch [231/5000], Train Loss: 2530.9044, Test Loss: 1058.2122\n",
      "Epoch [232/5000], Train Loss: 2567.5576, Test Loss: 1024.4802\n",
      "Epoch [233/5000], Train Loss: 2536.5893, Test Loss: 1036.2121\n",
      "Epoch [234/5000], Train Loss: 2552.5191, Test Loss: 1075.0503\n",
      "Epoch [235/5000], Train Loss: 2531.3395, Test Loss: 1016.9929\n",
      "Epoch [236/5000], Train Loss: 2503.0684, Test Loss: 1009.8932\n",
      "Epoch [237/5000], Train Loss: 2532.9397, Test Loss: 1019.8625\n",
      "Epoch [238/5000], Train Loss: 2504.5587, Test Loss: 1015.9343\n",
      "Epoch [239/5000], Train Loss: 2598.3288, Test Loss: 1038.5658\n",
      "Epoch [240/5000], Train Loss: 2587.6322, Test Loss: 1045.9839\n",
      "Epoch [241/5000], Train Loss: 2526.2887, Test Loss: 979.2597\n",
      "Epoch [242/5000], Train Loss: 2501.8928, Test Loss: 996.0521\n",
      "Epoch [243/5000], Train Loss: 2520.7713, Test Loss: 1009.8961\n",
      "Epoch [244/5000], Train Loss: 2536.4995, Test Loss: 1029.4052\n",
      "Epoch [245/5000], Train Loss: 2467.5163, Test Loss: 1031.4670\n",
      "Epoch [246/5000], Train Loss: 2460.2361, Test Loss: 1024.4350\n",
      "Epoch [247/5000], Train Loss: 2556.2043, Test Loss: 983.5355\n",
      "Epoch [248/5000], Train Loss: 2477.2497, Test Loss: 1024.1278\n",
      "Epoch [249/5000], Train Loss: 2449.9756, Test Loss: 1006.7871\n",
      "Epoch [250/5000], Train Loss: 2495.1536, Test Loss: 1038.1016\n",
      "Epoch [251/5000], Train Loss: 2423.5224, Test Loss: 1061.7291\n",
      "Epoch [252/5000], Train Loss: 2508.6040, Test Loss: 993.9836\n",
      "Epoch [253/5000], Train Loss: 2410.0725, Test Loss: 1020.5107\n",
      "Epoch [254/5000], Train Loss: 2450.1749, Test Loss: 990.3175\n",
      "Epoch [255/5000], Train Loss: 2471.8241, Test Loss: 1011.4385\n",
      "Epoch [256/5000], Train Loss: 2442.0569, Test Loss: 998.5310\n",
      "Epoch [257/5000], Train Loss: 2439.9331, Test Loss: 1011.7321\n",
      "Epoch [258/5000], Train Loss: 2522.3367, Test Loss: 1002.2712\n",
      "Epoch [259/5000], Train Loss: 2416.6231, Test Loss: 1037.1191\n",
      "Epoch [260/5000], Train Loss: 2408.5158, Test Loss: 1001.4226\n",
      "Epoch [261/5000], Train Loss: 2404.9146, Test Loss: 997.4637\n",
      "Epoch [262/5000], Train Loss: 2454.6685, Test Loss: 1038.0418\n",
      "Epoch [263/5000], Train Loss: 2456.8960, Test Loss: 1052.2301\n",
      "Epoch [264/5000], Train Loss: 2434.5424, Test Loss: 994.8023\n",
      "Epoch [265/5000], Train Loss: 2396.3441, Test Loss: 1020.7852\n",
      "Epoch [266/5000], Train Loss: 2421.3024, Test Loss: 1014.8317\n",
      "Epoch [267/5000], Train Loss: 2406.0671, Test Loss: 984.7471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [268/5000], Train Loss: 2478.8871, Test Loss: 1045.6263\n",
      "Epoch [269/5000], Train Loss: 2455.2137, Test Loss: 1002.8880\n",
      "Epoch [270/5000], Train Loss: 2443.9967, Test Loss: 998.6989\n",
      "Epoch [271/5000], Train Loss: 2396.4198, Test Loss: 1013.0104\n",
      "Epoch [272/5000], Train Loss: 2500.0734, Test Loss: 982.8272\n",
      "Epoch [273/5000], Train Loss: 2474.1132, Test Loss: 980.3998\n",
      "Epoch [274/5000], Train Loss: 2403.4789, Test Loss: 991.4379\n",
      "Epoch [275/5000], Train Loss: 2448.2713, Test Loss: 1058.2038\n",
      "Epoch [276/5000], Train Loss: 2423.4601, Test Loss: 1040.5408\n",
      "Epoch [277/5000], Train Loss: 2485.6409, Test Loss: 992.9324\n",
      "Epoch [278/5000], Train Loss: 2503.1897, Test Loss: 1038.2585\n",
      "Epoch [279/5000], Train Loss: 2393.8776, Test Loss: 992.2052\n",
      "Epoch [280/5000], Train Loss: 2398.1499, Test Loss: 1019.3390\n",
      "Epoch [281/5000], Train Loss: 2396.2965, Test Loss: 1029.5235\n",
      "Epoch [282/5000], Train Loss: 2434.4982, Test Loss: 1050.9133\n",
      "Epoch [283/5000], Train Loss: 2435.1193, Test Loss: 992.4439\n",
      "Epoch [284/5000], Train Loss: 2364.1944, Test Loss: 986.5329\n",
      "Epoch [285/5000], Train Loss: 2401.7508, Test Loss: 1026.4468\n",
      "Epoch [286/5000], Train Loss: 2410.0894, Test Loss: 1031.6969\n",
      "Epoch [287/5000], Train Loss: 2378.1971, Test Loss: 975.6764\n",
      "Epoch [288/5000], Train Loss: 2329.5283, Test Loss: 996.6164\n",
      "Epoch [289/5000], Train Loss: 2423.9130, Test Loss: 994.7989\n",
      "Epoch [290/5000], Train Loss: 2361.2374, Test Loss: 998.0947\n",
      "Epoch [291/5000], Train Loss: 2423.9276, Test Loss: 991.9407\n",
      "Epoch [292/5000], Train Loss: 2331.1826, Test Loss: 971.1361\n",
      "Epoch [293/5000], Train Loss: 2373.5959, Test Loss: 967.3099\n",
      "Epoch [294/5000], Train Loss: 2352.9429, Test Loss: 988.5181\n",
      "Epoch [295/5000], Train Loss: 2463.9137, Test Loss: 1064.1143\n",
      "Epoch [296/5000], Train Loss: 2359.8522, Test Loss: 1008.4531\n",
      "Epoch [297/5000], Train Loss: 2403.4306, Test Loss: 979.8816\n",
      "Epoch [298/5000], Train Loss: 2373.4583, Test Loss: 998.7372\n",
      "Epoch [299/5000], Train Loss: 2383.5724, Test Loss: 1010.1711\n",
      "Epoch [300/5000], Train Loss: 2306.8169, Test Loss: 986.9556\n",
      "Epoch [301/5000], Train Loss: 2286.5644, Test Loss: 1009.8270\n",
      "Epoch [302/5000], Train Loss: 2372.5436, Test Loss: 977.1974\n",
      "Epoch [303/5000], Train Loss: 2352.8753, Test Loss: 964.9338\n",
      "Epoch [304/5000], Train Loss: 2343.6547, Test Loss: 961.7089\n",
      "Epoch [305/5000], Train Loss: 2319.8271, Test Loss: 989.9248\n",
      "Epoch [306/5000], Train Loss: 2375.8727, Test Loss: 979.8372\n",
      "Epoch [307/5000], Train Loss: 2393.8359, Test Loss: 1011.7953\n",
      "Epoch [308/5000], Train Loss: 2350.0714, Test Loss: 1008.4861\n",
      "Epoch [309/5000], Train Loss: 2427.6795, Test Loss: 991.1462\n",
      "Epoch [310/5000], Train Loss: 2350.3356, Test Loss: 1015.7742\n",
      "Epoch [311/5000], Train Loss: 2304.3353, Test Loss: 980.3965\n",
      "Epoch [312/5000], Train Loss: 2325.3822, Test Loss: 995.4553\n",
      "Epoch [313/5000], Train Loss: 2345.8060, Test Loss: 955.5883\n",
      "Epoch [314/5000], Train Loss: 2357.4814, Test Loss: 994.0160\n",
      "Epoch [315/5000], Train Loss: 2288.4203, Test Loss: 991.4889\n",
      "Epoch [316/5000], Train Loss: 2347.4734, Test Loss: 1017.7345\n",
      "Epoch [317/5000], Train Loss: 2333.0584, Test Loss: 1003.9272\n",
      "Epoch [318/5000], Train Loss: 2291.5451, Test Loss: 996.4798\n",
      "Epoch [319/5000], Train Loss: 2369.2105, Test Loss: 1002.2168\n",
      "Epoch [320/5000], Train Loss: 2389.2944, Test Loss: 1020.7182\n",
      "Epoch [321/5000], Train Loss: 2316.4316, Test Loss: 1003.5359\n",
      "Epoch [322/5000], Train Loss: 2413.0201, Test Loss: 1017.4778\n",
      "Epoch [323/5000], Train Loss: 2326.6800, Test Loss: 1009.6757\n",
      "Epoch [324/5000], Train Loss: 2309.3953, Test Loss: 990.1676\n",
      "Epoch [325/5000], Train Loss: 2287.8604, Test Loss: 968.1835\n",
      "Epoch [326/5000], Train Loss: 2323.6360, Test Loss: 991.2560\n",
      "Epoch [327/5000], Train Loss: 2361.0459, Test Loss: 1019.2755\n",
      "Epoch [328/5000], Train Loss: 2321.6116, Test Loss: 996.9754\n",
      "Epoch [329/5000], Train Loss: 2331.6898, Test Loss: 977.8683\n",
      "Epoch [330/5000], Train Loss: 2287.2242, Test Loss: 990.3173\n",
      "Epoch [331/5000], Train Loss: 2299.9958, Test Loss: 1004.8688\n",
      "Epoch [332/5000], Train Loss: 2297.1069, Test Loss: 985.9643\n",
      "Epoch [333/5000], Train Loss: 2363.4362, Test Loss: 973.7083\n",
      "Epoch [334/5000], Train Loss: 2215.7393, Test Loss: 979.9430\n",
      "Epoch [335/5000], Train Loss: 2310.3190, Test Loss: 1049.3669\n",
      "Epoch [336/5000], Train Loss: 2348.8663, Test Loss: 987.6536\n",
      "Epoch [337/5000], Train Loss: 2287.6020, Test Loss: 955.2984\n",
      "Epoch [338/5000], Train Loss: 2300.2127, Test Loss: 972.4288\n",
      "Epoch [339/5000], Train Loss: 2318.2153, Test Loss: 990.0802\n",
      "Epoch [340/5000], Train Loss: 2329.9097, Test Loss: 1023.2491\n",
      "Epoch [341/5000], Train Loss: 2290.8865, Test Loss: 975.9216\n",
      "Epoch [342/5000], Train Loss: 2322.2003, Test Loss: 965.7671\n",
      "Epoch [343/5000], Train Loss: 2273.2115, Test Loss: 958.8164\n",
      "Epoch [344/5000], Train Loss: 2213.1701, Test Loss: 1007.7648\n",
      "Epoch [345/5000], Train Loss: 2254.8870, Test Loss: 1003.6275\n",
      "Epoch [346/5000], Train Loss: 2326.3683, Test Loss: 1010.9294\n",
      "Epoch [347/5000], Train Loss: 2277.6530, Test Loss: 967.2479\n",
      "Epoch [348/5000], Train Loss: 2324.8607, Test Loss: 976.2334\n",
      "Epoch [349/5000], Train Loss: 2295.5228, Test Loss: 963.8602\n",
      "Epoch [350/5000], Train Loss: 2246.3281, Test Loss: 933.9937\n",
      "Epoch [351/5000], Train Loss: 2246.5451, Test Loss: 1009.4835\n",
      "Epoch [352/5000], Train Loss: 2366.2114, Test Loss: 1029.5696\n",
      "Epoch [353/5000], Train Loss: 2319.1102, Test Loss: 981.1298\n",
      "Epoch [354/5000], Train Loss: 2292.1142, Test Loss: 987.1757\n",
      "Epoch [355/5000], Train Loss: 2227.9836, Test Loss: 957.7553\n",
      "Epoch [356/5000], Train Loss: 2290.8369, Test Loss: 991.7597\n",
      "Epoch [357/5000], Train Loss: 2308.3046, Test Loss: 948.7137\n",
      "Epoch [358/5000], Train Loss: 2226.0412, Test Loss: 961.3962\n",
      "Epoch [359/5000], Train Loss: 2352.8819, Test Loss: 1026.4490\n",
      "Epoch [360/5000], Train Loss: 2343.8366, Test Loss: 1046.9307\n",
      "Epoch [361/5000], Train Loss: 2358.3488, Test Loss: 951.2940\n",
      "Epoch [362/5000], Train Loss: 2276.2644, Test Loss: 1002.4469\n",
      "Epoch [363/5000], Train Loss: 2248.7669, Test Loss: 940.0744\n",
      "Epoch [364/5000], Train Loss: 2302.8579, Test Loss: 970.6091\n",
      "Epoch [365/5000], Train Loss: 2277.6522, Test Loss: 990.2111\n",
      "Epoch [366/5000], Train Loss: 2259.1507, Test Loss: 986.3586\n",
      "Epoch [367/5000], Train Loss: 2327.8843, Test Loss: 983.6863\n",
      "Epoch [368/5000], Train Loss: 2193.9231, Test Loss: 998.2244\n",
      "Epoch [369/5000], Train Loss: 2280.4729, Test Loss: 950.3550\n",
      "Epoch [370/5000], Train Loss: 2295.5568, Test Loss: 979.4061\n",
      "Epoch [371/5000], Train Loss: 2249.6070, Test Loss: 936.6126\n",
      "Epoch [372/5000], Train Loss: 2245.1029, Test Loss: 984.5180\n",
      "Epoch [373/5000], Train Loss: 2257.7788, Test Loss: 992.1668\n",
      "Epoch [374/5000], Train Loss: 2203.9623, Test Loss: 1036.5712\n",
      "Epoch [375/5000], Train Loss: 2242.5772, Test Loss: 943.7174\n",
      "Epoch [376/5000], Train Loss: 2288.8071, Test Loss: 978.2358\n",
      "Epoch [377/5000], Train Loss: 2228.5084, Test Loss: 967.7610\n",
      "Epoch [378/5000], Train Loss: 2242.2631, Test Loss: 979.2114\n",
      "Epoch [379/5000], Train Loss: 2293.1415, Test Loss: 982.5321\n",
      "Epoch [380/5000], Train Loss: 2271.5312, Test Loss: 991.8607\n",
      "Epoch [381/5000], Train Loss: 2232.5672, Test Loss: 965.7648\n",
      "Epoch [382/5000], Train Loss: 2250.1083, Test Loss: 957.3829\n",
      "Epoch [383/5000], Train Loss: 2209.5748, Test Loss: 965.6386\n",
      "Epoch [384/5000], Train Loss: 2205.7225, Test Loss: 938.7577\n",
      "Epoch [385/5000], Train Loss: 2246.0537, Test Loss: 977.2442\n",
      "Epoch [386/5000], Train Loss: 2239.6854, Test Loss: 1021.0757\n",
      "Epoch [387/5000], Train Loss: 2233.6575, Test Loss: 939.5794\n",
      "Epoch [388/5000], Train Loss: 2236.7084, Test Loss: 989.7084\n",
      "Epoch [389/5000], Train Loss: 2190.1243, Test Loss: 979.0353\n",
      "Epoch [390/5000], Train Loss: 2271.4806, Test Loss: 1030.0657\n",
      "Epoch [391/5000], Train Loss: 2222.2596, Test Loss: 984.2453\n",
      "Epoch [392/5000], Train Loss: 2177.2434, Test Loss: 941.9795\n",
      "Epoch [393/5000], Train Loss: 2158.4419, Test Loss: 959.2421\n",
      "Epoch [394/5000], Train Loss: 2251.8534, Test Loss: 971.5035\n",
      "Epoch [395/5000], Train Loss: 2194.2117, Test Loss: 1006.7445\n",
      "Epoch [396/5000], Train Loss: 2219.1194, Test Loss: 995.8845\n",
      "Epoch [397/5000], Train Loss: 2258.9863, Test Loss: 996.1518\n",
      "Epoch [398/5000], Train Loss: 2192.4760, Test Loss: 977.5530\n",
      "Epoch [399/5000], Train Loss: 2226.2908, Test Loss: 995.6749\n",
      "Epoch [400/5000], Train Loss: 2261.5317, Test Loss: 966.1445\n",
      "Early stopping at epoch 400\n",
      "Execution time: 455.60342597961426 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933.993713632226"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (lin1): Linear(in_features=46000, out_features=200, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (lin2): Linear(in_features=200, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  28.669338\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "49.92  - Concentrations: 0.004999999888241291\n",
      "6.27  - Concentrations: 0.02500000037252903\n",
      "2.62  - Concentrations: 0.10000000149011612\n",
      "2.72  - Concentrations: 0.25\n",
      "2.53  - Concentrations: 0.5\n",
      "3.71  - Concentrations: 1.0\n",
      "5.05  - Concentrations: 2.5\n",
      "5.73  - Concentrations: 10.0\n",
      "5.85  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  106.924416\n",
      "--------------------\n",
      "30.17  - Min Value: 0.6783  - Mean Value: 9.2\n",
      "704.82  - Min Value: 0.0096  - Mean Value: 10.3\n",
      "70.62  - Min Value: 0.147  - Mean Value: 10.5\n",
      "55.2  - Min Value: 0.5572  - Mean Value: 8.5\n",
      "21.66  - Min Value: 1.3567  - Mean Value: 10.6\n",
      "34.25  - Min Value: 0.6332  - Mean Value: 10.9\n",
      "39.0  - Min Value: 0.7017  - Mean Value: 11.0\n",
      "66.54  - Min Value: 0.3674  - Mean Value: 8.9\n",
      "26.91  - Min Value: 0.8387  - Mean Value: 9.8\n",
      "20.09  - Min Value: 1.0913  - Mean Value: 11.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  8.838423\n",
      "--------------------\n",
      "3.66  - Min Value: 0.0111  - Mean Value: 0.1\n",
      "11.29  - Min Value: 0.0103  - Mean Value: 0.1\n",
      "6.12  - Min Value: 0.0153  - Mean Value: 0.1\n",
      "9.97  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "12.03  - Min Value: 0.0089  - Mean Value: 0.1\n",
      "11.57  - Min Value: 0.0075  - Mean Value: 0.1\n",
      "8.38  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "9.3  - Min Value: 0.0052  - Mean Value: 0.1\n",
      "7.47  - Min Value: 0.008  - Mean Value: 0.1\n",
      "8.58  - Min Value: 0.0134  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  287.41888\n",
      "--------------------\n",
      "410.41  - Min Value: 0.008  - Mean Value: 0.8\n",
      "324.94  - Min Value: 0.009  - Mean Value: 0.9\n",
      "575.06  - Min Value: 0.0138  - Mean Value: 1.5\n",
      "154.17  - Min Value: 0.0107  - Mean Value: 0.7\n",
      "198.04  - Min Value: 0.0191  - Mean Value: 0.7\n",
      "467.04  - Min Value: 0.0186  - Mean Value: 0.8\n",
      "191.71  - Min Value: 0.0175  - Mean Value: 0.8\n",
      "183.15  - Min Value: 0.0238  - Mean Value: 1.3\n",
      "125.93  - Min Value: 0.0168  - Mean Value: 0.7\n",
      "243.73  - Min Value: 0.0171  - Mean Value: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  320.49445\n",
      "--------------------\n",
      "210.2  - Min Value: 0.013  - Mean Value: 0.6\n",
      "255.44  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "174.02  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "234.69  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "257.09  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "524.68  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "415.71  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "161.79  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "189.31  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "782.01  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  2426.1445\n",
      "--------------------\n",
      "1449.79  - Min Value: 0.013  - Mean Value: 0.6\n",
      "1926.65  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "2579.81  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "2075.89  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "2071.31  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "3443.37  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "2099.55  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "1648.03  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "3699.82  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "3267.22  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  2.5180692463546177\n",
      "--------------------\n",
      "2.49\n",
      "2.45\n",
      "2.45\n",
      "2.57\n",
      "2.48\n",
      "2.54\n",
      "2.46\n",
      "2.45\n",
      "2.68\n",
      "2.62\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  5.186301696628142\n",
      "--------------------\n",
      "2.49\n",
      "2.47\n",
      "2.43\n",
      "2.45\n",
      "4.43\n",
      "4.5\n",
      "4.54\n",
      "8.51\n",
      "9.49\n",
      "10.56\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"Singlet_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"Singlet_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  28.82834537439127\n",
      "--------------------\n",
      "2.31\n",
      "6.45\n",
      "12.71\n",
      "19.05\n",
      "25.37\n",
      "31.73\n",
      "38.11\n",
      "44.4\n",
      "50.91\n",
      "57.24\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal Baseline 1\n",
      "tensor([0.4380, 0.4545, 0.4307, 0.4398, 0.4579, 0.4226, 0.4224, 0.4594, 0.4620,\n",
      "        0.4550, 0.4388, 0.4634, 0.4627, 0.5688, 0.4312, 0.4697, 0.5075, 0.4272,\n",
      "        0.4349, 0.4199, 0.4338, 0.4924, 0.4428, 0.4584, 0.4570, 0.4574, 0.4308,\n",
      "        0.4525, 0.4469, 0.4388, 0.4333, 0.4709, 0.4646, 0.4105, 0.4376, 0.4312,\n",
      "        0.4415, 0.4960, 0.4784, 0.4222, 0.4280, 0.4744, 0.4538, 0.4353],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Sinusoidal Baseline 2\n",
      "tensor([0.4231, 0.4294, 0.3585, 0.4502, 0.4485, 0.4253, 0.4106, 0.3984, 0.4345,\n",
      "        0.4413, 0.4388, 0.4329, 0.4606, 0.4741, 0.4748, 0.3825, 0.4881, 0.4437,\n",
      "        0.4293, 0.4162, 0.4122, 0.4472, 0.4450, 0.4695, 0.4156, 0.4492, 0.4004,\n",
      "        0.4118, 0.4462, 0.4288, 0.4302, 0.4361, 0.4128, 0.4064, 0.4643, 0.4069,\n",
      "        0.4547, 0.4928, 0.4553, 0.4072, 0.4329, 0.4860, 0.4257, 0.3908],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "HD-Range 1 - 0.01s and 20s\n",
      "tensor([ 0.3786, 15.8472,  5.0395, 15.5262,  1.0597, 14.1240,  2.3474, 15.5262,\n",
      "         2.0184, 14.3691,  3.2449, 15.7128,  3.2249, 15.2767,  0.0000, 11.8120,\n",
      "         3.8212, 12.3150,  1.8426, 14.3559,  2.4725, 12.9925,  1.2125, 12.5567,\n",
      "         1.6536, 19.5121,  3.7401, 16.2745,  0.1863, 18.0877,  0.9896, 14.5511,\n",
      "         0.0000, 16.4587,  3.1517, 17.3401,  0.0000, 18.3474,  5.2474, 12.4559,\n",
      "         0.0000, 15.4524,  1.1408, 13.4495], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "HD-Range 2 - 0s and 20s\n",
      "tensor([ 0.3696, 15.8436,  5.0365, 15.5249,  1.0512, 14.1188,  2.3408, 15.5256,\n",
      "         2.0091, 14.3682,  3.2390, 15.7128,  3.2161, 15.2714,  0.0000, 11.8054,\n",
      "         3.8134, 12.3119,  1.8347, 14.3528,  2.4668, 12.9919,  1.2042, 12.5515,\n",
      "         1.6444, 19.5108,  3.7350, 16.2738,  0.1779, 18.0860,  0.9805, 14.5477,\n",
      "         0.0000, 16.4574,  3.1458, 17.3368,  0.0000, 18.3485,  5.2379, 12.4524,\n",
      "         0.0000, 15.4481,  1.1346, 13.4459], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(OtherValSpectra[0])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 0.01s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
