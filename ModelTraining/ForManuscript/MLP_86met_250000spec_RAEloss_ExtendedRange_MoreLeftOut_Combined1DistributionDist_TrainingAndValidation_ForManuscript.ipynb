{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 5000\n",
    "\n",
    "# Identification part of the filenames\n",
    "model_base_name = '250000spec_86Metabolites_RAE_ExtendedRange_MoreLeftOut_Combined1Distribution'\n",
    "base_name = '250000spec_ExtendedRange_MoreLeftOut_Combined1Distribution'    # This is the dataset base name\n",
    "base_dir = '/path/to/base/directory'   # Set base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP on dataset of 86 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"MLP_86Met_{model_base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to directory containing datasets\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra_filename = f'Dataset86_{base_name}_ForManuscript_Spec.dat'\n",
    "conc1_filename = f'Dataset86_{base_name}_ForManuscript_Conc.npy'\n",
    "\n",
    "spectra_shape = (249996, 46000)\n",
    "conc1_shape = (249996, 86)\n",
    "\n",
    "\n",
    "# Load the memmap arrays\n",
    "spectra_memmap = np.memmap(spectra_filename, dtype=np.float64, mode='r', shape=spectra_shape)\n",
    "conc1_memmap = np.load(conc1_filename)\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train_indices, X_test_indices, y_train_indices, y_test_indices = train_test_split(\n",
    "    np.arange(spectra_shape[0]), np.arange(conc1_shape[0]), test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# Create custom dataset class\n",
    "class NMRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, spectra_memmap, conc1_memmap, indices):\n",
    "        self.spectra_memmap = spectra_memmap\n",
    "        self.conc1_memmap = conc1_memmap\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        X = self.spectra_memmap[actual_idx]\n",
    "        y = self.conc1_memmap[actual_idx]\n",
    "        return torch.tensor(X).float().to(device), torch.tensor(y).float().to(device)\n",
    "    \n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NMRDataset(spectra_memmap, conc1_memmap, X_train_indices)\n",
    "test_dataset = NMRDataset(spectra_memmap, conc1_memmap, X_test_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 31  \n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "## Best params from Optuna study\n",
    "#{'n_layers': 2, \n",
    "# 'activation': 'LeakyReLU', \n",
    "# 'n_units_l0': 222, \n",
    "# 'n_units_l1': 463, \n",
    "# 'learning_rate': 0.0020767074281295714, \n",
    "# 'reg_strength': 0.009375024340091184, \n",
    "# 'batch_size': 31}\n",
    "\n",
    "# Define some model & training parameters\n",
    "size_hidden1 = 222\n",
    "size_hidden2 = 463\n",
    "size_hidden3 = 86\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(46000, size_hidden1)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n",
    "    def forward(self, input):\n",
    "        return (self.lin3(self.relu2(self.lin2(self.relu1(self.lin1(input))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = RelativeAbsoluteError()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = 0.0020767074281295714, weight_decay=0.009375024340091184)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in validation loss constitutes early stopping\n",
    "    accumulation_steps = 4\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # For timing cell run time\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        ## Training phase\n",
    "        # Instantiate the GradScaler\n",
    "        scaler = GradScaler()\n",
    "        optimizer.zero_grad()  # Only zero gradients here at the start of an epoch\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Enable autocasting for forward and backward passes\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Scale the loss to account for the accumulation steps\n",
    "                loss = loss / accumulation_steps\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Scale the loss and perform backpropagation\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                # Step the optimizer and update the scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()  # Zero gradients after accumulation_steps\n",
    "\n",
    "        # Testing phase\n",
    "        train_losses.append(train_loss)\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                # Move data to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Enable autocasting for forward passes\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "        \n",
    "        \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Epoch time: \",end-start)\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/5000], Train Loss: 20280.9631, Test Loss: 13814.9640\n",
      "Epoch time:  1463.4299216270447\n",
      "Epoch [2/5000], Train Loss: 11096.0236, Test Loss: 8800.0768\n",
      "Epoch time:  426.2634274959564\n",
      "Epoch [3/5000], Train Loss: 6777.6097, Test Loss: 5841.2142\n",
      "Epoch time:  402.8392524719238\n",
      "Epoch [4/5000], Train Loss: 5022.3068, Test Loss: 4500.0571\n",
      "Epoch time:  400.0149567127228\n",
      "Epoch [5/5000], Train Loss: 4379.7819, Test Loss: 4433.2164\n",
      "Epoch time:  399.5797176361084\n",
      "Epoch [6/5000], Train Loss: 4025.9677, Test Loss: 4124.2383\n",
      "Epoch time:  411.8033232688904\n",
      "Epoch [7/5000], Train Loss: 3806.4347, Test Loss: 3671.3277\n",
      "Epoch time:  397.2672402858734\n",
      "Epoch [8/5000], Train Loss: 3637.7353, Test Loss: 3437.6496\n",
      "Epoch time:  393.8951835632324\n",
      "Epoch [9/5000], Train Loss: 3478.2585, Test Loss: 3487.0662\n",
      "Epoch time:  395.2951440811157\n",
      "Epoch [10/5000], Train Loss: 3376.7514, Test Loss: 3608.9642\n",
      "Epoch time:  373.7318334579468\n",
      "Epoch [11/5000], Train Loss: 3301.1741, Test Loss: 3189.1946\n",
      "Epoch time:  379.7137362957001\n",
      "Epoch [12/5000], Train Loss: 3214.2639, Test Loss: 3136.6792\n",
      "Epoch time:  390.60756635665894\n",
      "Epoch [13/5000], Train Loss: 3161.2931, Test Loss: 3307.1912\n",
      "Epoch time:  356.6123378276825\n",
      "Epoch [14/5000], Train Loss: 3109.2861, Test Loss: 2933.9488\n",
      "Epoch time:  396.8901643753052\n",
      "Epoch [15/5000], Train Loss: 3053.7975, Test Loss: 3120.3302\n",
      "Epoch time:  393.3615038394928\n",
      "Epoch [16/5000], Train Loss: 3002.3030, Test Loss: 3107.2106\n",
      "Epoch time:  388.06798362731934\n",
      "Epoch [17/5000], Train Loss: 2970.0618, Test Loss: 2994.4122\n",
      "Epoch time:  385.1394944190979\n",
      "Epoch [18/5000], Train Loss: 2893.1218, Test Loss: 2823.9312\n",
      "Epoch time:  378.6841917037964\n",
      "Epoch [19/5000], Train Loss: 2862.3705, Test Loss: 2729.2857\n",
      "Epoch time:  395.1973612308502\n",
      "Epoch [20/5000], Train Loss: 2810.2141, Test Loss: 2802.0773\n",
      "Epoch time:  389.23738741874695\n",
      "Epoch [21/5000], Train Loss: 2779.1873, Test Loss: 2579.3016\n",
      "Epoch time:  379.18843150138855\n",
      "Epoch [22/5000], Train Loss: 2711.5216, Test Loss: 2705.1447\n",
      "Epoch time:  384.04996490478516\n",
      "Epoch [23/5000], Train Loss: 2662.9564, Test Loss: 2679.7169\n",
      "Epoch time:  391.6561665534973\n",
      "Epoch [24/5000], Train Loss: 2627.2254, Test Loss: 2558.3370\n",
      "Epoch time:  391.38383293151855\n",
      "Epoch [25/5000], Train Loss: 2582.3865, Test Loss: 2469.0155\n",
      "Epoch time:  402.10671281814575\n",
      "Epoch [26/5000], Train Loss: 2547.7507, Test Loss: 2533.7931\n",
      "Epoch time:  424.78326654434204\n",
      "Epoch [27/5000], Train Loss: 2501.3733, Test Loss: 2397.2816\n",
      "Epoch time:  421.71820068359375\n",
      "Epoch [28/5000], Train Loss: 2469.3968, Test Loss: 2425.9869\n",
      "Epoch time:  402.504691362381\n",
      "Epoch [29/5000], Train Loss: 2422.3877, Test Loss: 2485.3951\n",
      "Epoch time:  364.6480767726898\n",
      "Epoch [30/5000], Train Loss: 2388.2325, Test Loss: 2448.0394\n",
      "Epoch time:  383.9937665462494\n",
      "Epoch [31/5000], Train Loss: 2340.7008, Test Loss: 2298.5379\n",
      "Epoch time:  407.7614040374756\n",
      "Epoch [32/5000], Train Loss: 2313.2518, Test Loss: 2311.4191\n",
      "Epoch time:  415.20717191696167\n",
      "Epoch [33/5000], Train Loss: 2265.1846, Test Loss: 2183.8733\n",
      "Epoch time:  389.2297611236572\n",
      "Epoch [34/5000], Train Loss: 2228.1791, Test Loss: 2202.0978\n",
      "Epoch time:  396.00380873680115\n",
      "Epoch [35/5000], Train Loss: 2195.4413, Test Loss: 2242.2732\n",
      "Epoch time:  424.94953989982605\n",
      "Epoch [36/5000], Train Loss: 2174.9367, Test Loss: 2181.8486\n",
      "Epoch time:  389.08014464378357\n",
      "Epoch [37/5000], Train Loss: 2130.1113, Test Loss: 2222.7005\n",
      "Epoch time:  381.81514286994934\n",
      "Epoch [38/5000], Train Loss: 2101.1618, Test Loss: 2224.0332\n",
      "Epoch time:  395.21981287002563\n",
      "Epoch [39/5000], Train Loss: 2077.0855, Test Loss: 2141.6950\n",
      "Epoch time:  451.3483283519745\n",
      "Epoch [40/5000], Train Loss: 2043.4245, Test Loss: 1869.6470\n",
      "Epoch time:  386.465904712677\n",
      "Epoch [41/5000], Train Loss: 2025.0630, Test Loss: 2106.8846\n",
      "Epoch time:  383.0138759613037\n",
      "Epoch [42/5000], Train Loss: 1996.8301, Test Loss: 1978.3104\n",
      "Epoch time:  368.16267132759094\n",
      "Epoch [43/5000], Train Loss: 1980.3882, Test Loss: 1876.6734\n",
      "Epoch time:  389.45168256759644\n",
      "Epoch [44/5000], Train Loss: 1955.2654, Test Loss: 1934.7733\n",
      "Epoch time:  391.3757085800171\n",
      "Epoch [45/5000], Train Loss: 1926.3265, Test Loss: 2006.6840\n",
      "Epoch time:  377.3792119026184\n",
      "Epoch [46/5000], Train Loss: 1910.1321, Test Loss: 1848.0110\n",
      "Epoch time:  399.4087998867035\n",
      "Epoch [47/5000], Train Loss: 1889.3551, Test Loss: 1856.1989\n",
      "Epoch time:  425.3487374782562\n",
      "Epoch [48/5000], Train Loss: 1881.7961, Test Loss: 1871.5097\n",
      "Epoch time:  400.35427808761597\n",
      "Epoch [49/5000], Train Loss: 1866.5512, Test Loss: 1820.2917\n",
      "Epoch time:  368.98544454574585\n",
      "Epoch [50/5000], Train Loss: 1847.3509, Test Loss: 1866.3009\n",
      "Epoch time:  373.4911115169525\n",
      "Epoch [51/5000], Train Loss: 1834.3570, Test Loss: 1926.5365\n",
      "Epoch time:  377.03650546073914\n",
      "Epoch [52/5000], Train Loss: 1833.5173, Test Loss: 1849.8196\n",
      "Epoch time:  372.72435235977173\n",
      "Epoch [53/5000], Train Loss: 1811.6216, Test Loss: 1914.1354\n",
      "Epoch time:  382.9835605621338\n",
      "Epoch [54/5000], Train Loss: 1796.7864, Test Loss: 1872.7203\n",
      "Epoch time:  378.6001715660095\n",
      "Epoch [55/5000], Train Loss: 1797.9341, Test Loss: 1784.9917\n",
      "Epoch time:  384.2549681663513\n",
      "Epoch [56/5000], Train Loss: 1800.7470, Test Loss: 1753.5374\n",
      "Epoch time:  391.69182324409485\n",
      "Epoch [57/5000], Train Loss: 1790.2174, Test Loss: 1920.2651\n",
      "Epoch time:  376.84254360198975\n",
      "Epoch [58/5000], Train Loss: 1773.5146, Test Loss: 1819.7212\n",
      "Epoch time:  402.6692306995392\n",
      "Epoch [59/5000], Train Loss: 1773.5175, Test Loss: 1760.8098\n",
      "Epoch time:  366.62143301963806\n",
      "Epoch [60/5000], Train Loss: 1761.9788, Test Loss: 1802.9691\n",
      "Epoch time:  374.51278138160706\n",
      "Epoch [61/5000], Train Loss: 1759.1376, Test Loss: 1925.9699\n",
      "Epoch time:  383.2823324203491\n",
      "Epoch [62/5000], Train Loss: 1740.3375, Test Loss: 1748.0751\n",
      "Epoch time:  384.21630477905273\n",
      "Epoch [63/5000], Train Loss: 1741.0998, Test Loss: 1669.7691\n",
      "Epoch time:  378.72796630859375\n",
      "Epoch [64/5000], Train Loss: 1735.0171, Test Loss: 1765.8631\n",
      "Epoch time:  392.8089027404785\n",
      "Epoch [65/5000], Train Loss: 1729.0903, Test Loss: 1808.3085\n",
      "Epoch time:  395.59029746055603\n",
      "Epoch [66/5000], Train Loss: 1714.7483, Test Loss: 1625.3078\n",
      "Epoch time:  402.05869030952454\n",
      "Epoch [67/5000], Train Loss: 1717.7094, Test Loss: 1652.1838\n",
      "Epoch time:  398.6926074028015\n",
      "Epoch [68/5000], Train Loss: 1707.1515, Test Loss: 1771.2982\n",
      "Epoch time:  392.9097454547882\n",
      "Epoch [69/5000], Train Loss: 1709.2627, Test Loss: 1796.4140\n",
      "Epoch time:  378.2061276435852\n",
      "Epoch [70/5000], Train Loss: 1697.2655, Test Loss: 1650.7860\n",
      "Epoch time:  379.19820642471313\n",
      "Epoch [71/5000], Train Loss: 1699.5005, Test Loss: 1617.2360\n",
      "Epoch time:  357.64850640296936\n",
      "Epoch [72/5000], Train Loss: 1699.2811, Test Loss: 1709.4576\n",
      "Epoch time:  353.35897517204285\n",
      "Epoch [73/5000], Train Loss: 1685.3009, Test Loss: 1770.1761\n",
      "Epoch time:  381.3753809928894\n",
      "Epoch [74/5000], Train Loss: 1684.3609, Test Loss: 1696.3449\n",
      "Epoch time:  358.8639483451843\n",
      "Epoch [75/5000], Train Loss: 1686.1431, Test Loss: 1573.4936\n",
      "Epoch time:  372.2971715927124\n",
      "Epoch [76/5000], Train Loss: 1671.7618, Test Loss: 1562.6843\n",
      "Epoch time:  383.2181715965271\n",
      "Epoch [77/5000], Train Loss: 1678.6189, Test Loss: 1657.9495\n",
      "Epoch time:  382.03662180900574\n",
      "Epoch [78/5000], Train Loss: 1666.2340, Test Loss: 1682.4321\n",
      "Epoch time:  380.5380048751831\n",
      "Epoch [79/5000], Train Loss: 1673.0450, Test Loss: 1659.2659\n",
      "Epoch time:  385.22980427742004\n",
      "Epoch [80/5000], Train Loss: 1666.9934, Test Loss: 1682.7405\n",
      "Epoch time:  377.198073387146\n",
      "Epoch [81/5000], Train Loss: 1658.0159, Test Loss: 1664.2434\n",
      "Epoch time:  367.10120463371277\n",
      "Epoch [82/5000], Train Loss: 1660.7577, Test Loss: 1567.2541\n",
      "Epoch time:  385.8129482269287\n",
      "Epoch [83/5000], Train Loss: 1659.6977, Test Loss: 1646.7982\n",
      "Epoch time:  379.854279756546\n",
      "Epoch [84/5000], Train Loss: 1657.8528, Test Loss: 1616.5407\n",
      "Epoch time:  364.9540741443634\n",
      "Epoch [85/5000], Train Loss: 1652.9679, Test Loss: 1665.0192\n",
      "Epoch time:  389.2723038196564\n",
      "Epoch [86/5000], Train Loss: 1651.0430, Test Loss: 1596.6040\n",
      "Epoch time:  377.6375298500061\n",
      "Epoch [87/5000], Train Loss: 1646.9715, Test Loss: 1532.5084\n",
      "Epoch time:  392.5126678943634\n",
      "Epoch [88/5000], Train Loss: 1644.6316, Test Loss: 1574.6833\n",
      "Epoch time:  395.09743189811707\n",
      "Epoch [89/5000], Train Loss: 1636.0202, Test Loss: 1657.3487\n",
      "Epoch time:  400.4669020175934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/5000], Train Loss: 1649.1734, Test Loss: 1597.4747\n",
      "Epoch time:  383.581574678421\n",
      "Epoch [91/5000], Train Loss: 1632.5195, Test Loss: 1697.0881\n",
      "Epoch time:  406.80807304382324\n",
      "Epoch [92/5000], Train Loss: 1634.0541, Test Loss: 1620.5070\n",
      "Epoch time:  382.5830373764038\n",
      "Epoch [93/5000], Train Loss: 1634.6224, Test Loss: 1765.2272\n",
      "Epoch time:  372.61725211143494\n",
      "Epoch [94/5000], Train Loss: 1637.6749, Test Loss: 1664.1143\n",
      "Epoch time:  383.14458084106445\n",
      "Epoch [95/5000], Train Loss: 1636.5187, Test Loss: 1695.1279\n",
      "Epoch time:  373.85328817367554\n",
      "Epoch [96/5000], Train Loss: 1627.3193, Test Loss: 1589.7931\n",
      "Epoch time:  402.3304479122162\n",
      "Epoch [97/5000], Train Loss: 1623.5004, Test Loss: 1664.8143\n",
      "Epoch time:  439.95149636268616\n",
      "Epoch [98/5000], Train Loss: 1621.0822, Test Loss: 1771.6920\n",
      "Epoch time:  381.81434178352356\n",
      "Epoch [99/5000], Train Loss: 1625.3539, Test Loss: 1576.8046\n",
      "Epoch time:  375.54011368751526\n",
      "Epoch [100/5000], Train Loss: 1608.9835, Test Loss: 1561.1296\n",
      "Epoch time:  352.3027708530426\n",
      "Epoch [101/5000], Train Loss: 1613.2098, Test Loss: 1527.4995\n",
      "Epoch time:  361.76752638816833\n",
      "Epoch [102/5000], Train Loss: 1616.8709, Test Loss: 1593.5851\n",
      "Epoch time:  358.6672053337097\n",
      "Epoch [103/5000], Train Loss: 1621.9344, Test Loss: 1604.2889\n",
      "Epoch time:  362.8845019340515\n",
      "Epoch [104/5000], Train Loss: 1621.9845, Test Loss: 1625.9625\n",
      "Epoch time:  336.178190946579\n",
      "Epoch [105/5000], Train Loss: 1617.3386, Test Loss: 1735.4277\n",
      "Epoch time:  346.00332903862\n",
      "Epoch [106/5000], Train Loss: 1615.9286, Test Loss: 1571.4317\n",
      "Epoch time:  357.02054810523987\n",
      "Epoch [107/5000], Train Loss: 1618.9451, Test Loss: 1643.0058\n",
      "Epoch time:  356.37852907180786\n",
      "Epoch [108/5000], Train Loss: 1613.0610, Test Loss: 1685.1642\n",
      "Epoch time:  369.23467659950256\n",
      "Epoch [109/5000], Train Loss: 1612.7563, Test Loss: 1681.8355\n",
      "Epoch time:  395.44573879241943\n",
      "Epoch [110/5000], Train Loss: 1612.5887, Test Loss: 1618.2042\n",
      "Epoch time:  391.5516083240509\n",
      "Epoch [111/5000], Train Loss: 1609.8242, Test Loss: 1647.2618\n",
      "Epoch time:  370.46030950546265\n",
      "Epoch [112/5000], Train Loss: 1603.4684, Test Loss: 1657.5122\n",
      "Epoch time:  368.30267357826233\n",
      "Epoch [113/5000], Train Loss: 1607.2008, Test Loss: 1607.9893\n",
      "Epoch time:  381.19618678092957\n",
      "Epoch [114/5000], Train Loss: 1608.0671, Test Loss: 1578.4731\n",
      "Epoch time:  372.43828678131104\n",
      "Epoch [115/5000], Train Loss: 1611.7736, Test Loss: 1736.8993\n",
      "Epoch time:  359.2472679615021\n",
      "Epoch [116/5000], Train Loss: 1592.6947, Test Loss: 1669.2346\n",
      "Epoch time:  375.06111907958984\n",
      "Epoch [117/5000], Train Loss: 1610.2950, Test Loss: 1611.5875\n",
      "Epoch time:  380.2533612251282\n",
      "Epoch [118/5000], Train Loss: 1594.0869, Test Loss: 1565.1382\n",
      "Epoch time:  371.2967357635498\n",
      "Epoch [119/5000], Train Loss: 1598.7328, Test Loss: 1658.1301\n",
      "Epoch time:  373.44875717163086\n",
      "Epoch [120/5000], Train Loss: 1596.7448, Test Loss: 1578.9134\n",
      "Epoch time:  423.3421194553375\n",
      "Epoch [121/5000], Train Loss: 1596.3730, Test Loss: 1607.3484\n",
      "Epoch time:  380.5883333683014\n",
      "Epoch [122/5000], Train Loss: 1590.8464, Test Loss: 1666.3019\n",
      "Epoch time:  378.56930470466614\n",
      "Epoch [123/5000], Train Loss: 1587.3469, Test Loss: 1557.3111\n",
      "Epoch time:  384.6690948009491\n",
      "Epoch [124/5000], Train Loss: 1591.6942, Test Loss: 1789.3650\n",
      "Epoch time:  357.26558566093445\n",
      "Epoch [125/5000], Train Loss: 1598.1489, Test Loss: 1580.4926\n",
      "Epoch time:  380.77296805381775\n",
      "Epoch [126/5000], Train Loss: 1588.2100, Test Loss: 1549.2032\n",
      "Epoch time:  403.6748650074005\n",
      "Epoch [127/5000], Train Loss: 1587.2910, Test Loss: 1711.0549\n",
      "Epoch time:  398.1303882598877\n",
      "Epoch [128/5000], Train Loss: 1588.5843, Test Loss: 1537.6543\n",
      "Epoch time:  386.27393198013306\n",
      "Epoch [129/5000], Train Loss: 1589.8157, Test Loss: 1547.2927\n",
      "Epoch time:  415.82098507881165\n",
      "Epoch [130/5000], Train Loss: 1586.8949, Test Loss: 1496.5187\n",
      "Epoch time:  473.3953881263733\n",
      "Epoch [131/5000], Train Loss: 1589.7849, Test Loss: 1524.4561\n",
      "Epoch time:  376.0737838745117\n",
      "Epoch [132/5000], Train Loss: 1597.7305, Test Loss: 1563.8021\n",
      "Epoch time:  381.2683787345886\n",
      "Epoch [133/5000], Train Loss: 1582.0621, Test Loss: 1589.0655\n",
      "Epoch time:  556.3928043842316\n",
      "Epoch [134/5000], Train Loss: 1585.9438, Test Loss: 1598.7452\n",
      "Epoch time:  374.426052570343\n",
      "Epoch [135/5000], Train Loss: 1584.9558, Test Loss: 1679.2131\n",
      "Epoch time:  337.91863226890564\n",
      "Epoch [136/5000], Train Loss: 1570.3952, Test Loss: 1604.1308\n",
      "Epoch time:  479.138587474823\n",
      "Epoch [137/5000], Train Loss: 1587.6024, Test Loss: 1594.0956\n",
      "Epoch time:  430.49341225624084\n",
      "Epoch [138/5000], Train Loss: 1592.3637, Test Loss: 1584.7705\n",
      "Epoch time:  423.45010566711426\n",
      "Epoch [139/5000], Train Loss: 1565.9355, Test Loss: 1589.5353\n",
      "Epoch time:  433.83209705352783\n",
      "Epoch [140/5000], Train Loss: 1578.8861, Test Loss: 1611.0431\n",
      "Epoch time:  393.3442349433899\n",
      "Epoch [141/5000], Train Loss: 1575.3867, Test Loss: 1519.5461\n",
      "Epoch time:  416.3029136657715\n",
      "Epoch [142/5000], Train Loss: 1577.5207, Test Loss: 1522.7371\n",
      "Epoch time:  382.30806159973145\n",
      "Epoch [143/5000], Train Loss: 1561.6894, Test Loss: 1653.8413\n",
      "Epoch time:  442.1498143672943\n",
      "Epoch [144/5000], Train Loss: 1575.8841, Test Loss: 1550.7708\n",
      "Epoch time:  369.84879302978516\n",
      "Epoch [145/5000], Train Loss: 1566.7882, Test Loss: 1654.3300\n",
      "Epoch time:  381.4221000671387\n",
      "Epoch [146/5000], Train Loss: 1569.0435, Test Loss: 1611.9316\n",
      "Epoch time:  318.58447647094727\n",
      "Epoch [147/5000], Train Loss: 1568.1747, Test Loss: 1764.1622\n",
      "Epoch time:  358.69915080070496\n",
      "Epoch [148/5000], Train Loss: 1564.9790, Test Loss: 1717.1664\n",
      "Epoch time:  354.96200251579285\n",
      "Epoch [149/5000], Train Loss: 1558.9427, Test Loss: 1596.5646\n",
      "Epoch time:  358.7711582183838\n",
      "Epoch [150/5000], Train Loss: 1576.5945, Test Loss: 1608.9687\n",
      "Epoch time:  347.0396661758423\n",
      "Epoch [151/5000], Train Loss: 1559.9818, Test Loss: 1577.8549\n",
      "Epoch time:  393.26639890670776\n",
      "Epoch [152/5000], Train Loss: 1577.6134, Test Loss: 1631.4234\n",
      "Epoch time:  375.91156244277954\n",
      "Epoch [153/5000], Train Loss: 1570.5884, Test Loss: 1486.5272\n",
      "Epoch time:  406.6532738208771\n",
      "Epoch [154/5000], Train Loss: 1565.6182, Test Loss: 1571.6918\n",
      "Epoch time:  381.1067259311676\n",
      "Epoch [155/5000], Train Loss: 1562.8251, Test Loss: 1569.9064\n",
      "Epoch time:  373.85261702537537\n",
      "Epoch [156/5000], Train Loss: 1567.7425, Test Loss: 1564.2540\n",
      "Epoch time:  376.4207661151886\n",
      "Epoch [157/5000], Train Loss: 1563.7696, Test Loss: 1494.3276\n",
      "Epoch time:  339.49509716033936\n",
      "Epoch [158/5000], Train Loss: 1568.3858, Test Loss: 1559.4412\n",
      "Epoch time:  367.7058870792389\n",
      "Epoch [159/5000], Train Loss: 1558.4249, Test Loss: 1652.8975\n",
      "Epoch time:  423.1362030506134\n",
      "Epoch [160/5000], Train Loss: 1558.1013, Test Loss: 1543.6482\n",
      "Epoch time:  550.4345843791962\n",
      "Epoch [161/5000], Train Loss: 1558.2704, Test Loss: 1551.7057\n",
      "Epoch time:  508.2175679206848\n",
      "Epoch [162/5000], Train Loss: 1563.5870, Test Loss: 1597.9501\n",
      "Epoch time:  467.63516092300415\n",
      "Epoch [163/5000], Train Loss: 1563.7510, Test Loss: 1560.0449\n",
      "Epoch time:  392.5611734390259\n",
      "Epoch [164/5000], Train Loss: 1558.0478, Test Loss: 1574.7581\n",
      "Epoch time:  526.8626842498779\n",
      "Epoch [165/5000], Train Loss: 1549.2736, Test Loss: 1532.7297\n",
      "Epoch time:  513.00022315979\n",
      "Epoch [166/5000], Train Loss: 1563.9164, Test Loss: 1607.7340\n",
      "Epoch time:  448.7185380458832\n",
      "Epoch [167/5000], Train Loss: 1551.8041, Test Loss: 1564.3697\n",
      "Epoch time:  521.1138167381287\n",
      "Epoch [168/5000], Train Loss: 1557.9642, Test Loss: 1575.7708\n",
      "Epoch time:  537.6807055473328\n",
      "Epoch [169/5000], Train Loss: 1556.4214, Test Loss: 1620.5903\n",
      "Epoch time:  392.47893953323364\n",
      "Epoch [170/5000], Train Loss: 1551.5205, Test Loss: 1571.6831\n",
      "Epoch time:  406.78664088249207\n",
      "Epoch [171/5000], Train Loss: 1551.1481, Test Loss: 1543.9646\n",
      "Epoch time:  412.15131306648254\n",
      "Epoch [172/5000], Train Loss: 1551.7421, Test Loss: 1664.0960\n",
      "Epoch time:  482.48837995529175\n",
      "Epoch [173/5000], Train Loss: 1554.9782, Test Loss: 1476.7233\n",
      "Epoch time:  497.26936316490173\n",
      "Epoch [174/5000], Train Loss: 1549.6445, Test Loss: 1570.0169\n",
      "Epoch time:  394.8313045501709\n",
      "Epoch [175/5000], Train Loss: 1548.4190, Test Loss: 1540.5138\n",
      "Epoch time:  447.34673595428467\n",
      "Epoch [176/5000], Train Loss: 1554.8380, Test Loss: 1612.0015\n",
      "Epoch time:  410.31918573379517\n",
      "Epoch [177/5000], Train Loss: 1550.4530, Test Loss: 1466.3765\n",
      "Epoch time:  519.4275183677673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [178/5000], Train Loss: 1547.4355, Test Loss: 1715.1310\n",
      "Epoch time:  512.7827880382538\n",
      "Epoch [179/5000], Train Loss: 1554.0614, Test Loss: 1763.6040\n",
      "Epoch time:  442.3045873641968\n",
      "Epoch [180/5000], Train Loss: 1546.8735, Test Loss: 1566.9588\n",
      "Epoch time:  420.6811249256134\n",
      "Epoch [181/5000], Train Loss: 1556.3623, Test Loss: 1583.0640\n",
      "Epoch time:  438.58517599105835\n",
      "Epoch [182/5000], Train Loss: 1551.9638, Test Loss: 1510.1938\n",
      "Epoch time:  434.41526079177856\n",
      "Epoch [183/5000], Train Loss: 1552.8488, Test Loss: 1624.8265\n",
      "Epoch time:  505.1919515132904\n",
      "Epoch [184/5000], Train Loss: 1543.9050, Test Loss: 1555.7019\n",
      "Epoch time:  435.55029463768005\n",
      "Epoch [185/5000], Train Loss: 1545.4838, Test Loss: 1668.4957\n",
      "Epoch time:  523.2303369045258\n",
      "Epoch [186/5000], Train Loss: 1542.5043, Test Loss: 1848.3613\n",
      "Epoch time:  476.24966502189636\n",
      "Epoch [187/5000], Train Loss: 1544.0093, Test Loss: 1523.4948\n",
      "Epoch time:  492.5786643028259\n",
      "Epoch [188/5000], Train Loss: 1552.7191, Test Loss: 1472.5769\n",
      "Epoch time:  397.6242311000824\n",
      "Epoch [189/5000], Train Loss: 1536.3941, Test Loss: 1569.5626\n",
      "Epoch time:  468.390638589859\n",
      "Epoch [190/5000], Train Loss: 1542.5962, Test Loss: 1589.8921\n",
      "Epoch time:  499.45753622055054\n",
      "Epoch [191/5000], Train Loss: 1544.8750, Test Loss: 1463.3476\n",
      "Epoch time:  521.5364122390747\n",
      "Epoch [192/5000], Train Loss: 1544.8276, Test Loss: 1521.5477\n",
      "Epoch time:  466.768785238266\n",
      "Epoch [193/5000], Train Loss: 1541.9915, Test Loss: 1564.0192\n",
      "Epoch time:  499.569322347641\n",
      "Epoch [194/5000], Train Loss: 1543.9719, Test Loss: 1592.6196\n",
      "Epoch time:  420.8149445056915\n",
      "Epoch [195/5000], Train Loss: 1553.1051, Test Loss: 1512.8303\n",
      "Epoch time:  435.7418851852417\n",
      "Epoch [196/5000], Train Loss: 1542.5857, Test Loss: 1673.5023\n",
      "Epoch time:  425.966251373291\n",
      "Epoch [197/5000], Train Loss: 1542.2732, Test Loss: 1513.6006\n",
      "Epoch time:  453.79771733283997\n",
      "Epoch [198/5000], Train Loss: 1545.3048, Test Loss: 1527.3769\n",
      "Epoch time:  449.8958559036255\n",
      "Epoch [199/5000], Train Loss: 1541.9764, Test Loss: 1554.7693\n",
      "Epoch time:  439.407502412796\n",
      "Epoch [200/5000], Train Loss: 1532.3732, Test Loss: 1539.4534\n",
      "Epoch time:  398.0927634239197\n",
      "Epoch [201/5000], Train Loss: 1530.7583, Test Loss: 1436.7556\n",
      "Epoch time:  476.3624427318573\n",
      "Epoch [202/5000], Train Loss: 1543.0054, Test Loss: 1695.0292\n",
      "Epoch time:  461.46506571769714\n",
      "Epoch [203/5000], Train Loss: 1540.0734, Test Loss: 1579.7101\n",
      "Epoch time:  454.068815946579\n",
      "Epoch [204/5000], Train Loss: 1540.2832, Test Loss: 1526.8095\n",
      "Epoch time:  432.5064675807953\n",
      "Epoch [205/5000], Train Loss: 1535.6241, Test Loss: 1579.8563\n",
      "Epoch time:  429.64517068862915\n",
      "Epoch [206/5000], Train Loss: 1536.0981, Test Loss: 1444.2874\n",
      "Epoch time:  496.799028635025\n",
      "Epoch [207/5000], Train Loss: 1539.7801, Test Loss: 1652.0697\n",
      "Epoch time:  469.1463384628296\n",
      "Epoch [208/5000], Train Loss: 1540.6384, Test Loss: 1408.6343\n",
      "Epoch time:  476.23418521881104\n",
      "Epoch [209/5000], Train Loss: 1539.6641, Test Loss: 1713.7454\n",
      "Epoch time:  477.3361494541168\n",
      "Epoch [210/5000], Train Loss: 1537.9934, Test Loss: 1533.6291\n",
      "Epoch time:  422.30285358428955\n",
      "Epoch [211/5000], Train Loss: 1530.8797, Test Loss: 1464.6757\n",
      "Epoch time:  453.35028171539307\n",
      "Epoch [212/5000], Train Loss: 1534.0927, Test Loss: 1549.2276\n",
      "Epoch time:  455.8988358974457\n",
      "Epoch [213/5000], Train Loss: 1527.2516, Test Loss: 1621.6576\n",
      "Epoch time:  471.71707916259766\n",
      "Epoch [214/5000], Train Loss: 1528.9403, Test Loss: 1530.2557\n",
      "Epoch time:  488.4942572116852\n",
      "Epoch [215/5000], Train Loss: 1531.4418, Test Loss: 1492.9738\n",
      "Epoch time:  419.2610375881195\n",
      "Epoch [216/5000], Train Loss: 1525.3631, Test Loss: 1568.8403\n",
      "Epoch time:  513.8748743534088\n",
      "Epoch [217/5000], Train Loss: 1526.3234, Test Loss: 1573.9314\n",
      "Epoch time:  465.14784240722656\n",
      "Epoch [218/5000], Train Loss: 1532.0847, Test Loss: 1589.9561\n",
      "Epoch time:  451.8060715198517\n",
      "Epoch [219/5000], Train Loss: 1531.8533, Test Loss: 1659.5637\n",
      "Epoch time:  366.99637961387634\n",
      "Epoch [220/5000], Train Loss: 1532.6508, Test Loss: 1529.0527\n",
      "Epoch time:  377.26442885398865\n",
      "Epoch [221/5000], Train Loss: 1534.1324, Test Loss: 1470.7570\n",
      "Epoch time:  335.29459285736084\n",
      "Epoch [222/5000], Train Loss: 1529.1303, Test Loss: 1488.2897\n",
      "Epoch time:  360.99993205070496\n",
      "Epoch [223/5000], Train Loss: 1527.7741, Test Loss: 1590.5207\n",
      "Epoch time:  322.05677103996277\n",
      "Epoch [224/5000], Train Loss: 1537.5125, Test Loss: 1705.5695\n",
      "Epoch time:  400.03156661987305\n",
      "Epoch [225/5000], Train Loss: 1527.5053, Test Loss: 1538.0601\n",
      "Epoch time:  368.4187843799591\n",
      "Epoch [226/5000], Train Loss: 1529.9591, Test Loss: 1636.3195\n",
      "Epoch time:  341.66221141815186\n",
      "Epoch [227/5000], Train Loss: 1533.7361, Test Loss: 1508.0972\n",
      "Epoch time:  353.59867811203003\n",
      "Epoch [228/5000], Train Loss: 1525.9634, Test Loss: 1496.7421\n",
      "Epoch time:  377.51340079307556\n",
      "Epoch [229/5000], Train Loss: 1529.0930, Test Loss: 1600.5510\n",
      "Epoch time:  330.87531065940857\n",
      "Epoch [230/5000], Train Loss: 1530.3366, Test Loss: 1544.4819\n",
      "Epoch time:  367.11171889305115\n",
      "Epoch [231/5000], Train Loss: 1524.2850, Test Loss: 1523.3772\n",
      "Epoch time:  337.6962628364563\n",
      "Epoch [232/5000], Train Loss: 1518.7794, Test Loss: 1640.1076\n",
      "Epoch time:  339.0118260383606\n",
      "Epoch [233/5000], Train Loss: 1536.8504, Test Loss: 1526.4159\n",
      "Epoch time:  364.34394550323486\n",
      "Epoch [234/5000], Train Loss: 1516.5951, Test Loss: 1508.4781\n",
      "Epoch time:  340.8111324310303\n",
      "Epoch [235/5000], Train Loss: 1531.3624, Test Loss: 1636.3832\n",
      "Epoch time:  359.0503661632538\n",
      "Epoch [236/5000], Train Loss: 1527.0561, Test Loss: 1542.4671\n",
      "Epoch time:  392.0515215396881\n",
      "Epoch [237/5000], Train Loss: 1527.4272, Test Loss: 1763.4285\n",
      "Epoch time:  355.0270493030548\n",
      "Epoch [238/5000], Train Loss: 1516.6730, Test Loss: 1616.3349\n",
      "Epoch time:  386.16714310646057\n",
      "Epoch [239/5000], Train Loss: 1522.4692, Test Loss: 1648.7014\n",
      "Epoch time:  371.9815649986267\n",
      "Epoch [240/5000], Train Loss: 1522.1754, Test Loss: 1478.3402\n",
      "Epoch time:  338.37227034568787\n",
      "Epoch [241/5000], Train Loss: 1524.2819, Test Loss: 1531.1060\n",
      "Epoch time:  380.23757767677307\n",
      "Epoch [242/5000], Train Loss: 1531.3472, Test Loss: 1592.7572\n",
      "Epoch time:  398.8988616466522\n",
      "Epoch [243/5000], Train Loss: 1519.4866, Test Loss: 1577.4111\n",
      "Epoch time:  349.1762104034424\n",
      "Epoch [244/5000], Train Loss: 1524.1950, Test Loss: 1462.6798\n",
      "Epoch time:  365.39479541778564\n",
      "Epoch [245/5000], Train Loss: 1529.5388, Test Loss: 1608.1584\n",
      "Epoch time:  408.1686170101166\n",
      "Epoch [246/5000], Train Loss: 1522.2620, Test Loss: 1515.1642\n",
      "Epoch time:  362.20320105552673\n",
      "Epoch [247/5000], Train Loss: 1530.1814, Test Loss: 1579.5072\n",
      "Epoch time:  335.35030913352966\n",
      "Epoch [248/5000], Train Loss: 1520.7424, Test Loss: 1507.0435\n",
      "Epoch time:  372.03126096725464\n",
      "Epoch [249/5000], Train Loss: 1522.5070, Test Loss: 1567.7384\n",
      "Epoch time:  348.62717294692993\n",
      "Epoch [250/5000], Train Loss: 1523.5617, Test Loss: 1447.6652\n",
      "Epoch time:  359.19558668136597\n",
      "Epoch [251/5000], Train Loss: 1514.9577, Test Loss: 1401.7300\n",
      "Epoch time:  351.28092217445374\n",
      "Epoch [252/5000], Train Loss: 1527.7071, Test Loss: 1560.1634\n",
      "Epoch time:  358.9686698913574\n",
      "Epoch [253/5000], Train Loss: 1521.8675, Test Loss: 1514.0783\n",
      "Epoch time:  380.61121439933777\n",
      "Epoch [254/5000], Train Loss: 1534.6079, Test Loss: 1500.7663\n",
      "Epoch time:  352.8290092945099\n",
      "Epoch [255/5000], Train Loss: 1520.8215, Test Loss: 1516.4315\n",
      "Epoch time:  372.3518693447113\n",
      "Epoch [256/5000], Train Loss: 1522.6428, Test Loss: 1518.5991\n",
      "Epoch time:  343.50270795822144\n",
      "Epoch [257/5000], Train Loss: 1519.9679, Test Loss: 1514.4258\n",
      "Epoch time:  376.7087709903717\n",
      "Epoch [258/5000], Train Loss: 1518.7203, Test Loss: 1462.1506\n",
      "Epoch time:  373.3004631996155\n",
      "Epoch [259/5000], Train Loss: 1523.4209, Test Loss: 1520.3754\n",
      "Epoch time:  332.0786352157593\n",
      "Epoch [260/5000], Train Loss: 1522.6309, Test Loss: 1497.3125\n",
      "Epoch time:  352.9682161808014\n",
      "Epoch [261/5000], Train Loss: 1516.5127, Test Loss: 1565.8834\n",
      "Epoch time:  401.88601207733154\n",
      "Epoch [262/5000], Train Loss: 1524.6360, Test Loss: 1563.0118\n",
      "Epoch time:  371.39613461494446\n",
      "Epoch [263/5000], Train Loss: 1518.6280, Test Loss: 1549.5728\n",
      "Epoch time:  368.3295810222626\n",
      "Epoch [264/5000], Train Loss: 1512.4059, Test Loss: 1594.2078\n",
      "Epoch time:  399.54289054870605\n",
      "Epoch [265/5000], Train Loss: 1521.4823, Test Loss: 1532.0112\n",
      "Epoch time:  362.6710376739502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [266/5000], Train Loss: 1516.2341, Test Loss: 1439.0764\n",
      "Epoch time:  372.31919527053833\n",
      "Epoch [267/5000], Train Loss: 1519.7790, Test Loss: 1627.7897\n",
      "Epoch time:  386.8818738460541\n",
      "Epoch [268/5000], Train Loss: 1512.8061, Test Loss: 1576.6385\n",
      "Epoch time:  404.8900263309479\n",
      "Epoch [269/5000], Train Loss: 1515.6377, Test Loss: 1500.6312\n",
      "Epoch time:  363.0595359802246\n",
      "Epoch [270/5000], Train Loss: 1512.2858, Test Loss: 1460.8320\n",
      "Epoch time:  390.52900981903076\n",
      "Epoch [271/5000], Train Loss: 1526.3680, Test Loss: 1553.4576\n",
      "Epoch time:  392.9101400375366\n",
      "Epoch [272/5000], Train Loss: 1516.2152, Test Loss: 1563.6648\n",
      "Epoch time:  341.1570484638214\n",
      "Epoch [273/5000], Train Loss: 1518.1740, Test Loss: 1577.6428\n",
      "Epoch time:  388.4290256500244\n",
      "Epoch [274/5000], Train Loss: 1518.1697, Test Loss: 1525.1345\n",
      "Epoch time:  403.30230951309204\n",
      "Epoch [275/5000], Train Loss: 1515.8770, Test Loss: 1536.8865\n",
      "Epoch time:  347.5449163913727\n",
      "Epoch [276/5000], Train Loss: 1525.0732, Test Loss: 1530.0437\n",
      "Epoch time:  364.26972818374634\n",
      "Epoch [277/5000], Train Loss: 1522.2576, Test Loss: 1580.8100\n",
      "Epoch time:  385.77989768981934\n",
      "Epoch [278/5000], Train Loss: 1518.7639, Test Loss: 1438.3130\n",
      "Epoch time:  354.4891858100891\n",
      "Epoch [279/5000], Train Loss: 1521.1622, Test Loss: 1581.1395\n",
      "Epoch time:  372.77512288093567\n",
      "Epoch [280/5000], Train Loss: 1512.3049, Test Loss: 1542.3886\n",
      "Epoch time:  384.0791518688202\n",
      "Epoch [281/5000], Train Loss: 1515.9617, Test Loss: 1523.0398\n",
      "Epoch time:  364.75773906707764\n",
      "Epoch [282/5000], Train Loss: 1513.6793, Test Loss: 1522.1511\n",
      "Epoch time:  396.4575686454773\n",
      "Epoch [283/5000], Train Loss: 1515.0480, Test Loss: 1493.6452\n",
      "Epoch time:  390.2704350948334\n",
      "Epoch [284/5000], Train Loss: 1512.4841, Test Loss: 1466.2813\n",
      "Epoch time:  349.8252549171448\n",
      "Epoch [285/5000], Train Loss: 1518.3152, Test Loss: 1661.7314\n",
      "Epoch time:  350.51852083206177\n",
      "Epoch [286/5000], Train Loss: 1519.1888, Test Loss: 1501.5980\n",
      "Epoch time:  377.85013818740845\n",
      "Epoch [287/5000], Train Loss: 1517.0827, Test Loss: 1503.8604\n",
      "Epoch time:  372.4580521583557\n",
      "Epoch [288/5000], Train Loss: 1504.8546, Test Loss: 1557.0337\n",
      "Epoch time:  376.07780861854553\n",
      "Epoch [289/5000], Train Loss: 1517.8668, Test Loss: 1534.2790\n",
      "Epoch time:  406.78622245788574\n",
      "Epoch [290/5000], Train Loss: 1520.0310, Test Loss: 1468.3396\n",
      "Epoch time:  370.86546540260315\n",
      "Epoch [291/5000], Train Loss: 1510.5963, Test Loss: 1615.3989\n",
      "Epoch time:  340.5239818096161\n",
      "Epoch [292/5000], Train Loss: 1514.0603, Test Loss: 1470.2176\n",
      "Epoch time:  326.893107175827\n",
      "Epoch [293/5000], Train Loss: 1529.0771, Test Loss: 1465.3106\n",
      "Epoch time:  330.747549533844\n",
      "Epoch [294/5000], Train Loss: 1511.5673, Test Loss: 1552.6884\n",
      "Epoch time:  301.70863246917725\n",
      "Epoch [295/5000], Train Loss: 1517.7119, Test Loss: 1465.5500\n",
      "Epoch time:  307.30414628982544\n",
      "Epoch [296/5000], Train Loss: 1517.4446, Test Loss: 1603.4206\n",
      "Epoch time:  296.4284598827362\n",
      "Epoch [297/5000], Train Loss: 1514.6874, Test Loss: 1608.7536\n",
      "Epoch time:  317.6823012828827\n",
      "Epoch [298/5000], Train Loss: 1520.3912, Test Loss: 1534.8600\n",
      "Epoch time:  291.18846797943115\n",
      "Epoch [299/5000], Train Loss: 1515.1537, Test Loss: 1465.9319\n",
      "Epoch time:  336.6085262298584\n",
      "Epoch [300/5000], Train Loss: 1509.5908, Test Loss: 1538.1191\n",
      "Epoch time:  295.9499683380127\n",
      "Epoch [301/5000], Train Loss: 1513.5543, Test Loss: 1520.7222\n",
      "Early stopping at epoch 301\n",
      "Execution time: 119869.9954841137 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401.7300172820687"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset86_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset86_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_86met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_86met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_86met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_86met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_86met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_86met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_86met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_86met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_86met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_86met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_86met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_86met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_86met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_86met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_86met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_86met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_86met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_86met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_86met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_86met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (lin1): Linear(in_features=46000, out_features=222, bias=True)\n",
       "  (relu1): LeakyReLU(negative_slope=0.01)\n",
       "  (lin2): Linear(in_features=222, out_features=463, bias=True)\n",
       "  (relu2): LeakyReLU(negative_slope=0.01)\n",
       "  (lin3): Linear(in_features=463, out_features=86, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  54.00655\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(86):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "64.59  - Concentrations: 0.004999999888241291\n",
      "18.54  - Concentrations: 0.02500000037252903\n",
      "6.28  - Concentrations: 0.10000000149011612\n",
      "4.06  - Concentrations: 0.25\n",
      "2.96  - Concentrations: 0.5\n",
      "2.59  - Concentrations: 1.0\n",
      "2.48  - Concentrations: 2.5\n",
      "2.35  - Concentrations: 10.0\n",
      "2.48  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(86):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  7.4365363\n",
      "--------------------\n",
      "6.32  - Min Value: 0.2735  - Mean Value: 10.0\n",
      "7.61  - Min Value: 0.2585  - Mean Value: 10.3\n",
      "5.57  - Min Value: 1.1016  - Mean Value: 9.9\n",
      "14.36  - Min Value: 0.1107  - Mean Value: 10.2\n",
      "9.81  - Min Value: 0.099  - Mean Value: 9.7\n",
      "6.13  - Min Value: 0.5492  - Mean Value: 10.5\n",
      "6.23  - Min Value: 0.2504  - Mean Value: 9.4\n",
      "6.15  - Min Value: 0.2381  - Mean Value: 11.0\n",
      "6.18  - Min Value: 0.3283  - Mean Value: 9.9\n",
      "6.01  - Min Value: 0.6191  - Mean Value: 9.6\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(86):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  11.072065\n",
      "--------------------\n",
      "9.25  - Min Value: 0.0054  - Mean Value: 0.1\n",
      "11.08  - Min Value: 0.0173  - Mean Value: 0.1\n",
      "11.22  - Min Value: 0.0068  - Mean Value: 0.1\n",
      "15.8  - Min Value: 0.0089  - Mean Value: 0.1\n",
      "11.01  - Min Value: 0.0062  - Mean Value: 0.1\n",
      "9.96  - Min Value: 0.0059  - Mean Value: 0.1\n",
      "10.09  - Min Value: 0.008  - Mean Value: 0.1\n",
      "10.28  - Min Value: 0.0051  - Mean Value: 0.1\n",
      "10.19  - Min Value: 0.005  - Mean Value: 0.1\n",
      "11.84  - Min Value: 0.0063  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(86):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  36.18391\n",
      "--------------------\n",
      "47.19  - Min Value: 0.0062  - Mean Value: 1.3\n",
      "48.98  - Min Value: 0.0066  - Mean Value: 0.7\n",
      "19.76  - Min Value: 0.0127  - Mean Value: 0.6\n",
      "26.03  - Min Value: 0.0092  - Mean Value: 0.8\n",
      "35.02  - Min Value: 0.0055  - Mean Value: 0.6\n",
      "24.24  - Min Value: 0.0097  - Mean Value: 0.8\n",
      "54.56  - Min Value: 0.0063  - Mean Value: 0.9\n",
      "47.85  - Min Value: 0.0066  - Mean Value: 0.7\n",
      "22.9  - Min Value: 0.0055  - Mean Value: 0.7\n",
      "35.3  - Min Value: 0.0163  - Mean Value: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(86):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 44 is out of bounds for dimension 0 with size 44",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m APE \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metabolite \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m86\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     per_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(GroundTruth[metabolite] \u001b[38;5;241m-\u001b[39m Prediction_cpu[\u001b[38;5;241m0\u001b[39m][metabolite]) \u001b[38;5;241m/\u001b[39m GroundTruth[metabolite]\n\u001b[1;32m     18\u001b[0m     APE\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mabs\u001b[39m(per_err\u001b[38;5;241m.\u001b[39mcpu()))\n\u001b[1;32m     20\u001b[0m MAPE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(APE) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(APE)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 44 is out of bounds for dimension 0 with size 44"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(86):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(86):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x53500 and 46000x222)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m GroundTruth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.43\u001b[39m\n\u001b[1;32m      8\u001b[0m model_aq\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 9\u001b[0m Prediction \u001b[38;5;241m=\u001b[39m model_aq(SNR_Spec[i])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Move Prediction tensor to CPU and detach from computation graph\u001b[39;00m\n\u001b[1;32m     12\u001b[0m Prediction_cpu \u001b[38;5;241m=\u001b[39m Prediction\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mNMR_Model_Aq.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1(\u001b[38;5;28minput\u001b[39m))))))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x53500 and 46000x222)"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(86):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  11.643129335222753\n",
      "--------------------\n",
      "3.06\n",
      "3.21\n",
      "3.34\n",
      "5.2\n",
      "10.58\n",
      "10.73\n",
      "14.37\n",
      "17.76\n",
      "21.07\n",
      "27.11\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(86):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"Singlet_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"Singlet_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  30.838093632177873\n",
      "--------------------\n",
      "3.21\n",
      "3.93\n",
      "5.7\n",
      "12.37\n",
      "22.12\n",
      "32.49\n",
      "43.27\n",
      "53.58\n",
      "62.23\n",
      "69.48\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(86):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal Baseline 1\n",
      "tensor([0.4591, 0.5962, 0.4011, 0.7034, 0.5582, 0.5321, 0.5435, 0.6375, 0.8087,\n",
      "        0.4288, 0.4719, 0.5281, 0.6060, 1.3674, 0.0287, 0.7168, 0.7465, 0.4830,\n",
      "        0.5559, 0.4217, 0.4636, 1.0352, 0.2180, 0.5676, 0.6732, 0.5323, 0.6258,\n",
      "        0.2989, 0.7314, 0.4665, 0.4804, 0.4384, 0.5473, 0.3490, 0.4444, 0.4808,\n",
      "        0.4974, 0.4607, 0.0212, 0.4551, 0.3859, 0.5624, 0.6423, 0.6331, 0.6855,\n",
      "        0.3494, 0.3415, 0.4162, 0.6581, 0.6870, 0.5132, 0.4155, 0.5046, 0.9512,\n",
      "        1.2636, 0.6128, 0.4545, 0.4334, 0.4541, 0.2549, 0.6656, 0.5092, 0.7408,\n",
      "        0.4218, 0.3663, 0.5776, 0.3868, 0.6281, 0.4831, 0.3878, 0.3447, 0.5299,\n",
      "        0.3252, 0.4587, 0.4387, 0.4163, 0.4163, 0.3584, 0.5016, 0.5141, 0.5509,\n",
      "        0.4143, 0.3028, 0.2991, 0.4679, 0.5397], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Sinusoidal Baseline 2\n",
      "tensor([0.3600, 0.5402, 0.0042, 0.6725, 0.3129, 0.4688, 0.3461, 0.1266, 0.5532,\n",
      "        0.4718, 0.3278, 0.6460, 0.3736, 0.0114, 0.6974, 0.0126, 0.3312, 0.4839,\n",
      "        0.4997, 0.4505, 0.2568, 0.2851, 0.0173, 0.4552, 0.4552, 0.4664, 0.1694,\n",
      "        0.0103, 0.5085, 0.3033, 0.4485, 0.0266, 0.1528, 0.3971, 0.6446, 0.3025,\n",
      "        0.6487, 0.8446, 0.7231, 0.0178, 0.4556, 0.7709, 0.3005, 0.5534, 0.0055,\n",
      "        0.1786, 0.3846, 0.4396, 0.2941, 0.3405, 0.2607, 0.2190, 0.2944, 0.1991,\n",
      "        0.0000, 0.1069, 0.4734, 0.0719, 0.3777, 0.3488, 0.0057, 0.2838, 0.2761,\n",
      "        0.4595, 0.0687, 0.0121, 0.3815, 0.6666, 0.6747, 0.5402, 0.0113, 0.5357,\n",
      "        0.0220, 0.1799, 0.4539, 0.4060, 0.3984, 0.1054, 0.5222, 0.3434, 0.2680,\n",
      "        0.2426, 0.3237, 0.5602, 0.3908, 0.0080], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "HD-Range 1 - 0.01s and 20s\n",
      "tensor([0.0000e+00, 1.8579e+01, 0.0000e+00, 1.7604e+01, 1.1717e-01, 1.7747e+01,\n",
      "        1.8000e-01, 1.6763e+01, 6.5353e-02, 1.8278e+01, 2.9741e-01, 1.9821e+01,\n",
      "        9.0800e-03, 1.5866e+01, 3.1941e-01, 1.8003e+01, 1.1768e-01, 1.8797e+01,\n",
      "        9.1538e-02, 1.8486e+01, 2.1173e-01, 1.7923e+01, 8.7134e-02, 1.9193e+01,\n",
      "        2.8981e-01, 1.6838e+01, 1.2263e-01, 1.7925e+01, 1.2452e-01, 1.8475e+01,\n",
      "        2.8262e-02, 1.6636e+01, 6.6957e-02, 1.9236e+01, 2.3031e-01, 1.9480e+01,\n",
      "        1.9227e-01, 1.8801e+01, 1.7658e-01, 1.8489e+01, 7.2421e-02, 1.7666e+01,\n",
      "        1.4000e-01, 1.7981e+01, 0.0000e+00, 1.9418e+01, 0.0000e+00, 1.6911e+01,\n",
      "        1.4998e-01, 1.8813e+01, 1.7520e-01, 1.8349e+01, 1.9589e-01, 1.7501e+01,\n",
      "        8.5044e-02, 1.7329e+01, 5.4971e+00, 1.8058e+01, 1.9767e-01, 2.0405e+01,\n",
      "        1.1426e-01, 1.8907e+01, 1.1938e-01, 4.7390e+00, 6.5385e-02, 1.8342e+01,\n",
      "        4.6012e-02, 1.8296e+01, 1.7401e-01, 1.8522e+01, 1.5346e-01, 1.8157e+01,\n",
      "        3.5970e-01, 1.7779e+01, 7.6998e-02, 1.9194e+01, 6.1178e-02, 1.8910e+01,\n",
      "        9.6027e-02, 1.9385e+01, 1.2130e-01, 1.8147e+01, 8.3371e-02, 1.9309e+01,\n",
      "        6.3337e-02, 1.7324e+01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "HD-Range 2 - 0s and 20s\n",
      "tensor([0.0000e+00, 1.8580e+01, 0.0000e+00, 1.7607e+01, 1.1716e-01, 1.7748e+01,\n",
      "        1.7995e-01, 1.6765e+01, 6.5362e-02, 1.8278e+01, 2.9732e-01, 1.9819e+01,\n",
      "        9.0920e-03, 1.5860e+01, 3.1938e-01, 1.8003e+01, 1.1769e-01, 1.8798e+01,\n",
      "        9.1496e-02, 1.8485e+01, 2.1159e-01, 1.7927e+01, 8.7081e-02, 1.9193e+01,\n",
      "        2.8978e-01, 1.6837e+01, 1.2256e-01, 1.7928e+01, 1.2449e-01, 1.8476e+01,\n",
      "        2.8267e-02, 1.6639e+01, 6.6940e-02, 1.9238e+01, 2.3030e-01, 1.9475e+01,\n",
      "        1.9216e-01, 1.8805e+01, 1.7655e-01, 1.8492e+01, 7.2438e-02, 1.7666e+01,\n",
      "        1.4000e-01, 1.7984e+01, 0.0000e+00, 1.9416e+01, 0.0000e+00, 1.6913e+01,\n",
      "        1.4998e-01, 1.8814e+01, 1.7525e-01, 1.8348e+01, 1.9593e-01, 1.7498e+01,\n",
      "        8.3631e-02, 1.7327e+01, 5.4915e+00, 1.8057e+01, 1.9757e-01, 2.0402e+01,\n",
      "        1.1421e-01, 1.8910e+01, 1.1940e-01, 4.7334e+00, 6.5384e-02, 1.8347e+01,\n",
      "        4.5961e-02, 1.8299e+01, 1.7401e-01, 1.8525e+01, 1.5350e-01, 1.8158e+01,\n",
      "        3.5956e-01, 1.7778e+01, 7.6984e-02, 1.9191e+01, 6.1190e-02, 1.8911e+01,\n",
      "        9.6047e-02, 1.9384e+01, 1.2120e-01, 1.8146e+01, 8.3322e-02, 1.9310e+01,\n",
      "        6.3240e-02, 1.7321e+01], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(OtherValSpectra[0])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 0.01s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
