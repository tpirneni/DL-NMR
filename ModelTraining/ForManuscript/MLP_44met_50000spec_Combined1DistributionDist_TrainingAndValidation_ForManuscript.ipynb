{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 5000\n",
    "\n",
    "# Identification part of the filenames\n",
    "base_name = '50000spec_Combined1Distribution'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"MLP_44met_{base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load(f'Dataset44_{base_name}_ForManuscript_Spec.npy')\n",
    "conc1 = np.load(f'Dataset44_{base_name}_ForManuscript_Conc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 169, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 169, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "size_hidden1 = 200\n",
    "size_hidden2 = 44\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(46000, size_hidden1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "    def forward(self, input):\n",
    "        return (self.lin2(self.relu1(self.lin1(input))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = RelativeAbsoluteError()\n",
    "    optimizer = optim.AdamW(model.parameters(), weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in best validation loss constitutes early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "            \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/5000], Train Loss: 21417.1286, Test Loss: 4213.5225\n",
      "Epoch [2/5000], Train Loss: 12569.2860, Test Loss: 2256.2049\n",
      "Epoch [3/5000], Train Loss: 6662.2518, Test Loss: 1268.0228\n",
      "Epoch [4/5000], Train Loss: 4174.9626, Test Loss: 889.5434\n",
      "Epoch [5/5000], Train Loss: 2938.7411, Test Loss: 615.0765\n",
      "Epoch [6/5000], Train Loss: 2198.5147, Test Loss: 481.4365\n",
      "Epoch [7/5000], Train Loss: 1690.1104, Test Loss: 373.3532\n",
      "Epoch [8/5000], Train Loss: 1319.5855, Test Loss: 305.7330\n",
      "Epoch [9/5000], Train Loss: 1174.8816, Test Loss: 293.6016\n",
      "Epoch [10/5000], Train Loss: 1125.8596, Test Loss: 275.3477\n",
      "Epoch [11/5000], Train Loss: 1079.7478, Test Loss: 263.3047\n",
      "Epoch [12/5000], Train Loss: 1006.1618, Test Loss: 235.1190\n",
      "Epoch [13/5000], Train Loss: 997.5080, Test Loss: 228.6528\n",
      "Epoch [14/5000], Train Loss: 987.7367, Test Loss: 244.2012\n",
      "Epoch [15/5000], Train Loss: 969.7709, Test Loss: 270.3115\n",
      "Epoch [16/5000], Train Loss: 970.5833, Test Loss: 249.1929\n",
      "Epoch [17/5000], Train Loss: 890.4734, Test Loss: 218.0835\n",
      "Epoch [18/5000], Train Loss: 900.7646, Test Loss: 228.4251\n",
      "Epoch [19/5000], Train Loss: 885.2686, Test Loss: 240.1642\n",
      "Epoch [20/5000], Train Loss: 906.2631, Test Loss: 227.3387\n",
      "Epoch [21/5000], Train Loss: 894.4801, Test Loss: 218.8403\n",
      "Epoch [22/5000], Train Loss: 864.2529, Test Loss: 212.4691\n",
      "Epoch [23/5000], Train Loss: 860.4065, Test Loss: 221.9729\n",
      "Epoch [24/5000], Train Loss: 907.9938, Test Loss: 220.0584\n",
      "Epoch [25/5000], Train Loss: 863.4308, Test Loss: 202.5066\n",
      "Epoch [26/5000], Train Loss: 862.5216, Test Loss: 216.0430\n",
      "Epoch [27/5000], Train Loss: 857.7532, Test Loss: 217.4933\n",
      "Epoch [28/5000], Train Loss: 849.4241, Test Loss: 241.2148\n",
      "Epoch [29/5000], Train Loss: 886.5164, Test Loss: 219.0520\n",
      "Epoch [30/5000], Train Loss: 829.0152, Test Loss: 227.6998\n",
      "Epoch [31/5000], Train Loss: 852.5238, Test Loss: 210.8347\n",
      "Epoch [32/5000], Train Loss: 841.6616, Test Loss: 213.5454\n",
      "Epoch [33/5000], Train Loss: 828.5650, Test Loss: 207.9018\n",
      "Epoch [34/5000], Train Loss: 799.1895, Test Loss: 196.5507\n",
      "Epoch [35/5000], Train Loss: 823.5101, Test Loss: 204.6296\n",
      "Epoch [36/5000], Train Loss: 831.4542, Test Loss: 200.1436\n",
      "Epoch [37/5000], Train Loss: 798.9973, Test Loss: 228.4934\n",
      "Epoch [38/5000], Train Loss: 828.4574, Test Loss: 218.6158\n",
      "Epoch [39/5000], Train Loss: 790.5927, Test Loss: 211.6332\n",
      "Epoch [40/5000], Train Loss: 814.5821, Test Loss: 216.8259\n",
      "Epoch [41/5000], Train Loss: 821.0047, Test Loss: 218.7759\n",
      "Epoch [42/5000], Train Loss: 823.5188, Test Loss: 195.0412\n",
      "Epoch [43/5000], Train Loss: 816.5994, Test Loss: 231.6445\n",
      "Epoch [44/5000], Train Loss: 815.8739, Test Loss: 200.8114\n",
      "Epoch [45/5000], Train Loss: 823.7189, Test Loss: 201.3107\n",
      "Epoch [46/5000], Train Loss: 792.6455, Test Loss: 188.4887\n",
      "Epoch [47/5000], Train Loss: 832.6215, Test Loss: 216.7300\n",
      "Epoch [48/5000], Train Loss: 825.4206, Test Loss: 216.8207\n",
      "Epoch [49/5000], Train Loss: 770.3456, Test Loss: 197.0911\n",
      "Epoch [50/5000], Train Loss: 783.9488, Test Loss: 196.4611\n",
      "Epoch [51/5000], Train Loss: 784.2886, Test Loss: 186.9724\n",
      "Epoch [52/5000], Train Loss: 827.4263, Test Loss: 202.2160\n",
      "Epoch [53/5000], Train Loss: 810.2576, Test Loss: 216.1206\n",
      "Epoch [54/5000], Train Loss: 808.2692, Test Loss: 185.5880\n",
      "Epoch [55/5000], Train Loss: 774.2922, Test Loss: 211.9300\n",
      "Epoch [56/5000], Train Loss: 778.0530, Test Loss: 181.3262\n",
      "Epoch [57/5000], Train Loss: 760.0748, Test Loss: 221.2439\n",
      "Epoch [58/5000], Train Loss: 787.6168, Test Loss: 221.7800\n",
      "Epoch [59/5000], Train Loss: 787.4822, Test Loss: 198.6283\n",
      "Epoch [60/5000], Train Loss: 790.2939, Test Loss: 208.3913\n",
      "Epoch [61/5000], Train Loss: 769.0811, Test Loss: 181.2161\n",
      "Epoch [62/5000], Train Loss: 776.5178, Test Loss: 196.7063\n",
      "Epoch [63/5000], Train Loss: 778.8057, Test Loss: 209.3726\n",
      "Epoch [64/5000], Train Loss: 757.8373, Test Loss: 193.7172\n",
      "Epoch [65/5000], Train Loss: 767.5795, Test Loss: 216.9093\n",
      "Epoch [66/5000], Train Loss: 792.0511, Test Loss: 183.5523\n",
      "Epoch [67/5000], Train Loss: 745.7830, Test Loss: 194.2149\n",
      "Epoch [68/5000], Train Loss: 775.1203, Test Loss: 183.1444\n",
      "Epoch [69/5000], Train Loss: 785.3199, Test Loss: 196.0034\n",
      "Epoch [70/5000], Train Loss: 797.6784, Test Loss: 194.2258\n",
      "Epoch [71/5000], Train Loss: 754.7680, Test Loss: 210.9322\n",
      "Epoch [72/5000], Train Loss: 776.5992, Test Loss: 193.5099\n",
      "Epoch [73/5000], Train Loss: 800.1620, Test Loss: 208.8836\n",
      "Epoch [74/5000], Train Loss: 785.3030, Test Loss: 217.0292\n",
      "Epoch [75/5000], Train Loss: 763.1350, Test Loss: 192.6180\n",
      "Epoch [76/5000], Train Loss: 767.2009, Test Loss: 199.1547\n",
      "Epoch [77/5000], Train Loss: 752.7890, Test Loss: 186.5525\n",
      "Epoch [78/5000], Train Loss: 749.5388, Test Loss: 202.3890\n",
      "Epoch [79/5000], Train Loss: 754.6364, Test Loss: 225.3145\n",
      "Epoch [80/5000], Train Loss: 746.5999, Test Loss: 181.8860\n",
      "Epoch [81/5000], Train Loss: 812.3457, Test Loss: 220.4147\n",
      "Epoch [82/5000], Train Loss: 775.4026, Test Loss: 205.3667\n",
      "Epoch [83/5000], Train Loss: 777.4691, Test Loss: 182.1672\n",
      "Epoch [84/5000], Train Loss: 745.5115, Test Loss: 190.2925\n",
      "Epoch [85/5000], Train Loss: 748.3126, Test Loss: 205.3232\n",
      "Epoch [86/5000], Train Loss: 750.6320, Test Loss: 191.2630\n",
      "Epoch [87/5000], Train Loss: 790.9923, Test Loss: 181.9092\n",
      "Epoch [88/5000], Train Loss: 769.9131, Test Loss: 199.0706\n",
      "Epoch [89/5000], Train Loss: 804.4357, Test Loss: 214.7677\n",
      "Epoch [90/5000], Train Loss: 714.6883, Test Loss: 203.3106\n",
      "Epoch [91/5000], Train Loss: 750.6956, Test Loss: 220.9284\n",
      "Epoch [92/5000], Train Loss: 758.6070, Test Loss: 195.9882\n",
      "Epoch [93/5000], Train Loss: 706.4561, Test Loss: 184.4108\n",
      "Epoch [94/5000], Train Loss: 721.5991, Test Loss: 206.9092\n",
      "Epoch [95/5000], Train Loss: 731.4899, Test Loss: 203.9596\n",
      "Epoch [96/5000], Train Loss: 743.7105, Test Loss: 195.7314\n",
      "Epoch [97/5000], Train Loss: 763.1196, Test Loss: 185.4381\n",
      "Epoch [98/5000], Train Loss: 737.0970, Test Loss: 174.2914\n",
      "Epoch [99/5000], Train Loss: 768.3739, Test Loss: 186.9964\n",
      "Epoch [100/5000], Train Loss: 755.3865, Test Loss: 184.1471\n",
      "Epoch [101/5000], Train Loss: 714.2957, Test Loss: 187.7590\n",
      "Epoch [102/5000], Train Loss: 748.2813, Test Loss: 187.8438\n",
      "Epoch [103/5000], Train Loss: 733.8545, Test Loss: 175.8355\n",
      "Epoch [104/5000], Train Loss: 729.8758, Test Loss: 230.7003\n",
      "Epoch [105/5000], Train Loss: 737.7301, Test Loss: 199.7681\n",
      "Epoch [106/5000], Train Loss: 697.7184, Test Loss: 207.7301\n",
      "Epoch [107/5000], Train Loss: 757.2711, Test Loss: 203.8497\n",
      "Epoch [108/5000], Train Loss: 761.2560, Test Loss: 182.8621\n",
      "Epoch [109/5000], Train Loss: 729.0494, Test Loss: 226.0071\n",
      "Epoch [110/5000], Train Loss: 732.0969, Test Loss: 194.0919\n",
      "Epoch [111/5000], Train Loss: 709.3650, Test Loss: 192.1128\n",
      "Epoch [112/5000], Train Loss: 727.3879, Test Loss: 219.4264\n",
      "Epoch [113/5000], Train Loss: 748.2326, Test Loss: 198.9610\n",
      "Epoch [114/5000], Train Loss: 739.7251, Test Loss: 185.1233\n",
      "Epoch [115/5000], Train Loss: 707.2821, Test Loss: 195.6510\n",
      "Epoch [116/5000], Train Loss: 725.2693, Test Loss: 201.3996\n",
      "Epoch [117/5000], Train Loss: 738.1744, Test Loss: 233.1396\n",
      "Epoch [118/5000], Train Loss: 759.6242, Test Loss: 196.0952\n",
      "Epoch [119/5000], Train Loss: 700.2216, Test Loss: 209.6182\n",
      "Epoch [120/5000], Train Loss: 764.3022, Test Loss: 208.2749\n",
      "Epoch [121/5000], Train Loss: 720.1681, Test Loss: 179.8005\n",
      "Epoch [122/5000], Train Loss: 704.5598, Test Loss: 194.0300\n",
      "Epoch [123/5000], Train Loss: 703.8917, Test Loss: 189.4993\n",
      "Epoch [124/5000], Train Loss: 730.2251, Test Loss: 206.7321\n",
      "Epoch [125/5000], Train Loss: 741.6503, Test Loss: 188.9640\n",
      "Epoch [126/5000], Train Loss: 689.1925, Test Loss: 176.4941\n",
      "Epoch [127/5000], Train Loss: 745.2153, Test Loss: 216.8026\n",
      "Epoch [128/5000], Train Loss: 741.9555, Test Loss: 203.5175\n",
      "Epoch [129/5000], Train Loss: 725.3487, Test Loss: 188.0914\n",
      "Epoch [130/5000], Train Loss: 717.7639, Test Loss: 189.1087\n",
      "Epoch [131/5000], Train Loss: 733.7024, Test Loss: 197.5526\n",
      "Epoch [132/5000], Train Loss: 702.0980, Test Loss: 173.4456\n",
      "Epoch [133/5000], Train Loss: 702.8427, Test Loss: 179.3656\n",
      "Epoch [134/5000], Train Loss: 731.1155, Test Loss: 206.6887\n",
      "Epoch [135/5000], Train Loss: 710.2857, Test Loss: 210.4942\n",
      "Epoch [136/5000], Train Loss: 721.9493, Test Loss: 183.0605\n",
      "Epoch [137/5000], Train Loss: 693.1464, Test Loss: 184.3910\n",
      "Epoch [138/5000], Train Loss: 704.9194, Test Loss: 200.7151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [139/5000], Train Loss: 747.3224, Test Loss: 207.9188\n",
      "Epoch [140/5000], Train Loss: 731.0467, Test Loss: 210.8137\n",
      "Epoch [141/5000], Train Loss: 708.7581, Test Loss: 186.3177\n",
      "Epoch [142/5000], Train Loss: 724.7216, Test Loss: 194.0771\n",
      "Epoch [143/5000], Train Loss: 706.9124, Test Loss: 195.7964\n",
      "Epoch [144/5000], Train Loss: 736.3497, Test Loss: 174.7528\n",
      "Epoch [145/5000], Train Loss: 718.8152, Test Loss: 198.8471\n",
      "Epoch [146/5000], Train Loss: 729.4074, Test Loss: 179.4881\n",
      "Epoch [147/5000], Train Loss: 694.5175, Test Loss: 205.8876\n",
      "Epoch [148/5000], Train Loss: 733.7250, Test Loss: 185.8937\n",
      "Epoch [149/5000], Train Loss: 715.1273, Test Loss: 172.2386\n",
      "Epoch [150/5000], Train Loss: 736.1346, Test Loss: 206.3943\n",
      "Epoch [151/5000], Train Loss: 700.0877, Test Loss: 194.5187\n",
      "Epoch [152/5000], Train Loss: 729.1977, Test Loss: 168.0523\n",
      "Epoch [153/5000], Train Loss: 670.2806, Test Loss: 174.1594\n",
      "Epoch [154/5000], Train Loss: 696.3145, Test Loss: 171.1835\n",
      "Epoch [155/5000], Train Loss: 695.8834, Test Loss: 180.0086\n",
      "Epoch [156/5000], Train Loss: 676.4622, Test Loss: 173.4209\n",
      "Epoch [157/5000], Train Loss: 718.5590, Test Loss: 199.9784\n",
      "Epoch [158/5000], Train Loss: 715.7672, Test Loss: 198.8873\n",
      "Epoch [159/5000], Train Loss: 694.9896, Test Loss: 185.0801\n",
      "Epoch [160/5000], Train Loss: 703.6089, Test Loss: 184.3424\n",
      "Epoch [161/5000], Train Loss: 699.5483, Test Loss: 176.8180\n",
      "Epoch [162/5000], Train Loss: 698.2928, Test Loss: 188.2902\n",
      "Epoch [163/5000], Train Loss: 698.8099, Test Loss: 185.0302\n",
      "Epoch [164/5000], Train Loss: 744.6455, Test Loss: 203.5538\n",
      "Epoch [165/5000], Train Loss: 731.7616, Test Loss: 175.6188\n",
      "Epoch [166/5000], Train Loss: 676.0041, Test Loss: 195.6586\n",
      "Epoch [167/5000], Train Loss: 700.5722, Test Loss: 189.8692\n",
      "Epoch [168/5000], Train Loss: 703.5832, Test Loss: 196.4947\n",
      "Epoch [169/5000], Train Loss: 703.9580, Test Loss: 164.9445\n",
      "Epoch [170/5000], Train Loss: 675.2995, Test Loss: 181.6403\n",
      "Epoch [171/5000], Train Loss: 677.7244, Test Loss: 168.7709\n",
      "Epoch [172/5000], Train Loss: 702.8409, Test Loss: 181.7008\n",
      "Epoch [173/5000], Train Loss: 733.5374, Test Loss: 201.2415\n",
      "Epoch [174/5000], Train Loss: 676.1121, Test Loss: 189.6950\n",
      "Epoch [175/5000], Train Loss: 671.1684, Test Loss: 176.5683\n",
      "Epoch [176/5000], Train Loss: 685.3666, Test Loss: 186.1063\n",
      "Epoch [177/5000], Train Loss: 682.3248, Test Loss: 187.1856\n",
      "Epoch [178/5000], Train Loss: 678.6493, Test Loss: 188.8494\n",
      "Epoch [179/5000], Train Loss: 696.0258, Test Loss: 194.4010\n",
      "Epoch [180/5000], Train Loss: 668.3524, Test Loss: 187.7523\n",
      "Epoch [181/5000], Train Loss: 686.9620, Test Loss: 185.2487\n",
      "Epoch [182/5000], Train Loss: 694.6761, Test Loss: 187.5703\n",
      "Epoch [183/5000], Train Loss: 689.6399, Test Loss: 173.3033\n",
      "Epoch [184/5000], Train Loss: 679.4085, Test Loss: 163.9782\n",
      "Epoch [185/5000], Train Loss: 689.2416, Test Loss: 175.7761\n",
      "Epoch [186/5000], Train Loss: 686.9135, Test Loss: 180.4121\n",
      "Epoch [187/5000], Train Loss: 656.4075, Test Loss: 173.6329\n",
      "Epoch [188/5000], Train Loss: 684.6624, Test Loss: 163.8958\n",
      "Epoch [189/5000], Train Loss: 675.8749, Test Loss: 173.3241\n",
      "Epoch [190/5000], Train Loss: 690.6057, Test Loss: 201.4379\n",
      "Epoch [191/5000], Train Loss: 684.6344, Test Loss: 184.5967\n",
      "Epoch [192/5000], Train Loss: 690.2189, Test Loss: 191.7065\n",
      "Epoch [193/5000], Train Loss: 681.7107, Test Loss: 198.4857\n",
      "Epoch [194/5000], Train Loss: 694.6664, Test Loss: 187.6364\n",
      "Epoch [195/5000], Train Loss: 684.8544, Test Loss: 193.9195\n",
      "Epoch [196/5000], Train Loss: 660.0285, Test Loss: 186.9951\n",
      "Epoch [197/5000], Train Loss: 671.7412, Test Loss: 183.3318\n",
      "Epoch [198/5000], Train Loss: 689.5200, Test Loss: 175.9607\n",
      "Epoch [199/5000], Train Loss: 662.4837, Test Loss: 185.9342\n",
      "Epoch [200/5000], Train Loss: 699.2021, Test Loss: 180.3697\n",
      "Epoch [201/5000], Train Loss: 671.7750, Test Loss: 173.9157\n",
      "Epoch [202/5000], Train Loss: 671.1849, Test Loss: 183.2393\n",
      "Epoch [203/5000], Train Loss: 665.0854, Test Loss: 185.8000\n",
      "Epoch [204/5000], Train Loss: 666.1092, Test Loss: 182.5983\n",
      "Epoch [205/5000], Train Loss: 660.6801, Test Loss: 177.4334\n",
      "Epoch [206/5000], Train Loss: 681.5853, Test Loss: 188.6077\n",
      "Epoch [207/5000], Train Loss: 689.9876, Test Loss: 180.5555\n",
      "Epoch [208/5000], Train Loss: 670.7363, Test Loss: 202.7816\n",
      "Epoch [209/5000], Train Loss: 678.9893, Test Loss: 175.5052\n",
      "Epoch [210/5000], Train Loss: 666.1112, Test Loss: 180.4242\n",
      "Epoch [211/5000], Train Loss: 704.5200, Test Loss: 199.0165\n",
      "Epoch [212/5000], Train Loss: 694.1110, Test Loss: 175.5095\n",
      "Epoch [213/5000], Train Loss: 702.6858, Test Loss: 188.0184\n",
      "Epoch [214/5000], Train Loss: 664.6462, Test Loss: 162.6056\n",
      "Epoch [215/5000], Train Loss: 676.5147, Test Loss: 186.5553\n",
      "Epoch [216/5000], Train Loss: 674.8034, Test Loss: 190.4512\n",
      "Epoch [217/5000], Train Loss: 682.2342, Test Loss: 190.4241\n",
      "Epoch [218/5000], Train Loss: 678.6858, Test Loss: 182.9699\n",
      "Epoch [219/5000], Train Loss: 672.0510, Test Loss: 193.2753\n",
      "Epoch [220/5000], Train Loss: 693.8505, Test Loss: 174.0981\n",
      "Epoch [221/5000], Train Loss: 670.3410, Test Loss: 191.4925\n",
      "Epoch [222/5000], Train Loss: 665.5934, Test Loss: 186.5367\n",
      "Epoch [223/5000], Train Loss: 675.6559, Test Loss: 207.1928\n",
      "Epoch [224/5000], Train Loss: 674.0154, Test Loss: 186.9621\n",
      "Epoch [225/5000], Train Loss: 662.3714, Test Loss: 182.9103\n",
      "Epoch [226/5000], Train Loss: 666.5734, Test Loss: 175.5273\n",
      "Epoch [227/5000], Train Loss: 690.6021, Test Loss: 183.8810\n",
      "Epoch [228/5000], Train Loss: 659.5749, Test Loss: 187.8474\n",
      "Epoch [229/5000], Train Loss: 674.5737, Test Loss: 197.9026\n",
      "Epoch [230/5000], Train Loss: 645.5171, Test Loss: 177.0973\n",
      "Epoch [231/5000], Train Loss: 657.7987, Test Loss: 172.6175\n",
      "Epoch [232/5000], Train Loss: 670.9865, Test Loss: 167.6873\n",
      "Epoch [233/5000], Train Loss: 642.6299, Test Loss: 181.8445\n",
      "Epoch [234/5000], Train Loss: 659.0490, Test Loss: 178.0032\n",
      "Epoch [235/5000], Train Loss: 666.4763, Test Loss: 180.5603\n",
      "Epoch [236/5000], Train Loss: 657.8614, Test Loss: 178.5896\n",
      "Epoch [237/5000], Train Loss: 659.7899, Test Loss: 180.9952\n",
      "Epoch [238/5000], Train Loss: 695.5452, Test Loss: 199.0953\n",
      "Epoch [239/5000], Train Loss: 689.7019, Test Loss: 162.6417\n",
      "Epoch [240/5000], Train Loss: 666.7164, Test Loss: 190.4030\n",
      "Epoch [241/5000], Train Loss: 666.2854, Test Loss: 170.6147\n",
      "Epoch [242/5000], Train Loss: 647.8580, Test Loss: 174.0421\n",
      "Epoch [243/5000], Train Loss: 660.2175, Test Loss: 183.5444\n",
      "Epoch [244/5000], Train Loss: 676.4439, Test Loss: 189.1070\n",
      "Epoch [245/5000], Train Loss: 647.0456, Test Loss: 174.1265\n",
      "Epoch [246/5000], Train Loss: 667.1415, Test Loss: 178.5456\n",
      "Epoch [247/5000], Train Loss: 645.4236, Test Loss: 178.7700\n",
      "Epoch [248/5000], Train Loss: 654.0915, Test Loss: 165.6950\n",
      "Epoch [249/5000], Train Loss: 656.4715, Test Loss: 167.6876\n",
      "Epoch [250/5000], Train Loss: 634.9316, Test Loss: 163.7833\n",
      "Epoch [251/5000], Train Loss: 649.6146, Test Loss: 164.2750\n",
      "Epoch [252/5000], Train Loss: 627.0024, Test Loss: 194.4598\n",
      "Epoch [253/5000], Train Loss: 650.5099, Test Loss: 170.1036\n",
      "Epoch [254/5000], Train Loss: 634.4665, Test Loss: 176.2767\n",
      "Epoch [255/5000], Train Loss: 642.6055, Test Loss: 160.5329\n",
      "Epoch [256/5000], Train Loss: 664.4838, Test Loss: 195.7918\n",
      "Epoch [257/5000], Train Loss: 696.5393, Test Loss: 186.8057\n",
      "Epoch [258/5000], Train Loss: 675.9830, Test Loss: 179.0300\n",
      "Epoch [259/5000], Train Loss: 656.2215, Test Loss: 181.6188\n",
      "Epoch [260/5000], Train Loss: 628.2077, Test Loss: 183.6428\n",
      "Epoch [261/5000], Train Loss: 666.3656, Test Loss: 179.7874\n",
      "Epoch [262/5000], Train Loss: 617.9069, Test Loss: 168.6595\n",
      "Epoch [263/5000], Train Loss: 632.4100, Test Loss: 169.1956\n",
      "Epoch [264/5000], Train Loss: 651.2920, Test Loss: 183.6914\n",
      "Epoch [265/5000], Train Loss: 652.4237, Test Loss: 158.6412\n",
      "Epoch [266/5000], Train Loss: 638.9319, Test Loss: 181.2328\n",
      "Epoch [267/5000], Train Loss: 666.6395, Test Loss: 172.7909\n",
      "Epoch [268/5000], Train Loss: 626.4943, Test Loss: 182.2925\n",
      "Epoch [269/5000], Train Loss: 655.8441, Test Loss: 184.6455\n",
      "Epoch [270/5000], Train Loss: 657.5888, Test Loss: 186.5751\n",
      "Epoch [271/5000], Train Loss: 642.1412, Test Loss: 180.1501\n",
      "Epoch [272/5000], Train Loss: 655.8336, Test Loss: 182.0209\n",
      "Epoch [273/5000], Train Loss: 670.9074, Test Loss: 179.3165\n",
      "Epoch [274/5000], Train Loss: 628.0411, Test Loss: 175.2718\n",
      "Epoch [275/5000], Train Loss: 618.4783, Test Loss: 174.5712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [276/5000], Train Loss: 644.1439, Test Loss: 173.3174\n",
      "Epoch [277/5000], Train Loss: 638.5762, Test Loss: 175.4395\n",
      "Epoch [278/5000], Train Loss: 660.4875, Test Loss: 171.3578\n",
      "Epoch [279/5000], Train Loss: 663.7487, Test Loss: 151.9825\n",
      "Epoch [280/5000], Train Loss: 619.1740, Test Loss: 170.1770\n",
      "Epoch [281/5000], Train Loss: 661.3140, Test Loss: 162.1108\n",
      "Epoch [282/5000], Train Loss: 640.8527, Test Loss: 169.3395\n",
      "Epoch [283/5000], Train Loss: 622.2448, Test Loss: 170.0266\n",
      "Epoch [284/5000], Train Loss: 644.8830, Test Loss: 182.2741\n",
      "Epoch [285/5000], Train Loss: 659.4679, Test Loss: 176.5743\n",
      "Epoch [286/5000], Train Loss: 645.1112, Test Loss: 165.5422\n",
      "Epoch [287/5000], Train Loss: 612.6441, Test Loss: 168.3674\n",
      "Epoch [288/5000], Train Loss: 632.3728, Test Loss: 180.0065\n",
      "Epoch [289/5000], Train Loss: 638.2408, Test Loss: 181.7678\n",
      "Epoch [290/5000], Train Loss: 629.1970, Test Loss: 183.7174\n",
      "Epoch [291/5000], Train Loss: 628.8368, Test Loss: 171.2128\n",
      "Epoch [292/5000], Train Loss: 655.6710, Test Loss: 173.4753\n",
      "Epoch [293/5000], Train Loss: 643.8705, Test Loss: 160.4339\n",
      "Epoch [294/5000], Train Loss: 634.6160, Test Loss: 174.7269\n",
      "Epoch [295/5000], Train Loss: 640.0191, Test Loss: 184.3711\n",
      "Epoch [296/5000], Train Loss: 679.6735, Test Loss: 179.5655\n",
      "Epoch [297/5000], Train Loss: 640.0433, Test Loss: 158.0120\n",
      "Epoch [298/5000], Train Loss: 650.0363, Test Loss: 191.3157\n",
      "Epoch [299/5000], Train Loss: 652.3798, Test Loss: 178.8598\n",
      "Epoch [300/5000], Train Loss: 660.6357, Test Loss: 164.0315\n",
      "Epoch [301/5000], Train Loss: 628.4726, Test Loss: 183.2885\n",
      "Epoch [302/5000], Train Loss: 640.3162, Test Loss: 156.7808\n",
      "Epoch [303/5000], Train Loss: 625.8334, Test Loss: 162.1645\n",
      "Epoch [304/5000], Train Loss: 636.4054, Test Loss: 187.8987\n",
      "Epoch [305/5000], Train Loss: 632.0380, Test Loss: 173.4722\n",
      "Epoch [306/5000], Train Loss: 628.9368, Test Loss: 178.2824\n",
      "Epoch [307/5000], Train Loss: 645.8101, Test Loss: 194.9921\n",
      "Epoch [308/5000], Train Loss: 633.5349, Test Loss: 167.8210\n",
      "Epoch [309/5000], Train Loss: 624.1893, Test Loss: 171.0573\n",
      "Epoch [310/5000], Train Loss: 632.5900, Test Loss: 182.1274\n",
      "Epoch [311/5000], Train Loss: 626.9898, Test Loss: 166.6817\n",
      "Epoch [312/5000], Train Loss: 631.4797, Test Loss: 172.5264\n",
      "Epoch [313/5000], Train Loss: 637.9491, Test Loss: 163.5677\n",
      "Epoch [314/5000], Train Loss: 629.5998, Test Loss: 163.6196\n",
      "Epoch [315/5000], Train Loss: 599.5366, Test Loss: 170.6714\n",
      "Epoch [316/5000], Train Loss: 612.6541, Test Loss: 176.6179\n",
      "Epoch [317/5000], Train Loss: 622.5763, Test Loss: 175.4930\n",
      "Epoch [318/5000], Train Loss: 614.3685, Test Loss: 175.2361\n",
      "Epoch [319/5000], Train Loss: 631.5021, Test Loss: 158.7075\n",
      "Epoch [320/5000], Train Loss: 653.1501, Test Loss: 180.8724\n",
      "Epoch [321/5000], Train Loss: 668.3541, Test Loss: 174.8993\n",
      "Epoch [322/5000], Train Loss: 654.5663, Test Loss: 181.3759\n",
      "Epoch [323/5000], Train Loss: 633.2333, Test Loss: 176.9039\n",
      "Epoch [324/5000], Train Loss: 625.0055, Test Loss: 191.9750\n",
      "Epoch [325/5000], Train Loss: 623.1894, Test Loss: 149.4134\n",
      "Epoch [326/5000], Train Loss: 609.7401, Test Loss: 169.6455\n",
      "Epoch [327/5000], Train Loss: 613.3932, Test Loss: 165.8787\n",
      "Epoch [328/5000], Train Loss: 626.2828, Test Loss: 153.8520\n",
      "Epoch [329/5000], Train Loss: 624.9715, Test Loss: 186.0776\n",
      "Epoch [330/5000], Train Loss: 645.3860, Test Loss: 153.1129\n",
      "Epoch [331/5000], Train Loss: 617.5610, Test Loss: 174.5037\n",
      "Epoch [332/5000], Train Loss: 631.5759, Test Loss: 175.4983\n",
      "Epoch [333/5000], Train Loss: 641.1750, Test Loss: 177.8460\n",
      "Epoch [334/5000], Train Loss: 607.1856, Test Loss: 159.6797\n",
      "Epoch [335/5000], Train Loss: 608.1111, Test Loss: 163.4655\n",
      "Epoch [336/5000], Train Loss: 624.9105, Test Loss: 194.9912\n",
      "Epoch [337/5000], Train Loss: 657.4805, Test Loss: 185.4626\n",
      "Epoch [338/5000], Train Loss: 639.7498, Test Loss: 169.5701\n",
      "Epoch [339/5000], Train Loss: 610.1851, Test Loss: 172.9530\n",
      "Epoch [340/5000], Train Loss: 622.6434, Test Loss: 168.8936\n",
      "Epoch [341/5000], Train Loss: 615.7955, Test Loss: 151.5056\n",
      "Epoch [342/5000], Train Loss: 623.9755, Test Loss: 170.5052\n",
      "Epoch [343/5000], Train Loss: 623.9877, Test Loss: 173.2678\n",
      "Epoch [344/5000], Train Loss: 641.2009, Test Loss: 167.9299\n",
      "Epoch [345/5000], Train Loss: 632.3375, Test Loss: 169.6073\n",
      "Epoch [346/5000], Train Loss: 628.3842, Test Loss: 191.3457\n",
      "Epoch [347/5000], Train Loss: 631.8338, Test Loss: 180.6994\n",
      "Epoch [348/5000], Train Loss: 639.3049, Test Loss: 169.2562\n",
      "Epoch [349/5000], Train Loss: 616.3421, Test Loss: 172.8299\n",
      "Epoch [350/5000], Train Loss: 625.8685, Test Loss: 174.0340\n",
      "Epoch [351/5000], Train Loss: 617.0394, Test Loss: 168.0484\n",
      "Epoch [352/5000], Train Loss: 612.9843, Test Loss: 177.5205\n",
      "Epoch [353/5000], Train Loss: 644.0771, Test Loss: 176.4399\n",
      "Epoch [354/5000], Train Loss: 617.7453, Test Loss: 165.3233\n",
      "Epoch [355/5000], Train Loss: 622.8682, Test Loss: 159.8977\n",
      "Epoch [356/5000], Train Loss: 618.4542, Test Loss: 193.0769\n",
      "Epoch [357/5000], Train Loss: 643.3377, Test Loss: 173.6756\n",
      "Epoch [358/5000], Train Loss: 623.2397, Test Loss: 169.7391\n",
      "Epoch [359/5000], Train Loss: 625.1910, Test Loss: 149.1240\n",
      "Epoch [360/5000], Train Loss: 617.0033, Test Loss: 166.6847\n",
      "Epoch [361/5000], Train Loss: 636.1424, Test Loss: 158.6686\n",
      "Epoch [362/5000], Train Loss: 620.0031, Test Loss: 166.2740\n",
      "Epoch [363/5000], Train Loss: 610.7745, Test Loss: 146.2756\n",
      "Epoch [364/5000], Train Loss: 603.0277, Test Loss: 165.2839\n",
      "Epoch [365/5000], Train Loss: 613.2021, Test Loss: 183.1438\n",
      "Epoch [366/5000], Train Loss: 631.4886, Test Loss: 162.5499\n",
      "Epoch [367/5000], Train Loss: 635.4246, Test Loss: 174.9953\n",
      "Epoch [368/5000], Train Loss: 630.7992, Test Loss: 152.0462\n",
      "Epoch [369/5000], Train Loss: 596.6951, Test Loss: 169.5568\n",
      "Epoch [370/5000], Train Loss: 619.4242, Test Loss: 156.2789\n",
      "Epoch [371/5000], Train Loss: 628.7712, Test Loss: 173.3515\n",
      "Epoch [372/5000], Train Loss: 630.5785, Test Loss: 173.0707\n",
      "Epoch [373/5000], Train Loss: 625.8054, Test Loss: 161.6527\n",
      "Epoch [374/5000], Train Loss: 638.9820, Test Loss: 151.6594\n",
      "Epoch [375/5000], Train Loss: 601.8339, Test Loss: 154.6282\n",
      "Epoch [376/5000], Train Loss: 616.8114, Test Loss: 163.2160\n",
      "Epoch [377/5000], Train Loss: 602.0995, Test Loss: 152.6393\n",
      "Epoch [378/5000], Train Loss: 605.8043, Test Loss: 168.9750\n",
      "Epoch [379/5000], Train Loss: 602.6481, Test Loss: 158.9730\n",
      "Epoch [380/5000], Train Loss: 649.1011, Test Loss: 157.5787\n",
      "Epoch [381/5000], Train Loss: 591.8919, Test Loss: 149.9562\n",
      "Epoch [382/5000], Train Loss: 600.9905, Test Loss: 174.1833\n",
      "Epoch [383/5000], Train Loss: 621.6080, Test Loss: 160.5010\n",
      "Epoch [384/5000], Train Loss: 614.2483, Test Loss: 162.6122\n",
      "Epoch [385/5000], Train Loss: 590.8500, Test Loss: 163.6607\n",
      "Epoch [386/5000], Train Loss: 611.1782, Test Loss: 177.4257\n",
      "Epoch [387/5000], Train Loss: 610.5035, Test Loss: 160.6474\n",
      "Epoch [388/5000], Train Loss: 614.8400, Test Loss: 186.6375\n",
      "Epoch [389/5000], Train Loss: 615.8759, Test Loss: 177.2269\n",
      "Epoch [390/5000], Train Loss: 615.1006, Test Loss: 186.7701\n",
      "Epoch [391/5000], Train Loss: 579.2152, Test Loss: 168.9812\n",
      "Epoch [392/5000], Train Loss: 602.5653, Test Loss: 168.2085\n",
      "Epoch [393/5000], Train Loss: 602.4340, Test Loss: 160.4930\n",
      "Epoch [394/5000], Train Loss: 598.6622, Test Loss: 187.1129\n",
      "Epoch [395/5000], Train Loss: 603.0221, Test Loss: 158.6993\n",
      "Epoch [396/5000], Train Loss: 596.1449, Test Loss: 165.7816\n",
      "Epoch [397/5000], Train Loss: 602.3261, Test Loss: 173.9597\n",
      "Epoch [398/5000], Train Loss: 631.5901, Test Loss: 172.8026\n",
      "Epoch [399/5000], Train Loss: 633.5230, Test Loss: 176.1859\n",
      "Epoch [400/5000], Train Loss: 613.9183, Test Loss: 166.6468\n",
      "Epoch [401/5000], Train Loss: 614.9243, Test Loss: 162.6483\n",
      "Epoch [402/5000], Train Loss: 591.2034, Test Loss: 167.8144\n",
      "Epoch [403/5000], Train Loss: 609.2593, Test Loss: 179.0975\n",
      "Epoch [404/5000], Train Loss: 608.5890, Test Loss: 157.0672\n",
      "Epoch [405/5000], Train Loss: 601.5070, Test Loss: 158.4180\n",
      "Epoch [406/5000], Train Loss: 593.3967, Test Loss: 174.2493\n",
      "Epoch [407/5000], Train Loss: 623.9639, Test Loss: 178.4951\n",
      "Epoch [408/5000], Train Loss: 628.9574, Test Loss: 156.4001\n",
      "Epoch [409/5000], Train Loss: 585.1970, Test Loss: 166.4042\n",
      "Epoch [410/5000], Train Loss: 611.7688, Test Loss: 172.5745\n",
      "Epoch [411/5000], Train Loss: 612.0837, Test Loss: 159.0274\n",
      "Epoch [412/5000], Train Loss: 609.2779, Test Loss: 164.0842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [413/5000], Train Loss: 587.3278, Test Loss: 149.1589\n",
      "Early stopping at epoch 413\n",
      "Execution time: 1238.8447577953339 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146.27558944933116"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (lin1): Linear(in_features=46000, out_features=200, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (lin2): Linear(in_features=200, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  65.71007\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "47.29  - Concentrations: 0.004999999888241291\n",
      "11.02  - Concentrations: 0.02500000037252903\n",
      "3.63  - Concentrations: 0.10000000149011612\n",
      "2.08  - Concentrations: 0.25\n",
      "1.51  - Concentrations: 0.5\n",
      "1.38  - Concentrations: 1.0\n",
      "1.28  - Concentrations: 2.5\n",
      "1.21  - Concentrations: 10.0\n",
      "1.2  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  4.8258157\n",
      "--------------------\n",
      "1.94  - Min Value: 0.6783  - Mean Value: 9.2\n",
      "23.59  - Min Value: 0.0096  - Mean Value: 10.3\n",
      "3.25  - Min Value: 0.147  - Mean Value: 10.5\n",
      "3.0  - Min Value: 0.5572  - Mean Value: 8.5\n",
      "1.76  - Min Value: 1.3567  - Mean Value: 10.6\n",
      "3.31  - Min Value: 0.6332  - Mean Value: 10.9\n",
      "3.18  - Min Value: 0.7017  - Mean Value: 11.0\n",
      "4.53  - Min Value: 0.3674  - Mean Value: 8.9\n",
      "1.97  - Min Value: 0.8387  - Mean Value: 9.8\n",
      "1.72  - Min Value: 1.0913  - Mean Value: 11.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  6.0907736\n",
      "--------------------\n",
      "5.96  - Min Value: 0.0111  - Mean Value: 0.1\n",
      "4.21  - Min Value: 0.0103  - Mean Value: 0.1\n",
      "4.64  - Min Value: 0.0153  - Mean Value: 0.1\n",
      "4.91  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "7.3  - Min Value: 0.0089  - Mean Value: 0.1\n",
      "8.23  - Min Value: 0.0075  - Mean Value: 0.1\n",
      "6.75  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "7.69  - Min Value: 0.0052  - Mean Value: 0.1\n",
      "6.31  - Min Value: 0.008  - Mean Value: 0.1\n",
      "4.9  - Min Value: 0.0134  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  15.042109\n",
      "--------------------\n",
      "23.54  - Min Value: 0.008  - Mean Value: 0.8\n",
      "20.99  - Min Value: 0.009  - Mean Value: 0.9\n",
      "27.08  - Min Value: 0.0138  - Mean Value: 1.5\n",
      "10.22  - Min Value: 0.0107  - Mean Value: 0.7\n",
      "12.58  - Min Value: 0.0191  - Mean Value: 0.7\n",
      "15.13  - Min Value: 0.0186  - Mean Value: 0.8\n",
      "11.56  - Min Value: 0.0175  - Mean Value: 0.8\n",
      "7.99  - Min Value: 0.0238  - Mean Value: 1.3\n",
      "7.64  - Min Value: 0.0168  - Mean Value: 0.7\n",
      "13.69  - Min Value: 0.0171  - Mean Value: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  21.728615\n",
      "--------------------\n",
      "14.89  - Min Value: 0.013  - Mean Value: 0.6\n",
      "24.14  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "13.18  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "15.69  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "17.35  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "35.88  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "38.04  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "16.26  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "16.01  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "25.85  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  169.96179\n",
      "--------------------\n",
      "200.87  - Min Value: 0.013  - Mean Value: 0.6\n",
      "95.14  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "138.59  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "123.0  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "203.13  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "204.53  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "151.33  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "108.22  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "206.61  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "268.19  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  1.8230060927459604\n",
      "--------------------\n",
      "1.66\n",
      "1.65\n",
      "1.76\n",
      "1.79\n",
      "1.87\n",
      "1.99\n",
      "1.75\n",
      "1.78\n",
      "2.01\n",
      "1.97\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  11.789368927856833\n",
      "--------------------\n",
      "1.61\n",
      "1.91\n",
      "2.07\n",
      "4.41\n",
      "13.47\n",
      "13.37\n",
      "13.76\n",
      "18.71\n",
      "21.29\n",
      "27.29\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"Singlet_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"Singlet_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  29.92695073811751\n",
      "--------------------\n",
      "2.83\n",
      "8.67\n",
      "14.65\n",
      "20.74\n",
      "26.78\n",
      "33.01\n",
      "39.04\n",
      "45.07\n",
      "51.11\n",
      "57.38\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal Baseline 1\n",
      "tensor([0.5144, 0.6226, 0.4595, 0.6441, 0.5138, 0.5492, 0.5087, 0.6457, 0.7192,\n",
      "        0.4054, 0.4355, 0.6664, 0.5436, 1.0148, 0.4709, 0.7028, 0.7290, 0.4757,\n",
      "        0.5423, 0.4424, 0.4703, 0.8734, 0.3356, 0.4740, 0.6360, 0.5271, 0.6272,\n",
      "        0.5541, 0.6264, 0.4646, 0.4348, 0.8917, 0.5396, 0.3947, 0.4032, 0.5254,\n",
      "        0.4525, 0.4928, 0.5839, 0.4094, 0.3704, 0.5212, 0.5399, 0.4821],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Sinusoidal Baseline 2\n",
      "tensor([0.3636, 0.5071, 0.0054, 0.5357, 0.3880, 0.4506, 0.3985, 0.2155, 0.4206,\n",
      "        0.5608, 0.4034, 0.5720, 0.7630, 0.2114, 0.7268, 0.1170, 0.6530, 0.5360,\n",
      "        0.4889, 0.4197, 0.3668, 0.3254, 0.5751, 0.4936, 0.4228, 0.4649, 0.2227,\n",
      "        0.0761, 0.6608, 0.3798, 0.3848, 0.2728, 0.2957, 0.4577, 0.5500, 0.4350,\n",
      "        0.6035, 0.6662, 0.5753, 0.3003, 0.4174, 0.7205, 0.3000, 0.0927],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "HD-Range 1 - 0.01s and 20s\n",
      "tensor([ 0.6414, 20.4607,  0.5475, 20.6440,  0.7499, 20.6537,  0.5794, 20.7004,\n",
      "         0.0000, 20.3252,  0.6388, 21.1680,  0.4324, 20.6665,  1.4413, 21.1188,\n",
      "         0.3573, 20.7760,  0.7898, 20.3473,  1.2115, 20.7304,  0.3875, 20.4762,\n",
      "         0.3701, 20.6632,  0.8293, 20.6418,  0.6648, 21.1496,  0.5252, 21.1841,\n",
      "         1.5375, 20.4598,  1.1807, 21.4016,  0.4748, 21.0045,  0.5911, 19.8616,\n",
      "         0.5696, 20.0690,  0.8014, 20.5188], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "HD-Range 2 - 0s and 20s\n",
      "tensor([ 0.6297, 20.4588,  0.5402, 20.6483,  0.7423, 20.6549,  0.5703, 20.7027,\n",
      "         0.0000, 20.3257,  0.6309, 21.1703,  0.4265, 20.6609,  1.4317, 21.1210,\n",
      "         0.3468, 20.7759,  0.7796, 20.3484,  1.2010, 20.7347,  0.3821, 20.4760,\n",
      "         0.3618, 20.6655,  0.8189, 20.6463,  0.6544, 21.1504,  0.5154, 21.1853,\n",
      "         1.5248, 20.4603,  1.1713, 21.3989,  0.4656, 21.0090,  0.5859, 19.8663,\n",
      "         0.5613, 20.0702,  0.7951, 20.5192], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(OtherValSpectra[0])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 0.01s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
