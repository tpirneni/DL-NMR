{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 500\n",
    "\n",
    "# Identification part of the filenames\n",
    "base_name = 'MimicTissueRange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"CNN_44met_{base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load(f'Dataset44_{base_name}_ForManuscript_Spec.npy')\n",
    "conc1 = np.load(f'Dataset44_{base_name}_ForManuscript_Conc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in datasets]\n",
    "test_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in Test_datasets]\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset_reshaped, batch_size = 64, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset_reshaped, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NMR_Model_Aq, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 42, kernel_size=6, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(120708, 200)\n",
    "        self.fc2 = nn.Linear(200, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)                  \n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = RelativeAbsoluteError()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = 5.287243368897864e-05, weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in validation loss constitutes early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "            \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/500], Train Loss: 12614.4837, Test Loss: 3142.2398\n",
      "Epoch [2/500], Train Loss: 12553.8001, Test Loss: 3106.5738\n",
      "Epoch [3/500], Train Loss: 12110.0431, Test Loss: 2939.6234\n",
      "Epoch [4/500], Train Loss: 11206.5018, Test Loss: 2620.4253\n",
      "Epoch [5/500], Train Loss: 9583.1250, Test Loss: 2136.0228\n",
      "Epoch [6/500], Train Loss: 7675.4973, Test Loss: 1767.7362\n",
      "Epoch [7/500], Train Loss: 6707.1495, Test Loss: 1591.2306\n",
      "Epoch [8/500], Train Loss: 6130.9544, Test Loss: 1477.4780\n",
      "Epoch [9/500], Train Loss: 5726.4609, Test Loss: 1380.0143\n",
      "Epoch [10/500], Train Loss: 5384.1487, Test Loss: 1316.4397\n",
      "Epoch [11/500], Train Loss: 5133.2856, Test Loss: 1259.2830\n",
      "Epoch [12/500], Train Loss: 4945.3371, Test Loss: 1209.3278\n",
      "Epoch [13/500], Train Loss: 4760.1604, Test Loss: 1177.4876\n",
      "Epoch [14/500], Train Loss: 4612.1821, Test Loss: 1147.0681\n",
      "Epoch [15/500], Train Loss: 4528.6051, Test Loss: 1115.6795\n",
      "Epoch [16/500], Train Loss: 4388.4907, Test Loss: 1081.6940\n",
      "Epoch [17/500], Train Loss: 4279.9852, Test Loss: 1062.8006\n",
      "Epoch [18/500], Train Loss: 4211.3137, Test Loss: 1039.4164\n",
      "Epoch [19/500], Train Loss: 4125.0629, Test Loss: 1046.8731\n",
      "Epoch [20/500], Train Loss: 4062.4404, Test Loss: 1003.9683\n",
      "Epoch [21/500], Train Loss: 3997.0507, Test Loss: 991.1747\n",
      "Epoch [22/500], Train Loss: 3928.4193, Test Loss: 978.5384\n",
      "Epoch [23/500], Train Loss: 3876.9318, Test Loss: 967.4202\n",
      "Epoch [24/500], Train Loss: 3855.9356, Test Loss: 958.9573\n",
      "Epoch [25/500], Train Loss: 3799.9702, Test Loss: 962.2599\n",
      "Epoch [26/500], Train Loss: 3769.7694, Test Loss: 936.1748\n",
      "Epoch [27/500], Train Loss: 3773.6130, Test Loss: 935.6733\n",
      "Epoch [28/500], Train Loss: 3732.9888, Test Loss: 922.9002\n",
      "Epoch [29/500], Train Loss: 3694.4833, Test Loss: 927.7666\n",
      "Epoch [30/500], Train Loss: 3688.1667, Test Loss: 911.1638\n",
      "Epoch [31/500], Train Loss: 3652.5419, Test Loss: 911.7370\n",
      "Epoch [32/500], Train Loss: 3659.8282, Test Loss: 915.2881\n",
      "Epoch [33/500], Train Loss: 3639.2081, Test Loss: 901.7131\n",
      "Epoch [34/500], Train Loss: 3625.7438, Test Loss: 905.9687\n",
      "Epoch [35/500], Train Loss: 3626.7219, Test Loss: 895.0717\n",
      "Epoch [36/500], Train Loss: 3603.4458, Test Loss: 901.8228\n",
      "Epoch [37/500], Train Loss: 3590.8356, Test Loss: 897.4225\n",
      "Epoch [38/500], Train Loss: 3587.5896, Test Loss: 907.3126\n",
      "Epoch [39/500], Train Loss: 3579.7711, Test Loss: 890.7743\n",
      "Epoch [40/500], Train Loss: 3586.9796, Test Loss: 888.9457\n",
      "Epoch [41/500], Train Loss: 3566.2386, Test Loss: 885.9366\n",
      "Epoch [42/500], Train Loss: 3556.9092, Test Loss: 885.3788\n",
      "Epoch [43/500], Train Loss: 3562.2822, Test Loss: 890.4301\n",
      "Epoch [44/500], Train Loss: 3535.7470, Test Loss: 877.3908\n",
      "Epoch [45/500], Train Loss: 3535.2793, Test Loss: 883.4223\n",
      "Epoch [46/500], Train Loss: 3540.6048, Test Loss: 879.2996\n",
      "Epoch [47/500], Train Loss: 3553.8954, Test Loss: 899.9178\n",
      "Epoch [48/500], Train Loss: 3537.8331, Test Loss: 880.7813\n",
      "Epoch [49/500], Train Loss: 3524.2911, Test Loss: 879.0430\n",
      "Epoch [50/500], Train Loss: 3511.2161, Test Loss: 873.6355\n",
      "Epoch [51/500], Train Loss: 3512.5376, Test Loss: 881.7145\n",
      "Epoch [52/500], Train Loss: 3512.5574, Test Loss: 872.7195\n",
      "Epoch [53/500], Train Loss: 3486.9855, Test Loss: 867.1988\n",
      "Epoch [54/500], Train Loss: 3498.2547, Test Loss: 882.6307\n",
      "Epoch [55/500], Train Loss: 3500.3751, Test Loss: 872.5185\n",
      "Epoch [56/500], Train Loss: 3502.1133, Test Loss: 881.8216\n",
      "Epoch [57/500], Train Loss: 3493.8185, Test Loss: 871.9365\n",
      "Epoch [58/500], Train Loss: 3480.0392, Test Loss: 869.3423\n",
      "Epoch [59/500], Train Loss: 3464.6045, Test Loss: 879.8746\n",
      "Epoch [60/500], Train Loss: 3487.3682, Test Loss: 872.4639\n",
      "Epoch [61/500], Train Loss: 3473.7886, Test Loss: 867.9739\n",
      "Epoch [62/500], Train Loss: 3481.9668, Test Loss: 887.8250\n",
      "Epoch [63/500], Train Loss: 3460.5165, Test Loss: 863.0758\n",
      "Epoch [64/500], Train Loss: 3449.7749, Test Loss: 874.0252\n",
      "Epoch [65/500], Train Loss: 3450.2001, Test Loss: 881.7317\n",
      "Epoch [66/500], Train Loss: 3492.5553, Test Loss: 860.1381\n",
      "Epoch [67/500], Train Loss: 3447.1642, Test Loss: 852.7008\n",
      "Epoch [68/500], Train Loss: 3440.9547, Test Loss: 863.7932\n",
      "Epoch [69/500], Train Loss: 3449.4105, Test Loss: 869.4324\n",
      "Epoch [70/500], Train Loss: 3430.7901, Test Loss: 854.9601\n",
      "Epoch [71/500], Train Loss: 3443.2478, Test Loss: 858.6032\n",
      "Epoch [72/500], Train Loss: 3419.4203, Test Loss: 855.3388\n",
      "Epoch [73/500], Train Loss: 3420.6731, Test Loss: 868.8182\n",
      "Epoch [74/500], Train Loss: 3437.0038, Test Loss: 871.3193\n",
      "Epoch [75/500], Train Loss: 3407.0273, Test Loss: 867.2501\n",
      "Epoch [76/500], Train Loss: 3407.7159, Test Loss: 858.0195\n",
      "Epoch [77/500], Train Loss: 3432.4118, Test Loss: 842.1958\n",
      "Epoch [78/500], Train Loss: 3377.4174, Test Loss: 850.3565\n",
      "Epoch [79/500], Train Loss: 3416.6299, Test Loss: 844.2319\n",
      "Epoch [80/500], Train Loss: 3377.9172, Test Loss: 852.8961\n",
      "Epoch [81/500], Train Loss: 3382.6909, Test Loss: 844.3408\n",
      "Epoch [82/500], Train Loss: 3371.9377, Test Loss: 853.0967\n",
      "Epoch [83/500], Train Loss: 3361.1561, Test Loss: 840.4270\n",
      "Epoch [84/500], Train Loss: 3346.0315, Test Loss: 838.4165\n",
      "Epoch [85/500], Train Loss: 3352.6768, Test Loss: 868.5640\n",
      "Epoch [86/500], Train Loss: 3341.3129, Test Loss: 837.1696\n",
      "Epoch [87/500], Train Loss: 3328.8259, Test Loss: 841.8463\n",
      "Epoch [88/500], Train Loss: 3313.5292, Test Loss: 824.8605\n",
      "Epoch [89/500], Train Loss: 3297.2296, Test Loss: 823.9049\n",
      "Epoch [90/500], Train Loss: 3292.5777, Test Loss: 830.9748\n",
      "Epoch [91/500], Train Loss: 3266.5122, Test Loss: 814.4184\n",
      "Epoch [92/500], Train Loss: 3275.9526, Test Loss: 841.0356\n",
      "Epoch [93/500], Train Loss: 3256.7404, Test Loss: 814.4809\n",
      "Epoch [94/500], Train Loss: 3235.4103, Test Loss: 803.5701\n",
      "Epoch [95/500], Train Loss: 3228.8951, Test Loss: 825.2968\n",
      "Epoch [96/500], Train Loss: 3215.0037, Test Loss: 804.2642\n",
      "Epoch [97/500], Train Loss: 3194.2573, Test Loss: 807.1441\n",
      "Epoch [98/500], Train Loss: 3180.0097, Test Loss: 796.8285\n",
      "Epoch [99/500], Train Loss: 3224.1095, Test Loss: 805.0466\n",
      "Epoch [100/500], Train Loss: 3175.6249, Test Loss: 803.5820\n",
      "Epoch [101/500], Train Loss: 3174.8654, Test Loss: 808.4253\n",
      "Epoch [102/500], Train Loss: 3161.2332, Test Loss: 803.7682\n",
      "Epoch [103/500], Train Loss: 3172.1343, Test Loss: 796.3485\n",
      "Epoch [104/500], Train Loss: 3171.3596, Test Loss: 803.4227\n",
      "Epoch [105/500], Train Loss: 3170.1225, Test Loss: 799.4079\n",
      "Epoch [106/500], Train Loss: 3156.1548, Test Loss: 798.8914\n",
      "Epoch [107/500], Train Loss: 3152.2310, Test Loss: 785.9314\n",
      "Epoch [108/500], Train Loss: 3157.3320, Test Loss: 806.5205\n",
      "Epoch [109/500], Train Loss: 3145.2167, Test Loss: 801.3198\n",
      "Epoch [110/500], Train Loss: 3142.2137, Test Loss: 809.3382\n",
      "Epoch [111/500], Train Loss: 3147.6744, Test Loss: 805.2766\n",
      "Epoch [112/500], Train Loss: 3130.7131, Test Loss: 785.7039\n",
      "Epoch [113/500], Train Loss: 3138.9800, Test Loss: 794.0943\n",
      "Epoch [114/500], Train Loss: 3133.3249, Test Loss: 793.4647\n",
      "Epoch [115/500], Train Loss: 3140.6196, Test Loss: 784.4522\n",
      "Epoch [116/500], Train Loss: 3125.8750, Test Loss: 791.4463\n",
      "Epoch [117/500], Train Loss: 3128.9224, Test Loss: 789.3199\n",
      "Epoch [118/500], Train Loss: 3115.4037, Test Loss: 802.2958\n",
      "Epoch [119/500], Train Loss: 3133.8162, Test Loss: 786.0779\n",
      "Epoch [120/500], Train Loss: 3116.7652, Test Loss: 784.8591\n",
      "Epoch [121/500], Train Loss: 3116.1941, Test Loss: 785.8418\n",
      "Epoch [122/500], Train Loss: 3100.8229, Test Loss: 785.1738\n",
      "Epoch [123/500], Train Loss: 3090.2794, Test Loss: 788.6078\n",
      "Epoch [124/500], Train Loss: 3105.1577, Test Loss: 787.7087\n",
      "Epoch [125/500], Train Loss: 3111.1632, Test Loss: 798.0845\n",
      "Epoch [126/500], Train Loss: 3099.2159, Test Loss: 773.5143\n",
      "Epoch [127/500], Train Loss: 3097.9108, Test Loss: 775.2708\n",
      "Epoch [128/500], Train Loss: 3088.9135, Test Loss: 780.8687\n",
      "Epoch [129/500], Train Loss: 3079.0305, Test Loss: 782.1078\n",
      "Epoch [130/500], Train Loss: 3079.4038, Test Loss: 775.5549\n",
      "Epoch [131/500], Train Loss: 3081.6686, Test Loss: 778.0212\n",
      "Epoch [132/500], Train Loss: 3076.8167, Test Loss: 792.2871\n",
      "Epoch [133/500], Train Loss: 3079.2090, Test Loss: 782.7105\n",
      "Epoch [134/500], Train Loss: 3078.7798, Test Loss: 774.4963\n",
      "Epoch [135/500], Train Loss: 3078.9883, Test Loss: 765.6020\n",
      "Epoch [136/500], Train Loss: 3069.8345, Test Loss: 767.5300\n",
      "Epoch [137/500], Train Loss: 3068.6299, Test Loss: 764.6358\n",
      "Epoch [138/500], Train Loss: 3051.1359, Test Loss: 771.6478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [139/500], Train Loss: 3047.4267, Test Loss: 770.8373\n",
      "Epoch [140/500], Train Loss: 3052.9876, Test Loss: 765.5852\n",
      "Epoch [141/500], Train Loss: 3053.7532, Test Loss: 763.1804\n",
      "Epoch [142/500], Train Loss: 3042.1477, Test Loss: 761.7003\n",
      "Epoch [143/500], Train Loss: 3030.6748, Test Loss: 777.1768\n",
      "Epoch [144/500], Train Loss: 3031.9807, Test Loss: 767.6815\n",
      "Epoch [145/500], Train Loss: 3044.9913, Test Loss: 765.2500\n",
      "Epoch [146/500], Train Loss: 3018.6460, Test Loss: 763.3547\n",
      "Epoch [147/500], Train Loss: 3020.4130, Test Loss: 772.6291\n",
      "Epoch [148/500], Train Loss: 3009.5895, Test Loss: 763.0341\n",
      "Epoch [149/500], Train Loss: 3020.8634, Test Loss: 758.8015\n",
      "Epoch [150/500], Train Loss: 2988.1089, Test Loss: 755.8831\n",
      "Epoch [151/500], Train Loss: 2991.9664, Test Loss: 748.5508\n",
      "Epoch [152/500], Train Loss: 2981.2324, Test Loss: 754.8715\n",
      "Epoch [153/500], Train Loss: 2991.0788, Test Loss: 762.9151\n",
      "Epoch [154/500], Train Loss: 2974.8467, Test Loss: 755.9848\n",
      "Epoch [155/500], Train Loss: 2969.0589, Test Loss: 739.8422\n",
      "Epoch [156/500], Train Loss: 2961.1321, Test Loss: 739.3331\n",
      "Epoch [157/500], Train Loss: 2956.1548, Test Loss: 750.3464\n",
      "Epoch [158/500], Train Loss: 2940.9176, Test Loss: 735.2992\n",
      "Epoch [159/500], Train Loss: 2950.0966, Test Loss: 744.4720\n",
      "Epoch [160/500], Train Loss: 2936.0800, Test Loss: 733.8008\n",
      "Epoch [161/500], Train Loss: 2922.8997, Test Loss: 739.1169\n",
      "Epoch [162/500], Train Loss: 2914.0575, Test Loss: 734.8880\n",
      "Epoch [163/500], Train Loss: 2907.8843, Test Loss: 730.7942\n",
      "Epoch [164/500], Train Loss: 2904.0611, Test Loss: 736.2935\n",
      "Epoch [165/500], Train Loss: 2899.6040, Test Loss: 729.5161\n",
      "Epoch [166/500], Train Loss: 2904.5030, Test Loss: 728.8709\n",
      "Epoch [167/500], Train Loss: 2874.7503, Test Loss: 727.9955\n",
      "Epoch [168/500], Train Loss: 2893.1038, Test Loss: 733.5659\n",
      "Epoch [169/500], Train Loss: 2873.9124, Test Loss: 723.7347\n",
      "Epoch [170/500], Train Loss: 2870.9200, Test Loss: 724.7768\n",
      "Epoch [171/500], Train Loss: 2863.0363, Test Loss: 720.3205\n",
      "Epoch [172/500], Train Loss: 2870.8938, Test Loss: 732.4567\n",
      "Epoch [173/500], Train Loss: 2868.4401, Test Loss: 723.7773\n",
      "Epoch [174/500], Train Loss: 2857.3097, Test Loss: 718.6802\n",
      "Epoch [175/500], Train Loss: 2848.8467, Test Loss: 719.8446\n",
      "Epoch [176/500], Train Loss: 2851.5524, Test Loss: 727.7530\n",
      "Epoch [177/500], Train Loss: 2842.1559, Test Loss: 716.9608\n",
      "Epoch [178/500], Train Loss: 2841.0286, Test Loss: 726.8635\n",
      "Epoch [179/500], Train Loss: 2834.7489, Test Loss: 711.4602\n",
      "Epoch [180/500], Train Loss: 2834.9016, Test Loss: 714.7642\n",
      "Epoch [181/500], Train Loss: 2819.0223, Test Loss: 715.4352\n",
      "Epoch [182/500], Train Loss: 2824.4445, Test Loss: 714.6454\n",
      "Epoch [183/500], Train Loss: 2823.9982, Test Loss: 721.0884\n",
      "Epoch [184/500], Train Loss: 2816.7137, Test Loss: 726.1530\n",
      "Epoch [185/500], Train Loss: 2823.1570, Test Loss: 722.4027\n",
      "Epoch [186/500], Train Loss: 2820.9317, Test Loss: 713.5956\n",
      "Epoch [187/500], Train Loss: 2790.7335, Test Loss: 711.5219\n",
      "Epoch [188/500], Train Loss: 2792.9345, Test Loss: 699.5337\n",
      "Epoch [189/500], Train Loss: 2808.3180, Test Loss: 704.8007\n",
      "Epoch [190/500], Train Loss: 2794.5206, Test Loss: 704.3249\n",
      "Epoch [191/500], Train Loss: 2794.2193, Test Loss: 706.9603\n",
      "Epoch [192/500], Train Loss: 2783.3940, Test Loss: 708.0212\n",
      "Epoch [193/500], Train Loss: 2796.0465, Test Loss: 703.6020\n",
      "Epoch [194/500], Train Loss: 2780.4256, Test Loss: 709.4679\n",
      "Epoch [195/500], Train Loss: 2772.6891, Test Loss: 703.8565\n",
      "Epoch [196/500], Train Loss: 2770.9967, Test Loss: 697.9036\n",
      "Epoch [197/500], Train Loss: 2775.1535, Test Loss: 704.2401\n",
      "Epoch [198/500], Train Loss: 2760.2833, Test Loss: 703.3201\n",
      "Epoch [199/500], Train Loss: 2778.2469, Test Loss: 702.4754\n",
      "Epoch [200/500], Train Loss: 2755.2381, Test Loss: 694.8879\n",
      "Epoch [201/500], Train Loss: 2751.0142, Test Loss: 703.5468\n",
      "Epoch [202/500], Train Loss: 2752.1840, Test Loss: 697.1039\n",
      "Epoch [203/500], Train Loss: 2758.2686, Test Loss: 701.9414\n",
      "Epoch [204/500], Train Loss: 2743.8283, Test Loss: 692.8639\n",
      "Epoch [205/500], Train Loss: 2750.4295, Test Loss: 694.3358\n",
      "Epoch [206/500], Train Loss: 2749.1878, Test Loss: 692.2955\n",
      "Epoch [207/500], Train Loss: 2744.4612, Test Loss: 693.0456\n",
      "Epoch [208/500], Train Loss: 2728.3083, Test Loss: 694.7608\n",
      "Epoch [209/500], Train Loss: 2733.8967, Test Loss: 711.4531\n",
      "Epoch [210/500], Train Loss: 2745.7947, Test Loss: 692.7268\n",
      "Epoch [211/500], Train Loss: 2732.5911, Test Loss: 690.5820\n",
      "Epoch [212/500], Train Loss: 2725.5747, Test Loss: 694.2026\n",
      "Epoch [213/500], Train Loss: 2723.5269, Test Loss: 700.6494\n",
      "Epoch [214/500], Train Loss: 2718.3141, Test Loss: 693.3733\n",
      "Epoch [215/500], Train Loss: 2720.7526, Test Loss: 692.9781\n",
      "Epoch [216/500], Train Loss: 2709.5959, Test Loss: 696.5462\n",
      "Epoch [217/500], Train Loss: 2710.3075, Test Loss: 692.0732\n",
      "Epoch [218/500], Train Loss: 2716.5051, Test Loss: 687.2864\n",
      "Epoch [219/500], Train Loss: 2711.6837, Test Loss: 702.4192\n",
      "Epoch [220/500], Train Loss: 2706.6985, Test Loss: 692.2442\n",
      "Epoch [221/500], Train Loss: 2706.4550, Test Loss: 689.4740\n",
      "Epoch [222/500], Train Loss: 2700.5507, Test Loss: 686.7238\n",
      "Epoch [223/500], Train Loss: 2698.0439, Test Loss: 693.6449\n",
      "Epoch [224/500], Train Loss: 2708.0010, Test Loss: 679.5222\n",
      "Epoch [225/500], Train Loss: 2689.2249, Test Loss: 684.0226\n",
      "Epoch [226/500], Train Loss: 2701.9713, Test Loss: 686.1306\n",
      "Epoch [227/500], Train Loss: 2685.4253, Test Loss: 692.2999\n",
      "Epoch [228/500], Train Loss: 2696.6822, Test Loss: 687.4720\n",
      "Epoch [229/500], Train Loss: 2691.6819, Test Loss: 685.2228\n",
      "Epoch [230/500], Train Loss: 2685.1312, Test Loss: 680.2734\n",
      "Epoch [231/500], Train Loss: 2682.8239, Test Loss: 688.4876\n",
      "Epoch [232/500], Train Loss: 2683.1312, Test Loss: 679.5152\n",
      "Epoch [233/500], Train Loss: 2676.9592, Test Loss: 679.7533\n",
      "Epoch [234/500], Train Loss: 2673.9461, Test Loss: 681.3439\n",
      "Epoch [235/500], Train Loss: 2680.8545, Test Loss: 677.7765\n",
      "Epoch [236/500], Train Loss: 2680.6942, Test Loss: 674.5160\n",
      "Epoch [237/500], Train Loss: 2680.6248, Test Loss: 677.0237\n",
      "Epoch [238/500], Train Loss: 2676.6138, Test Loss: 684.3193\n",
      "Epoch [239/500], Train Loss: 2672.0177, Test Loss: 691.9671\n",
      "Epoch [240/500], Train Loss: 2669.4607, Test Loss: 674.3914\n",
      "Epoch [241/500], Train Loss: 2683.1217, Test Loss: 686.2973\n",
      "Epoch [242/500], Train Loss: 2670.5392, Test Loss: 683.5539\n",
      "Epoch [243/500], Train Loss: 2661.4338, Test Loss: 681.0689\n",
      "Epoch [244/500], Train Loss: 2667.3118, Test Loss: 682.2632\n",
      "Epoch [245/500], Train Loss: 2676.3943, Test Loss: 675.4769\n",
      "Epoch [246/500], Train Loss: 2663.7067, Test Loss: 674.1374\n",
      "Epoch [247/500], Train Loss: 2650.2165, Test Loss: 676.9016\n",
      "Epoch [248/500], Train Loss: 2655.7080, Test Loss: 677.5979\n",
      "Epoch [249/500], Train Loss: 2675.1812, Test Loss: 683.0662\n",
      "Epoch [250/500], Train Loss: 2657.4405, Test Loss: 671.2048\n",
      "Epoch [251/500], Train Loss: 2644.0848, Test Loss: 680.5285\n",
      "Epoch [252/500], Train Loss: 2661.6886, Test Loss: 677.9536\n",
      "Epoch [253/500], Train Loss: 2652.7365, Test Loss: 677.7905\n",
      "Epoch [254/500], Train Loss: 2657.0941, Test Loss: 677.5348\n",
      "Epoch [255/500], Train Loss: 2654.7388, Test Loss: 684.6678\n",
      "Epoch [256/500], Train Loss: 2649.4407, Test Loss: 672.8875\n",
      "Epoch [257/500], Train Loss: 2652.0207, Test Loss: 683.8123\n",
      "Epoch [258/500], Train Loss: 2647.8805, Test Loss: 676.8956\n",
      "Epoch [259/500], Train Loss: 2651.0730, Test Loss: 678.9596\n",
      "Epoch [260/500], Train Loss: 2657.2661, Test Loss: 679.1680\n",
      "Epoch [261/500], Train Loss: 2648.6918, Test Loss: 680.9964\n",
      "Epoch [262/500], Train Loss: 2638.7570, Test Loss: 668.6631\n",
      "Epoch [263/500], Train Loss: 2638.3939, Test Loss: 671.8876\n",
      "Epoch [264/500], Train Loss: 2645.1601, Test Loss: 673.9839\n",
      "Epoch [265/500], Train Loss: 2641.4117, Test Loss: 671.9272\n",
      "Epoch [266/500], Train Loss: 2646.5275, Test Loss: 673.3322\n",
      "Epoch [267/500], Train Loss: 2635.2863, Test Loss: 672.5397\n",
      "Epoch [268/500], Train Loss: 2636.5577, Test Loss: 669.9349\n",
      "Epoch [269/500], Train Loss: 2645.0836, Test Loss: 690.1643\n",
      "Epoch [270/500], Train Loss: 2637.2908, Test Loss: 670.0957\n",
      "Epoch [271/500], Train Loss: 2643.8098, Test Loss: 670.3099\n",
      "Epoch [272/500], Train Loss: 2631.6038, Test Loss: 681.0071\n",
      "Epoch [273/500], Train Loss: 2643.9251, Test Loss: 677.9751\n",
      "Epoch [274/500], Train Loss: 2639.2444, Test Loss: 677.7396\n",
      "Epoch [275/500], Train Loss: 2634.6787, Test Loss: 678.8147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [276/500], Train Loss: 2633.1275, Test Loss: 674.3287\n",
      "Epoch [277/500], Train Loss: 2638.4163, Test Loss: 677.6157\n",
      "Epoch [278/500], Train Loss: 2634.6331, Test Loss: 668.9853\n",
      "Epoch [279/500], Train Loss: 2629.1212, Test Loss: 668.1106\n",
      "Epoch [280/500], Train Loss: 2637.5317, Test Loss: 666.7353\n",
      "Epoch [281/500], Train Loss: 2624.6034, Test Loss: 666.8094\n",
      "Epoch [282/500], Train Loss: 2627.2774, Test Loss: 667.2864\n",
      "Epoch [283/500], Train Loss: 2636.6591, Test Loss: 667.1516\n",
      "Epoch [284/500], Train Loss: 2632.3724, Test Loss: 681.1483\n",
      "Epoch [285/500], Train Loss: 2622.8346, Test Loss: 671.7507\n",
      "Epoch [286/500], Train Loss: 2622.5582, Test Loss: 669.2438\n",
      "Epoch [287/500], Train Loss: 2644.3958, Test Loss: 670.0925\n",
      "Epoch [288/500], Train Loss: 2628.2203, Test Loss: 670.8435\n",
      "Epoch [289/500], Train Loss: 2628.8234, Test Loss: 664.7448\n",
      "Epoch [290/500], Train Loss: 2618.9453, Test Loss: 670.0969\n",
      "Epoch [291/500], Train Loss: 2621.4568, Test Loss: 664.0780\n",
      "Epoch [292/500], Train Loss: 2627.2444, Test Loss: 663.1994\n",
      "Epoch [293/500], Train Loss: 2634.4799, Test Loss: 684.1449\n",
      "Epoch [294/500], Train Loss: 2613.6582, Test Loss: 664.4800\n",
      "Epoch [295/500], Train Loss: 2621.1495, Test Loss: 664.3443\n",
      "Epoch [296/500], Train Loss: 2625.4628, Test Loss: 666.0197\n",
      "Epoch [297/500], Train Loss: 2621.9944, Test Loss: 675.6112\n",
      "Epoch [298/500], Train Loss: 2623.1748, Test Loss: 665.6973\n",
      "Epoch [299/500], Train Loss: 2620.7275, Test Loss: 664.2707\n",
      "Epoch [300/500], Train Loss: 2615.9021, Test Loss: 671.4856\n",
      "Epoch [301/500], Train Loss: 2615.9266, Test Loss: 666.9573\n",
      "Epoch [302/500], Train Loss: 2619.9368, Test Loss: 671.2615\n",
      "Epoch [303/500], Train Loss: 2604.3355, Test Loss: 670.4984\n",
      "Epoch [304/500], Train Loss: 2616.7863, Test Loss: 664.0201\n",
      "Epoch [305/500], Train Loss: 2612.0440, Test Loss: 675.4081\n",
      "Epoch [306/500], Train Loss: 2618.3063, Test Loss: 674.2101\n",
      "Epoch [307/500], Train Loss: 2615.8898, Test Loss: 663.7314\n",
      "Epoch [308/500], Train Loss: 2609.7584, Test Loss: 664.2585\n",
      "Epoch [309/500], Train Loss: 2616.6346, Test Loss: 674.5418\n",
      "Epoch [310/500], Train Loss: 2612.9756, Test Loss: 670.2294\n",
      "Epoch [311/500], Train Loss: 2617.2172, Test Loss: 674.6238\n",
      "Epoch [312/500], Train Loss: 2613.8289, Test Loss: 673.9476\n",
      "Epoch [313/500], Train Loss: 2609.7871, Test Loss: 674.6655\n",
      "Epoch [314/500], Train Loss: 2613.7975, Test Loss: 673.5774\n",
      "Epoch [315/500], Train Loss: 2609.8702, Test Loss: 671.0290\n",
      "Epoch [316/500], Train Loss: 2608.6537, Test Loss: 667.4017\n",
      "Epoch [317/500], Train Loss: 2609.0193, Test Loss: 669.0143\n",
      "Epoch [318/500], Train Loss: 2614.0897, Test Loss: 681.9102\n",
      "Epoch [319/500], Train Loss: 2613.4438, Test Loss: 669.6913\n",
      "Epoch [320/500], Train Loss: 2609.3690, Test Loss: 665.1926\n",
      "Epoch [321/500], Train Loss: 2609.0040, Test Loss: 664.4719\n",
      "Epoch [322/500], Train Loss: 2604.1699, Test Loss: 667.4244\n",
      "Epoch [323/500], Train Loss: 2621.7530, Test Loss: 667.7230\n",
      "Epoch [324/500], Train Loss: 2600.6659, Test Loss: 674.2696\n",
      "Epoch [325/500], Train Loss: 2607.5434, Test Loss: 667.2756\n",
      "Epoch [326/500], Train Loss: 2596.0469, Test Loss: 664.2526\n",
      "Epoch [327/500], Train Loss: 2594.3443, Test Loss: 665.2842\n",
      "Epoch [328/500], Train Loss: 2603.7074, Test Loss: 667.2690\n",
      "Epoch [329/500], Train Loss: 2602.7545, Test Loss: 662.8795\n",
      "Epoch [330/500], Train Loss: 2605.0316, Test Loss: 663.2800\n",
      "Epoch [331/500], Train Loss: 2602.3606, Test Loss: 656.5693\n",
      "Epoch [332/500], Train Loss: 2603.1372, Test Loss: 664.2668\n",
      "Epoch [333/500], Train Loss: 2593.8450, Test Loss: 660.0654\n",
      "Epoch [334/500], Train Loss: 2609.3692, Test Loss: 659.0960\n",
      "Epoch [335/500], Train Loss: 2610.3310, Test Loss: 666.3715\n",
      "Epoch [336/500], Train Loss: 2602.9436, Test Loss: 664.3540\n",
      "Epoch [337/500], Train Loss: 2599.0523, Test Loss: 660.6525\n",
      "Epoch [338/500], Train Loss: 2592.0470, Test Loss: 661.9709\n",
      "Epoch [339/500], Train Loss: 2603.5636, Test Loss: 660.7098\n",
      "Epoch [340/500], Train Loss: 2599.7110, Test Loss: 663.2514\n",
      "Epoch [341/500], Train Loss: 2595.0211, Test Loss: 658.2114\n",
      "Epoch [342/500], Train Loss: 2599.9917, Test Loss: 662.3877\n",
      "Epoch [343/500], Train Loss: 2594.4919, Test Loss: 666.0701\n",
      "Epoch [344/500], Train Loss: 2599.1034, Test Loss: 664.1527\n",
      "Epoch [345/500], Train Loss: 2596.9268, Test Loss: 656.7621\n",
      "Epoch [346/500], Train Loss: 2598.2724, Test Loss: 658.1112\n",
      "Epoch [347/500], Train Loss: 2596.6098, Test Loss: 662.5741\n",
      "Epoch [348/500], Train Loss: 2605.7332, Test Loss: 662.4354\n",
      "Epoch [349/500], Train Loss: 2594.9692, Test Loss: 662.3858\n",
      "Epoch [350/500], Train Loss: 2590.7659, Test Loss: 663.9078\n",
      "Epoch [351/500], Train Loss: 2593.9465, Test Loss: 667.1446\n",
      "Epoch [352/500], Train Loss: 2593.1896, Test Loss: 662.8601\n",
      "Epoch [353/500], Train Loss: 2594.2627, Test Loss: 670.9279\n",
      "Epoch [354/500], Train Loss: 2593.3385, Test Loss: 669.8329\n",
      "Epoch [355/500], Train Loss: 2596.3010, Test Loss: 659.4314\n",
      "Epoch [356/500], Train Loss: 2595.6573, Test Loss: 673.2326\n",
      "Epoch [357/500], Train Loss: 2593.2551, Test Loss: 661.9995\n",
      "Epoch [358/500], Train Loss: 2593.5273, Test Loss: 665.9036\n",
      "Epoch [359/500], Train Loss: 2591.4084, Test Loss: 663.7060\n",
      "Epoch [360/500], Train Loss: 2590.0938, Test Loss: 664.3024\n",
      "Epoch [361/500], Train Loss: 2583.4920, Test Loss: 659.8441\n",
      "Epoch [362/500], Train Loss: 2584.6864, Test Loss: 665.5825\n",
      "Epoch [363/500], Train Loss: 2595.6212, Test Loss: 662.0243\n",
      "Epoch [364/500], Train Loss: 2590.8464, Test Loss: 663.3267\n",
      "Epoch [365/500], Train Loss: 2598.7337, Test Loss: 657.9255\n",
      "Epoch [366/500], Train Loss: 2583.9261, Test Loss: 657.5036\n",
      "Epoch [367/500], Train Loss: 2587.8220, Test Loss: 662.2249\n",
      "Epoch [368/500], Train Loss: 2586.4850, Test Loss: 659.6032\n",
      "Epoch [369/500], Train Loss: 2597.4485, Test Loss: 659.4753\n",
      "Epoch [370/500], Train Loss: 2587.2251, Test Loss: 658.0337\n",
      "Epoch [371/500], Train Loss: 2579.4937, Test Loss: 658.4597\n",
      "Epoch [372/500], Train Loss: 2585.9550, Test Loss: 666.6069\n",
      "Epoch [373/500], Train Loss: 2592.4306, Test Loss: 666.9501\n",
      "Epoch [374/500], Train Loss: 2593.5247, Test Loss: 658.3804\n",
      "Epoch [375/500], Train Loss: 2587.0873, Test Loss: 658.3477\n",
      "Epoch [376/500], Train Loss: 2595.8033, Test Loss: 660.9280\n",
      "Epoch [377/500], Train Loss: 2581.6307, Test Loss: 657.4536\n",
      "Epoch [378/500], Train Loss: 2588.0909, Test Loss: 662.2758\n",
      "Epoch [379/500], Train Loss: 2581.7203, Test Loss: 658.7747\n",
      "Epoch [380/500], Train Loss: 2584.6923, Test Loss: 661.6814\n",
      "Epoch [381/500], Train Loss: 2583.3012, Test Loss: 662.2574\n",
      "Early stopping at epoch 381\n",
      "Execution time: 9442.438372850418 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656.5693218708038"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (conv1): Conv1d(1, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=120708, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  53.24725194433484\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i].cpu().numpy()\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i].unsqueeze(0).unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "176.53  - Concentrations: 0.004999999888241291\n",
      "44.43  - Concentrations: 0.02500000037252903\n",
      "20.76  - Concentrations: 0.10000000149011612\n",
      "16.3  - Concentrations: 0.25\n",
      "15.27  - Concentrations: 0.5\n",
      "14.54  - Concentrations: 1.0\n",
      "14.21  - Concentrations: 2.5\n",
      "14.13  - Concentrations: 10.0\n",
      "14.16  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  22.72035\n",
      "--------------------\n",
      "14.11  - Min Value: 0.6783  - Mean Value: 9.2\n",
      "77.79  - Min Value: 0.0096  - Mean Value: 10.3\n",
      "15.29  - Min Value: 0.147  - Mean Value: 10.5\n",
      "19.08  - Min Value: 0.5572  - Mean Value: 8.5\n",
      "15.39  - Min Value: 1.3567  - Mean Value: 10.6\n",
      "20.87  - Min Value: 0.6332  - Mean Value: 10.9\n",
      "24.09  - Min Value: 0.7017  - Mean Value: 11.0\n",
      "12.45  - Min Value: 0.3674  - Mean Value: 8.9\n",
      "13.51  - Min Value: 0.8387  - Mean Value: 9.8\n",
      "14.64  - Min Value: 1.0913  - Mean Value: 11.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  20.853594\n",
      "--------------------\n",
      "21.65  - Min Value: 0.0111  - Mean Value: 0.1\n",
      "19.04  - Min Value: 0.0103  - Mean Value: 0.1\n",
      "21.08  - Min Value: 0.0153  - Mean Value: 0.1\n",
      "18.47  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "19.27  - Min Value: 0.0089  - Mean Value: 0.1\n",
      "24.09  - Min Value: 0.0075  - Mean Value: 0.1\n",
      "19.43  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "21.01  - Min Value: 0.0052  - Mean Value: 0.1\n",
      "23.09  - Min Value: 0.008  - Mean Value: 0.1\n",
      "21.39  - Min Value: 0.0134  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  39.088715\n",
      "--------------------\n",
      "66.42  - Min Value: 0.008  - Mean Value: 0.8\n",
      "53.89  - Min Value: 0.009  - Mean Value: 0.9\n",
      "32.21  - Min Value: 0.0138  - Mean Value: 1.5\n",
      "28.85  - Min Value: 0.0107  - Mean Value: 0.7\n",
      "27.04  - Min Value: 0.0191  - Mean Value: 0.7\n",
      "51.17  - Min Value: 0.0186  - Mean Value: 0.8\n",
      "31.99  - Min Value: 0.0175  - Mean Value: 0.8\n",
      "23.52  - Min Value: 0.0238  - Mean Value: 1.3\n",
      "29.6  - Min Value: 0.0168  - Mean Value: 0.7\n",
      "46.19  - Min Value: 0.0171  - Mean Value: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  59.749554\n",
      "--------------------\n",
      "47.36  - Min Value: 0.013  - Mean Value: 0.6\n",
      "40.96  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "25.43  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "82.74  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "46.21  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "126.48  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "99.08  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "49.56  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "22.37  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "57.3  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  481.63568\n",
      "--------------------\n",
      "294.89  - Min Value: 0.0062  - Mean Value: 2.1\n",
      "480.65  - Min Value: 0.006  - Mean Value: 3.7\n",
      "393.71  - Min Value: 0.0066  - Mean Value: 4.3\n",
      "693.55  - Min Value: 0.0094  - Mean Value: 4.3\n",
      "587.33  - Min Value: 0.0068  - Mean Value: 4.9\n",
      "722.4  - Min Value: 0.005  - Mean Value: 3.8\n",
      "280.09  - Min Value: 0.0101  - Mean Value: 3.2\n",
      "328.88  - Min Value: 0.0062  - Mean Value: 3.2\n",
      "426.68  - Min Value: 0.0053  - Mean Value: 5.3\n",
      "608.18  - Min Value: 0.0054  - Mean Value: 2.5\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(HighDynamicRange2Conc[i].min().item(),4), \" - Mean Value:\", np.round(HighDynamicRange2Conc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  15.466890393364505\n",
      "--------------------\n",
      "15.42\n",
      "15.44\n",
      "15.34\n",
      "15.37\n",
      "15.39\n",
      "15.4\n",
      "15.41\n",
      "15.68\n",
      "15.62\n",
      "15.61\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  17.611712719181885\n",
      "--------------------\n",
      "15.38\n",
      "15.35\n",
      "15.41\n",
      "16.02\n",
      "16.73\n",
      "16.95\n",
      "17.28\n",
      "18.05\n",
      "20.17\n",
      "24.79\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a dataset with singlets added at random\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  77.54279018718411\n",
      "--------------------\n",
      "19.01\n",
      "32.05\n",
      "45.0\n",
      "58.06\n",
      "71.13\n",
      "84.08\n",
      "96.94\n",
      "109.99\n",
      "123.13\n",
      "136.05\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal Baseline 1\n",
      "tensor([0.4445, 0.5221, 0.4242, 0.4739, 0.4723, 0.4459, 0.4674, 0.5246, 0.5682,\n",
      "        0.3951, 0.4056, 0.1230, 0.1191, 0.1099, 0.1044, 0.5750, 0.5590, 0.4507,\n",
      "        0.4657, 0.3937, 0.0864, 0.6571, 0.3619, 0.4467, 0.5164, 0.4536, 0.5128,\n",
      "        0.4843, 0.4691, 0.4531, 0.4321, 0.0975, 0.0653, 0.4095, 0.4322, 0.4530,\n",
      "        0.4322, 0.4513, 0.1079, 0.3963, 0.4014, 0.4548, 0.5009, 0.4607],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Sinusoidal Baseline 2\n",
      "tensor([0.3778, 0.5041, 0.1562, 0.4923, 0.4242, 0.4450, 0.4264, 0.2987, 0.4344,\n",
      "        0.5197, 0.4193, 0.1167, 0.1133, 0.0988, 0.0973, 0.2404, 0.6406, 0.4830,\n",
      "        0.4495, 0.3972, 0.0781, 0.4197, 0.5369, 0.5552, 0.4335, 0.4594, 0.3276,\n",
      "        0.2579, 0.4874, 0.3986, 0.4224, 0.0898, 0.0592, 0.4315, 0.5581, 0.3599,\n",
      "        0.5110, 0.6261, 0.0998, 0.3752, 0.4352, 0.6412, 0.3659, 0.2774],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "HD-Range 1 - 1s and 20s\n",
      "tensor([ 0.7396, 18.1748,  0.0000, 17.9852,  0.0000, 29.8781,  0.0000, 20.0779,\n",
      "         0.0000, 18.9761,  0.0000,  3.0426,  2.8053,  2.7107,  2.2410, 18.2897,\n",
      "         0.0000, 17.6386,  0.0000, 19.2591,  2.2620, 16.3656,  0.0000, 19.7181,\n",
      "         0.0000, 19.2592,  0.0000, 17.6565,  0.0000, 21.1345,  0.0000,  2.5624,\n",
      "         1.8476, 19.4506,  0.7698, 21.0798,  0.0000, 22.3850,  2.8444, 18.1190,\n",
      "         0.0000, 17.6979,  0.0000, 18.8801], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "HD-Range 2 - 0s and 20s\n",
      "tensor([ 0.7299, 18.1736,  0.0000, 17.9878,  0.0000, 29.8770,  0.0000, 20.0802,\n",
      "         0.0000, 18.9766,  0.0000,  3.0414,  2.8040,  2.7095,  2.2395, 18.2901,\n",
      "         0.0000, 17.6380,  0.0000, 19.2587,  2.2609, 16.3699,  0.0000, 19.7190,\n",
      "         0.0000, 19.2587,  0.0000, 17.6594,  0.0000, 21.1353,  0.0000,  2.5612,\n",
      "         1.8466, 19.4514,  0.7617, 21.0764,  0.0000, 22.3885,  2.8432, 18.1194,\n",
      "         0.0000, 17.6979,  0.0000, 18.8782], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(OtherValSpectra[0].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 1s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
