{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 500\n",
    "\n",
    "# Identification part of the filenames\n",
    "model_base_name = '20000spec_RAE_ExtendedRange_MoreLeftOut_Combined1Distribution_AMPtest_TestArchitecture_IncreaseKernels'\n",
    "base_name = 'ExtendedRange_MoreLeftOut_Combined1Distribution'    # This is the dataset base name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"CNN_44met_{model_base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load(f'Dataset44_{base_name}_ForManuscript_Spec.npy')\n",
    "conc1 = np.load(f'Dataset44_{base_name}_ForManuscript_Conc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in datasets]\n",
    "test_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in Test_datasets]\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset_reshaped, batch_size = 128, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset_reshaped, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NMR_Model_Aq, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=12, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=12, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=12, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=12, padding=1)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(367104, 200)\n",
    "        self.fc2 = nn.Linear(200, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)                  \n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path, accumulation_steps=4):\n",
    "    criterion = RelativeAbsoluteError()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5.287243368897864e-05, weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in validation loss constitutes early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # For timing cell run time\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        ## Training phase\n",
    "        # Instantiate the GradScaler\n",
    "        scaler = GradScaler()\n",
    "        optimizer.zero_grad()  # Only zero gradients here at the start of an epoch\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Enable autocasting for forward and backward passes\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Scale the loss to account for the accumulation steps\n",
    "                loss = loss / accumulation_steps\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Scale the loss and perform backpropagation\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                # Step the optimizer and update the scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()  # Zero gradients after accumulation_steps\n",
    "\n",
    "        # Testing phase\n",
    "        train_losses.append(train_loss)\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                # Move data to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Enable autocasting for forward passes\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "        \n",
    "        \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            #torch.save({\n",
    "            #    'model_state_dict': model.state_dict(),\n",
    "            #    'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #}, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Epoch time: \",end-start)\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/500], Train Loss: 2565.6935, Test Loss: 2570.5602\n",
      "Epoch time:  22.818714141845703\n",
      "Epoch [2/500], Train Loss: 2563.0035, Test Loss: 2569.0334\n",
      "Epoch time:  19.050933122634888\n",
      "Epoch [3/500], Train Loss: 2562.1449, Test Loss: 2568.6975\n",
      "Epoch time:  19.067742824554443\n",
      "Epoch [4/500], Train Loss: 2558.3245, Test Loss: 2559.7756\n",
      "Epoch time:  19.09325337409973\n",
      "Epoch [5/500], Train Loss: 2531.4490, Test Loss: 2488.4994\n",
      "Epoch time:  19.082038640975952\n",
      "Epoch [6/500], Train Loss: 2389.4598, Test Loss: 2246.8717\n",
      "Epoch time:  19.063095331192017\n",
      "Epoch [7/500], Train Loss: 2177.0974, Test Loss: 2110.0360\n",
      "Epoch time:  19.06724500656128\n",
      "Epoch [8/500], Train Loss: 2097.2746, Test Loss: 2064.3823\n",
      "Epoch time:  19.074313402175903\n",
      "Epoch [9/500], Train Loss: 2066.3466, Test Loss: 2045.6269\n",
      "Epoch time:  19.10302972793579\n",
      "Epoch [10/500], Train Loss: 2045.1898, Test Loss: 2023.9655\n",
      "Epoch time:  19.09554958343506\n",
      "Epoch [11/500], Train Loss: 2025.7765, Test Loss: 2013.1366\n",
      "Epoch time:  19.08934450149536\n",
      "Epoch [12/500], Train Loss: 2012.8274, Test Loss: 1995.3273\n",
      "Epoch time:  19.119890451431274\n",
      "Epoch [13/500], Train Loss: 1995.5528, Test Loss: 1975.2776\n",
      "Epoch time:  19.130242109298706\n",
      "Epoch [14/500], Train Loss: 1973.4146, Test Loss: 1948.5184\n",
      "Epoch time:  19.1331045627594\n",
      "Epoch [15/500], Train Loss: 1948.6212, Test Loss: 1918.5334\n",
      "Epoch time:  19.120806455612183\n",
      "Epoch [16/500], Train Loss: 1921.7146, Test Loss: 1894.3576\n",
      "Epoch time:  19.107383012771606\n",
      "Epoch [17/500], Train Loss: 1891.1577, Test Loss: 1851.9907\n",
      "Epoch time:  19.09908938407898\n",
      "Epoch [18/500], Train Loss: 1851.6196, Test Loss: 1811.5322\n",
      "Epoch time:  19.11135196685791\n",
      "Epoch [19/500], Train Loss: 1807.6170, Test Loss: 1766.9349\n",
      "Epoch time:  19.12096118927002\n",
      "Epoch [20/500], Train Loss: 1758.2061, Test Loss: 1713.1315\n",
      "Epoch time:  19.103496551513672\n",
      "Epoch [21/500], Train Loss: 1694.5465, Test Loss: 1639.3694\n",
      "Epoch time:  19.11189103126526\n",
      "Epoch [22/500], Train Loss: 1609.7488, Test Loss: 1544.5337\n",
      "Epoch time:  19.106748819351196\n",
      "Epoch [23/500], Train Loss: 1505.3674, Test Loss: 1435.3616\n",
      "Epoch time:  19.12716054916382\n",
      "Epoch [24/500], Train Loss: 1388.0899, Test Loss: 1324.9790\n",
      "Epoch time:  19.100204944610596\n",
      "Epoch [25/500], Train Loss: 1290.5330, Test Loss: 1234.6871\n",
      "Epoch time:  19.095258235931396\n",
      "Epoch [26/500], Train Loss: 1204.2531, Test Loss: 1158.8019\n",
      "Epoch time:  19.108426809310913\n",
      "Epoch [27/500], Train Loss: 1132.2300, Test Loss: 1092.1098\n",
      "Epoch time:  19.13090944290161\n",
      "Epoch [28/500], Train Loss: 1074.4149, Test Loss: 1068.6155\n",
      "Epoch time:  19.077319383621216\n",
      "Epoch [29/500], Train Loss: 1037.7865, Test Loss: 994.2073\n",
      "Epoch time:  19.05911135673523\n",
      "Epoch [30/500], Train Loss: 979.2497, Test Loss: 952.9263\n",
      "Epoch time:  19.13010835647583\n",
      "Epoch [31/500], Train Loss: 939.5510, Test Loss: 917.1363\n",
      "Epoch time:  19.085452795028687\n",
      "Epoch [32/500], Train Loss: 911.2857, Test Loss: 887.5631\n",
      "Epoch time:  19.107765674591064\n",
      "Epoch [33/500], Train Loss: 876.8289, Test Loss: 852.7653\n",
      "Epoch time:  19.135979890823364\n",
      "Epoch [34/500], Train Loss: 846.2198, Test Loss: 824.8294\n",
      "Epoch time:  19.132776498794556\n",
      "Epoch [35/500], Train Loss: 829.3471, Test Loss: 812.9515\n",
      "Epoch time:  19.113118171691895\n",
      "Epoch [36/500], Train Loss: 807.1389, Test Loss: 796.0414\n",
      "Epoch time:  19.103536128997803\n",
      "Epoch [37/500], Train Loss: 791.9925, Test Loss: 771.8689\n",
      "Epoch time:  19.10556435585022\n",
      "Epoch [38/500], Train Loss: 772.0840, Test Loss: 769.7205\n",
      "Epoch time:  19.11912751197815\n",
      "Epoch [39/500], Train Loss: 766.3054, Test Loss: 748.0484\n",
      "Epoch time:  19.085855722427368\n",
      "Epoch [40/500], Train Loss: 745.5690, Test Loss: 736.1931\n",
      "Epoch time:  19.109397172927856\n",
      "Epoch [41/500], Train Loss: 737.4975, Test Loss: 723.5122\n",
      "Epoch time:  19.108463048934937\n",
      "Epoch [42/500], Train Loss: 718.1058, Test Loss: 708.5370\n",
      "Epoch time:  19.139909267425537\n",
      "Epoch [43/500], Train Loss: 704.8462, Test Loss: 708.7371\n",
      "Epoch time:  19.131978273391724\n",
      "Epoch [44/500], Train Loss: 698.3127, Test Loss: 685.0738\n",
      "Epoch time:  19.10698914527893\n",
      "Epoch [45/500], Train Loss: 690.9700, Test Loss: 688.0253\n",
      "Epoch time:  19.102197408676147\n",
      "Epoch [46/500], Train Loss: 688.2559, Test Loss: 677.7031\n",
      "Epoch time:  19.107731103897095\n",
      "Epoch [47/500], Train Loss: 671.6687, Test Loss: 667.4962\n",
      "Epoch time:  19.130648374557495\n",
      "Epoch [48/500], Train Loss: 663.1150, Test Loss: 657.3518\n",
      "Epoch time:  19.093003749847412\n",
      "Epoch [49/500], Train Loss: 655.1517, Test Loss: 647.4855\n",
      "Epoch time:  19.085524797439575\n",
      "Epoch [50/500], Train Loss: 646.4182, Test Loss: 638.3679\n",
      "Epoch time:  19.107171297073364\n",
      "Epoch [51/500], Train Loss: 627.9652, Test Loss: 616.0586\n",
      "Epoch time:  19.10804843902588\n",
      "Epoch [52/500], Train Loss: 616.6422, Test Loss: 602.2322\n",
      "Epoch time:  19.106983423233032\n",
      "Epoch [53/500], Train Loss: 608.1062, Test Loss: 591.1480\n",
      "Epoch time:  19.085751056671143\n",
      "Epoch [54/500], Train Loss: 589.7471, Test Loss: 580.4640\n",
      "Epoch time:  19.090705394744873\n",
      "Epoch [55/500], Train Loss: 582.2168, Test Loss: 594.5645\n",
      "Epoch time:  19.114303588867188\n",
      "Epoch [56/500], Train Loss: 582.0200, Test Loss: 567.8562\n",
      "Epoch time:  19.111680269241333\n",
      "Epoch [57/500], Train Loss: 567.0439, Test Loss: 567.4352\n",
      "Epoch time:  19.100123643875122\n",
      "Epoch [58/500], Train Loss: 561.8110, Test Loss: 555.3356\n",
      "Epoch time:  19.098339557647705\n",
      "Epoch [59/500], Train Loss: 552.1841, Test Loss: 548.4495\n",
      "Epoch time:  19.125303506851196\n",
      "Epoch [60/500], Train Loss: 548.9871, Test Loss: 546.3853\n",
      "Epoch time:  19.109333515167236\n",
      "Epoch [61/500], Train Loss: 546.2264, Test Loss: 535.8784\n",
      "Epoch time:  19.09932255744934\n",
      "Epoch [62/500], Train Loss: 531.1280, Test Loss: 532.0746\n",
      "Epoch time:  19.129101753234863\n",
      "Epoch [63/500], Train Loss: 529.5379, Test Loss: 523.7493\n",
      "Epoch time:  19.09181523323059\n",
      "Epoch [64/500], Train Loss: 526.2904, Test Loss: 514.3321\n",
      "Epoch time:  19.09279203414917\n",
      "Epoch [65/500], Train Loss: 528.3852, Test Loss: 512.0487\n",
      "Epoch time:  19.08813238143921\n",
      "Epoch [66/500], Train Loss: 513.8079, Test Loss: 506.1719\n",
      "Epoch time:  19.09895420074463\n",
      "Epoch [67/500], Train Loss: 508.3302, Test Loss: 507.1179\n",
      "Epoch time:  19.102505683898926\n",
      "Epoch [68/500], Train Loss: 505.5643, Test Loss: 502.7495\n",
      "Epoch time:  19.124905109405518\n",
      "Epoch [69/500], Train Loss: 502.6173, Test Loss: 505.5673\n",
      "Epoch time:  19.107224702835083\n",
      "Epoch [70/500], Train Loss: 506.9717, Test Loss: 497.0163\n",
      "Epoch time:  19.12453007698059\n",
      "Epoch [71/500], Train Loss: 492.7447, Test Loss: 489.4133\n",
      "Epoch time:  19.10791254043579\n",
      "Epoch [72/500], Train Loss: 490.1006, Test Loss: 494.5872\n",
      "Epoch time:  19.128047704696655\n",
      "Epoch [73/500], Train Loss: 487.7747, Test Loss: 476.2389\n",
      "Epoch time:  19.09744381904602\n",
      "Epoch [74/500], Train Loss: 483.2037, Test Loss: 480.1468\n",
      "Epoch time:  19.148332118988037\n",
      "Epoch [75/500], Train Loss: 478.6558, Test Loss: 474.2773\n",
      "Epoch time:  19.106937170028687\n",
      "Epoch [76/500], Train Loss: 482.2149, Test Loss: 481.2210\n",
      "Epoch time:  19.11450433731079\n",
      "Epoch [77/500], Train Loss: 477.8702, Test Loss: 465.6247\n",
      "Epoch time:  19.10375952720642\n",
      "Epoch [78/500], Train Loss: 475.9025, Test Loss: 474.8516\n",
      "Epoch time:  19.146782636642456\n",
      "Epoch [79/500], Train Loss: 479.6113, Test Loss: 486.3941\n",
      "Epoch time:  19.119296073913574\n",
      "Epoch [80/500], Train Loss: 472.4175, Test Loss: 461.4608\n",
      "Epoch time:  19.095242500305176\n",
      "Epoch [81/500], Train Loss: 467.5687, Test Loss: 458.6301\n",
      "Epoch time:  19.095183849334717\n",
      "Epoch [82/500], Train Loss: 459.9631, Test Loss: 452.8176\n",
      "Epoch time:  19.10394525527954\n",
      "Epoch [83/500], Train Loss: 449.0291, Test Loss: 445.2237\n",
      "Epoch time:  19.11267066001892\n",
      "Epoch [84/500], Train Loss: 450.9145, Test Loss: 445.9476\n",
      "Epoch time:  19.119295120239258\n",
      "Epoch [85/500], Train Loss: 446.7205, Test Loss: 460.9053\n",
      "Epoch time:  19.062751531600952\n",
      "Epoch [86/500], Train Loss: 446.9863, Test Loss: 437.0881\n",
      "Epoch time:  19.089518547058105\n",
      "Epoch [87/500], Train Loss: 445.7793, Test Loss: 444.5167\n",
      "Epoch time:  19.101755142211914\n",
      "Epoch [88/500], Train Loss: 444.7243, Test Loss: 439.3784\n",
      "Epoch time:  19.127522706985474\n",
      "Epoch [89/500], Train Loss: 446.7453, Test Loss: 441.7151\n",
      "Epoch time:  19.097332000732422\n",
      "Epoch [90/500], Train Loss: 444.0685, Test Loss: 434.4163\n",
      "Epoch time:  19.08943009376526\n",
      "Epoch [91/500], Train Loss: 440.3382, Test Loss: 459.5128\n",
      "Epoch time:  19.129852533340454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/500], Train Loss: 438.0186, Test Loss: 430.2207\n",
      "Epoch time:  19.08893656730652\n",
      "Epoch [93/500], Train Loss: 432.1052, Test Loss: 429.3708\n",
      "Epoch time:  19.115848064422607\n",
      "Epoch [94/500], Train Loss: 424.9620, Test Loss: 417.1995\n",
      "Epoch time:  19.10280990600586\n",
      "Epoch [95/500], Train Loss: 418.0917, Test Loss: 418.4866\n",
      "Epoch time:  19.126959562301636\n",
      "Epoch [96/500], Train Loss: 417.8561, Test Loss: 411.8643\n",
      "Epoch time:  19.120280981063843\n",
      "Epoch [97/500], Train Loss: 420.0239, Test Loss: 434.6636\n",
      "Epoch time:  19.099167108535767\n",
      "Epoch [98/500], Train Loss: 424.0600, Test Loss: 423.0759\n",
      "Epoch time:  19.1032772064209\n",
      "Epoch [99/500], Train Loss: 416.5483, Test Loss: 413.4223\n",
      "Epoch time:  19.106268405914307\n",
      "Epoch [100/500], Train Loss: 420.3368, Test Loss: 422.1357\n",
      "Epoch time:  19.11230969429016\n",
      "Epoch [101/500], Train Loss: 414.5106, Test Loss: 412.4922\n",
      "Epoch time:  19.09398078918457\n",
      "Epoch [102/500], Train Loss: 417.1725, Test Loss: 413.2634\n",
      "Epoch time:  19.097806453704834\n",
      "Epoch [103/500], Train Loss: 419.7917, Test Loss: 438.3059\n",
      "Epoch time:  19.09416890144348\n",
      "Epoch [104/500], Train Loss: 418.9377, Test Loss: 412.9051\n",
      "Epoch time:  19.100409030914307\n",
      "Epoch [105/500], Train Loss: 403.3748, Test Loss: 399.2937\n",
      "Epoch time:  19.127393007278442\n",
      "Epoch [106/500], Train Loss: 407.8561, Test Loss: 413.4905\n",
      "Epoch time:  19.107063055038452\n",
      "Epoch [107/500], Train Loss: 412.5492, Test Loss: 405.7127\n",
      "Epoch time:  19.106624364852905\n",
      "Epoch [108/500], Train Loss: 409.8092, Test Loss: 411.5284\n",
      "Epoch time:  19.109273672103882\n",
      "Epoch [109/500], Train Loss: 409.6853, Test Loss: 405.2074\n",
      "Epoch time:  19.084056854248047\n",
      "Epoch [110/500], Train Loss: 404.5539, Test Loss: 408.5309\n",
      "Epoch time:  19.128821849822998\n",
      "Epoch [111/500], Train Loss: 403.7007, Test Loss: 400.0367\n",
      "Epoch time:  19.11767292022705\n",
      "Epoch [112/500], Train Loss: 397.6458, Test Loss: 395.6057\n",
      "Epoch time:  19.152134895324707\n",
      "Epoch [113/500], Train Loss: 394.5953, Test Loss: 391.3984\n",
      "Epoch time:  19.093374729156494\n",
      "Epoch [114/500], Train Loss: 392.0026, Test Loss: 391.4597\n",
      "Epoch time:  19.111023426055908\n",
      "Epoch [115/500], Train Loss: 391.8613, Test Loss: 386.7945\n",
      "Epoch time:  19.096526861190796\n",
      "Epoch [116/500], Train Loss: 398.6517, Test Loss: 410.0792\n",
      "Epoch time:  19.114487886428833\n",
      "Epoch [117/500], Train Loss: 399.0690, Test Loss: 385.3771\n",
      "Epoch time:  19.07529067993164\n",
      "Epoch [118/500], Train Loss: 384.9297, Test Loss: 381.0113\n",
      "Epoch time:  19.119490385055542\n",
      "Epoch [119/500], Train Loss: 385.3248, Test Loss: 387.3000\n",
      "Epoch time:  19.10481572151184\n",
      "Epoch [120/500], Train Loss: 388.3063, Test Loss: 382.5165\n",
      "Epoch time:  19.106356382369995\n",
      "Epoch [121/500], Train Loss: 384.9514, Test Loss: 380.6122\n",
      "Epoch time:  19.117900609970093\n",
      "Epoch [122/500], Train Loss: 382.3409, Test Loss: 377.8123\n",
      "Epoch time:  19.116776704788208\n",
      "Epoch [123/500], Train Loss: 382.3764, Test Loss: 383.1207\n",
      "Epoch time:  19.09380793571472\n",
      "Epoch [124/500], Train Loss: 383.6830, Test Loss: 387.8415\n",
      "Epoch time:  19.13192892074585\n",
      "Epoch [125/500], Train Loss: 387.9527, Test Loss: 385.0525\n",
      "Epoch time:  19.104185819625854\n",
      "Epoch [126/500], Train Loss: 380.4107, Test Loss: 380.4034\n",
      "Epoch time:  19.14783024787903\n",
      "Epoch [127/500], Train Loss: 376.4551, Test Loss: 378.8190\n",
      "Epoch time:  19.125226974487305\n",
      "Epoch [128/500], Train Loss: 377.4438, Test Loss: 374.5965\n",
      "Epoch time:  19.107192516326904\n",
      "Epoch [129/500], Train Loss: 378.3262, Test Loss: 378.3902\n",
      "Epoch time:  19.10339117050171\n",
      "Epoch [130/500], Train Loss: 377.4247, Test Loss: 365.9950\n",
      "Epoch time:  19.10692048072815\n",
      "Epoch [131/500], Train Loss: 379.5193, Test Loss: 385.6108\n",
      "Epoch time:  19.079346418380737\n",
      "Epoch [132/500], Train Loss: 378.0944, Test Loss: 380.3711\n",
      "Epoch time:  19.10105013847351\n",
      "Epoch [133/500], Train Loss: 372.8871, Test Loss: 364.6462\n",
      "Epoch time:  19.104817628860474\n",
      "Epoch [134/500], Train Loss: 365.3312, Test Loss: 366.6191\n",
      "Epoch time:  19.125890970230103\n",
      "Epoch [135/500], Train Loss: 367.6784, Test Loss: 373.4624\n",
      "Epoch time:  19.11972403526306\n",
      "Epoch [136/500], Train Loss: 368.2945, Test Loss: 373.7312\n",
      "Epoch time:  19.10473871231079\n",
      "Epoch [137/500], Train Loss: 369.2460, Test Loss: 365.0698\n",
      "Epoch time:  19.121640920639038\n",
      "Epoch [138/500], Train Loss: 369.1267, Test Loss: 369.2816\n",
      "Epoch time:  19.087883710861206\n",
      "Epoch [139/500], Train Loss: 373.9402, Test Loss: 369.3488\n",
      "Epoch time:  19.10100030899048\n",
      "Epoch [140/500], Train Loss: 375.7551, Test Loss: 366.1721\n",
      "Epoch time:  19.111892223358154\n",
      "Epoch [141/500], Train Loss: 366.3316, Test Loss: 372.0059\n",
      "Epoch time:  19.118242025375366\n",
      "Epoch [142/500], Train Loss: 362.8535, Test Loss: 366.1705\n",
      "Epoch time:  19.132603883743286\n",
      "Epoch [143/500], Train Loss: 361.9016, Test Loss: 361.9868\n",
      "Epoch time:  19.12174963951111\n",
      "Epoch [144/500], Train Loss: 363.0718, Test Loss: 369.5416\n",
      "Epoch time:  19.118160009384155\n",
      "Epoch [145/500], Train Loss: 366.0405, Test Loss: 368.3972\n",
      "Epoch time:  19.120507955551147\n",
      "Epoch [146/500], Train Loss: 366.9173, Test Loss: 356.1911\n",
      "Epoch time:  19.0962917804718\n",
      "Epoch [147/500], Train Loss: 363.4017, Test Loss: 368.6729\n",
      "Epoch time:  19.113612174987793\n",
      "Epoch [148/500], Train Loss: 370.0382, Test Loss: 364.6119\n",
      "Epoch time:  19.09781551361084\n",
      "Epoch [149/500], Train Loss: 365.3912, Test Loss: 360.6305\n",
      "Epoch time:  19.108933448791504\n",
      "Epoch [150/500], Train Loss: 367.0207, Test Loss: 366.9895\n",
      "Epoch time:  19.09439730644226\n",
      "Epoch [151/500], Train Loss: 357.3899, Test Loss: 352.8944\n",
      "Epoch time:  19.097349643707275\n",
      "Epoch [152/500], Train Loss: 350.6462, Test Loss: 348.6881\n",
      "Epoch time:  19.134272575378418\n",
      "Epoch [153/500], Train Loss: 351.6403, Test Loss: 357.1315\n",
      "Epoch time:  19.119947910308838\n",
      "Epoch [154/500], Train Loss: 355.8988, Test Loss: 365.5714\n",
      "Epoch time:  19.12145972251892\n",
      "Epoch [155/500], Train Loss: 358.0565, Test Loss: 359.6806\n",
      "Epoch time:  19.086244344711304\n",
      "Epoch [156/500], Train Loss: 353.7313, Test Loss: 351.4411\n",
      "Epoch time:  19.119539976119995\n",
      "Epoch [157/500], Train Loss: 351.8090, Test Loss: 347.1941\n",
      "Epoch time:  19.124255418777466\n",
      "Epoch [158/500], Train Loss: 346.9712, Test Loss: 347.5060\n",
      "Epoch time:  19.140960693359375\n",
      "Epoch [159/500], Train Loss: 349.5386, Test Loss: 347.7421\n",
      "Epoch time:  19.103842735290527\n",
      "Epoch [160/500], Train Loss: 349.3431, Test Loss: 346.8359\n",
      "Epoch time:  19.119235515594482\n",
      "Epoch [161/500], Train Loss: 349.4274, Test Loss: 349.5501\n",
      "Epoch time:  19.09728193283081\n",
      "Epoch [162/500], Train Loss: 346.1016, Test Loss: 339.5284\n",
      "Epoch time:  19.096712112426758\n",
      "Epoch [163/500], Train Loss: 348.1873, Test Loss: 346.7388\n",
      "Epoch time:  19.11436438560486\n",
      "Epoch [164/500], Train Loss: 352.1623, Test Loss: 347.7702\n",
      "Epoch time:  19.056865692138672\n",
      "Epoch [165/500], Train Loss: 344.6920, Test Loss: 349.0619\n",
      "Epoch time:  19.131218433380127\n",
      "Epoch [166/500], Train Loss: 348.3314, Test Loss: 353.4862\n",
      "Epoch time:  19.1059467792511\n",
      "Epoch [167/500], Train Loss: 351.2405, Test Loss: 348.3226\n",
      "Epoch time:  19.11292052268982\n",
      "Epoch [168/500], Train Loss: 342.3934, Test Loss: 337.1922\n",
      "Epoch time:  19.12868022918701\n",
      "Epoch [169/500], Train Loss: 338.3244, Test Loss: 339.1608\n",
      "Epoch time:  19.124301195144653\n",
      "Epoch [170/500], Train Loss: 340.2205, Test Loss: 344.8978\n",
      "Epoch time:  19.121417999267578\n",
      "Epoch [171/500], Train Loss: 345.2351, Test Loss: 362.1538\n",
      "Epoch time:  19.098958492279053\n",
      "Epoch [172/500], Train Loss: 346.9507, Test Loss: 346.4994\n",
      "Epoch time:  19.11319923400879\n",
      "Epoch [173/500], Train Loss: 339.2116, Test Loss: 339.4220\n",
      "Epoch time:  19.125184774398804\n",
      "Epoch [174/500], Train Loss: 343.0337, Test Loss: 348.8764\n",
      "Epoch time:  19.096975803375244\n",
      "Epoch [175/500], Train Loss: 348.4248, Test Loss: 345.9184\n",
      "Epoch time:  19.105539321899414\n",
      "Epoch [176/500], Train Loss: 339.2244, Test Loss: 344.1196\n",
      "Epoch time:  19.12731909751892\n",
      "Epoch [177/500], Train Loss: 341.7861, Test Loss: 336.2378\n",
      "Epoch time:  19.11206889152527\n",
      "Epoch [178/500], Train Loss: 334.3523, Test Loss: 338.0001\n",
      "Epoch time:  19.10100245475769\n",
      "Epoch [179/500], Train Loss: 332.2314, Test Loss: 335.0844\n",
      "Epoch time:  19.113617181777954\n",
      "Epoch [180/500], Train Loss: 332.2022, Test Loss: 330.7188\n",
      "Epoch time:  19.122280597686768\n",
      "Epoch [181/500], Train Loss: 330.3988, Test Loss: 338.7448\n",
      "Epoch time:  19.14122247695923\n",
      "Epoch [182/500], Train Loss: 339.0118, Test Loss: 335.1913\n",
      "Epoch time:  19.13335943222046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [183/500], Train Loss: 339.8320, Test Loss: 340.2681\n",
      "Epoch time:  19.088457584381104\n",
      "Epoch [184/500], Train Loss: 343.6875, Test Loss: 334.3348\n",
      "Epoch time:  19.09818387031555\n",
      "Epoch [185/500], Train Loss: 330.5210, Test Loss: 331.4667\n",
      "Epoch time:  19.133544206619263\n",
      "Epoch [186/500], Train Loss: 335.8402, Test Loss: 343.5501\n",
      "Epoch time:  19.115595817565918\n",
      "Epoch [187/500], Train Loss: 338.2668, Test Loss: 340.1734\n",
      "Epoch time:  19.125763654708862\n",
      "Epoch [188/500], Train Loss: 341.4355, Test Loss: 339.9870\n",
      "Epoch time:  19.107369422912598\n",
      "Epoch [189/500], Train Loss: 335.3449, Test Loss: 345.5144\n",
      "Epoch time:  19.103564262390137\n",
      "Epoch [190/500], Train Loss: 330.3827, Test Loss: 332.7844\n",
      "Epoch time:  19.10868239402771\n",
      "Epoch [191/500], Train Loss: 329.6473, Test Loss: 333.2593\n",
      "Epoch time:  19.134068250656128\n",
      "Epoch [192/500], Train Loss: 335.8838, Test Loss: 334.1582\n",
      "Epoch time:  19.108398914337158\n",
      "Epoch [193/500], Train Loss: 326.1011, Test Loss: 320.4081\n",
      "Epoch time:  19.099606037139893\n",
      "Epoch [194/500], Train Loss: 323.6482, Test Loss: 327.5973\n",
      "Epoch time:  19.125696182250977\n",
      "Epoch [195/500], Train Loss: 328.4736, Test Loss: 334.3774\n",
      "Epoch time:  19.103709936141968\n",
      "Epoch [196/500], Train Loss: 332.6699, Test Loss: 343.8165\n",
      "Epoch time:  19.094967365264893\n",
      "Epoch [197/500], Train Loss: 327.6021, Test Loss: 333.4660\n",
      "Epoch time:  19.110634088516235\n",
      "Epoch [198/500], Train Loss: 325.9491, Test Loss: 319.4505\n",
      "Epoch time:  19.09905743598938\n",
      "Epoch [199/500], Train Loss: 325.1379, Test Loss: 325.6726\n",
      "Epoch time:  19.13074827194214\n",
      "Epoch [200/500], Train Loss: 326.3403, Test Loss: 328.3541\n",
      "Epoch time:  19.113763093948364\n",
      "Epoch [201/500], Train Loss: 327.9760, Test Loss: 334.5936\n",
      "Epoch time:  19.106720209121704\n",
      "Epoch [202/500], Train Loss: 325.6109, Test Loss: 320.4192\n",
      "Epoch time:  19.094313383102417\n",
      "Epoch [203/500], Train Loss: 319.1058, Test Loss: 329.1856\n",
      "Epoch time:  19.10030722618103\n",
      "Epoch [204/500], Train Loss: 325.2354, Test Loss: 319.6311\n",
      "Epoch time:  19.09561061859131\n",
      "Epoch [205/500], Train Loss: 325.2614, Test Loss: 325.2412\n",
      "Epoch time:  19.109716415405273\n",
      "Epoch [206/500], Train Loss: 340.4427, Test Loss: 341.8009\n",
      "Epoch time:  19.047579765319824\n",
      "Epoch [207/500], Train Loss: 323.8495, Test Loss: 320.8103\n",
      "Epoch time:  19.106045246124268\n",
      "Epoch [208/500], Train Loss: 315.3362, Test Loss: 318.2608\n",
      "Epoch time:  19.112693548202515\n",
      "Epoch [209/500], Train Loss: 322.4208, Test Loss: 323.6251\n",
      "Epoch time:  19.099647521972656\n",
      "Epoch [210/500], Train Loss: 319.4084, Test Loss: 324.5913\n",
      "Epoch time:  19.146973133087158\n",
      "Epoch [211/500], Train Loss: 327.9063, Test Loss: 321.7707\n",
      "Epoch time:  19.057106971740723\n",
      "Epoch [212/500], Train Loss: 313.5256, Test Loss: 312.5051\n",
      "Epoch time:  19.08928632736206\n",
      "Epoch [213/500], Train Loss: 310.7464, Test Loss: 316.1094\n",
      "Epoch time:  19.121214389801025\n",
      "Epoch [214/500], Train Loss: 319.5636, Test Loss: 333.8470\n",
      "Epoch time:  19.115559577941895\n",
      "Epoch [215/500], Train Loss: 322.4739, Test Loss: 329.6916\n",
      "Epoch time:  19.108636140823364\n",
      "Epoch [216/500], Train Loss: 317.5540, Test Loss: 310.0930\n",
      "Epoch time:  19.10644841194153\n",
      "Epoch [217/500], Train Loss: 313.0028, Test Loss: 322.9510\n",
      "Epoch time:  19.126481771469116\n",
      "Epoch [218/500], Train Loss: 320.1543, Test Loss: 321.0669\n",
      "Epoch time:  19.07012367248535\n",
      "Epoch [219/500], Train Loss: 317.7762, Test Loss: 308.4166\n",
      "Epoch time:  19.114243507385254\n",
      "Epoch [220/500], Train Loss: 304.7198, Test Loss: 308.4488\n",
      "Epoch time:  19.12541365623474\n",
      "Epoch [221/500], Train Loss: 308.3305, Test Loss: 310.2184\n",
      "Epoch time:  19.100124835968018\n",
      "Epoch [222/500], Train Loss: 313.9168, Test Loss: 316.2607\n",
      "Epoch time:  19.124886512756348\n",
      "Epoch [223/500], Train Loss: 315.2254, Test Loss: 322.6688\n",
      "Epoch time:  19.113268613815308\n",
      "Epoch [224/500], Train Loss: 315.1293, Test Loss: 321.3497\n",
      "Epoch time:  19.11698269844055\n",
      "Epoch [225/500], Train Loss: 322.8513, Test Loss: 313.6171\n",
      "Epoch time:  19.079914093017578\n",
      "Epoch [226/500], Train Loss: 311.0952, Test Loss: 312.1700\n",
      "Epoch time:  19.10334587097168\n",
      "Epoch [227/500], Train Loss: 306.7768, Test Loss: 304.4434\n",
      "Epoch time:  19.115325689315796\n",
      "Epoch [228/500], Train Loss: 308.7298, Test Loss: 310.1870\n",
      "Epoch time:  19.142953395843506\n",
      "Epoch [229/500], Train Loss: 313.6829, Test Loss: 311.6390\n",
      "Epoch time:  19.11928939819336\n",
      "Epoch [230/500], Train Loss: 311.5412, Test Loss: 300.0732\n",
      "Epoch time:  19.07086443901062\n",
      "Epoch [231/500], Train Loss: 299.4313, Test Loss: 303.6321\n",
      "Epoch time:  19.100549697875977\n",
      "Epoch [232/500], Train Loss: 301.2199, Test Loss: 312.0423\n",
      "Epoch time:  19.119782209396362\n",
      "Epoch [233/500], Train Loss: 305.3804, Test Loss: 307.3330\n",
      "Epoch time:  19.127886056900024\n",
      "Epoch [234/500], Train Loss: 304.0279, Test Loss: 310.1030\n",
      "Epoch time:  19.10742211341858\n",
      "Epoch [235/500], Train Loss: 300.8369, Test Loss: 301.3694\n",
      "Epoch time:  19.09715962409973\n",
      "Epoch [236/500], Train Loss: 299.9834, Test Loss: 305.5408\n",
      "Epoch time:  19.12049150466919\n",
      "Epoch [237/500], Train Loss: 301.9010, Test Loss: 311.1932\n",
      "Epoch time:  19.13381814956665\n",
      "Epoch [238/500], Train Loss: 305.2439, Test Loss: 314.8228\n",
      "Epoch time:  19.116777420043945\n",
      "Epoch [239/500], Train Loss: 313.6527, Test Loss: 316.0000\n",
      "Epoch time:  19.07487201690674\n",
      "Epoch [240/500], Train Loss: 306.9226, Test Loss: 318.1248\n",
      "Epoch time:  19.09675359725952\n",
      "Epoch [241/500], Train Loss: 307.5551, Test Loss: 300.7001\n",
      "Epoch time:  19.082425117492676\n",
      "Epoch [242/500], Train Loss: 298.8225, Test Loss: 299.1798\n",
      "Epoch time:  19.14363956451416\n",
      "Epoch [243/500], Train Loss: 296.8330, Test Loss: 304.4101\n",
      "Epoch time:  19.122087001800537\n",
      "Epoch [244/500], Train Loss: 300.5156, Test Loss: 303.7545\n",
      "Epoch time:  19.09343981742859\n",
      "Epoch [245/500], Train Loss: 305.6119, Test Loss: 319.1848\n",
      "Epoch time:  19.088730573654175\n",
      "Epoch [246/500], Train Loss: 311.5483, Test Loss: 309.3045\n",
      "Epoch time:  19.09701681137085\n",
      "Epoch [247/500], Train Loss: 302.4996, Test Loss: 316.9994\n",
      "Epoch time:  19.109584093093872\n",
      "Epoch [248/500], Train Loss: 303.0051, Test Loss: 316.4835\n",
      "Epoch time:  19.10780692100525\n",
      "Epoch [249/500], Train Loss: 300.7230, Test Loss: 306.3904\n",
      "Epoch time:  19.089401483535767\n",
      "Epoch [250/500], Train Loss: 300.6060, Test Loss: 293.5809\n",
      "Epoch time:  19.093453884124756\n",
      "Epoch [251/500], Train Loss: 301.9717, Test Loss: 295.1746\n",
      "Epoch time:  19.106871604919434\n",
      "Epoch [252/500], Train Loss: 293.8426, Test Loss: 289.6956\n",
      "Epoch time:  19.133833646774292\n",
      "Epoch [253/500], Train Loss: 286.5432, Test Loss: 299.7828\n",
      "Epoch time:  19.12397313117981\n",
      "Epoch [254/500], Train Loss: 289.6593, Test Loss: 296.7747\n",
      "Epoch time:  19.102272748947144\n",
      "Epoch [255/500], Train Loss: 292.0430, Test Loss: 289.3722\n",
      "Epoch time:  19.101924180984497\n",
      "Epoch [256/500], Train Loss: 300.5065, Test Loss: 297.6629\n",
      "Epoch time:  19.114598035812378\n",
      "Epoch [257/500], Train Loss: 290.1104, Test Loss: 288.0539\n",
      "Epoch time:  19.09905505180359\n",
      "Epoch [258/500], Train Loss: 287.7391, Test Loss: 300.0450\n",
      "Epoch time:  19.125452280044556\n",
      "Epoch [259/500], Train Loss: 291.8407, Test Loss: 305.3945\n",
      "Epoch time:  19.098935842514038\n",
      "Epoch [260/500], Train Loss: 299.6173, Test Loss: 295.9600\n",
      "Epoch time:  19.090234756469727\n",
      "Epoch [261/500], Train Loss: 286.8519, Test Loss: 293.4849\n",
      "Epoch time:  19.1119704246521\n",
      "Epoch [262/500], Train Loss: 285.4992, Test Loss: 283.7656\n",
      "Epoch time:  19.129068613052368\n",
      "Epoch [263/500], Train Loss: 289.5492, Test Loss: 298.3985\n",
      "Epoch time:  19.090317249298096\n",
      "Epoch [264/500], Train Loss: 292.6523, Test Loss: 297.3970\n",
      "Epoch time:  19.10102677345276\n",
      "Epoch [265/500], Train Loss: 293.0641, Test Loss: 299.7347\n",
      "Epoch time:  19.094237327575684\n",
      "Epoch [266/500], Train Loss: 290.9417, Test Loss: 287.5509\n",
      "Epoch time:  19.11693787574768\n",
      "Epoch [267/500], Train Loss: 283.7104, Test Loss: 291.5428\n",
      "Epoch time:  19.13690972328186\n",
      "Epoch [268/500], Train Loss: 286.9246, Test Loss: 281.5071\n",
      "Epoch time:  19.13557267189026\n",
      "Epoch [269/500], Train Loss: 283.3400, Test Loss: 285.7350\n",
      "Epoch time:  19.08968758583069\n",
      "Epoch [270/500], Train Loss: 287.1056, Test Loss: 298.8960\n",
      "Epoch time:  19.142377138137817\n",
      "Epoch [271/500], Train Loss: 286.7268, Test Loss: 283.3193\n",
      "Epoch time:  19.10534358024597\n",
      "Epoch [272/500], Train Loss: 282.7733, Test Loss: 279.5491\n",
      "Epoch time:  19.129499435424805\n",
      "Epoch [273/500], Train Loss: 284.1768, Test Loss: 281.9401\n",
      "Epoch time:  19.111531019210815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [274/500], Train Loss: 290.2255, Test Loss: 300.1674\n",
      "Epoch time:  19.103816986083984\n",
      "Epoch [275/500], Train Loss: 289.3428, Test Loss: 285.0937\n",
      "Epoch time:  19.11132311820984\n",
      "Epoch [276/500], Train Loss: 283.4250, Test Loss: 289.5984\n",
      "Epoch time:  19.11005711555481\n",
      "Epoch [277/500], Train Loss: 288.9496, Test Loss: 294.3730\n",
      "Epoch time:  19.099936962127686\n",
      "Epoch [278/500], Train Loss: 286.6528, Test Loss: 279.0203\n",
      "Epoch time:  19.09408950805664\n",
      "Epoch [279/500], Train Loss: 278.0010, Test Loss: 285.6666\n",
      "Epoch time:  19.104430675506592\n",
      "Epoch [280/500], Train Loss: 278.0589, Test Loss: 278.8506\n",
      "Epoch time:  19.13499116897583\n",
      "Epoch [281/500], Train Loss: 275.0831, Test Loss: 287.7968\n",
      "Epoch time:  19.121995210647583\n",
      "Epoch [282/500], Train Loss: 277.2161, Test Loss: 287.2363\n",
      "Epoch time:  19.093090295791626\n",
      "Epoch [283/500], Train Loss: 284.7369, Test Loss: 283.3997\n",
      "Epoch time:  19.112690448760986\n",
      "Epoch [284/500], Train Loss: 282.0594, Test Loss: 281.2896\n",
      "Epoch time:  19.121855974197388\n",
      "Epoch [285/500], Train Loss: 281.9521, Test Loss: 280.5304\n",
      "Epoch time:  19.111167669296265\n",
      "Epoch [286/500], Train Loss: 280.2921, Test Loss: 283.0060\n",
      "Epoch time:  19.097935438156128\n",
      "Epoch [287/500], Train Loss: 282.7067, Test Loss: 280.0850\n",
      "Epoch time:  19.09765911102295\n",
      "Epoch [288/500], Train Loss: 282.5734, Test Loss: 280.6146\n",
      "Epoch time:  19.09401249885559\n",
      "Epoch [289/500], Train Loss: 279.5257, Test Loss: 279.7728\n",
      "Epoch time:  19.143101930618286\n",
      "Epoch [290/500], Train Loss: 281.0369, Test Loss: 275.9376\n",
      "Epoch time:  19.09586191177368\n",
      "Epoch [291/500], Train Loss: 281.1293, Test Loss: 279.7762\n",
      "Epoch time:  19.096508264541626\n",
      "Epoch [292/500], Train Loss: 284.4363, Test Loss: 293.4662\n",
      "Epoch time:  19.09851861000061\n",
      "Epoch [293/500], Train Loss: 283.6183, Test Loss: 284.3631\n",
      "Epoch time:  19.110692977905273\n",
      "Epoch [294/500], Train Loss: 280.4905, Test Loss: 273.6859\n",
      "Epoch time:  19.104523181915283\n",
      "Epoch [295/500], Train Loss: 275.3564, Test Loss: 275.9567\n",
      "Epoch time:  19.117574214935303\n",
      "Epoch [296/500], Train Loss: 275.6167, Test Loss: 277.3123\n",
      "Epoch time:  19.134244680404663\n",
      "Epoch [297/500], Train Loss: 276.5897, Test Loss: 274.3071\n",
      "Epoch time:  19.10468363761902\n",
      "Epoch [298/500], Train Loss: 272.8592, Test Loss: 271.9542\n",
      "Epoch time:  19.140055656433105\n",
      "Epoch [299/500], Train Loss: 270.3352, Test Loss: 269.3316\n",
      "Epoch time:  19.12352991104126\n",
      "Epoch [300/500], Train Loss: 278.6125, Test Loss: 284.4924\n",
      "Epoch time:  19.130550622940063\n",
      "Epoch [301/500], Train Loss: 284.1071, Test Loss: 289.9134\n",
      "Epoch time:  19.088735342025757\n",
      "Epoch [302/500], Train Loss: 281.6182, Test Loss: 277.6513\n",
      "Epoch time:  19.09916114807129\n",
      "Epoch [303/500], Train Loss: 276.4841, Test Loss: 274.5772\n",
      "Epoch time:  19.1153404712677\n",
      "Epoch [304/500], Train Loss: 279.6790, Test Loss: 283.0908\n",
      "Epoch time:  19.099077939987183\n",
      "Epoch [305/500], Train Loss: 276.8628, Test Loss: 278.6450\n",
      "Epoch time:  19.08261466026306\n",
      "Epoch [306/500], Train Loss: 271.9856, Test Loss: 283.5893\n",
      "Epoch time:  19.08848214149475\n",
      "Epoch [307/500], Train Loss: 275.6117, Test Loss: 272.2357\n",
      "Epoch time:  19.09413456916809\n",
      "Epoch [308/500], Train Loss: 278.8115, Test Loss: 281.2838\n",
      "Epoch time:  19.121309995651245\n",
      "Epoch [309/500], Train Loss: 272.4808, Test Loss: 268.5695\n",
      "Epoch time:  19.10559582710266\n",
      "Epoch [310/500], Train Loss: 274.4541, Test Loss: 279.3108\n",
      "Epoch time:  19.099231481552124\n",
      "Epoch [311/500], Train Loss: 271.9704, Test Loss: 271.0693\n",
      "Epoch time:  19.124683618545532\n",
      "Epoch [312/500], Train Loss: 273.8411, Test Loss: 283.1280\n",
      "Epoch time:  19.115262031555176\n",
      "Epoch [313/500], Train Loss: 277.7109, Test Loss: 285.5701\n",
      "Epoch time:  19.117003440856934\n",
      "Epoch [314/500], Train Loss: 279.5309, Test Loss: 289.1654\n",
      "Epoch time:  19.11791467666626\n",
      "Epoch [315/500], Train Loss: 281.7953, Test Loss: 286.6760\n",
      "Epoch time:  19.087416410446167\n",
      "Epoch [316/500], Train Loss: 279.5694, Test Loss: 271.1025\n",
      "Epoch time:  19.08279323577881\n",
      "Epoch [317/500], Train Loss: 275.6499, Test Loss: 277.0954\n",
      "Epoch time:  19.11278486251831\n",
      "Epoch [318/500], Train Loss: 279.7146, Test Loss: 272.6069\n",
      "Epoch time:  19.075886249542236\n",
      "Epoch [319/500], Train Loss: 272.7052, Test Loss: 275.6193\n",
      "Epoch time:  19.12628936767578\n",
      "Epoch [320/500], Train Loss: 276.9595, Test Loss: 268.0785\n",
      "Epoch time:  19.102659225463867\n",
      "Epoch [321/500], Train Loss: 271.7137, Test Loss: 271.8785\n",
      "Epoch time:  19.123311042785645\n",
      "Epoch [322/500], Train Loss: 273.2940, Test Loss: 273.2225\n",
      "Epoch time:  19.104429006576538\n",
      "Epoch [323/500], Train Loss: 268.5357, Test Loss: 270.0320\n",
      "Epoch time:  19.108977556228638\n",
      "Epoch [324/500], Train Loss: 271.0849, Test Loss: 275.7914\n",
      "Epoch time:  19.09587264060974\n",
      "Epoch [325/500], Train Loss: 279.3321, Test Loss: 282.2322\n",
      "Epoch time:  19.09383988380432\n",
      "Epoch [326/500], Train Loss: 274.2196, Test Loss: 272.6137\n",
      "Epoch time:  19.12502694129944\n",
      "Epoch [327/500], Train Loss: 270.8821, Test Loss: 267.7883\n",
      "Epoch time:  19.13260006904602\n",
      "Epoch [328/500], Train Loss: 271.8335, Test Loss: 272.0130\n",
      "Epoch time:  19.102921962738037\n",
      "Epoch [329/500], Train Loss: 270.4730, Test Loss: 281.2164\n",
      "Epoch time:  19.109533548355103\n",
      "Epoch [330/500], Train Loss: 275.9947, Test Loss: 284.7041\n",
      "Epoch time:  19.10119390487671\n",
      "Epoch [331/500], Train Loss: 276.3816, Test Loss: 283.7029\n",
      "Epoch time:  19.109697103500366\n",
      "Epoch [332/500], Train Loss: 278.8912, Test Loss: 277.6583\n",
      "Epoch time:  19.094351768493652\n",
      "Epoch [333/500], Train Loss: 273.0551, Test Loss: 280.1939\n",
      "Epoch time:  19.09631657600403\n",
      "Epoch [334/500], Train Loss: 272.9293, Test Loss: 283.4204\n",
      "Epoch time:  19.107893466949463\n",
      "Epoch [335/500], Train Loss: 272.6428, Test Loss: 273.7597\n",
      "Epoch time:  19.106370449066162\n",
      "Epoch [336/500], Train Loss: 269.9455, Test Loss: 277.0743\n",
      "Epoch time:  19.14022707939148\n",
      "Epoch [337/500], Train Loss: 271.3939, Test Loss: 285.2406\n",
      "Epoch time:  19.11814570426941\n",
      "Epoch [338/500], Train Loss: 276.2124, Test Loss: 276.5587\n",
      "Epoch time:  19.122949600219727\n",
      "Epoch [339/500], Train Loss: 271.3601, Test Loss: 272.1767\n",
      "Epoch time:  19.093220233917236\n",
      "Epoch [340/500], Train Loss: 260.0024, Test Loss: 266.3763\n",
      "Epoch time:  19.131546020507812\n",
      "Epoch [341/500], Train Loss: 263.8822, Test Loss: 272.4926\n",
      "Epoch time:  19.137696981430054\n",
      "Epoch [342/500], Train Loss: 269.3246, Test Loss: 264.7780\n",
      "Epoch time:  19.089909315109253\n",
      "Epoch [343/500], Train Loss: 259.6959, Test Loss: 262.2528\n",
      "Epoch time:  19.10497736930847\n",
      "Epoch [344/500], Train Loss: 259.5637, Test Loss: 266.6594\n",
      "Epoch time:  19.133728504180908\n",
      "Epoch [345/500], Train Loss: 263.5913, Test Loss: 264.6287\n",
      "Epoch time:  19.120018482208252\n",
      "Epoch [346/500], Train Loss: 263.6830, Test Loss: 266.3281\n",
      "Epoch time:  19.108583211898804\n",
      "Epoch [347/500], Train Loss: 264.5899, Test Loss: 279.0807\n",
      "Epoch time:  19.118151426315308\n",
      "Epoch [348/500], Train Loss: 274.3116, Test Loss: 259.2464\n",
      "Epoch time:  19.086823225021362\n",
      "Epoch [349/500], Train Loss: 264.7449, Test Loss: 267.8886\n",
      "Epoch time:  19.108710527420044\n",
      "Epoch [350/500], Train Loss: 270.8946, Test Loss: 268.7460\n",
      "Epoch time:  19.115143060684204\n",
      "Epoch [351/500], Train Loss: 269.4615, Test Loss: 269.4727\n",
      "Epoch time:  19.114975452423096\n",
      "Epoch [352/500], Train Loss: 271.9849, Test Loss: 271.4350\n",
      "Epoch time:  19.108977794647217\n",
      "Epoch [353/500], Train Loss: 268.0093, Test Loss: 263.1029\n",
      "Epoch time:  19.107258319854736\n",
      "Epoch [354/500], Train Loss: 269.3304, Test Loss: 270.4050\n",
      "Epoch time:  19.120970726013184\n",
      "Epoch [355/500], Train Loss: 273.8257, Test Loss: 272.5472\n",
      "Epoch time:  19.111405849456787\n",
      "Epoch [356/500], Train Loss: 269.2543, Test Loss: 265.3051\n",
      "Epoch time:  19.131582975387573\n",
      "Epoch [357/500], Train Loss: 263.9015, Test Loss: 274.4447\n",
      "Epoch time:  19.10249924659729\n",
      "Epoch [358/500], Train Loss: 267.4047, Test Loss: 269.0717\n",
      "Epoch time:  19.105950593948364\n",
      "Epoch [359/500], Train Loss: 268.4037, Test Loss: 271.6726\n",
      "Epoch time:  19.124738216400146\n",
      "Epoch [360/500], Train Loss: 268.8239, Test Loss: 264.5895\n",
      "Epoch time:  19.100786447525024\n",
      "Epoch [361/500], Train Loss: 266.4547, Test Loss: 263.9298\n",
      "Epoch time:  19.13191509246826\n",
      "Epoch [362/500], Train Loss: 259.2391, Test Loss: 267.4813\n",
      "Epoch time:  19.12690782546997\n",
      "Epoch [363/500], Train Loss: 264.1158, Test Loss: 261.1729\n",
      "Epoch time:  19.10181474685669\n",
      "Epoch [364/500], Train Loss: 259.5550, Test Loss: 265.1613\n",
      "Epoch time:  19.14087963104248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [365/500], Train Loss: 269.4400, Test Loss: 266.1531\n",
      "Epoch time:  19.129394054412842\n",
      "Epoch [366/500], Train Loss: 262.3281, Test Loss: 261.5785\n",
      "Epoch time:  19.121861934661865\n",
      "Epoch [367/500], Train Loss: 267.9242, Test Loss: 263.4360\n",
      "Epoch time:  19.117942333221436\n",
      "Epoch [368/500], Train Loss: 260.6297, Test Loss: 265.1284\n",
      "Epoch time:  19.131232738494873\n",
      "Epoch [369/500], Train Loss: 264.0387, Test Loss: 266.2937\n",
      "Epoch time:  19.103527784347534\n",
      "Epoch [370/500], Train Loss: 267.7764, Test Loss: 267.6665\n",
      "Epoch time:  19.117979764938354\n",
      "Epoch [371/500], Train Loss: 261.4468, Test Loss: 257.9885\n",
      "Epoch time:  19.093849658966064\n",
      "Epoch [372/500], Train Loss: 267.0983, Test Loss: 262.4952\n",
      "Epoch time:  19.098336696624756\n",
      "Epoch [373/500], Train Loss: 275.4996, Test Loss: 262.3198\n",
      "Epoch time:  19.11436676979065\n",
      "Epoch [374/500], Train Loss: 261.0557, Test Loss: 267.0385\n",
      "Epoch time:  19.1365487575531\n",
      "Epoch [375/500], Train Loss: 260.8467, Test Loss: 276.4383\n",
      "Epoch time:  19.09872579574585\n",
      "Epoch [376/500], Train Loss: 262.3154, Test Loss: 265.4460\n",
      "Epoch time:  19.09882164001465\n",
      "Epoch [377/500], Train Loss: 265.9125, Test Loss: 275.7220\n",
      "Epoch time:  19.105536222457886\n",
      "Epoch [378/500], Train Loss: 262.5444, Test Loss: 264.4403\n",
      "Epoch time:  19.108065605163574\n",
      "Epoch [379/500], Train Loss: 262.0022, Test Loss: 266.0953\n",
      "Epoch time:  19.12720251083374\n",
      "Epoch [380/500], Train Loss: 262.7244, Test Loss: 273.2974\n",
      "Epoch time:  19.10276508331299\n",
      "Epoch [381/500], Train Loss: 266.1441, Test Loss: 256.1165\n",
      "Epoch time:  19.100515365600586\n",
      "Epoch [382/500], Train Loss: 261.5069, Test Loss: 261.8012\n",
      "Epoch time:  19.12107276916504\n",
      "Epoch [383/500], Train Loss: 260.8096, Test Loss: 262.8004\n",
      "Epoch time:  19.10855770111084\n",
      "Epoch [384/500], Train Loss: 258.4311, Test Loss: 255.3513\n",
      "Epoch time:  19.097899675369263\n",
      "Epoch [385/500], Train Loss: 259.4004, Test Loss: 267.7206\n",
      "Epoch time:  19.1062753200531\n",
      "Epoch [386/500], Train Loss: 270.9481, Test Loss: 276.3776\n",
      "Epoch time:  19.073832035064697\n",
      "Epoch [387/500], Train Loss: 264.8244, Test Loss: 265.9693\n",
      "Epoch time:  19.117249965667725\n",
      "Epoch [388/500], Train Loss: 262.6080, Test Loss: 264.1408\n",
      "Epoch time:  19.09107208251953\n",
      "Epoch [389/500], Train Loss: 258.6880, Test Loss: 255.0812\n",
      "Epoch time:  19.132452726364136\n",
      "Epoch [390/500], Train Loss: 264.4640, Test Loss: 268.8323\n",
      "Epoch time:  19.072439908981323\n",
      "Epoch [391/500], Train Loss: 262.2201, Test Loss: 259.0008\n",
      "Epoch time:  19.085550546646118\n",
      "Epoch [392/500], Train Loss: 259.5998, Test Loss: 259.4095\n",
      "Epoch time:  19.13928747177124\n",
      "Epoch [393/500], Train Loss: 257.1547, Test Loss: 257.3703\n",
      "Epoch time:  19.129352807998657\n",
      "Epoch [394/500], Train Loss: 256.6685, Test Loss: 262.3553\n",
      "Epoch time:  19.09733772277832\n",
      "Epoch [395/500], Train Loss: 264.5198, Test Loss: 258.7169\n",
      "Epoch time:  19.105574131011963\n",
      "Epoch [396/500], Train Loss: 256.6535, Test Loss: 258.4621\n",
      "Epoch time:  19.138614177703857\n",
      "Epoch [397/500], Train Loss: 258.6227, Test Loss: 265.4037\n",
      "Epoch time:  19.100587129592896\n",
      "Epoch [398/500], Train Loss: 262.2602, Test Loss: 263.3211\n",
      "Epoch time:  19.100966453552246\n",
      "Epoch [399/500], Train Loss: 260.2987, Test Loss: 258.6297\n",
      "Epoch time:  19.09534764289856\n",
      "Epoch [400/500], Train Loss: 256.8171, Test Loss: 262.4803\n",
      "Epoch time:  19.0881564617157\n",
      "Epoch [401/500], Train Loss: 257.0533, Test Loss: 260.0577\n",
      "Epoch time:  19.15093469619751\n",
      "Epoch [402/500], Train Loss: 259.8789, Test Loss: 272.0794\n",
      "Epoch time:  19.113821268081665\n",
      "Epoch [403/500], Train Loss: 268.3582, Test Loss: 270.4737\n",
      "Epoch time:  19.100224494934082\n",
      "Epoch [404/500], Train Loss: 262.4382, Test Loss: 262.7714\n",
      "Epoch time:  19.091975450515747\n",
      "Epoch [405/500], Train Loss: 261.7322, Test Loss: 289.7685\n",
      "Epoch time:  19.10360622406006\n",
      "Epoch [406/500], Train Loss: 266.0243, Test Loss: 255.6162\n",
      "Epoch time:  19.089315176010132\n",
      "Epoch [407/500], Train Loss: 258.8966, Test Loss: 259.2371\n",
      "Epoch time:  19.102733373641968\n",
      "Epoch [408/500], Train Loss: 265.8752, Test Loss: 274.3158\n",
      "Epoch time:  19.105875730514526\n",
      "Epoch [409/500], Train Loss: 265.6047, Test Loss: 273.4067\n",
      "Epoch time:  19.095670461654663\n",
      "Epoch [410/500], Train Loss: 265.2642, Test Loss: 276.2948\n",
      "Epoch time:  19.117177724838257\n",
      "Epoch [411/500], Train Loss: 269.7429, Test Loss: 262.6982\n",
      "Epoch time:  19.099170684814453\n",
      "Epoch [412/500], Train Loss: 262.6311, Test Loss: 262.3680\n",
      "Epoch time:  19.08902907371521\n",
      "Epoch [413/500], Train Loss: 258.3845, Test Loss: 255.2340\n",
      "Epoch time:  19.100672245025635\n",
      "Epoch [414/500], Train Loss: 262.0906, Test Loss: 269.0784\n",
      "Epoch time:  19.115437507629395\n",
      "Epoch [415/500], Train Loss: 267.2181, Test Loss: 266.1290\n",
      "Epoch time:  19.111116647720337\n",
      "Epoch [416/500], Train Loss: 267.2434, Test Loss: 279.9888\n",
      "Epoch time:  19.074138164520264\n",
      "Epoch [417/500], Train Loss: 265.6532, Test Loss: 265.8399\n",
      "Epoch time:  19.081692695617676\n",
      "Epoch [418/500], Train Loss: 257.4733, Test Loss: 254.8655\n",
      "Epoch time:  19.119145393371582\n",
      "Epoch [419/500], Train Loss: 253.6948, Test Loss: 255.3510\n",
      "Epoch time:  19.129662036895752\n",
      "Epoch [420/500], Train Loss: 258.9348, Test Loss: 265.4001\n",
      "Epoch time:  19.13798213005066\n",
      "Epoch [421/500], Train Loss: 262.5772, Test Loss: 259.0314\n",
      "Epoch time:  19.099748611450195\n",
      "Epoch [422/500], Train Loss: 255.4181, Test Loss: 253.7430\n",
      "Epoch time:  19.13638734817505\n",
      "Epoch [423/500], Train Loss: 254.2566, Test Loss: 256.5823\n",
      "Epoch time:  19.13127040863037\n",
      "Epoch [424/500], Train Loss: 257.1780, Test Loss: 263.4334\n",
      "Epoch time:  19.138263702392578\n",
      "Epoch [425/500], Train Loss: 262.4218, Test Loss: 266.8500\n",
      "Epoch time:  19.10010075569153\n",
      "Epoch [426/500], Train Loss: 256.3875, Test Loss: 257.7089\n",
      "Epoch time:  19.10955834388733\n",
      "Epoch [427/500], Train Loss: 257.0197, Test Loss: 257.0748\n",
      "Epoch time:  19.082931756973267\n",
      "Epoch [428/500], Train Loss: 259.5036, Test Loss: 266.2255\n",
      "Epoch time:  19.112906217575073\n",
      "Epoch [429/500], Train Loss: 259.1127, Test Loss: 257.7918\n",
      "Epoch time:  19.121198654174805\n",
      "Epoch [430/500], Train Loss: 249.9713, Test Loss: 249.1589\n",
      "Epoch time:  19.112375020980835\n",
      "Epoch [431/500], Train Loss: 255.2479, Test Loss: 259.2556\n",
      "Epoch time:  19.0947322845459\n",
      "Epoch [432/500], Train Loss: 255.9367, Test Loss: 268.9241\n",
      "Epoch time:  19.13172721862793\n",
      "Epoch [433/500], Train Loss: 256.3981, Test Loss: 263.8199\n",
      "Epoch time:  19.10437560081482\n",
      "Epoch [434/500], Train Loss: 255.2092, Test Loss: 256.0999\n",
      "Epoch time:  19.119311094284058\n",
      "Epoch [435/500], Train Loss: 260.5994, Test Loss: 265.7325\n",
      "Epoch time:  9.028329133987427\n",
      "Epoch [436/500], Train Loss: 265.6813, Test Loss: 265.9980\n",
      "Epoch time:  8.822808265686035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m save_path \u001b[38;5;241m=\u001b[39m ModelName \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Params.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_losses, test_losses, is_model_trained \u001b[38;5;241m=\u001b[39m train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Finish timing cell run time\u001b[39;00m\n\u001b[1;32m     24\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[9], line 101\u001b[0m, in \u001b[0;36mtrain_or_load_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pretrained model found. Training from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#optimizer = optim.Adam(model.parameters())  \u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n\u001b[1;32m    102\u001b[0m is_model_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set flag to True after training\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Save losses per epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mtrain_and_save_best_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path, accumulation_steps)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Scale the loss to account for the accumulation steps\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[0;32m---> 31\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Scale the loss and perform backpropagation\u001b[39;00m\n\u001b[1;32m     33\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i].cpu().numpy()\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i].unsqueeze(0).unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(HighDynamicRange2Conc[i].min().item(),4), \" - Mean Value:\", np.round(HighDynamicRange2Conc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on a dataset with singlets added at random\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred = model_aq(OtherValSpectra[0].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 1s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
