{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 500\n",
    "\n",
    "# Identification part of the filenames\n",
    "model_base_name = '20000spec_RAE_ExtendedRange_MoreLeftOut_Combined1Distribution_AMPtest_TestMostComplexArchitecture'\n",
    "base_name = 'ExtendedRange_MoreLeftOut_Combined1Distribution'    # This is the dataset base name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"CNN_44met_{model_base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load(f'Dataset44_{base_name}_ForManuscript_Spec.npy')\n",
    "conc1 = np.load(f'Dataset44_{base_name}_ForManuscript_Conc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in datasets]\n",
    "test_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in Test_datasets]\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset_reshaped, batch_size = 128, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset_reshaped, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NMR_Model_Aq, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 44, kernel_size=12, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(44, 44, kernel_size=12, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(44, 44, kernel_size=12, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(44, 44, kernel_size=12, padding=1)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(126192, 200)\n",
    "        self.fc2 = nn.Linear(200, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)                  \n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path, accumulation_steps=4):\n",
    "    criterion = RelativeAbsoluteError()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5.287243368897864e-05, weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in validation loss constitutes early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # For timing cell run time\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        ## Training phase\n",
    "        # Instantiate the GradScaler\n",
    "        scaler = GradScaler()\n",
    "        optimizer.zero_grad()  # Only zero gradients here at the start of an epoch\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Enable autocasting for forward and backward passes\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Scale the loss to account for the accumulation steps\n",
    "                loss = loss / accumulation_steps\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Scale the loss and perform backpropagation\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                # Step the optimizer and update the scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()  # Zero gradients after accumulation_steps\n",
    "\n",
    "        # Testing phase\n",
    "        train_losses.append(train_loss)\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                # Move data to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Enable autocasting for forward passes\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "        \n",
    "        \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            #torch.save({\n",
    "            #    'model_state_dict': model.state_dict(),\n",
    "            #    'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #}, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Epoch time: \",end-start)\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/500], Train Loss: 2565.1322, Test Loss: 2569.6918\n",
      "Epoch time:  12.75424575805664\n",
      "Epoch [2/500], Train Loss: 2562.8196, Test Loss: 2569.2752\n",
      "Epoch time:  11.319423913955688\n",
      "Epoch [3/500], Train Loss: 2560.5660, Test Loss: 2563.3835\n",
      "Epoch time:  11.3486647605896\n",
      "Epoch [4/500], Train Loss: 2545.0403, Test Loss: 2527.4833\n",
      "Epoch time:  11.374839305877686\n",
      "Epoch [5/500], Train Loss: 2477.3375, Test Loss: 2393.2477\n",
      "Epoch time:  11.373445987701416\n",
      "Epoch [6/500], Train Loss: 2291.6306, Test Loss: 2161.4004\n",
      "Epoch time:  11.821260452270508\n",
      "Epoch [7/500], Train Loss: 2132.3538, Test Loss: 2084.3563\n",
      "Epoch time:  11.412518739700317\n",
      "Epoch [8/500], Train Loss: 2079.6656, Test Loss: 2052.3380\n",
      "Epoch time:  11.380270004272461\n",
      "Epoch [9/500], Train Loss: 2061.6596, Test Loss: 2040.3071\n",
      "Epoch time:  14.41676664352417\n",
      "Epoch [10/500], Train Loss: 2048.0824, Test Loss: 2030.0057\n",
      "Epoch time:  24.27259373664856\n",
      "Epoch [11/500], Train Loss: 2034.4809, Test Loss: 2017.5998\n",
      "Epoch time:  24.284343242645264\n",
      "Epoch [12/500], Train Loss: 2028.5620, Test Loss: 2011.5954\n",
      "Epoch time:  24.25197982788086\n",
      "Epoch [13/500], Train Loss: 2016.6198, Test Loss: 1999.6330\n",
      "Epoch time:  24.306557178497314\n",
      "Epoch [14/500], Train Loss: 2001.6648, Test Loss: 1986.2632\n",
      "Epoch time:  24.283957481384277\n",
      "Epoch [15/500], Train Loss: 1988.0815, Test Loss: 1970.2688\n",
      "Epoch time:  24.293803691864014\n",
      "Epoch [16/500], Train Loss: 1968.6590, Test Loss: 1948.1976\n",
      "Epoch time:  24.280996322631836\n",
      "Epoch [17/500], Train Loss: 1944.6212, Test Loss: 1918.5184\n",
      "Epoch time:  24.305670738220215\n",
      "Epoch [18/500], Train Loss: 1911.1270, Test Loss: 1877.9730\n",
      "Epoch time:  24.29180073738098\n",
      "Epoch [19/500], Train Loss: 1867.4879, Test Loss: 1816.2612\n",
      "Epoch time:  24.273293018341064\n",
      "Epoch [20/500], Train Loss: 1800.3797, Test Loss: 1745.5798\n",
      "Epoch time:  24.291587352752686\n",
      "Epoch [21/500], Train Loss: 1711.8216, Test Loss: 1646.4641\n",
      "Epoch time:  24.313347578048706\n",
      "Epoch [22/500], Train Loss: 1598.2406, Test Loss: 1512.4532\n",
      "Epoch time:  24.336411237716675\n",
      "Epoch [23/500], Train Loss: 1457.0699, Test Loss: 1371.3401\n",
      "Epoch time:  24.306952953338623\n",
      "Epoch [24/500], Train Loss: 1300.4410, Test Loss: 1216.8767\n",
      "Epoch time:  24.308721780776978\n",
      "Epoch [25/500], Train Loss: 1151.1803, Test Loss: 1079.2273\n",
      "Epoch time:  24.323775053024292\n",
      "Epoch [26/500], Train Loss: 1025.3307, Test Loss: 975.8444\n",
      "Epoch time:  24.314932346343994\n",
      "Epoch [27/500], Train Loss: 926.6434, Test Loss: 882.4160\n",
      "Epoch time:  24.31173849105835\n",
      "Epoch [28/500], Train Loss: 836.0245, Test Loss: 798.0046\n",
      "Epoch time:  24.32724666595459\n",
      "Epoch [29/500], Train Loss: 769.1112, Test Loss: 748.5981\n",
      "Epoch time:  24.336132287979126\n",
      "Epoch [30/500], Train Loss: 718.8836, Test Loss: 699.4611\n",
      "Epoch time:  24.314363718032837\n",
      "Epoch [31/500], Train Loss: 679.1781, Test Loss: 660.1484\n",
      "Epoch time:  24.33266854286194\n",
      "Epoch [32/500], Train Loss: 642.9132, Test Loss: 625.1961\n",
      "Epoch time:  24.322741508483887\n",
      "Epoch [33/500], Train Loss: 613.2499, Test Loss: 597.5821\n",
      "Epoch time:  24.32706379890442\n",
      "Epoch [34/500], Train Loss: 589.6998, Test Loss: 585.5476\n",
      "Epoch time:  24.289588451385498\n",
      "Epoch [35/500], Train Loss: 569.2253, Test Loss: 560.9605\n",
      "Epoch time:  24.335474729537964\n",
      "Epoch [36/500], Train Loss: 555.2686, Test Loss: 546.8835\n",
      "Epoch time:  24.325757026672363\n",
      "Epoch [37/500], Train Loss: 537.3830, Test Loss: 537.0369\n",
      "Epoch time:  24.31366753578186\n",
      "Epoch [38/500], Train Loss: 521.4565, Test Loss: 515.1170\n",
      "Epoch time:  24.31440234184265\n",
      "Epoch [39/500], Train Loss: 503.2216, Test Loss: 503.6671\n",
      "Epoch time:  24.322928428649902\n",
      "Epoch [40/500], Train Loss: 490.8103, Test Loss: 485.6127\n",
      "Epoch time:  24.34795069694519\n",
      "Epoch [41/500], Train Loss: 481.7684, Test Loss: 470.4037\n",
      "Epoch time:  24.305960178375244\n",
      "Epoch [42/500], Train Loss: 468.4592, Test Loss: 469.0302\n",
      "Epoch time:  24.33422613143921\n",
      "Epoch [43/500], Train Loss: 461.2246, Test Loss: 452.5369\n",
      "Epoch time:  24.336034536361694\n",
      "Epoch [44/500], Train Loss: 452.7073, Test Loss: 463.6548\n",
      "Epoch time:  24.32686710357666\n",
      "Epoch [45/500], Train Loss: 440.4921, Test Loss: 436.3688\n",
      "Epoch time:  24.316017866134644\n",
      "Epoch [46/500], Train Loss: 433.2507, Test Loss: 430.6492\n",
      "Epoch time:  24.34805178642273\n",
      "Epoch [47/500], Train Loss: 423.5431, Test Loss: 418.3686\n",
      "Epoch time:  24.347929000854492\n",
      "Epoch [48/500], Train Loss: 416.8816, Test Loss: 415.1792\n",
      "Epoch time:  24.349710702896118\n",
      "Epoch [49/500], Train Loss: 414.6421, Test Loss: 404.4545\n",
      "Epoch time:  24.32020139694214\n",
      "Epoch [50/500], Train Loss: 405.4056, Test Loss: 401.1839\n",
      "Epoch time:  24.337576866149902\n",
      "Epoch [51/500], Train Loss: 396.1534, Test Loss: 387.6732\n",
      "Epoch time:  24.338589906692505\n",
      "Epoch [52/500], Train Loss: 392.9215, Test Loss: 388.3890\n",
      "Epoch time:  24.3097722530365\n",
      "Epoch [53/500], Train Loss: 391.6781, Test Loss: 392.8036\n",
      "Epoch time:  24.33038020133972\n",
      "Epoch [54/500], Train Loss: 384.3603, Test Loss: 380.5454\n",
      "Epoch time:  24.34005379676819\n",
      "Epoch [55/500], Train Loss: 374.8195, Test Loss: 374.4350\n",
      "Epoch time:  24.332838773727417\n",
      "Epoch [56/500], Train Loss: 369.4015, Test Loss: 363.2729\n",
      "Epoch time:  24.297454357147217\n",
      "Epoch [57/500], Train Loss: 369.1041, Test Loss: 368.8266\n",
      "Epoch time:  24.32216787338257\n",
      "Epoch [58/500], Train Loss: 365.1606, Test Loss: 357.1930\n",
      "Epoch time:  24.327523708343506\n",
      "Epoch [59/500], Train Loss: 359.2594, Test Loss: 359.1833\n",
      "Epoch time:  24.339205026626587\n",
      "Epoch [60/500], Train Loss: 352.5845, Test Loss: 373.3365\n",
      "Epoch time:  24.295852661132812\n",
      "Epoch [61/500], Train Loss: 359.4551, Test Loss: 348.5608\n",
      "Epoch time:  24.328768014907837\n",
      "Epoch [62/500], Train Loss: 352.4391, Test Loss: 343.5559\n",
      "Epoch time:  24.325571537017822\n",
      "Epoch [63/500], Train Loss: 340.4506, Test Loss: 335.8192\n",
      "Epoch time:  24.303927898406982\n",
      "Epoch [64/500], Train Loss: 334.3411, Test Loss: 329.9570\n",
      "Epoch time:  24.31639266014099\n",
      "Epoch [65/500], Train Loss: 331.0331, Test Loss: 325.8489\n",
      "Epoch time:  24.321542739868164\n",
      "Epoch [66/500], Train Loss: 330.3774, Test Loss: 332.9702\n",
      "Epoch time:  24.339914083480835\n",
      "Epoch [67/500], Train Loss: 328.0110, Test Loss: 325.8640\n",
      "Epoch time:  24.288124561309814\n",
      "Epoch [68/500], Train Loss: 322.2197, Test Loss: 315.5387\n",
      "Epoch time:  24.333228826522827\n",
      "Epoch [69/500], Train Loss: 313.7179, Test Loss: 315.4260\n",
      "Epoch time:  24.335389852523804\n",
      "Epoch [70/500], Train Loss: 312.0586, Test Loss: 311.9562\n",
      "Epoch time:  24.317899227142334\n",
      "Epoch [71/500], Train Loss: 313.3420, Test Loss: 317.0747\n",
      "Epoch time:  24.307814121246338\n",
      "Epoch [72/500], Train Loss: 309.4703, Test Loss: 312.9491\n",
      "Epoch time:  24.3394033908844\n",
      "Epoch [73/500], Train Loss: 308.9856, Test Loss: 309.7879\n",
      "Epoch time:  24.342174768447876\n",
      "Epoch [74/500], Train Loss: 312.0833, Test Loss: 303.5566\n",
      "Epoch time:  24.309301614761353\n",
      "Epoch [75/500], Train Loss: 301.2149, Test Loss: 296.6158\n",
      "Epoch time:  24.309421062469482\n",
      "Epoch [76/500], Train Loss: 289.9633, Test Loss: 290.4572\n",
      "Epoch time:  24.341119289398193\n",
      "Epoch [77/500], Train Loss: 288.2925, Test Loss: 292.6137\n",
      "Epoch time:  24.33117961883545\n",
      "Epoch [78/500], Train Loss: 284.5746, Test Loss: 279.3287\n",
      "Epoch time:  24.29279661178589\n",
      "Epoch [79/500], Train Loss: 285.1214, Test Loss: 278.9669\n",
      "Epoch time:  24.349649906158447\n",
      "Epoch [80/500], Train Loss: 283.7879, Test Loss: 288.4423\n",
      "Epoch time:  24.33287215232849\n",
      "Epoch [81/500], Train Loss: 286.5218, Test Loss: 287.6776\n",
      "Epoch time:  24.331712007522583\n",
      "Epoch [82/500], Train Loss: 285.2058, Test Loss: 278.0090\n",
      "Epoch time:  24.304070949554443\n",
      "Epoch [83/500], Train Loss: 277.3309, Test Loss: 268.6352\n",
      "Epoch time:  24.336504459381104\n",
      "Epoch [84/500], Train Loss: 267.4894, Test Loss: 272.4299\n",
      "Epoch time:  24.329895973205566\n",
      "Epoch [85/500], Train Loss: 266.7963, Test Loss: 265.5787\n",
      "Epoch time:  24.309689044952393\n",
      "Epoch [86/500], Train Loss: 263.3507, Test Loss: 260.5496\n",
      "Epoch time:  24.326284170150757\n",
      "Epoch [87/500], Train Loss: 261.0514, Test Loss: 263.9480\n",
      "Epoch time:  24.30913734436035\n",
      "Epoch [88/500], Train Loss: 259.1515, Test Loss: 261.2691\n",
      "Epoch time:  24.318870782852173\n",
      "Epoch [89/500], Train Loss: 255.7337, Test Loss: 255.8666\n",
      "Epoch time:  24.30153203010559\n",
      "Epoch [90/500], Train Loss: 255.5365, Test Loss: 255.8551\n",
      "Epoch time:  24.330737590789795\n",
      "Epoch [91/500], Train Loss: 253.6743, Test Loss: 253.0426\n",
      "Epoch time:  24.330638647079468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/500], Train Loss: 249.3841, Test Loss: 244.4320\n",
      "Epoch time:  24.317248821258545\n",
      "Epoch [93/500], Train Loss: 248.8860, Test Loss: 252.9363\n",
      "Epoch time:  24.288378715515137\n",
      "Epoch [94/500], Train Loss: 247.1037, Test Loss: 251.6866\n",
      "Epoch time:  24.31494426727295\n",
      "Epoch [95/500], Train Loss: 255.1291, Test Loss: 256.4298\n",
      "Epoch time:  24.33279013633728\n",
      "Epoch [96/500], Train Loss: 246.1546, Test Loss: 238.2784\n",
      "Epoch time:  24.290052890777588\n",
      "Epoch [97/500], Train Loss: 245.5665, Test Loss: 241.8891\n",
      "Epoch time:  24.320196628570557\n",
      "Epoch [98/500], Train Loss: 243.9445, Test Loss: 240.3752\n",
      "Epoch time:  24.324111461639404\n",
      "Epoch [99/500], Train Loss: 236.0575, Test Loss: 232.5214\n",
      "Epoch time:  24.317954301834106\n",
      "Epoch [100/500], Train Loss: 228.7290, Test Loss: 232.9335\n",
      "Epoch time:  24.28373908996582\n",
      "Epoch [101/500], Train Loss: 231.8611, Test Loss: 236.5742\n",
      "Epoch time:  24.33281683921814\n",
      "Epoch [102/500], Train Loss: 232.8216, Test Loss: 228.1218\n",
      "Epoch time:  24.33885645866394\n",
      "Epoch [103/500], Train Loss: 232.4658, Test Loss: 225.1806\n",
      "Epoch time:  24.318769931793213\n",
      "Epoch [104/500], Train Loss: 233.0745, Test Loss: 229.4473\n",
      "Epoch time:  24.28770661354065\n",
      "Epoch [105/500], Train Loss: 228.1640, Test Loss: 230.6067\n",
      "Epoch time:  24.33110213279724\n",
      "Epoch [106/500], Train Loss: 225.9138, Test Loss: 222.5579\n",
      "Epoch time:  24.33550786972046\n",
      "Epoch [107/500], Train Loss: 220.0731, Test Loss: 217.1984\n",
      "Epoch time:  24.30313801765442\n",
      "Epoch [108/500], Train Loss: 217.7525, Test Loss: 221.6227\n",
      "Epoch time:  24.31374979019165\n",
      "Epoch [109/500], Train Loss: 220.6774, Test Loss: 220.8545\n",
      "Epoch time:  24.31903100013733\n",
      "Epoch [110/500], Train Loss: 216.0481, Test Loss: 231.8982\n",
      "Epoch time:  24.31380343437195\n",
      "Epoch [111/500], Train Loss: 218.2280, Test Loss: 223.7580\n",
      "Epoch time:  24.3019802570343\n",
      "Epoch [112/500], Train Loss: 221.2923, Test Loss: 221.9846\n",
      "Epoch time:  24.3268940448761\n",
      "Epoch [113/500], Train Loss: 229.8712, Test Loss: 227.0442\n",
      "Epoch time:  24.320891618728638\n",
      "Epoch [114/500], Train Loss: 218.8064, Test Loss: 208.0454\n",
      "Epoch time:  24.320082187652588\n",
      "Epoch [115/500], Train Loss: 210.5489, Test Loss: 213.8024\n",
      "Epoch time:  24.305060625076294\n",
      "Epoch [116/500], Train Loss: 208.6011, Test Loss: 210.3770\n",
      "Epoch time:  24.323660135269165\n",
      "Epoch [117/500], Train Loss: 208.1349, Test Loss: 203.9678\n",
      "Epoch time:  24.326465368270874\n",
      "Epoch [118/500], Train Loss: 205.3738, Test Loss: 207.6568\n",
      "Epoch time:  24.327314376831055\n",
      "Epoch [119/500], Train Loss: 209.3165, Test Loss: 206.8224\n",
      "Epoch time:  24.317517518997192\n",
      "Epoch [120/500], Train Loss: 203.0577, Test Loss: 203.7530\n",
      "Epoch time:  24.33725929260254\n",
      "Epoch [121/500], Train Loss: 206.3186, Test Loss: 217.8233\n",
      "Epoch time:  24.328086853027344\n",
      "Epoch [122/500], Train Loss: 203.4064, Test Loss: 203.0922\n",
      "Epoch time:  24.294113159179688\n",
      "Epoch [123/500], Train Loss: 200.6954, Test Loss: 205.6094\n",
      "Epoch time:  24.32164454460144\n",
      "Epoch [124/500], Train Loss: 201.8685, Test Loss: 215.7616\n",
      "Epoch time:  24.319628953933716\n",
      "Epoch [125/500], Train Loss: 207.7700, Test Loss: 201.2342\n",
      "Epoch time:  24.318516731262207\n",
      "Epoch [126/500], Train Loss: 207.6850, Test Loss: 202.7809\n",
      "Epoch time:  24.2976496219635\n",
      "Epoch [127/500], Train Loss: 193.9634, Test Loss: 195.5765\n",
      "Epoch time:  24.337283611297607\n",
      "Epoch [128/500], Train Loss: 193.4168, Test Loss: 189.6158\n",
      "Epoch time:  24.325541019439697\n",
      "Epoch [129/500], Train Loss: 186.3265, Test Loss: 192.3600\n",
      "Epoch time:  24.291815280914307\n",
      "Epoch [130/500], Train Loss: 190.9566, Test Loss: 188.5496\n",
      "Epoch time:  24.314575910568237\n",
      "Epoch [131/500], Train Loss: 185.5654, Test Loss: 186.3354\n",
      "Epoch time:  24.3209125995636\n",
      "Epoch [132/500], Train Loss: 184.2402, Test Loss: 185.8240\n",
      "Epoch time:  24.327482223510742\n",
      "Epoch [133/500], Train Loss: 186.3808, Test Loss: 187.4012\n",
      "Epoch time:  24.282276391983032\n",
      "Epoch [134/500], Train Loss: 183.2625, Test Loss: 182.2882\n",
      "Epoch time:  24.315560817718506\n",
      "Epoch [135/500], Train Loss: 181.5223, Test Loss: 180.5388\n",
      "Epoch time:  24.321550607681274\n",
      "Epoch [136/500], Train Loss: 183.2945, Test Loss: 187.0679\n",
      "Epoch time:  24.319138288497925\n",
      "Epoch [137/500], Train Loss: 183.0067, Test Loss: 180.7932\n",
      "Epoch time:  24.293747663497925\n",
      "Epoch [138/500], Train Loss: 179.8924, Test Loss: 181.0190\n",
      "Epoch time:  24.34091854095459\n",
      "Epoch [139/500], Train Loss: 180.8623, Test Loss: 182.3336\n",
      "Epoch time:  24.312072277069092\n",
      "Epoch [140/500], Train Loss: 178.6855, Test Loss: 176.3308\n",
      "Epoch time:  24.29868721961975\n",
      "Epoch [141/500], Train Loss: 179.7894, Test Loss: 179.2269\n",
      "Epoch time:  24.314886808395386\n",
      "Epoch [142/500], Train Loss: 184.0433, Test Loss: 180.2380\n",
      "Epoch time:  24.331783056259155\n",
      "Epoch [143/500], Train Loss: 178.2613, Test Loss: 183.3367\n",
      "Epoch time:  24.33720564842224\n",
      "Epoch [144/500], Train Loss: 180.6720, Test Loss: 176.6641\n",
      "Epoch time:  24.311366081237793\n",
      "Epoch [145/500], Train Loss: 177.0892, Test Loss: 175.5702\n",
      "Epoch time:  24.331451654434204\n",
      "Epoch [146/500], Train Loss: 180.4237, Test Loss: 183.3721\n",
      "Epoch time:  24.329394817352295\n",
      "Epoch [147/500], Train Loss: 178.2743, Test Loss: 179.9208\n",
      "Epoch time:  24.315184831619263\n",
      "Epoch [148/500], Train Loss: 176.1321, Test Loss: 177.3621\n",
      "Epoch time:  24.313732624053955\n",
      "Epoch [149/500], Train Loss: 175.7064, Test Loss: 172.2001\n",
      "Epoch time:  24.343485116958618\n",
      "Epoch [150/500], Train Loss: 171.9214, Test Loss: 170.5578\n",
      "Epoch time:  24.33482813835144\n",
      "Epoch [151/500], Train Loss: 168.2518, Test Loss: 166.2811\n",
      "Epoch time:  24.29746961593628\n",
      "Epoch [152/500], Train Loss: 166.8779, Test Loss: 170.8780\n",
      "Epoch time:  24.32129144668579\n",
      "Epoch [153/500], Train Loss: 166.6259, Test Loss: 167.2735\n",
      "Epoch time:  24.33519220352173\n",
      "Epoch [154/500], Train Loss: 170.1399, Test Loss: 176.2941\n",
      "Epoch time:  24.317935705184937\n",
      "Epoch [155/500], Train Loss: 177.0563, Test Loss: 176.0343\n",
      "Epoch time:  24.279872179031372\n",
      "Epoch [156/500], Train Loss: 176.9086, Test Loss: 175.2554\n",
      "Epoch time:  24.336462259292603\n",
      "Epoch [157/500], Train Loss: 174.9559, Test Loss: 177.8450\n",
      "Epoch time:  24.325284481048584\n",
      "Epoch [158/500], Train Loss: 166.4306, Test Loss: 161.5099\n",
      "Epoch time:  24.3070547580719\n",
      "Epoch [159/500], Train Loss: 162.4721, Test Loss: 162.2596\n",
      "Epoch time:  24.294280290603638\n",
      "Epoch [160/500], Train Loss: 165.6631, Test Loss: 176.0038\n",
      "Epoch time:  24.31682586669922\n",
      "Epoch [161/500], Train Loss: 167.5787, Test Loss: 163.0377\n",
      "Epoch time:  24.324012756347656\n",
      "Epoch [162/500], Train Loss: 168.6621, Test Loss: 177.4057\n",
      "Epoch time:  24.309571266174316\n",
      "Epoch [163/500], Train Loss: 171.6801, Test Loss: 175.1782\n",
      "Epoch time:  24.319880723953247\n",
      "Epoch [164/500], Train Loss: 167.4405, Test Loss: 163.6491\n",
      "Epoch time:  24.326234817504883\n",
      "Epoch [165/500], Train Loss: 163.6955, Test Loss: 159.9702\n",
      "Epoch time:  24.308256149291992\n",
      "Epoch [166/500], Train Loss: 160.5091, Test Loss: 158.3654\n",
      "Epoch time:  24.302436590194702\n",
      "Epoch [167/500], Train Loss: 162.4869, Test Loss: 163.5483\n",
      "Epoch time:  24.327788591384888\n",
      "Epoch [168/500], Train Loss: 160.6702, Test Loss: 155.2058\n",
      "Epoch time:  24.327274322509766\n",
      "Epoch [169/500], Train Loss: 157.4421, Test Loss: 153.6172\n",
      "Epoch time:  24.33035635948181\n",
      "Epoch [170/500], Train Loss: 158.5872, Test Loss: 166.1950\n",
      "Epoch time:  24.312079668045044\n",
      "Epoch [171/500], Train Loss: 156.0619, Test Loss: 155.3768\n",
      "Epoch time:  24.333375930786133\n",
      "Epoch [172/500], Train Loss: 153.0964, Test Loss: 160.4461\n",
      "Epoch time:  24.31639266014099\n",
      "Epoch [173/500], Train Loss: 155.1606, Test Loss: 153.6103\n",
      "Epoch time:  24.31991219520569\n",
      "Epoch [174/500], Train Loss: 151.1985, Test Loss: 151.4382\n",
      "Epoch time:  24.3240008354187\n",
      "Epoch [175/500], Train Loss: 151.7745, Test Loss: 157.1635\n",
      "Epoch time:  24.33802342414856\n",
      "Epoch [176/500], Train Loss: 152.4422, Test Loss: 150.3663\n",
      "Epoch time:  24.336541891098022\n",
      "Epoch [177/500], Train Loss: 152.2375, Test Loss: 150.5411\n",
      "Epoch time:  24.298444271087646\n",
      "Epoch [178/500], Train Loss: 152.7636, Test Loss: 159.3537\n",
      "Epoch time:  24.31767201423645\n",
      "Epoch [179/500], Train Loss: 153.6144, Test Loss: 157.4481\n",
      "Epoch time:  24.328099489212036\n",
      "Epoch [180/500], Train Loss: 156.0334, Test Loss: 157.3815\n",
      "Epoch time:  24.327820539474487\n",
      "Epoch [181/500], Train Loss: 149.5938, Test Loss: 144.8088\n",
      "Epoch time:  24.306870222091675\n",
      "Epoch [182/500], Train Loss: 149.1558, Test Loss: 150.9270\n",
      "Epoch time:  24.331952333450317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [183/500], Train Loss: 146.5093, Test Loss: 146.4037\n",
      "Epoch time:  24.30994200706482\n",
      "Epoch [184/500], Train Loss: 146.1998, Test Loss: 147.9938\n",
      "Epoch time:  24.298155069351196\n",
      "Epoch [185/500], Train Loss: 146.6713, Test Loss: 157.7888\n",
      "Epoch time:  24.299978971481323\n",
      "Epoch [186/500], Train Loss: 149.0218, Test Loss: 152.2226\n",
      "Epoch time:  24.311444520950317\n",
      "Epoch [187/500], Train Loss: 148.3049, Test Loss: 143.0809\n",
      "Epoch time:  24.321279525756836\n",
      "Epoch [188/500], Train Loss: 145.8869, Test Loss: 162.1534\n",
      "Epoch time:  24.281370878219604\n",
      "Epoch [189/500], Train Loss: 152.6771, Test Loss: 148.2966\n",
      "Epoch time:  24.312223434448242\n",
      "Epoch [190/500], Train Loss: 144.9135, Test Loss: 142.2214\n",
      "Epoch time:  24.338101863861084\n",
      "Epoch [191/500], Train Loss: 143.1633, Test Loss: 146.1029\n",
      "Epoch time:  24.32896590232849\n",
      "Epoch [192/500], Train Loss: 138.5987, Test Loss: 137.6078\n",
      "Epoch time:  24.29345679283142\n",
      "Epoch [193/500], Train Loss: 139.3130, Test Loss: 139.2016\n",
      "Epoch time:  24.333669662475586\n",
      "Epoch [194/500], Train Loss: 137.8417, Test Loss: 143.2997\n",
      "Epoch time:  24.32964015007019\n",
      "Epoch [195/500], Train Loss: 137.6062, Test Loss: 135.8343\n",
      "Epoch time:  24.313909769058228\n",
      "Epoch [196/500], Train Loss: 138.7345, Test Loss: 142.5619\n",
      "Epoch time:  24.312204360961914\n",
      "Epoch [197/500], Train Loss: 142.6468, Test Loss: 144.7112\n",
      "Epoch time:  24.326748371124268\n",
      "Epoch [198/500], Train Loss: 145.9359, Test Loss: 144.8065\n",
      "Epoch time:  24.331214904785156\n",
      "Epoch [199/500], Train Loss: 144.5310, Test Loss: 144.0380\n",
      "Epoch time:  24.287041902542114\n",
      "Epoch [200/500], Train Loss: 141.0048, Test Loss: 134.9650\n",
      "Epoch time:  24.341098070144653\n",
      "Epoch [201/500], Train Loss: 134.2211, Test Loss: 130.7869\n",
      "Epoch time:  24.341896057128906\n",
      "Epoch [202/500], Train Loss: 134.8728, Test Loss: 131.8405\n",
      "Epoch time:  24.332706212997437\n",
      "Epoch [203/500], Train Loss: 131.3363, Test Loss: 127.5646\n",
      "Epoch time:  24.313344478607178\n",
      "Epoch [204/500], Train Loss: 136.7665, Test Loss: 136.2900\n",
      "Epoch time:  24.33855152130127\n",
      "Epoch [205/500], Train Loss: 138.8552, Test Loss: 131.4354\n",
      "Epoch time:  24.343640089035034\n",
      "Epoch [206/500], Train Loss: 128.8899, Test Loss: 126.3964\n",
      "Epoch time:  24.299550533294678\n",
      "Epoch [207/500], Train Loss: 127.9981, Test Loss: 129.2565\n",
      "Epoch time:  24.329546213150024\n",
      "Epoch [208/500], Train Loss: 128.4393, Test Loss: 135.6843\n",
      "Epoch time:  24.34208655357361\n",
      "Epoch [209/500], Train Loss: 129.9139, Test Loss: 131.5393\n",
      "Epoch time:  24.328601360321045\n",
      "Epoch [210/500], Train Loss: 128.7779, Test Loss: 128.7718\n",
      "Epoch time:  24.28646969795227\n",
      "Epoch [211/500], Train Loss: 126.6044, Test Loss: 128.3149\n",
      "Epoch time:  24.336211442947388\n",
      "Epoch [212/500], Train Loss: 126.5515, Test Loss: 128.8481\n",
      "Epoch time:  24.330549955368042\n",
      "Epoch [213/500], Train Loss: 130.7519, Test Loss: 132.2369\n",
      "Epoch time:  24.341059684753418\n",
      "Epoch [214/500], Train Loss: 126.8253, Test Loss: 124.8832\n",
      "Epoch time:  24.306626558303833\n",
      "Epoch [215/500], Train Loss: 123.5531, Test Loss: 130.8323\n",
      "Epoch time:  24.320894479751587\n",
      "Epoch [216/500], Train Loss: 127.8673, Test Loss: 124.3876\n",
      "Epoch time:  24.342442989349365\n",
      "Epoch [217/500], Train Loss: 124.1289, Test Loss: 131.0502\n",
      "Epoch time:  24.327504634857178\n",
      "Epoch [218/500], Train Loss: 128.2172, Test Loss: 122.0608\n",
      "Epoch time:  24.319875955581665\n",
      "Epoch [219/500], Train Loss: 131.1824, Test Loss: 140.7748\n",
      "Epoch time:  24.32420301437378\n",
      "Epoch [220/500], Train Loss: 133.0603, Test Loss: 129.8572\n",
      "Epoch time:  24.35693621635437\n",
      "Epoch [221/500], Train Loss: 127.7304, Test Loss: 132.3731\n",
      "Epoch time:  24.313325881958008\n",
      "Epoch [222/500], Train Loss: 125.5263, Test Loss: 126.8768\n",
      "Epoch time:  24.331998109817505\n",
      "Epoch [223/500], Train Loss: 126.7487, Test Loss: 125.0178\n",
      "Epoch time:  24.334502458572388\n",
      "Epoch [224/500], Train Loss: 121.8463, Test Loss: 121.4292\n",
      "Epoch time:  24.323317527770996\n",
      "Epoch [225/500], Train Loss: 119.9223, Test Loss: 120.9023\n",
      "Epoch time:  24.324488162994385\n",
      "Epoch [226/500], Train Loss: 120.6628, Test Loss: 121.6013\n",
      "Epoch time:  24.324973821640015\n",
      "Epoch [227/500], Train Loss: 122.6226, Test Loss: 123.3007\n",
      "Epoch time:  24.35785436630249\n",
      "Epoch [228/500], Train Loss: 122.9660, Test Loss: 120.5147\n",
      "Epoch time:  24.316097259521484\n",
      "Epoch [229/500], Train Loss: 127.1024, Test Loss: 133.6610\n",
      "Epoch time:  24.34392237663269\n",
      "Epoch [230/500], Train Loss: 122.6525, Test Loss: 120.1477\n",
      "Epoch time:  24.340579509735107\n",
      "Epoch [231/500], Train Loss: 116.9407, Test Loss: 115.8387\n",
      "Epoch time:  24.34889841079712\n",
      "Epoch [232/500], Train Loss: 115.2161, Test Loss: 119.1105\n",
      "Epoch time:  24.310845851898193\n",
      "Epoch [233/500], Train Loss: 116.5202, Test Loss: 118.4359\n",
      "Epoch time:  24.33725595474243\n",
      "Epoch [234/500], Train Loss: 113.9517, Test Loss: 114.3517\n",
      "Epoch time:  24.333411931991577\n",
      "Epoch [235/500], Train Loss: 113.0447, Test Loss: 113.9862\n",
      "Epoch time:  24.330137729644775\n",
      "Epoch [236/500], Train Loss: 112.1525, Test Loss: 108.5500\n",
      "Epoch time:  24.30782914161682\n",
      "Epoch [237/500], Train Loss: 109.6042, Test Loss: 112.8219\n",
      "Epoch time:  24.349073886871338\n",
      "Epoch [238/500], Train Loss: 111.0397, Test Loss: 110.8544\n",
      "Epoch time:  24.33223843574524\n",
      "Epoch [239/500], Train Loss: 110.0724, Test Loss: 106.9188\n",
      "Epoch time:  24.301000118255615\n",
      "Epoch [240/500], Train Loss: 111.5237, Test Loss: 108.5848\n",
      "Epoch time:  24.344273805618286\n",
      "Epoch [241/500], Train Loss: 107.6095, Test Loss: 105.9526\n",
      "Epoch time:  24.321155309677124\n",
      "Epoch [242/500], Train Loss: 106.2000, Test Loss: 106.6709\n",
      "Epoch time:  24.327756643295288\n",
      "Epoch [243/500], Train Loss: 108.9308, Test Loss: 116.5448\n",
      "Epoch time:  24.283650636672974\n",
      "Epoch [244/500], Train Loss: 114.2535, Test Loss: 112.3874\n",
      "Epoch time:  24.327378273010254\n",
      "Epoch [245/500], Train Loss: 115.9407, Test Loss: 120.0477\n",
      "Epoch time:  24.323493480682373\n",
      "Epoch [246/500], Train Loss: 117.3411, Test Loss: 130.9975\n",
      "Epoch time:  24.314957857131958\n",
      "Epoch [247/500], Train Loss: 120.3457, Test Loss: 102.5428\n",
      "Epoch time:  24.288928747177124\n",
      "Epoch [248/500], Train Loss: 105.9116, Test Loss: 106.7584\n",
      "Epoch time:  24.331382989883423\n",
      "Epoch [249/500], Train Loss: 106.9857, Test Loss: 108.9936\n",
      "Epoch time:  24.33367943763733\n",
      "Epoch [250/500], Train Loss: 106.5229, Test Loss: 114.7376\n",
      "Epoch time:  24.303713083267212\n",
      "Epoch [251/500], Train Loss: 109.9219, Test Loss: 103.2805\n",
      "Epoch time:  24.30741024017334\n",
      "Epoch [252/500], Train Loss: 109.1022, Test Loss: 111.1493\n",
      "Epoch time:  24.32563614845276\n",
      "Epoch [253/500], Train Loss: 108.4312, Test Loss: 106.2123\n",
      "Epoch time:  24.308940172195435\n",
      "Epoch [254/500], Train Loss: 111.5191, Test Loss: 129.9189\n",
      "Epoch time:  24.299243450164795\n",
      "Epoch [255/500], Train Loss: 118.2822, Test Loss: 109.4058\n",
      "Epoch time:  24.32605814933777\n",
      "Epoch [256/500], Train Loss: 112.1000, Test Loss: 114.9999\n",
      "Epoch time:  24.337586402893066\n",
      "Epoch [257/500], Train Loss: 112.0003, Test Loss: 104.8089\n",
      "Epoch time:  24.332308769226074\n",
      "Epoch [258/500], Train Loss: 103.8635, Test Loss: 104.4028\n",
      "Epoch time:  24.312554359436035\n",
      "Epoch [259/500], Train Loss: 102.9285, Test Loss: 101.4298\n",
      "Epoch time:  24.332886457443237\n",
      "Epoch [260/500], Train Loss: 98.0667, Test Loss: 97.1748\n",
      "Epoch time:  24.338404893875122\n",
      "Epoch [261/500], Train Loss: 99.2739, Test Loss: 107.8780\n",
      "Epoch time:  24.331186056137085\n",
      "Epoch [262/500], Train Loss: 101.6918, Test Loss: 97.8741\n",
      "Epoch time:  24.336849689483643\n",
      "Epoch [263/500], Train Loss: 98.4047, Test Loss: 97.0874\n",
      "Epoch time:  24.349878787994385\n",
      "Epoch [264/500], Train Loss: 100.5557, Test Loss: 99.4162\n",
      "Epoch time:  24.33932113647461\n",
      "Epoch [265/500], Train Loss: 104.4957, Test Loss: 100.4061\n",
      "Epoch time:  24.286165714263916\n",
      "Epoch [266/500], Train Loss: 106.9051, Test Loss: 99.8900\n",
      "Epoch time:  24.324220895767212\n",
      "Epoch [267/500], Train Loss: 108.1476, Test Loss: 103.9075\n",
      "Epoch time:  24.350098848342896\n",
      "Epoch [268/500], Train Loss: 104.4484, Test Loss: 100.6783\n",
      "Epoch time:  24.331976652145386\n",
      "Epoch [269/500], Train Loss: 99.8587, Test Loss: 109.9211\n",
      "Epoch time:  24.312073230743408\n",
      "Epoch [270/500], Train Loss: 100.1195, Test Loss: 100.4859\n",
      "Epoch time:  24.345930814743042\n",
      "Epoch [271/500], Train Loss: 97.5217, Test Loss: 101.6782\n",
      "Epoch time:  24.323699712753296\n",
      "Epoch [272/500], Train Loss: 102.0732, Test Loss: 105.2607\n",
      "Epoch time:  24.311338901519775\n",
      "Epoch [273/500], Train Loss: 104.5461, Test Loss: 111.9585\n",
      "Epoch time:  24.32424545288086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [274/500], Train Loss: 103.0637, Test Loss: 98.0418\n",
      "Epoch time:  24.351504802703857\n",
      "Epoch [275/500], Train Loss: 101.2997, Test Loss: 90.9206\n",
      "Epoch time:  24.333804845809937\n",
      "Epoch [276/500], Train Loss: 96.0227, Test Loss: 100.4047\n",
      "Epoch time:  24.3104031085968\n",
      "Epoch [277/500], Train Loss: 96.1836, Test Loss: 93.9343\n",
      "Epoch time:  24.342685222625732\n",
      "Epoch [278/500], Train Loss: 100.0841, Test Loss: 101.8682\n",
      "Epoch time:  24.34253716468811\n",
      "Epoch [279/500], Train Loss: 98.7022, Test Loss: 101.5667\n",
      "Epoch time:  24.335207223892212\n",
      "Epoch [280/500], Train Loss: 95.6495, Test Loss: 97.3141\n",
      "Epoch time:  24.303277492523193\n",
      "Epoch [281/500], Train Loss: 93.2290, Test Loss: 91.3273\n",
      "Epoch time:  24.335595846176147\n",
      "Epoch [282/500], Train Loss: 95.6801, Test Loss: 92.1787\n",
      "Epoch time:  24.34469771385193\n",
      "Epoch [283/500], Train Loss: 96.8988, Test Loss: 101.4040\n",
      "Epoch time:  24.312679290771484\n",
      "Epoch [284/500], Train Loss: 94.9560, Test Loss: 91.2613\n",
      "Epoch time:  24.34318447113037\n",
      "Epoch [285/500], Train Loss: 91.5772, Test Loss: 98.1765\n",
      "Epoch time:  24.334078550338745\n",
      "Epoch [286/500], Train Loss: 97.1580, Test Loss: 103.9465\n",
      "Epoch time:  24.34163212776184\n",
      "Epoch [287/500], Train Loss: 95.5642, Test Loss: 93.2898\n",
      "Epoch time:  24.30170178413391\n",
      "Epoch [288/500], Train Loss: 95.3029, Test Loss: 98.3934\n",
      "Epoch time:  24.334131717681885\n",
      "Epoch [289/500], Train Loss: 98.1147, Test Loss: 98.3291\n",
      "Epoch time:  24.335288047790527\n",
      "Epoch [290/500], Train Loss: 98.5678, Test Loss: 98.2944\n",
      "Epoch time:  24.33421277999878\n",
      "Epoch [291/500], Train Loss: 93.6426, Test Loss: 92.0540\n",
      "Epoch time:  24.296462535858154\n",
      "Epoch [292/500], Train Loss: 91.5599, Test Loss: 87.8377\n",
      "Epoch time:  24.320606470108032\n",
      "Epoch [293/500], Train Loss: 94.8472, Test Loss: 103.7315\n",
      "Epoch time:  24.33677864074707\n",
      "Epoch [294/500], Train Loss: 103.6062, Test Loss: 98.0423\n",
      "Epoch time:  24.309624671936035\n",
      "Epoch [295/500], Train Loss: 95.2320, Test Loss: 93.4228\n",
      "Epoch time:  24.334959745407104\n",
      "Epoch [296/500], Train Loss: 97.8109, Test Loss: 99.5668\n",
      "Epoch time:  24.328011751174927\n",
      "Epoch [297/500], Train Loss: 103.8404, Test Loss: 102.0692\n",
      "Epoch time:  24.31459331512451\n",
      "Epoch [298/500], Train Loss: 102.1684, Test Loss: 104.6974\n",
      "Epoch time:  24.312402486801147\n",
      "Epoch [299/500], Train Loss: 99.3119, Test Loss: 103.8915\n",
      "Epoch time:  24.3424129486084\n",
      "Epoch [300/500], Train Loss: 101.3320, Test Loss: 107.2690\n",
      "Epoch time:  24.316412925720215\n",
      "Epoch [301/500], Train Loss: 94.2075, Test Loss: 94.2651\n",
      "Epoch time:  24.34441351890564\n",
      "Epoch [302/500], Train Loss: 93.5688, Test Loss: 89.9481\n",
      "Epoch time:  24.321874856948853\n",
      "Epoch [303/500], Train Loss: 88.5198, Test Loss: 91.6118\n",
      "Epoch time:  24.328855752944946\n",
      "Epoch [304/500], Train Loss: 89.2461, Test Loss: 94.7079\n",
      "Epoch time:  24.348215341567993\n",
      "Epoch [305/500], Train Loss: 88.6820, Test Loss: 89.6317\n",
      "Epoch time:  24.30968713760376\n",
      "Epoch [306/500], Train Loss: 96.2740, Test Loss: 99.3524\n",
      "Epoch time:  24.336050271987915\n",
      "Epoch [307/500], Train Loss: 96.4435, Test Loss: 97.0688\n",
      "Epoch time:  24.337807416915894\n",
      "Epoch [308/500], Train Loss: 93.5949, Test Loss: 99.5756\n",
      "Epoch time:  24.33181858062744\n",
      "Epoch [309/500], Train Loss: 93.8316, Test Loss: 95.1463\n",
      "Epoch time:  24.308069944381714\n",
      "Epoch [310/500], Train Loss: 90.5184, Test Loss: 89.3358\n",
      "Epoch time:  24.34907579421997\n",
      "Epoch [311/500], Train Loss: 91.4148, Test Loss: 103.1760\n",
      "Epoch time:  24.341374397277832\n",
      "Epoch [312/500], Train Loss: 100.8609, Test Loss: 94.7440\n",
      "Epoch time:  24.32310390472412\n",
      "Epoch [313/500], Train Loss: 105.6802, Test Loss: 120.8302\n",
      "Epoch time:  24.2991886138916\n",
      "Epoch [314/500], Train Loss: 105.2573, Test Loss: 115.4843\n",
      "Epoch time:  24.355042457580566\n",
      "Epoch [315/500], Train Loss: 99.4295, Test Loss: 102.9982\n",
      "Epoch time:  24.338443517684937\n",
      "Epoch [316/500], Train Loss: 92.4020, Test Loss: 87.7673\n",
      "Epoch time:  24.311140775680542\n",
      "Epoch [317/500], Train Loss: 87.4012, Test Loss: 91.3221\n",
      "Epoch time:  24.353477239608765\n",
      "Epoch [318/500], Train Loss: 91.3788, Test Loss: 89.5088\n",
      "Epoch time:  24.35390615463257\n",
      "Epoch [319/500], Train Loss: 89.7713, Test Loss: 92.7117\n",
      "Epoch time:  24.339348793029785\n",
      "Epoch [320/500], Train Loss: 90.8919, Test Loss: 94.0939\n",
      "Epoch time:  24.31778120994568\n",
      "Epoch [321/500], Train Loss: 97.7603, Test Loss: 93.2945\n",
      "Epoch time:  24.34786295890808\n",
      "Epoch [322/500], Train Loss: 98.7685, Test Loss: 111.0364\n",
      "Epoch time:  24.341036558151245\n",
      "Epoch [323/500], Train Loss: 103.9270, Test Loss: 96.3881\n",
      "Epoch time:  24.33574676513672\n",
      "Epoch [324/500], Train Loss: 100.0361, Test Loss: 95.3958\n",
      "Epoch time:  24.297656297683716\n",
      "Epoch [325/500], Train Loss: 90.8164, Test Loss: 83.4692\n",
      "Epoch time:  24.346688270568848\n",
      "Epoch [326/500], Train Loss: 84.5737, Test Loss: 86.9852\n",
      "Epoch time:  24.35639524459839\n",
      "Epoch [327/500], Train Loss: 88.4214, Test Loss: 94.6414\n",
      "Epoch time:  24.308146238327026\n",
      "Epoch [328/500], Train Loss: 93.2844, Test Loss: 91.7554\n",
      "Epoch time:  24.34638500213623\n",
      "Epoch [329/500], Train Loss: 90.6614, Test Loss: 90.0361\n",
      "Epoch time:  24.34563112258911\n",
      "Epoch [330/500], Train Loss: 86.3407, Test Loss: 84.0902\n",
      "Epoch time:  24.341933965682983\n",
      "Epoch [331/500], Train Loss: 86.3345, Test Loss: 87.8712\n",
      "Epoch time:  24.313348293304443\n",
      "Epoch [332/500], Train Loss: 84.6641, Test Loss: 84.7809\n",
      "Epoch time:  24.354719638824463\n",
      "Epoch [333/500], Train Loss: 87.9682, Test Loss: 89.3424\n",
      "Epoch time:  24.3432674407959\n",
      "Epoch [334/500], Train Loss: 87.7053, Test Loss: 87.2289\n",
      "Epoch time:  24.318888425827026\n",
      "Epoch [335/500], Train Loss: 88.8604, Test Loss: 92.0201\n",
      "Epoch time:  24.313401222229004\n",
      "Epoch [336/500], Train Loss: 94.9197, Test Loss: 102.7329\n",
      "Epoch time:  24.341294527053833\n",
      "Epoch [337/500], Train Loss: 99.7896, Test Loss: 89.6536\n",
      "Epoch time:  24.31385087966919\n",
      "Epoch [338/500], Train Loss: 83.5783, Test Loss: 89.3460\n",
      "Epoch time:  24.298895359039307\n",
      "Epoch [339/500], Train Loss: 88.6586, Test Loss: 82.5133\n",
      "Epoch time:  24.32156252861023\n",
      "Epoch [340/500], Train Loss: 85.0090, Test Loss: 91.9362\n",
      "Epoch time:  24.344516277313232\n",
      "Epoch [341/500], Train Loss: 89.7302, Test Loss: 89.4537\n",
      "Epoch time:  24.33012628555298\n",
      "Epoch [342/500], Train Loss: 89.0203, Test Loss: 83.9683\n",
      "Epoch time:  24.31026816368103\n",
      "Epoch [343/500], Train Loss: 87.1779, Test Loss: 86.8042\n",
      "Epoch time:  24.354543924331665\n",
      "Epoch [344/500], Train Loss: 84.9171, Test Loss: 90.7346\n",
      "Epoch time:  24.33408832550049\n",
      "Epoch [345/500], Train Loss: 89.8435, Test Loss: 97.3100\n",
      "Epoch time:  24.33227801322937\n",
      "Epoch [346/500], Train Loss: 92.3761, Test Loss: 86.0844\n",
      "Epoch time:  24.300618648529053\n",
      "Epoch [347/500], Train Loss: 87.6553, Test Loss: 86.9100\n",
      "Epoch time:  24.320266485214233\n",
      "Epoch [348/500], Train Loss: 88.5360, Test Loss: 92.9131\n",
      "Epoch time:  24.338149309158325\n",
      "Epoch [349/500], Train Loss: 92.0322, Test Loss: 89.2480\n",
      "Epoch time:  24.301818132400513\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m save_path \u001b[38;5;241m=\u001b[39m ModelName \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Params.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_losses, test_losses, is_model_trained \u001b[38;5;241m=\u001b[39m train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Finish timing cell run time\u001b[39;00m\n\u001b[1;32m     24\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[9], line 101\u001b[0m, in \u001b[0;36mtrain_or_load_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pretrained model found. Training from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#optimizer = optim.Adam(model.parameters())  \u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n\u001b[1;32m    102\u001b[0m is_model_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set flag to True after training\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Save losses per epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m, in \u001b[0;36mtrain_and_save_best_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path, accumulation_steps)\u001b[0m\n\u001b[1;32m     33\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Step the optimizer and update the scaler\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     38\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero gradients after accumulation_steps\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:452\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    450\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    454\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  117.35176830175116\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i].cpu().numpy()\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i].unsqueeze(0).unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "380.67  - Concentrations: 0.004999999888241291\n",
      "75.8  - Concentrations: 0.02500000037252903\n",
      "19.37  - Concentrations: 0.10000000149011612\n",
      "8.46  - Concentrations: 0.25\n",
      "4.94  - Concentrations: 0.5\n",
      "2.84  - Concentrations: 1.0\n",
      "2.23  - Concentrations: 2.5\n",
      "1.43  - Concentrations: 10.0\n",
      "1.31  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  5.516701\n",
      "--------------------\n",
      "2.84  - Min Value: 0.6783  - Mean Value: 9.2\n",
      "27.55  - Min Value: 0.0096  - Mean Value: 10.3\n",
      "5.86  - Min Value: 0.147  - Mean Value: 10.5\n",
      "2.66  - Min Value: 0.5572  - Mean Value: 8.5\n",
      "2.57  - Min Value: 1.3567  - Mean Value: 10.6\n",
      "2.93  - Min Value: 0.6332  - Mean Value: 10.9\n",
      "3.05  - Min Value: 0.7017  - Mean Value: 11.0\n",
      "2.92  - Min Value: 0.3674  - Mean Value: 8.9\n",
      "2.94  - Min Value: 0.8387  - Mean Value: 9.8\n",
      "1.83  - Min Value: 1.0913  - Mean Value: 11.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  29.880344\n",
      "--------------------\n",
      "22.73  - Min Value: 0.0111  - Mean Value: 0.1\n",
      "37.05  - Min Value: 0.0103  - Mean Value: 0.1\n",
      "21.05  - Min Value: 0.0153  - Mean Value: 0.1\n",
      "30.54  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "29.51  - Min Value: 0.0089  - Mean Value: 0.1\n",
      "31.43  - Min Value: 0.0075  - Mean Value: 0.1\n",
      "28.49  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "35.65  - Min Value: 0.0052  - Mean Value: 0.1\n",
      "35.79  - Min Value: 0.008  - Mean Value: 0.1\n",
      "26.55  - Min Value: 0.0134  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  28.521595\n",
      "--------------------\n",
      "56.75  - Min Value: 0.008  - Mean Value: 0.8\n",
      "30.41  - Min Value: 0.009  - Mean Value: 0.9\n",
      "34.44  - Min Value: 0.0138  - Mean Value: 1.5\n",
      "26.64  - Min Value: 0.0107  - Mean Value: 0.7\n",
      "19.5  - Min Value: 0.0191  - Mean Value: 0.7\n",
      "34.73  - Min Value: 0.0186  - Mean Value: 0.8\n",
      "22.77  - Min Value: 0.0175  - Mean Value: 0.8\n",
      "17.29  - Min Value: 0.0238  - Mean Value: 1.3\n",
      "18.86  - Min Value: 0.0168  - Mean Value: 0.7\n",
      "23.83  - Min Value: 0.0171  - Mean Value: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  43.38544\n",
      "--------------------\n",
      "25.26  - Min Value: 0.013  - Mean Value: 0.6\n",
      "79.6  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "61.13  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "47.74  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "32.45  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "35.25  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "44.87  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "28.16  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "26.58  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "52.79  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  160.27888\n",
      "--------------------\n",
      "117.82  - Min Value: 0.0062  - Mean Value: 2.1\n",
      "165.24  - Min Value: 0.006  - Mean Value: 3.7\n",
      "142.3  - Min Value: 0.0066  - Mean Value: 4.3\n",
      "175.38  - Min Value: 0.0094  - Mean Value: 4.3\n",
      "164.9  - Min Value: 0.0068  - Mean Value: 4.9\n",
      "162.08  - Min Value: 0.005  - Mean Value: 3.8\n",
      "123.07  - Min Value: 0.0101  - Mean Value: 3.2\n",
      "92.3  - Min Value: 0.0062  - Mean Value: 3.2\n",
      "185.22  - Min Value: 0.0053  - Mean Value: 5.3\n",
      "274.49  - Min Value: 0.0054  - Mean Value: 2.5\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(HighDynamicRange2Conc[i].min().item(),4), \" - Mean Value:\", np.round(HighDynamicRange2Conc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on a dataset with singlets added at random\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred = model_aq(OtherValSpectra[0].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 1s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
