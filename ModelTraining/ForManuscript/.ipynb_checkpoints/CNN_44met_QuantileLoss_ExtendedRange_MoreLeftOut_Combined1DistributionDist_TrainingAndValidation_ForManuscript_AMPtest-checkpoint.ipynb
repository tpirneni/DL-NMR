{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 500\n",
    "\n",
    "# Identification part of the filenames\n",
    "model_base_name = 'Quantile_ExtendedRange_MoreLeftOut_Combined1Distribution_AMPtest'\n",
    "base_name = 'ExtendedRange_MoreLeftOut_Combined1Distribution'    # This is the dataset base name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"CNN_44met_{model_base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load(f'Dataset44_{base_name}_ForManuscript_Spec.npy')\n",
    "conc1 = np.load(f'Dataset44_{base_name}_ForManuscript_Conc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in datasets]\n",
    "test_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in Test_datasets]\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset_reshaped, batch_size = 64, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset_reshaped, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectraVal = np.load('Dataset44_ExtendedRange_MoreLeftOut_Combined1Distribution_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load('Dataset44_ExtendedRange_MoreLeftOut_Combined1Distribution_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "Val_datasets = torch.utils.data.TensorDataset(spectraVal, concVal)\n",
    "val_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in Val_datasets]\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset_reshaped, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NMR_Model_Aq, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 42, kernel_size=6, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(120708, 200)\n",
    "        self.fc2 = nn.Linear(200, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)                  \n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile=0.5):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        error = y_true - y_pred\n",
    "        loss = torch.mean(torch.max(self.quantile * error, (self.quantile - 1) * error))\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "# MAPE loss function for directly comparing models despite loss function used\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "        return loss * 100  # To get percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = QuantileLoss()\n",
    "    criterion2 = MAPELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5.287243368897864e-05, weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in validation loss constitutes early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # For timing cell run time\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        \n",
    "         # Instantiate the GradScaler\n",
    "        scaler = GradScaler()\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move data to GPU\n",
    "             inputs, labels = inputs.to(device), labels.to(device)\n",
    "             # Zero the parameter gradients\n",
    "             optimizer.zero_grad()\n",
    "            # Enable autocasting for forward and backward passes\n",
    "             with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "             train_loss += loss.item() * inputs.size(0)\n",
    "             # Scale the loss and perform backpropagation\n",
    "             scaler.scale(loss).backward()\n",
    "             # Step the optimizer\n",
    "             scaler.step(optimizer)\n",
    "              # Update the scaler\n",
    "             scaler.update()\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                # Move data to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Enable autocasting for forward passes\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "\n",
    "            \n",
    "        running_test_loss2 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss2 = criterion2(outputs, labels)\n",
    "                running_test_loss2 += loss2.item() * inputs.size(0)\n",
    "        \n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.3f} | Test Loss: {test_loss:.3f} | Test Loss [MAPE]: {running_test_loss2:.3f}')\n",
    "\n",
    "            \n",
    "    \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Epoch time: \",end-start)\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch 1/500 | Train Loss: 20320.551 | Test Loss: 4924.886 | Test Loss [MAPE]: 2109279.130\n",
      "Epoch time:  13.876266717910767\n",
      "Epoch 2/500 | Train Loss: 17129.566 | Test Loss: 4118.349 | Test Loss [MAPE]: 2441874.725\n",
      "Epoch time:  13.910306930541992\n",
      "Epoch 3/500 | Train Loss: 16301.256 | Test Loss: 4054.310 | Test Loss [MAPE]: 1728865.051\n",
      "Epoch time:  13.979425430297852\n",
      "Epoch 4/500 | Train Loss: 16064.103 | Test Loss: 3994.246 | Test Loss [MAPE]: 2197488.384\n",
      "Epoch time:  13.997628688812256\n",
      "Epoch 5/500 | Train Loss: 15791.026 | Test Loss: 3907.092 | Test Loss [MAPE]: 2844145.331\n",
      "Epoch time:  13.992154836654663\n",
      "Epoch 6/500 | Train Loss: 15400.915 | Test Loss: 3786.654 | Test Loss [MAPE]: 4207335.635\n",
      "Epoch time:  13.998713493347168\n",
      "Epoch 7/500 | Train Loss: 14836.864 | Test Loss: 3612.948 | Test Loss [MAPE]: 5908330.109\n",
      "Epoch time:  13.977831840515137\n",
      "Epoch 8/500 | Train Loss: 13940.027 | Test Loss: 3320.214 | Test Loss [MAPE]: 7437338.555\n",
      "Epoch time:  13.978948831558228\n",
      "Epoch 9/500 | Train Loss: 12175.876 | Test Loss: 2729.336 | Test Loss [MAPE]: 7955782.878\n",
      "Epoch time:  14.021043062210083\n",
      "Epoch 10/500 | Train Loss: 9602.279 | Test Loss: 2144.752 | Test Loss [MAPE]: 6527853.171\n",
      "Epoch time:  13.642829895019531\n",
      "Epoch 11/500 | Train Loss: 7615.434 | Test Loss: 1730.598 | Test Loss [MAPE]: 5106286.193\n",
      "Epoch time:  14.039459228515625\n",
      "Epoch 12/500 | Train Loss: 6365.852 | Test Loss: 1509.955 | Test Loss [MAPE]: 4674522.045\n",
      "Epoch time:  13.969909429550171\n",
      "Epoch 13/500 | Train Loss: 5587.020 | Test Loss: 1338.737 | Test Loss [MAPE]: 3946871.341\n",
      "Epoch time:  14.031167030334473\n",
      "Epoch 14/500 | Train Loss: 5093.108 | Test Loss: 1244.623 | Test Loss [MAPE]: 3678798.292\n",
      "Epoch time:  14.048810005187988\n",
      "Epoch 15/500 | Train Loss: 4733.049 | Test Loss: 1156.877 | Test Loss [MAPE]: 3512068.179\n",
      "Epoch time:  13.96571159362793\n",
      "Epoch 16/500 | Train Loss: 4472.930 | Test Loss: 1089.562 | Test Loss [MAPE]: 3195531.018\n",
      "Epoch time:  14.025810241699219\n",
      "Epoch 17/500 | Train Loss: 4270.092 | Test Loss: 1050.074 | Test Loss [MAPE]: 3089670.306\n",
      "Epoch time:  14.044320821762085\n",
      "Epoch 18/500 | Train Loss: 4099.775 | Test Loss: 1021.692 | Test Loss [MAPE]: 2919082.083\n",
      "Epoch time:  14.01371455192566\n",
      "Epoch 19/500 | Train Loss: 3972.333 | Test Loss: 982.646 | Test Loss [MAPE]: 2851517.141\n",
      "Epoch time:  14.034850597381592\n",
      "Epoch 20/500 | Train Loss: 3860.508 | Test Loss: 965.787 | Test Loss [MAPE]: 2820197.992\n",
      "Epoch time:  13.98702597618103\n",
      "Epoch 21/500 | Train Loss: 3769.696 | Test Loss: 947.275 | Test Loss [MAPE]: 2703470.044\n",
      "Epoch time:  13.991204261779785\n",
      "Epoch 22/500 | Train Loss: 3684.320 | Test Loss: 925.451 | Test Loss [MAPE]: 2680933.821\n",
      "Epoch time:  14.009334564208984\n",
      "Epoch 23/500 | Train Loss: 3616.545 | Test Loss: 908.610 | Test Loss [MAPE]: 2525822.813\n",
      "Epoch time:  13.96482801437378\n",
      "Epoch 24/500 | Train Loss: 3552.264 | Test Loss: 882.318 | Test Loss [MAPE]: 2502709.574\n",
      "Epoch time:  13.97739577293396\n",
      "Epoch 25/500 | Train Loss: 3490.677 | Test Loss: 903.519 | Test Loss [MAPE]: 2589607.258\n",
      "Epoch time:  12.460724592208862\n",
      "Epoch 26/500 | Train Loss: 3451.979 | Test Loss: 867.746 | Test Loss [MAPE]: 2403415.731\n",
      "Epoch time:  13.999216556549072\n",
      "Epoch 27/500 | Train Loss: 3393.127 | Test Loss: 865.538 | Test Loss [MAPE]: 2419340.589\n",
      "Epoch time:  14.033648252487183\n",
      "Epoch 28/500 | Train Loss: 3352.533 | Test Loss: 839.718 | Test Loss [MAPE]: 2280059.816\n",
      "Epoch time:  14.01944088935852\n",
      "Epoch 29/500 | Train Loss: 3312.132 | Test Loss: 834.140 | Test Loss [MAPE]: 2237572.669\n",
      "Epoch time:  14.029979467391968\n",
      "Epoch 30/500 | Train Loss: 3277.602 | Test Loss: 832.598 | Test Loss [MAPE]: 2247813.332\n",
      "Epoch time:  14.009812116622925\n",
      "Epoch 31/500 | Train Loss: 3246.998 | Test Loss: 813.083 | Test Loss [MAPE]: 2169749.405\n",
      "Epoch time:  13.999141216278076\n",
      "Epoch 32/500 | Train Loss: 3205.437 | Test Loss: 805.818 | Test Loss [MAPE]: 2191851.382\n",
      "Epoch time:  13.962525129318237\n",
      "Epoch 33/500 | Train Loss: 3177.586 | Test Loss: 806.721 | Test Loss [MAPE]: 2179600.928\n",
      "Epoch time:  12.462831497192383\n",
      "Epoch 34/500 | Train Loss: 3154.551 | Test Loss: 788.596 | Test Loss [MAPE]: 2118440.000\n",
      "Epoch time:  13.99993634223938\n",
      "Epoch 35/500 | Train Loss: 3125.599 | Test Loss: 783.058 | Test Loss [MAPE]: 2084325.529\n",
      "Epoch time:  14.018723249435425\n",
      "Epoch 36/500 | Train Loss: 3101.999 | Test Loss: 800.780 | Test Loss [MAPE]: 2122774.037\n",
      "Epoch time:  12.467092037200928\n",
      "Epoch 37/500 | Train Loss: 3081.159 | Test Loss: 793.080 | Test Loss [MAPE]: 2105991.704\n",
      "Epoch time:  12.468301057815552\n",
      "Epoch 38/500 | Train Loss: 3056.062 | Test Loss: 778.581 | Test Loss [MAPE]: 2011462.045\n",
      "Epoch time:  14.017164707183838\n",
      "Epoch 39/500 | Train Loss: 3033.589 | Test Loss: 764.717 | Test Loss [MAPE]: 1970766.183\n",
      "Epoch time:  14.004916667938232\n",
      "Epoch 40/500 | Train Loss: 3018.827 | Test Loss: 762.214 | Test Loss [MAPE]: 1989019.038\n",
      "Epoch time:  14.018929243087769\n",
      "Epoch 41/500 | Train Loss: 3007.696 | Test Loss: 751.873 | Test Loss [MAPE]: 1913315.888\n",
      "Epoch time:  14.01197361946106\n",
      "Epoch 42/500 | Train Loss: 2978.976 | Test Loss: 759.666 | Test Loss [MAPE]: 1938055.086\n",
      "Epoch time:  12.465457439422607\n",
      "Epoch 43/500 | Train Loss: 2950.486 | Test Loss: 744.051 | Test Loss [MAPE]: 1887947.362\n",
      "Epoch time:  13.972927808761597\n",
      "Epoch 44/500 | Train Loss: 2933.797 | Test Loss: 746.805 | Test Loss [MAPE]: 1890732.712\n",
      "Epoch time:  12.466382503509521\n",
      "Epoch 45/500 | Train Loss: 2916.353 | Test Loss: 735.071 | Test Loss [MAPE]: 1833822.995\n",
      "Epoch time:  14.309457302093506\n",
      "Epoch 46/500 | Train Loss: 2898.260 | Test Loss: 735.183 | Test Loss [MAPE]: 1826455.688\n",
      "Epoch time:  12.484832048416138\n",
      "Epoch 47/500 | Train Loss: 2887.992 | Test Loss: 747.176 | Test Loss [MAPE]: 1895358.370\n",
      "Epoch time:  12.47109842300415\n",
      "Epoch 48/500 | Train Loss: 2862.941 | Test Loss: 718.574 | Test Loss [MAPE]: 1833859.639\n",
      "Epoch time:  14.008649110794067\n",
      "Epoch 49/500 | Train Loss: 2841.446 | Test Loss: 723.677 | Test Loss [MAPE]: 1825564.818\n",
      "Epoch time:  12.486306190490723\n",
      "Epoch 50/500 | Train Loss: 2824.108 | Test Loss: 713.942 | Test Loss [MAPE]: 1787922.509\n",
      "Epoch time:  13.999974966049194\n",
      "Epoch 51/500 | Train Loss: 2811.732 | Test Loss: 720.334 | Test Loss [MAPE]: 1814038.380\n",
      "Epoch time:  12.481244802474976\n",
      "Epoch 52/500 | Train Loss: 2794.308 | Test Loss: 717.081 | Test Loss [MAPE]: 1769068.523\n",
      "Epoch time:  12.4886155128479\n",
      "Epoch 53/500 | Train Loss: 2773.286 | Test Loss: 701.311 | Test Loss [MAPE]: 1765281.214\n",
      "Epoch time:  13.996411085128784\n",
      "Epoch 54/500 | Train Loss: 2765.108 | Test Loss: 698.418 | Test Loss [MAPE]: 1681061.202\n",
      "Epoch time:  14.01790976524353\n",
      "Epoch 55/500 | Train Loss: 2724.965 | Test Loss: 684.059 | Test Loss [MAPE]: 1627498.260\n",
      "Epoch time:  14.05574655532837\n",
      "Epoch 56/500 | Train Loss: 2708.188 | Test Loss: 689.179 | Test Loss [MAPE]: 1661444.983\n",
      "Epoch time:  12.496150970458984\n",
      "Epoch 57/500 | Train Loss: 2695.149 | Test Loss: 685.475 | Test Loss [MAPE]: 1577067.412\n",
      "Epoch time:  12.501732349395752\n",
      "Epoch 58/500 | Train Loss: 2700.722 | Test Loss: 685.414 | Test Loss [MAPE]: 1730590.235\n",
      "Epoch time:  12.496701002120972\n",
      "Epoch 59/500 | Train Loss: 2680.652 | Test Loss: 671.603 | Test Loss [MAPE]: 1569799.135\n",
      "Epoch time:  14.051136493682861\n",
      "Epoch 60/500 | Train Loss: 2659.324 | Test Loss: 676.804 | Test Loss [MAPE]: 1650478.552\n",
      "Epoch time:  12.479846000671387\n",
      "Epoch 61/500 | Train Loss: 2648.493 | Test Loss: 661.254 | Test Loss [MAPE]: 1603893.217\n",
      "Epoch time:  14.158057689666748\n",
      "Epoch 62/500 | Train Loss: 2633.745 | Test Loss: 666.709 | Test Loss [MAPE]: 1545335.242\n",
      "Epoch time:  12.478120803833008\n",
      "Epoch 63/500 | Train Loss: 2630.073 | Test Loss: 667.561 | Test Loss [MAPE]: 1528320.934\n",
      "Epoch time:  12.488309144973755\n",
      "Epoch 64/500 | Train Loss: 2613.290 | Test Loss: 668.866 | Test Loss [MAPE]: 1531364.035\n",
      "Epoch time:  12.48808479309082\n",
      "Epoch 65/500 | Train Loss: 2599.756 | Test Loss: 664.745 | Test Loss [MAPE]: 1582347.451\n",
      "Epoch time:  12.496442794799805\n",
      "Epoch 66/500 | Train Loss: 2594.844 | Test Loss: 665.407 | Test Loss [MAPE]: 1589480.696\n",
      "Epoch time:  12.492285013198853\n",
      "Epoch 67/500 | Train Loss: 2590.786 | Test Loss: 650.872 | Test Loss [MAPE]: 1492929.646\n",
      "Epoch time:  14.025491714477539\n",
      "Epoch 68/500 | Train Loss: 2570.044 | Test Loss: 648.597 | Test Loss [MAPE]: 1488015.959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time:  13.982146501541138\n",
      "Epoch 69/500 | Train Loss: 2581.176 | Test Loss: 668.165 | Test Loss [MAPE]: 1533140.967\n",
      "Epoch time:  12.484933614730835\n",
      "Epoch 70/500 | Train Loss: 2567.636 | Test Loss: 643.506 | Test Loss [MAPE]: 1520353.183\n",
      "Epoch time:  14.086224794387817\n",
      "Epoch 71/500 | Train Loss: 2568.186 | Test Loss: 652.302 | Test Loss [MAPE]: 1449881.989\n",
      "Epoch time:  12.477717161178589\n",
      "Epoch 72/500 | Train Loss: 2559.643 | Test Loss: 657.576 | Test Loss [MAPE]: 1544063.581\n",
      "Epoch time:  12.489020109176636\n",
      "Epoch 73/500 | Train Loss: 2539.163 | Test Loss: 645.693 | Test Loss [MAPE]: 1480685.173\n",
      "Epoch time:  12.489769458770752\n",
      "Epoch 74/500 | Train Loss: 2546.127 | Test Loss: 642.766 | Test Loss [MAPE]: 1386200.536\n",
      "Epoch time:  14.092162132263184\n",
      "Epoch 75/500 | Train Loss: 2530.428 | Test Loss: 645.899 | Test Loss [MAPE]: 1493723.327\n",
      "Epoch time:  12.482490062713623\n",
      "Epoch 76/500 | Train Loss: 2523.006 | Test Loss: 641.642 | Test Loss [MAPE]: 1465332.249\n",
      "Epoch time:  14.05443525314331\n",
      "Epoch 77/500 | Train Loss: 2523.361 | Test Loss: 647.310 | Test Loss [MAPE]: 1433147.653\n",
      "Epoch time:  12.481926918029785\n",
      "Epoch 78/500 | Train Loss: 2526.366 | Test Loss: 651.673 | Test Loss [MAPE]: 1540746.455\n",
      "Epoch time:  12.488949537277222\n",
      "Epoch 79/500 | Train Loss: 2517.787 | Test Loss: 636.984 | Test Loss [MAPE]: 1455697.995\n",
      "Epoch time:  14.060693264007568\n",
      "Epoch 80/500 | Train Loss: 2494.350 | Test Loss: 625.703 | Test Loss [MAPE]: 1378539.807\n",
      "Epoch time:  14.02150583267212\n",
      "Epoch 81/500 | Train Loss: 2506.708 | Test Loss: 632.354 | Test Loss [MAPE]: 1347393.176\n",
      "Epoch time:  12.481433629989624\n",
      "Epoch 82/500 | Train Loss: 2494.222 | Test Loss: 627.913 | Test Loss [MAPE]: 1405489.384\n",
      "Epoch time:  12.496743440628052\n",
      "Epoch 83/500 | Train Loss: 2509.611 | Test Loss: 636.345 | Test Loss [MAPE]: 1475870.673\n",
      "Epoch time:  12.490451574325562\n",
      "Epoch 84/500 | Train Loss: 2493.439 | Test Loss: 657.881 | Test Loss [MAPE]: 1471472.165\n",
      "Epoch time:  12.486536979675293\n",
      "Epoch 85/500 | Train Loss: 2493.183 | Test Loss: 624.508 | Test Loss [MAPE]: 1380943.168\n",
      "Epoch time:  14.059977054595947\n",
      "Epoch 86/500 | Train Loss: 2487.280 | Test Loss: 639.631 | Test Loss [MAPE]: 1471720.511\n",
      "Epoch time:  12.482390642166138\n",
      "Epoch 87/500 | Train Loss: 2479.158 | Test Loss: 625.063 | Test Loss [MAPE]: 1348627.190\n",
      "Epoch time:  12.492472410202026\n",
      "Epoch 88/500 | Train Loss: 2480.268 | Test Loss: 626.102 | Test Loss [MAPE]: 1410594.377\n",
      "Epoch time:  12.485503435134888\n",
      "Epoch 89/500 | Train Loss: 2476.399 | Test Loss: 628.649 | Test Loss [MAPE]: 1375552.764\n",
      "Epoch time:  12.493107318878174\n",
      "Epoch 90/500 | Train Loss: 2482.157 | Test Loss: 637.556 | Test Loss [MAPE]: 1484715.661\n",
      "Epoch time:  12.491016149520874\n",
      "Epoch 91/500 | Train Loss: 2476.026 | Test Loss: 620.402 | Test Loss [MAPE]: 1363477.286\n",
      "Epoch time:  14.054887771606445\n",
      "Epoch 92/500 | Train Loss: 2478.979 | Test Loss: 624.314 | Test Loss [MAPE]: 1411530.204\n",
      "Epoch time:  12.475981950759888\n",
      "Epoch 93/500 | Train Loss: 2461.252 | Test Loss: 625.614 | Test Loss [MAPE]: 1384672.460\n",
      "Epoch time:  12.48313593864441\n",
      "Epoch 94/500 | Train Loss: 2469.673 | Test Loss: 642.212 | Test Loss [MAPE]: 1486945.869\n",
      "Epoch time:  12.492335557937622\n",
      "Epoch 95/500 | Train Loss: 2468.204 | Test Loss: 626.344 | Test Loss [MAPE]: 1405115.558\n",
      "Epoch time:  12.489253759384155\n",
      "Epoch 96/500 | Train Loss: 2468.078 | Test Loss: 635.881 | Test Loss [MAPE]: 1435881.305\n",
      "Epoch time:  12.49121379852295\n",
      "Epoch 97/500 | Train Loss: 2464.603 | Test Loss: 626.881 | Test Loss [MAPE]: 1379187.009\n",
      "Epoch time:  12.490764379501343\n",
      "Epoch 98/500 | Train Loss: 2461.751 | Test Loss: 633.574 | Test Loss [MAPE]: 1407872.078\n",
      "Epoch time:  12.49477243423462\n",
      "Epoch 99/500 | Train Loss: 2461.582 | Test Loss: 630.340 | Test Loss [MAPE]: 1465667.986\n",
      "Epoch time:  12.491145372390747\n",
      "Epoch 100/500 | Train Loss: 2455.378 | Test Loss: 620.908 | Test Loss [MAPE]: 1327458.128\n",
      "Epoch time:  12.488343000411987\n",
      "Epoch 101/500 | Train Loss: 2454.346 | Test Loss: 619.823 | Test Loss [MAPE]: 1427506.536\n",
      "Epoch time:  14.046762466430664\n",
      "Epoch 102/500 | Train Loss: 2440.483 | Test Loss: 627.331 | Test Loss [MAPE]: 1429809.725\n",
      "Epoch time:  12.487433195114136\n",
      "Epoch 103/500 | Train Loss: 2456.827 | Test Loss: 618.132 | Test Loss [MAPE]: 1351317.354\n",
      "Epoch time:  14.002057075500488\n",
      "Epoch 104/500 | Train Loss: 2435.660 | Test Loss: 617.843 | Test Loss [MAPE]: 1381079.883\n",
      "Epoch time:  14.011980533599854\n",
      "Epoch 105/500 | Train Loss: 2437.840 | Test Loss: 627.298 | Test Loss [MAPE]: 1407186.435\n",
      "Epoch time:  12.479081153869629\n",
      "Epoch 106/500 | Train Loss: 2443.352 | Test Loss: 647.609 | Test Loss [MAPE]: 1498098.350\n",
      "Epoch time:  12.491761922836304\n",
      "Epoch 107/500 | Train Loss: 2443.454 | Test Loss: 613.368 | Test Loss [MAPE]: 1322252.784\n",
      "Epoch time:  13.998563289642334\n",
      "Epoch 108/500 | Train Loss: 2446.782 | Test Loss: 632.161 | Test Loss [MAPE]: 1343300.941\n",
      "Epoch time:  12.483898162841797\n",
      "Epoch 109/500 | Train Loss: 2441.579 | Test Loss: 621.120 | Test Loss [MAPE]: 1341956.417\n",
      "Epoch time:  12.4913170337677\n",
      "Epoch 110/500 | Train Loss: 2425.015 | Test Loss: 622.533 | Test Loss [MAPE]: 1288539.834\n",
      "Epoch time:  12.492435455322266\n",
      "Epoch 111/500 | Train Loss: 2436.656 | Test Loss: 642.894 | Test Loss [MAPE]: 1374357.901\n",
      "Epoch time:  12.496214866638184\n",
      "Epoch 112/500 | Train Loss: 2428.594 | Test Loss: 615.167 | Test Loss [MAPE]: 1340296.595\n",
      "Epoch time:  12.490358829498291\n",
      "Epoch 113/500 | Train Loss: 2430.951 | Test Loss: 631.167 | Test Loss [MAPE]: 1334457.286\n",
      "Epoch time:  12.48859453201294\n",
      "Epoch 114/500 | Train Loss: 2438.089 | Test Loss: 636.198 | Test Loss [MAPE]: 1451670.389\n",
      "Epoch time:  12.493386268615723\n",
      "Epoch 115/500 | Train Loss: 2431.679 | Test Loss: 612.513 | Test Loss [MAPE]: 1315746.130\n",
      "Epoch time:  14.020497560501099\n",
      "Epoch 116/500 | Train Loss: 2412.326 | Test Loss: 614.540 | Test Loss [MAPE]: 1316632.162\n",
      "Epoch time:  12.486675500869751\n",
      "Epoch 117/500 | Train Loss: 2423.198 | Test Loss: 615.717 | Test Loss [MAPE]: 1334187.459\n",
      "Epoch time:  12.48926067352295\n",
      "Epoch 118/500 | Train Loss: 2425.861 | Test Loss: 614.212 | Test Loss [MAPE]: 1279582.722\n",
      "Epoch time:  12.488500833511353\n",
      "Epoch 119/500 | Train Loss: 2420.608 | Test Loss: 620.257 | Test Loss [MAPE]: 1355709.025\n",
      "Epoch time:  12.494369506835938\n",
      "Epoch 120/500 | Train Loss: 2420.100 | Test Loss: 628.930 | Test Loss [MAPE]: 1369348.921\n",
      "Epoch time:  12.492063999176025\n",
      "Epoch 121/500 | Train Loss: 2414.663 | Test Loss: 606.946 | Test Loss [MAPE]: 1265062.468\n",
      "Epoch time:  14.073749780654907\n",
      "Epoch 122/500 | Train Loss: 2412.217 | Test Loss: 618.420 | Test Loss [MAPE]: 1325401.544\n",
      "Epoch time:  12.482444763183594\n",
      "Epoch 123/500 | Train Loss: 2411.679 | Test Loss: 612.030 | Test Loss [MAPE]: 1303127.060\n",
      "Epoch time:  12.484287023544312\n",
      "Epoch 124/500 | Train Loss: 2416.340 | Test Loss: 615.269 | Test Loss [MAPE]: 1320242.099\n",
      "Epoch time:  12.486084699630737\n",
      "Epoch 125/500 | Train Loss: 2417.204 | Test Loss: 605.671 | Test Loss [MAPE]: 1365549.240\n",
      "Epoch time:  14.018725633621216\n",
      "Epoch 126/500 | Train Loss: 2408.484 | Test Loss: 608.799 | Test Loss [MAPE]: 1273230.729\n",
      "Epoch time:  12.47337007522583\n",
      "Epoch 127/500 | Train Loss: 2405.870 | Test Loss: 634.228 | Test Loss [MAPE]: 1429455.138\n",
      "Epoch time:  12.481641292572021\n",
      "Epoch 128/500 | Train Loss: 2404.488 | Test Loss: 623.179 | Test Loss [MAPE]: 1347607.065\n",
      "Epoch time:  12.48294448852539\n",
      "Epoch 129/500 | Train Loss: 2408.437 | Test Loss: 607.089 | Test Loss [MAPE]: 1289952.886\n",
      "Epoch time:  12.48313045501709\n",
      "Epoch 130/500 | Train Loss: 2399.154 | Test Loss: 603.780 | Test Loss [MAPE]: 1209109.952\n",
      "Epoch time:  14.05327844619751\n",
      "Epoch 131/500 | Train Loss: 2394.981 | Test Loss: 610.191 | Test Loss [MAPE]: 1310580.520\n",
      "Epoch time:  12.710773468017578\n",
      "Epoch 132/500 | Train Loss: 2391.554 | Test Loss: 609.521 | Test Loss [MAPE]: 1364921.556\n",
      "Epoch time:  12.486639976501465\n",
      "Epoch 133/500 | Train Loss: 2393.963 | Test Loss: 620.765 | Test Loss [MAPE]: 1333322.616\n",
      "Epoch time:  12.484846353530884\n",
      "Epoch 134/500 | Train Loss: 2399.073 | Test Loss: 605.598 | Test Loss [MAPE]: 1258908.732\n",
      "Epoch time:  12.48693060874939\n",
      "Epoch 135/500 | Train Loss: 2384.502 | Test Loss: 605.983 | Test Loss [MAPE]: 1338416.921\n",
      "Epoch time:  12.486443281173706\n",
      "Epoch 136/500 | Train Loss: 2377.410 | Test Loss: 601.456 | Test Loss [MAPE]: 1279938.207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time:  14.01552438735962\n",
      "Epoch 137/500 | Train Loss: 2380.772 | Test Loss: 603.864 | Test Loss [MAPE]: 1272609.887\n",
      "Epoch time:  12.47874927520752\n",
      "Epoch 138/500 | Train Loss: 2382.238 | Test Loss: 612.463 | Test Loss [MAPE]: 1291948.890\n",
      "Epoch time:  12.4853196144104\n",
      "Epoch 139/500 | Train Loss: 2389.877 | Test Loss: 613.793 | Test Loss [MAPE]: 1289264.060\n",
      "Epoch time:  12.494025707244873\n",
      "Epoch 140/500 | Train Loss: 2383.154 | Test Loss: 622.043 | Test Loss [MAPE]: 1365925.988\n",
      "Epoch time:  12.491093158721924\n",
      "Epoch 141/500 | Train Loss: 2377.380 | Test Loss: 625.896 | Test Loss [MAPE]: 1409569.016\n",
      "Epoch time:  12.494292497634888\n",
      "Epoch 142/500 | Train Loss: 2376.306 | Test Loss: 603.564 | Test Loss [MAPE]: 1359516.847\n",
      "Epoch time:  12.492424964904785\n",
      "Epoch 143/500 | Train Loss: 2378.365 | Test Loss: 623.585 | Test Loss [MAPE]: 1304551.085\n",
      "Epoch time:  12.770832538604736\n",
      "Epoch 144/500 | Train Loss: 2388.178 | Test Loss: 609.714 | Test Loss [MAPE]: 1300786.593\n",
      "Epoch time:  23.893407344818115\n",
      "Epoch 145/500 | Train Loss: 2361.399 | Test Loss: 601.094 | Test Loss [MAPE]: 1245437.780\n",
      "Epoch time:  27.042007207870483\n",
      "Epoch 146/500 | Train Loss: 2367.183 | Test Loss: 606.042 | Test Loss [MAPE]: 1253948.990\n",
      "Epoch time:  25.511287689208984\n",
      "Epoch 147/500 | Train Loss: 2366.433 | Test Loss: 595.472 | Test Loss [MAPE]: 1257370.712\n",
      "Epoch time:  26.989206314086914\n",
      "Epoch 148/500 | Train Loss: 2351.368 | Test Loss: 590.536 | Test Loss [MAPE]: 1206473.093\n",
      "Epoch time:  26.97554326057434\n",
      "Epoch 149/500 | Train Loss: 2361.646 | Test Loss: 620.873 | Test Loss [MAPE]: 1288763.144\n",
      "Epoch time:  25.52499485015869\n",
      "Epoch 150/500 | Train Loss: 2356.797 | Test Loss: 614.890 | Test Loss [MAPE]: 1339044.539\n",
      "Epoch time:  25.496876001358032\n",
      "Epoch 151/500 | Train Loss: 2346.728 | Test Loss: 595.042 | Test Loss [MAPE]: 1241649.034\n",
      "Epoch time:  25.476044178009033\n",
      "Epoch 152/500 | Train Loss: 2350.543 | Test Loss: 595.949 | Test Loss [MAPE]: 1275008.885\n",
      "Epoch time:  25.461408138275146\n",
      "Epoch 153/500 | Train Loss: 2351.305 | Test Loss: 604.116 | Test Loss [MAPE]: 1303854.658\n",
      "Epoch time:  25.475011587142944\n",
      "Epoch 154/500 | Train Loss: 2349.795 | Test Loss: 597.834 | Test Loss [MAPE]: 1238555.620\n",
      "Epoch time:  25.562801122665405\n",
      "Epoch 155/500 | Train Loss: 2346.615 | Test Loss: 596.178 | Test Loss [MAPE]: 1136873.782\n",
      "Epoch time:  25.51948380470276\n",
      "Epoch 156/500 | Train Loss: 2337.250 | Test Loss: 606.105 | Test Loss [MAPE]: 1267132.086\n",
      "Epoch time:  25.50271964073181\n",
      "Epoch 157/500 | Train Loss: 2334.211 | Test Loss: 599.935 | Test Loss [MAPE]: 1278803.289\n",
      "Epoch time:  25.48113775253296\n",
      "Epoch 158/500 | Train Loss: 2336.904 | Test Loss: 595.363 | Test Loss [MAPE]: 1258122.970\n",
      "Epoch time:  25.470221757888794\n",
      "Epoch 159/500 | Train Loss: 2335.133 | Test Loss: 596.919 | Test Loss [MAPE]: 1279515.039\n",
      "Epoch time:  25.483567237854004\n",
      "Epoch 160/500 | Train Loss: 2325.990 | Test Loss: 599.095 | Test Loss [MAPE]: 1267971.831\n",
      "Epoch time:  25.538414001464844\n",
      "Epoch 161/500 | Train Loss: 2331.431 | Test Loss: 591.709 | Test Loss [MAPE]: 1156781.837\n",
      "Epoch time:  25.533451795578003\n",
      "Epoch 162/500 | Train Loss: 2323.311 | Test Loss: 598.058 | Test Loss [MAPE]: 1182646.965\n",
      "Epoch time:  25.495441198349\n",
      "Epoch 163/500 | Train Loss: 2327.332 | Test Loss: 594.428 | Test Loss [MAPE]: 1225284.160\n",
      "Epoch time:  25.485724687576294\n",
      "Epoch 164/500 | Train Loss: 2314.823 | Test Loss: 593.502 | Test Loss [MAPE]: 1302386.271\n",
      "Epoch time:  25.465628385543823\n",
      "Epoch 165/500 | Train Loss: 2312.378 | Test Loss: 602.948 | Test Loss [MAPE]: 1261511.531\n",
      "Epoch time:  25.481845140457153\n",
      "Epoch 166/500 | Train Loss: 2306.173 | Test Loss: 606.167 | Test Loss [MAPE]: 1298846.389\n",
      "Epoch time:  25.52720594406128\n",
      "Epoch 167/500 | Train Loss: 2312.880 | Test Loss: 595.859 | Test Loss [MAPE]: 1253092.035\n",
      "Epoch time:  25.53284788131714\n",
      "Epoch 168/500 | Train Loss: 2303.465 | Test Loss: 596.531 | Test Loss [MAPE]: 1099845.334\n",
      "Epoch time:  25.490947008132935\n",
      "Epoch 169/500 | Train Loss: 2301.047 | Test Loss: 591.741 | Test Loss [MAPE]: 1212592.497\n",
      "Epoch time:  25.471642017364502\n",
      "Epoch 170/500 | Train Loss: 2290.265 | Test Loss: 581.324 | Test Loss [MAPE]: 1187095.542\n",
      "Epoch time:  26.99019765853882\n",
      "Epoch 171/500 | Train Loss: 2275.101 | Test Loss: 580.162 | Test Loss [MAPE]: 1179485.297\n",
      "Epoch time:  26.823827266693115\n",
      "Epoch 172/500 | Train Loss: 2292.604 | Test Loss: 579.988 | Test Loss [MAPE]: 1145824.501\n",
      "Epoch time:  26.881092309951782\n",
      "Epoch 173/500 | Train Loss: 2278.421 | Test Loss: 578.174 | Test Loss [MAPE]: 1123740.436\n",
      "Epoch time:  27.107857704162598\n",
      "Epoch 174/500 | Train Loss: 2273.178 | Test Loss: 578.646 | Test Loss [MAPE]: 1165839.926\n",
      "Epoch time:  25.557310819625854\n",
      "Epoch 175/500 | Train Loss: 2277.455 | Test Loss: 577.489 | Test Loss [MAPE]: 1105216.719\n",
      "Epoch time:  27.009986400604248\n",
      "Epoch 176/500 | Train Loss: 2262.750 | Test Loss: 570.502 | Test Loss [MAPE]: 1090472.375\n",
      "Epoch time:  27.094497442245483\n",
      "Epoch 177/500 | Train Loss: 2271.955 | Test Loss: 579.572 | Test Loss [MAPE]: 1065191.136\n",
      "Epoch time:  25.46299386024475\n",
      "Epoch 178/500 | Train Loss: 2273.379 | Test Loss: 585.553 | Test Loss [MAPE]: 1092466.162\n",
      "Epoch time:  25.555030584335327\n",
      "Epoch 179/500 | Train Loss: 2268.249 | Test Loss: 574.114 | Test Loss [MAPE]: 1174052.754\n",
      "Epoch time:  25.530679941177368\n",
      "Epoch 180/500 | Train Loss: 2255.483 | Test Loss: 587.334 | Test Loss [MAPE]: 1133417.239\n",
      "Epoch time:  25.48500394821167\n",
      "Epoch 181/500 | Train Loss: 2249.732 | Test Loss: 570.610 | Test Loss [MAPE]: 1076905.442\n",
      "Epoch time:  25.476587057113647\n",
      "Epoch 182/500 | Train Loss: 2246.195 | Test Loss: 572.027 | Test Loss [MAPE]: 1082746.134\n",
      "Epoch time:  25.46727442741394\n",
      "Epoch 183/500 | Train Loss: 2244.135 | Test Loss: 582.142 | Test Loss [MAPE]: 1139667.853\n",
      "Epoch time:  25.479785442352295\n",
      "Epoch 184/500 | Train Loss: 2245.308 | Test Loss: 577.839 | Test Loss [MAPE]: 1063125.523\n",
      "Epoch time:  25.529640436172485\n",
      "Epoch 185/500 | Train Loss: 2250.342 | Test Loss: 580.849 | Test Loss [MAPE]: 1156002.390\n",
      "Epoch time:  25.536468505859375\n",
      "Epoch 186/500 | Train Loss: 2233.478 | Test Loss: 568.612 | Test Loss [MAPE]: 1064608.546\n",
      "Epoch time:  27.020111799240112\n",
      "Epoch 187/500 | Train Loss: 2232.748 | Test Loss: 575.445 | Test Loss [MAPE]: 1161966.250\n",
      "Epoch time:  25.53186559677124\n",
      "Epoch 188/500 | Train Loss: 2230.030 | Test Loss: 574.145 | Test Loss [MAPE]: 1029441.001\n",
      "Epoch time:  25.46881937980652\n",
      "Epoch 189/500 | Train Loss: 2214.511 | Test Loss: 566.679 | Test Loss [MAPE]: 1040460.509\n",
      "Epoch time:  26.92185616493225\n",
      "Epoch 190/500 | Train Loss: 2221.895 | Test Loss: 560.392 | Test Loss [MAPE]: 989614.096\n",
      "Epoch time:  26.975162267684937\n",
      "Epoch 191/500 | Train Loss: 2212.692 | Test Loss: 572.526 | Test Loss [MAPE]: 1165963.153\n",
      "Epoch time:  25.572978734970093\n",
      "Epoch 192/500 | Train Loss: 2214.376 | Test Loss: 563.513 | Test Loss [MAPE]: 1031256.859\n",
      "Epoch time:  25.452298879623413\n",
      "Epoch 193/500 | Train Loss: 2217.606 | Test Loss: 569.454 | Test Loss [MAPE]: 954750.986\n",
      "Epoch time:  25.499571561813354\n",
      "Epoch 194/500 | Train Loss: 2211.060 | Test Loss: 562.843 | Test Loss [MAPE]: 1019863.808\n",
      "Epoch time:  25.4737765789032\n",
      "Epoch 195/500 | Train Loss: 2208.646 | Test Loss: 563.720 | Test Loss [MAPE]: 1018836.475\n",
      "Epoch time:  25.459201097488403\n",
      "Epoch 196/500 | Train Loss: 2212.794 | Test Loss: 557.754 | Test Loss [MAPE]: 959731.192\n",
      "Epoch time:  26.977519512176514\n",
      "Epoch 197/500 | Train Loss: 2197.279 | Test Loss: 563.966 | Test Loss [MAPE]: 1030370.352\n",
      "Epoch time:  25.47016668319702\n",
      "Epoch 198/500 | Train Loss: 2195.465 | Test Loss: 564.567 | Test Loss [MAPE]: 1012041.409\n",
      "Epoch time:  25.466123342514038\n",
      "Epoch 199/500 | Train Loss: 2203.336 | Test Loss: 566.841 | Test Loss [MAPE]: 980237.790\n",
      "Epoch time:  25.540256023406982\n",
      "Epoch 200/500 | Train Loss: 2199.636 | Test Loss: 561.000 | Test Loss [MAPE]: 945657.306\n",
      "Epoch time:  25.539153337478638\n",
      "Epoch 201/500 | Train Loss: 2186.393 | Test Loss: 560.248 | Test Loss [MAPE]: 1023084.897\n",
      "Epoch time:  25.49497079849243\n",
      "Epoch 202/500 | Train Loss: 2182.552 | Test Loss: 561.906 | Test Loss [MAPE]: 978091.273\n",
      "Epoch time:  25.46883273124695\n",
      "Epoch 203/500 | Train Loss: 2186.050 | Test Loss: 565.334 | Test Loss [MAPE]: 996097.394\n",
      "Epoch time:  25.454978704452515\n",
      "Epoch 204/500 | Train Loss: 2194.936 | Test Loss: 559.916 | Test Loss [MAPE]: 1044758.206\n",
      "Epoch time:  25.471363067626953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/500 | Train Loss: 2199.522 | Test Loss: 557.907 | Test Loss [MAPE]: 921499.410\n",
      "Epoch time:  25.52650475502014\n",
      "Epoch 206/500 | Train Loss: 2179.082 | Test Loss: 553.323 | Test Loss [MAPE]: 937444.171\n",
      "Epoch time:  27.068787336349487\n",
      "Epoch 207/500 | Train Loss: 2181.267 | Test Loss: 583.718 | Test Loss [MAPE]: 1035174.334\n",
      "Epoch time:  25.57300066947937\n",
      "Epoch 208/500 | Train Loss: 2180.245 | Test Loss: 555.124 | Test Loss [MAPE]: 918110.998\n",
      "Epoch time:  25.494573831558228\n",
      "Epoch 209/500 | Train Loss: 2171.138 | Test Loss: 552.438 | Test Loss [MAPE]: 915809.197\n",
      "Epoch time:  26.91838240623474\n",
      "Epoch 210/500 | Train Loss: 2177.156 | Test Loss: 568.415 | Test Loss [MAPE]: 1017096.132\n",
      "Epoch time:  25.509474992752075\n",
      "Epoch 211/500 | Train Loss: 2180.272 | Test Loss: 561.575 | Test Loss [MAPE]: 992544.124\n",
      "Epoch time:  25.477346658706665\n",
      "Epoch 212/500 | Train Loss: 2182.464 | Test Loss: 567.161 | Test Loss [MAPE]: 1021396.484\n",
      "Epoch time:  25.48612880706787\n",
      "Epoch 213/500 | Train Loss: 2171.922 | Test Loss: 549.959 | Test Loss [MAPE]: 955426.199\n",
      "Epoch time:  26.98177719116211\n",
      "Epoch 214/500 | Train Loss: 2181.608 | Test Loss: 554.300 | Test Loss [MAPE]: 947575.702\n",
      "Epoch time:  25.475268840789795\n",
      "Epoch 215/500 | Train Loss: 2163.977 | Test Loss: 575.865 | Test Loss [MAPE]: 1014014.571\n",
      "Epoch time:  25.440441608428955\n",
      "Epoch 216/500 | Train Loss: 2167.959 | Test Loss: 564.677 | Test Loss [MAPE]: 1017659.550\n",
      "Epoch time:  25.44422960281372\n",
      "Epoch 217/500 | Train Loss: 2161.379 | Test Loss: 556.256 | Test Loss [MAPE]: 947977.627\n",
      "Epoch time:  25.52463936805725\n",
      "Epoch 218/500 | Train Loss: 2174.562 | Test Loss: 557.492 | Test Loss [MAPE]: 1009318.339\n",
      "Epoch time:  25.518953561782837\n",
      "Epoch 219/500 | Train Loss: 2163.241 | Test Loss: 556.820 | Test Loss [MAPE]: 949679.926\n",
      "Epoch time:  25.480096101760864\n",
      "Epoch 220/500 | Train Loss: 2168.286 | Test Loss: 586.981 | Test Loss [MAPE]: 990315.989\n",
      "Epoch time:  25.46250009536743\n",
      "Epoch 221/500 | Train Loss: 2164.300 | Test Loss: 547.112 | Test Loss [MAPE]: 961599.078\n",
      "Epoch time:  27.010799646377563\n",
      "Epoch 222/500 | Train Loss: 2160.681 | Test Loss: 557.516 | Test Loss [MAPE]: 953477.761\n",
      "Epoch time:  25.474962949752808\n",
      "Epoch 223/500 | Train Loss: 2159.430 | Test Loss: 554.965 | Test Loss [MAPE]: 959189.866\n",
      "Epoch time:  25.44493007659912\n",
      "Epoch 224/500 | Train Loss: 2168.328 | Test Loss: 563.940 | Test Loss [MAPE]: 989634.292\n",
      "Epoch time:  25.470107078552246\n",
      "Epoch 225/500 | Train Loss: 2142.678 | Test Loss: 549.317 | Test Loss [MAPE]: 864427.163\n",
      "Epoch time:  25.48197627067566\n",
      "Epoch 226/500 | Train Loss: 2155.090 | Test Loss: 553.491 | Test Loss [MAPE]: 983515.699\n",
      "Epoch time:  25.544793367385864\n",
      "Epoch 227/500 | Train Loss: 2157.754 | Test Loss: 560.505 | Test Loss [MAPE]: 967038.854\n",
      "Epoch time:  25.46013855934143\n",
      "Epoch 228/500 | Train Loss: 2148.645 | Test Loss: 552.304 | Test Loss [MAPE]: 962436.701\n",
      "Epoch time:  25.466469287872314\n",
      "Epoch 229/500 | Train Loss: 2151.647 | Test Loss: 551.621 | Test Loss [MAPE]: 911304.387\n",
      "Epoch time:  25.45278525352478\n",
      "Epoch 230/500 | Train Loss: 2162.983 | Test Loss: 564.335 | Test Loss [MAPE]: 854583.175\n",
      "Epoch time:  25.462782382965088\n",
      "Epoch 231/500 | Train Loss: 2154.797 | Test Loss: 549.512 | Test Loss [MAPE]: 908742.251\n",
      "Epoch time:  25.47598695755005\n",
      "Epoch 232/500 | Train Loss: 2144.026 | Test Loss: 560.908 | Test Loss [MAPE]: 977214.901\n",
      "Epoch time:  25.56657648086548\n",
      "Epoch 233/500 | Train Loss: 2163.817 | Test Loss: 556.976 | Test Loss [MAPE]: 964675.631\n",
      "Epoch time:  25.50487208366394\n",
      "Epoch 234/500 | Train Loss: 2156.890 | Test Loss: 552.716 | Test Loss [MAPE]: 927819.847\n",
      "Epoch time:  25.512763500213623\n",
      "Epoch 235/500 | Train Loss: 2142.814 | Test Loss: 551.832 | Test Loss [MAPE]: 886195.498\n",
      "Epoch time:  25.470870971679688\n",
      "Epoch 236/500 | Train Loss: 2158.968 | Test Loss: 549.352 | Test Loss [MAPE]: 973518.205\n",
      "Epoch time:  25.455732107162476\n",
      "Epoch 237/500 | Train Loss: 2147.492 | Test Loss: 551.086 | Test Loss [MAPE]: 919026.735\n",
      "Epoch time:  25.45231032371521\n",
      "Epoch 238/500 | Train Loss: 2152.475 | Test Loss: 550.475 | Test Loss [MAPE]: 883157.219\n",
      "Epoch time:  25.54407787322998\n",
      "Epoch 239/500 | Train Loss: 2139.851 | Test Loss: 547.933 | Test Loss [MAPE]: 918269.787\n",
      "Epoch time:  25.465254068374634\n",
      "Epoch 240/500 | Train Loss: 2142.091 | Test Loss: 553.012 | Test Loss [MAPE]: 956605.613\n",
      "Epoch time:  25.47706174850464\n",
      "Epoch 241/500 | Train Loss: 2136.620 | Test Loss: 560.662 | Test Loss [MAPE]: 997069.752\n",
      "Epoch time:  25.486183166503906\n",
      "Epoch 242/500 | Train Loss: 2141.009 | Test Loss: 550.742 | Test Loss [MAPE]: 811243.417\n",
      "Epoch time:  25.4564311504364\n",
      "Epoch 243/500 | Train Loss: 2143.843 | Test Loss: 548.245 | Test Loss [MAPE]: 938930.323\n",
      "Epoch time:  25.441834449768066\n",
      "Epoch 244/500 | Train Loss: 2149.445 | Test Loss: 551.660 | Test Loss [MAPE]: 888038.926\n",
      "Epoch time:  25.530497312545776\n",
      "Epoch 245/500 | Train Loss: 2134.989 | Test Loss: 546.942 | Test Loss [MAPE]: 926302.476\n",
      "Epoch time:  27.020599842071533\n",
      "Epoch 246/500 | Train Loss: 2130.769 | Test Loss: 557.666 | Test Loss [MAPE]: 935842.781\n",
      "Epoch time:  25.530892848968506\n",
      "Epoch 247/500 | Train Loss: 2140.141 | Test Loss: 548.891 | Test Loss [MAPE]: 841725.702\n",
      "Epoch time:  25.489741563796997\n",
      "Epoch 248/500 | Train Loss: 2130.173 | Test Loss: 540.710 | Test Loss [MAPE]: 879973.290\n",
      "Epoch time:  26.892574071884155\n",
      "Epoch 249/500 | Train Loss: 2139.457 | Test Loss: 551.727 | Test Loss [MAPE]: 936842.440\n",
      "Epoch time:  25.50015163421631\n",
      "Epoch 250/500 | Train Loss: 2131.420 | Test Loss: 542.330 | Test Loss [MAPE]: 886140.879\n",
      "Epoch time:  25.48059105873108\n",
      "Epoch 251/500 | Train Loss: 2132.523 | Test Loss: 550.558 | Test Loss [MAPE]: 988831.713\n",
      "Epoch time:  25.450372457504272\n",
      "Epoch 252/500 | Train Loss: 2140.550 | Test Loss: 553.450 | Test Loss [MAPE]: 851144.006\n",
      "Epoch time:  25.47693634033203\n",
      "Epoch 253/500 | Train Loss: 2126.318 | Test Loss: 556.292 | Test Loss [MAPE]: 1030038.052\n",
      "Epoch time:  25.460612058639526\n",
      "Epoch 254/500 | Train Loss: 2128.281 | Test Loss: 563.406 | Test Loss [MAPE]: 962554.641\n",
      "Epoch time:  25.513272762298584\n",
      "Epoch 255/500 | Train Loss: 2123.816 | Test Loss: 570.836 | Test Loss [MAPE]: 1018328.312\n",
      "Epoch time:  25.513750791549683\n",
      "Epoch 256/500 | Train Loss: 2145.132 | Test Loss: 539.936 | Test Loss [MAPE]: 850883.557\n",
      "Epoch time:  27.055876970291138\n",
      "Epoch 257/500 | Train Loss: 2125.592 | Test Loss: 558.955 | Test Loss [MAPE]: 946324.801\n",
      "Epoch time:  25.54452347755432\n",
      "Epoch 258/500 | Train Loss: 2119.603 | Test Loss: 540.523 | Test Loss [MAPE]: 924540.000\n",
      "Epoch time:  25.481494188308716\n",
      "Epoch 259/500 | Train Loss: 2122.884 | Test Loss: 552.534 | Test Loss [MAPE]: 1032540.784\n",
      "Epoch time:  25.46219277381897\n",
      "Epoch 260/500 | Train Loss: 2123.129 | Test Loss: 553.181 | Test Loss [MAPE]: 967708.500\n",
      "Epoch time:  25.4500892162323\n",
      "Epoch 261/500 | Train Loss: 2119.686 | Test Loss: 553.793 | Test Loss [MAPE]: 993312.625\n",
      "Epoch time:  25.444265127182007\n",
      "Epoch 262/500 | Train Loss: 2124.801 | Test Loss: 542.263 | Test Loss [MAPE]: 802671.915\n",
      "Epoch time:  25.482110023498535\n",
      "Epoch 263/500 | Train Loss: 2116.351 | Test Loss: 547.805 | Test Loss [MAPE]: 920783.813\n",
      "Epoch time:  25.564394235610962\n",
      "Epoch 264/500 | Train Loss: 2129.868 | Test Loss: 535.561 | Test Loss [MAPE]: 802549.965\n",
      "Epoch time:  27.007072925567627\n",
      "Epoch 265/500 | Train Loss: 2114.124 | Test Loss: 535.367 | Test Loss [MAPE]: 875333.320\n",
      "Epoch time:  27.081083297729492\n",
      "Epoch 266/500 | Train Loss: 2113.347 | Test Loss: 546.205 | Test Loss [MAPE]: 832648.996\n",
      "Epoch time:  25.44809627532959\n",
      "Epoch 267/500 | Train Loss: 2114.712 | Test Loss: 545.886 | Test Loss [MAPE]: 799853.624\n",
      "Epoch time:  25.53626799583435\n",
      "Epoch 268/500 | Train Loss: 2117.548 | Test Loss: 543.147 | Test Loss [MAPE]: 986063.038\n",
      "Epoch time:  15.789844274520874\n",
      "Epoch 269/500 | Train Loss: 2117.867 | Test Loss: 537.218 | Test Loss [MAPE]: 778147.950\n",
      "Epoch time:  12.506265878677368\n",
      "Epoch 270/500 | Train Loss: 2105.328 | Test Loss: 547.184 | Test Loss [MAPE]: 843478.726\n",
      "Epoch time:  12.50887656211853\n",
      "Epoch 271/500 | Train Loss: 2120.678 | Test Loss: 537.097 | Test Loss [MAPE]: 794980.065\n",
      "Epoch time:  12.509926795959473\n",
      "Epoch 272/500 | Train Loss: 2108.486 | Test Loss: 561.458 | Test Loss [MAPE]: 1004670.768\n",
      "Epoch time:  12.506884336471558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273/500 | Train Loss: 2112.615 | Test Loss: 544.665 | Test Loss [MAPE]: 910679.810\n",
      "Epoch time:  12.506157636642456\n",
      "Epoch 274/500 | Train Loss: 2120.726 | Test Loss: 551.179 | Test Loss [MAPE]: 858773.113\n",
      "Epoch time:  12.500523090362549\n",
      "Epoch 275/500 | Train Loss: 2105.963 | Test Loss: 545.113 | Test Loss [MAPE]: 768230.012\n",
      "Epoch time:  12.50065016746521\n",
      "Epoch 276/500 | Train Loss: 2122.291 | Test Loss: 555.813 | Test Loss [MAPE]: 953121.889\n",
      "Epoch time:  12.496757507324219\n",
      "Epoch 277/500 | Train Loss: 2119.290 | Test Loss: 550.973 | Test Loss [MAPE]: 916776.400\n",
      "Epoch time:  12.488107919692993\n",
      "Epoch 278/500 | Train Loss: 2103.110 | Test Loss: 541.877 | Test Loss [MAPE]: 845768.093\n",
      "Epoch time:  12.494056224822998\n",
      "Epoch 279/500 | Train Loss: 2103.582 | Test Loss: 542.459 | Test Loss [MAPE]: 881573.976\n",
      "Epoch time:  12.495055198669434\n",
      "Epoch 280/500 | Train Loss: 2111.619 | Test Loss: 536.596 | Test Loss [MAPE]: 799450.520\n",
      "Epoch time:  12.49289059638977\n",
      "Epoch 281/500 | Train Loss: 2093.769 | Test Loss: 543.752 | Test Loss [MAPE]: 876739.476\n",
      "Epoch time:  12.486692190170288\n",
      "Epoch 282/500 | Train Loss: 2107.478 | Test Loss: 546.119 | Test Loss [MAPE]: 848159.319\n",
      "Epoch time:  12.490910530090332\n",
      "Epoch 283/500 | Train Loss: 2103.443 | Test Loss: 553.258 | Test Loss [MAPE]: 861710.911\n",
      "Epoch time:  12.48949909210205\n",
      "Epoch 284/500 | Train Loss: 2106.645 | Test Loss: 555.077 | Test Loss [MAPE]: 835755.784\n",
      "Epoch time:  12.494833707809448\n",
      "Epoch 285/500 | Train Loss: 2095.858 | Test Loss: 549.628 | Test Loss [MAPE]: 903730.054\n",
      "Epoch time:  12.488200187683105\n",
      "Epoch 286/500 | Train Loss: 2112.359 | Test Loss: 544.214 | Test Loss [MAPE]: 808963.201\n",
      "Epoch time:  12.494036436080933\n",
      "Epoch 287/500 | Train Loss: 2099.539 | Test Loss: 534.080 | Test Loss [MAPE]: 805941.842\n",
      "Epoch time:  14.087641477584839\n",
      "Epoch 288/500 | Train Loss: 2107.116 | Test Loss: 538.282 | Test Loss [MAPE]: 902001.044\n",
      "Epoch time:  12.484612703323364\n",
      "Epoch 289/500 | Train Loss: 2113.349 | Test Loss: 552.867 | Test Loss [MAPE]: 865786.551\n",
      "Epoch time:  12.488483190536499\n",
      "Epoch 290/500 | Train Loss: 2098.817 | Test Loss: 555.642 | Test Loss [MAPE]: 918034.486\n",
      "Epoch time:  12.491568803787231\n",
      "Epoch 291/500 | Train Loss: 2102.584 | Test Loss: 549.585 | Test Loss [MAPE]: 968732.807\n",
      "Epoch time:  12.494410037994385\n",
      "Epoch 292/500 | Train Loss: 2094.503 | Test Loss: 556.418 | Test Loss [MAPE]: 908538.312\n",
      "Epoch time:  12.492493152618408\n",
      "Epoch 293/500 | Train Loss: 2093.870 | Test Loss: 552.198 | Test Loss [MAPE]: 862528.176\n",
      "Epoch time:  12.493829250335693\n",
      "Epoch 294/500 | Train Loss: 2103.010 | Test Loss: 550.245 | Test Loss [MAPE]: 969799.694\n",
      "Epoch time:  12.488997459411621\n",
      "Epoch 295/500 | Train Loss: 2100.514 | Test Loss: 535.427 | Test Loss [MAPE]: 846704.607\n",
      "Epoch time:  12.494950771331787\n",
      "Epoch 296/500 | Train Loss: 2096.839 | Test Loss: 555.415 | Test Loss [MAPE]: 948271.649\n",
      "Epoch time:  12.494469404220581\n",
      "Epoch 297/500 | Train Loss: 2105.666 | Test Loss: 542.470 | Test Loss [MAPE]: 891660.384\n",
      "Epoch time:  12.491611242294312\n",
      "Epoch 298/500 | Train Loss: 2097.689 | Test Loss: 543.917 | Test Loss [MAPE]: 972289.893\n",
      "Epoch time:  12.493380308151245\n",
      "Epoch 299/500 | Train Loss: 2082.798 | Test Loss: 542.564 | Test Loss [MAPE]: 920196.485\n",
      "Epoch time:  12.497970342636108\n",
      "Epoch 300/500 | Train Loss: 2089.079 | Test Loss: 546.700 | Test Loss [MAPE]: 907443.146\n",
      "Epoch time:  12.493458032608032\n",
      "Epoch 301/500 | Train Loss: 2089.102 | Test Loss: 545.546 | Test Loss [MAPE]: 880479.180\n",
      "Epoch time:  12.488213062286377\n",
      "Epoch 302/500 | Train Loss: 2100.292 | Test Loss: 538.457 | Test Loss [MAPE]: 859899.219\n",
      "Epoch time:  12.493349552154541\n",
      "Epoch 303/500 | Train Loss: 2106.227 | Test Loss: 543.449 | Test Loss [MAPE]: 825305.634\n",
      "Epoch time:  12.492332935333252\n",
      "Epoch 304/500 | Train Loss: 2099.019 | Test Loss: 532.642 | Test Loss [MAPE]: 813767.867\n",
      "Epoch time:  14.059376955032349\n",
      "Epoch 305/500 | Train Loss: 2088.962 | Test Loss: 538.726 | Test Loss [MAPE]: 736585.518\n",
      "Epoch time:  12.484503746032715\n",
      "Epoch 306/500 | Train Loss: 2094.413 | Test Loss: 545.572 | Test Loss [MAPE]: 841637.064\n",
      "Epoch time:  12.488977670669556\n",
      "Epoch 307/500 | Train Loss: 2081.204 | Test Loss: 551.685 | Test Loss [MAPE]: 938958.732\n",
      "Epoch time:  16.0420560836792\n",
      "Epoch 308/500 | Train Loss: 2097.214 | Test Loss: 539.018 | Test Loss [MAPE]: 880799.356\n",
      "Epoch time:  25.506288528442383\n",
      "Epoch 309/500 | Train Loss: 2097.940 | Test Loss: 555.459 | Test Loss [MAPE]: 945851.501\n",
      "Epoch time:  25.502943754196167\n",
      "Epoch 310/500 | Train Loss: 2092.956 | Test Loss: 532.465 | Test Loss [MAPE]: 809722.522\n",
      "Epoch time:  26.975877285003662\n",
      "Epoch 311/500 | Train Loss: 2083.303 | Test Loss: 536.693 | Test Loss [MAPE]: 808907.874\n",
      "Epoch time:  25.46654224395752\n",
      "Epoch 312/500 | Train Loss: 2082.277 | Test Loss: 535.402 | Test Loss [MAPE]: 825400.118\n",
      "Epoch time:  25.487513303756714\n",
      "Epoch 313/500 | Train Loss: 2077.512 | Test Loss: 534.536 | Test Loss [MAPE]: 901532.350\n",
      "Epoch time:  25.462387084960938\n",
      "Epoch 314/500 | Train Loss: 2084.941 | Test Loss: 545.644 | Test Loss [MAPE]: 870177.079\n",
      "Epoch time:  25.516578435897827\n",
      "Epoch 315/500 | Train Loss: 2074.862 | Test Loss: 533.766 | Test Loss [MAPE]: 842017.708\n",
      "Epoch time:  25.58076310157776\n",
      "Epoch 316/500 | Train Loss: 2079.636 | Test Loss: 535.953 | Test Loss [MAPE]: 810259.692\n",
      "Epoch time:  25.4856858253479\n",
      "Epoch 317/500 | Train Loss: 2090.885 | Test Loss: 537.809 | Test Loss [MAPE]: 846758.315\n",
      "Epoch time:  25.487075805664062\n",
      "Epoch 318/500 | Train Loss: 2095.566 | Test Loss: 560.231 | Test Loss [MAPE]: 972489.090\n",
      "Epoch time:  25.473555088043213\n",
      "Epoch 319/500 | Train Loss: 2075.220 | Test Loss: 537.716 | Test Loss [MAPE]: 856843.396\n",
      "Epoch time:  25.45837712287903\n",
      "Epoch 320/500 | Train Loss: 2078.330 | Test Loss: 558.815 | Test Loss [MAPE]: 831721.170\n",
      "Epoch time:  25.504679679870605\n",
      "Epoch 321/500 | Train Loss: 2079.986 | Test Loss: 548.426 | Test Loss [MAPE]: 986562.919\n",
      "Epoch time:  25.548476934432983\n",
      "Epoch 322/500 | Train Loss: 2076.725 | Test Loss: 535.620 | Test Loss [MAPE]: 897336.435\n",
      "Epoch time:  25.497899055480957\n",
      "Epoch 323/500 | Train Loss: 2073.349 | Test Loss: 533.897 | Test Loss [MAPE]: 902537.007\n",
      "Epoch time:  25.464895009994507\n",
      "Epoch 324/500 | Train Loss: 2072.590 | Test Loss: 539.783 | Test Loss [MAPE]: 847124.970\n",
      "Epoch time:  25.485961437225342\n",
      "Epoch 325/500 | Train Loss: 2088.298 | Test Loss: 565.013 | Test Loss [MAPE]: 941762.923\n",
      "Epoch time:  25.472925901412964\n",
      "Epoch 326/500 | Train Loss: 2072.942 | Test Loss: 538.281 | Test Loss [MAPE]: 772976.960\n",
      "Epoch time:  25.497366666793823\n",
      "Epoch 327/500 | Train Loss: 2073.692 | Test Loss: 534.240 | Test Loss [MAPE]: 825815.323\n",
      "Epoch time:  25.56589436531067\n",
      "Epoch 328/500 | Train Loss: 2071.271 | Test Loss: 535.300 | Test Loss [MAPE]: 909062.184\n",
      "Epoch time:  25.48307514190674\n",
      "Epoch 329/500 | Train Loss: 2081.795 | Test Loss: 534.071 | Test Loss [MAPE]: 869485.613\n",
      "Epoch time:  25.493677854537964\n",
      "Epoch 330/500 | Train Loss: 2078.811 | Test Loss: 542.316 | Test Loss [MAPE]: 922556.739\n",
      "Epoch time:  25.480491399765015\n",
      "Epoch 331/500 | Train Loss: 2066.081 | Test Loss: 545.830 | Test Loss [MAPE]: 841474.250\n",
      "Epoch time:  25.47888207435608\n",
      "Epoch 332/500 | Train Loss: 2073.473 | Test Loss: 531.558 | Test Loss [MAPE]: 856737.042\n",
      "Epoch time:  27.007408618927002\n",
      "Epoch 333/500 | Train Loss: 2076.152 | Test Loss: 536.897 | Test Loss [MAPE]: 899962.300\n",
      "Epoch time:  25.454554319381714\n",
      "Epoch 334/500 | Train Loss: 2067.725 | Test Loss: 536.524 | Test Loss [MAPE]: 844119.346\n",
      "Epoch time:  13.378682851791382\n",
      "Epoch 335/500 | Train Loss: 2082.659 | Test Loss: 548.683 | Test Loss [MAPE]: 898240.977\n",
      "Epoch time:  12.529788970947266\n",
      "Epoch 336/500 | Train Loss: 2064.075 | Test Loss: 530.055 | Test Loss [MAPE]: 902850.987\n",
      "Epoch time:  15.071305513381958\n",
      "Epoch 337/500 | Train Loss: 2058.881 | Test Loss: 535.934 | Test Loss [MAPE]: 1001356.493\n",
      "Epoch time:  24.115298986434937\n",
      "Epoch 338/500 | Train Loss: 2066.922 | Test Loss: 535.959 | Test Loss [MAPE]: 884077.113\n",
      "Epoch time:  24.146210193634033\n",
      "Epoch 339/500 | Train Loss: 2072.509 | Test Loss: 533.948 | Test Loss [MAPE]: 783238.453\n",
      "Epoch time:  24.135401725769043\n",
      "Epoch 340/500 | Train Loss: 2069.491 | Test Loss: 532.834 | Test Loss [MAPE]: 935929.569\n",
      "Epoch time:  24.150747299194336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341/500 | Train Loss: 2055.590 | Test Loss: 535.981 | Test Loss [MAPE]: 880023.175\n",
      "Epoch time:  24.163448572158813\n",
      "Epoch 342/500 | Train Loss: 2057.913 | Test Loss: 529.269 | Test Loss [MAPE]: 843950.408\n",
      "Epoch time:  25.655386447906494\n",
      "Epoch 343/500 | Train Loss: 2062.931 | Test Loss: 548.998 | Test Loss [MAPE]: 858335.488\n",
      "Epoch time:  24.16085982322693\n",
      "Epoch 344/500 | Train Loss: 2062.761 | Test Loss: 543.590 | Test Loss [MAPE]: 961779.355\n",
      "Epoch time:  21.12982153892517\n",
      "Epoch 345/500 | Train Loss: 2052.235 | Test Loss: 530.619 | Test Loss [MAPE]: 819355.942\n",
      "Epoch time:  17.032115936279297\n",
      "Epoch 346/500 | Train Loss: 2057.365 | Test Loss: 537.531 | Test Loss [MAPE]: 861298.164\n",
      "Epoch time:  24.19945764541626\n",
      "Epoch 347/500 | Train Loss: 2061.453 | Test Loss: 535.931 | Test Loss [MAPE]: 883517.933\n",
      "Epoch time:  24.147069454193115\n",
      "Epoch 348/500 | Train Loss: 2063.453 | Test Loss: 538.480 | Test Loss [MAPE]: 916404.732\n",
      "Epoch time:  23.3261501789093\n",
      "Epoch 349/500 | Train Loss: 2056.191 | Test Loss: 530.486 | Test Loss [MAPE]: 855503.801\n",
      "Epoch time:  16.904797554016113\n",
      "Epoch 350/500 | Train Loss: 2061.765 | Test Loss: 530.361 | Test Loss [MAPE]: 806091.780\n",
      "Epoch time:  25.4831280708313\n",
      "Epoch 351/500 | Train Loss: 2067.874 | Test Loss: 536.483 | Test Loss [MAPE]: 835759.493\n",
      "Epoch time:  25.522817850112915\n",
      "Epoch 352/500 | Train Loss: 2053.434 | Test Loss: 532.727 | Test Loss [MAPE]: 923454.463\n",
      "Epoch time:  25.60279679298401\n",
      "Epoch 353/500 | Train Loss: 2045.306 | Test Loss: 525.276 | Test Loss [MAPE]: 956451.083\n",
      "Epoch time:  27.049673795700073\n",
      "Epoch 354/500 | Train Loss: 2053.248 | Test Loss: 547.546 | Test Loss [MAPE]: 949544.602\n",
      "Epoch time:  25.61167311668396\n",
      "Epoch 355/500 | Train Loss: 2049.051 | Test Loss: 544.313 | Test Loss [MAPE]: 910377.779\n",
      "Epoch time:  25.542535305023193\n",
      "Epoch 356/500 | Train Loss: 2049.514 | Test Loss: 532.177 | Test Loss [MAPE]: 871252.521\n",
      "Epoch time:  24.993360996246338\n",
      "Epoch 357/500 | Train Loss: 2049.408 | Test Loss: 535.661 | Test Loss [MAPE]: 861634.227\n",
      "Epoch time:  12.49487042427063\n",
      "Epoch 358/500 | Train Loss: 2042.229 | Test Loss: 531.997 | Test Loss [MAPE]: 851641.779\n",
      "Epoch time:  12.492698431015015\n",
      "Epoch 359/500 | Train Loss: 2060.460 | Test Loss: 533.287 | Test Loss [MAPE]: 834117.269\n",
      "Epoch time:  12.4905104637146\n",
      "Epoch 360/500 | Train Loss: 2049.135 | Test Loss: 532.352 | Test Loss [MAPE]: 861568.183\n",
      "Epoch time:  12.486659288406372\n",
      "Epoch 361/500 | Train Loss: 2044.963 | Test Loss: 528.186 | Test Loss [MAPE]: 848628.175\n",
      "Epoch time:  12.489410638809204\n",
      "Epoch 362/500 | Train Loss: 2043.474 | Test Loss: 530.408 | Test Loss [MAPE]: 905512.492\n",
      "Epoch time:  12.486001014709473\n",
      "Epoch 363/500 | Train Loss: 2032.856 | Test Loss: 541.605 | Test Loss [MAPE]: 882779.693\n",
      "Epoch time:  12.489128112792969\n",
      "Epoch 364/500 | Train Loss: 2041.097 | Test Loss: 539.156 | Test Loss [MAPE]: 762878.116\n",
      "Epoch time:  12.656271934509277\n",
      "Epoch 365/500 | Train Loss: 2045.534 | Test Loss: 538.581 | Test Loss [MAPE]: 981694.878\n",
      "Epoch time:  12.500196695327759\n",
      "Epoch 366/500 | Train Loss: 2054.198 | Test Loss: 527.273 | Test Loss [MAPE]: 831485.728\n",
      "Epoch time:  12.488985538482666\n",
      "Epoch 367/500 | Train Loss: 2036.193 | Test Loss: 530.923 | Test Loss [MAPE]: 841962.755\n",
      "Epoch time:  12.488707780838013\n",
      "Epoch 368/500 | Train Loss: 2041.217 | Test Loss: 536.161 | Test Loss [MAPE]: 891230.797\n",
      "Epoch time:  12.487982273101807\n",
      "Epoch 369/500 | Train Loss: 2043.872 | Test Loss: 526.581 | Test Loss [MAPE]: 862378.993\n",
      "Epoch time:  12.48536205291748\n",
      "Epoch 370/500 | Train Loss: 2041.225 | Test Loss: 544.397 | Test Loss [MAPE]: 918957.904\n",
      "Epoch time:  12.762505769729614\n",
      "Epoch 371/500 | Train Loss: 2049.992 | Test Loss: 522.341 | Test Loss [MAPE]: 776030.391\n",
      "Epoch time:  14.266916036605835\n",
      "Epoch 372/500 | Train Loss: 2039.488 | Test Loss: 548.053 | Test Loss [MAPE]: 889213.738\n",
      "Epoch time:  12.473844528198242\n",
      "Epoch 373/500 | Train Loss: 2041.216 | Test Loss: 526.019 | Test Loss [MAPE]: 929680.693\n",
      "Epoch time:  12.489387035369873\n",
      "Epoch 374/500 | Train Loss: 2039.329 | Test Loss: 536.652 | Test Loss [MAPE]: 940238.519\n",
      "Epoch time:  12.697152614593506\n",
      "Epoch 375/500 | Train Loss: 2037.559 | Test Loss: 518.153 | Test Loss [MAPE]: 826073.845\n",
      "Epoch time:  13.99927806854248\n",
      "Epoch 376/500 | Train Loss: 2028.271 | Test Loss: 521.966 | Test Loss [MAPE]: 964111.081\n",
      "Epoch time:  12.74221420288086\n",
      "Epoch 377/500 | Train Loss: 2027.089 | Test Loss: 531.046 | Test Loss [MAPE]: 852515.599\n",
      "Epoch time:  12.504542589187622\n",
      "Epoch 378/500 | Train Loss: 2023.344 | Test Loss: 536.525 | Test Loss [MAPE]: 885300.501\n",
      "Epoch time:  12.505290508270264\n",
      "Epoch 379/500 | Train Loss: 2047.713 | Test Loss: 530.414 | Test Loss [MAPE]: 753800.467\n",
      "Epoch time:  12.49744987487793\n",
      "Epoch 380/500 | Train Loss: 2031.475 | Test Loss: 526.250 | Test Loss [MAPE]: 889541.856\n",
      "Epoch time:  12.505073547363281\n",
      "Epoch 381/500 | Train Loss: 2030.308 | Test Loss: 532.404 | Test Loss [MAPE]: 915517.995\n",
      "Epoch time:  12.498779058456421\n",
      "Epoch 382/500 | Train Loss: 2032.550 | Test Loss: 525.723 | Test Loss [MAPE]: 915530.426\n",
      "Epoch time:  12.505424499511719\n",
      "Epoch 383/500 | Train Loss: 2029.143 | Test Loss: 527.408 | Test Loss [MAPE]: 928931.450\n",
      "Epoch time:  12.49913239479065\n",
      "Epoch 384/500 | Train Loss: 2033.332 | Test Loss: 533.615 | Test Loss [MAPE]: 877227.867\n",
      "Epoch time:  12.496483325958252\n",
      "Epoch 385/500 | Train Loss: 2028.824 | Test Loss: 533.168 | Test Loss [MAPE]: 900461.934\n",
      "Epoch time:  12.506426334381104\n",
      "Epoch 386/500 | Train Loss: 2029.935 | Test Loss: 525.740 | Test Loss [MAPE]: 933342.903\n",
      "Epoch time:  12.504273891448975\n",
      "Epoch 387/500 | Train Loss: 2020.249 | Test Loss: 528.354 | Test Loss [MAPE]: 838595.128\n",
      "Epoch time:  12.490204095840454\n",
      "Epoch 388/500 | Train Loss: 2014.680 | Test Loss: 520.845 | Test Loss [MAPE]: 765374.930\n",
      "Epoch time:  12.496139764785767\n",
      "Epoch 389/500 | Train Loss: 2025.971 | Test Loss: 523.199 | Test Loss [MAPE]: 863733.978\n",
      "Epoch time:  12.491147756576538\n",
      "Epoch 390/500 | Train Loss: 2019.425 | Test Loss: 527.246 | Test Loss [MAPE]: 825727.113\n",
      "Epoch time:  12.49033808708191\n",
      "Epoch 391/500 | Train Loss: 2021.347 | Test Loss: 529.045 | Test Loss [MAPE]: 746122.789\n",
      "Epoch time:  12.487398862838745\n",
      "Epoch 392/500 | Train Loss: 2024.121 | Test Loss: 521.654 | Test Loss [MAPE]: 790117.509\n",
      "Epoch time:  12.487528800964355\n",
      "Epoch 393/500 | Train Loss: 2025.035 | Test Loss: 522.790 | Test Loss [MAPE]: 887015.953\n",
      "Epoch time:  12.49050760269165\n",
      "Epoch 394/500 | Train Loss: 2017.273 | Test Loss: 515.775 | Test Loss [MAPE]: 805430.453\n",
      "Epoch time:  14.012338876724243\n",
      "Epoch 395/500 | Train Loss: 2014.858 | Test Loss: 515.111 | Test Loss [MAPE]: 805755.748\n",
      "Epoch time:  14.039194583892822\n",
      "Epoch 396/500 | Train Loss: 2006.843 | Test Loss: 516.899 | Test Loss [MAPE]: 919085.272\n",
      "Epoch time:  12.481989860534668\n",
      "Epoch 397/500 | Train Loss: 2018.074 | Test Loss: 523.624 | Test Loss [MAPE]: 827805.127\n",
      "Epoch time:  12.48888897895813\n",
      "Epoch 398/500 | Train Loss: 2019.386 | Test Loss: 519.107 | Test Loss [MAPE]: 852960.081\n",
      "Epoch time:  12.490513324737549\n",
      "Epoch 399/500 | Train Loss: 2016.188 | Test Loss: 525.343 | Test Loss [MAPE]: 761445.844\n",
      "Epoch time:  12.489201784133911\n",
      "Epoch 400/500 | Train Loss: 2028.158 | Test Loss: 521.110 | Test Loss [MAPE]: 893045.772\n",
      "Epoch time:  12.494266986846924\n",
      "Epoch 401/500 | Train Loss: 2007.281 | Test Loss: 539.971 | Test Loss [MAPE]: 959653.988\n",
      "Epoch time:  12.492568731307983\n",
      "Epoch 402/500 | Train Loss: 2016.553 | Test Loss: 531.073 | Test Loss [MAPE]: 872386.685\n",
      "Epoch time:  12.491440296173096\n",
      "Epoch 403/500 | Train Loss: 2003.834 | Test Loss: 519.813 | Test Loss [MAPE]: 778993.831\n",
      "Epoch time:  12.495444536209106\n",
      "Epoch 404/500 | Train Loss: 2024.338 | Test Loss: 524.505 | Test Loss [MAPE]: 888341.052\n",
      "Epoch time:  12.491982221603394\n",
      "Epoch 405/500 | Train Loss: 2008.056 | Test Loss: 534.033 | Test Loss [MAPE]: 893745.110\n",
      "Epoch time:  12.493271112442017\n",
      "Epoch 406/500 | Train Loss: 2018.670 | Test Loss: 513.407 | Test Loss [MAPE]: 771276.365\n",
      "Epoch time:  14.061923265457153\n",
      "Epoch 407/500 | Train Loss: 1994.601 | Test Loss: 526.667 | Test Loss [MAPE]: 955564.452\n",
      "Epoch time:  12.478731155395508\n",
      "Epoch 408/500 | Train Loss: 1993.522 | Test Loss: 526.852 | Test Loss [MAPE]: 803452.070\n",
      "Epoch time:  12.49159550666809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 409/500 | Train Loss: 1984.988 | Test Loss: 514.537 | Test Loss [MAPE]: 812431.110\n",
      "Epoch time:  12.490722179412842\n",
      "Epoch 410/500 | Train Loss: 2002.090 | Test Loss: 516.701 | Test Loss [MAPE]: 934032.623\n",
      "Epoch time:  12.490797758102417\n",
      "Epoch 411/500 | Train Loss: 2001.188 | Test Loss: 520.908 | Test Loss [MAPE]: 807798.307\n",
      "Epoch time:  12.489996433258057\n",
      "Epoch 412/500 | Train Loss: 1995.645 | Test Loss: 515.761 | Test Loss [MAPE]: 756464.335\n",
      "Epoch time:  12.490440607070923\n",
      "Epoch 413/500 | Train Loss: 2003.496 | Test Loss: 512.654 | Test Loss [MAPE]: 865275.135\n",
      "Epoch time:  14.023651599884033\n",
      "Epoch 414/500 | Train Loss: 1994.832 | Test Loss: 515.544 | Test Loss [MAPE]: 832347.622\n",
      "Epoch time:  12.479747295379639\n",
      "Epoch 415/500 | Train Loss: 2002.232 | Test Loss: 526.484 | Test Loss [MAPE]: 900459.961\n",
      "Epoch time:  12.491463422775269\n",
      "Epoch 416/500 | Train Loss: 1998.532 | Test Loss: 520.484 | Test Loss [MAPE]: 876639.121\n",
      "Epoch time:  12.498869180679321\n",
      "Epoch 417/500 | Train Loss: 1998.194 | Test Loss: 521.356 | Test Loss [MAPE]: 919603.766\n",
      "Epoch time:  12.490122318267822\n",
      "Epoch 418/500 | Train Loss: 1980.994 | Test Loss: 521.947 | Test Loss [MAPE]: 910078.483\n",
      "Epoch time:  12.480664491653442\n",
      "Epoch 419/500 | Train Loss: 1977.671 | Test Loss: 509.612 | Test Loss [MAPE]: 830958.968\n",
      "Epoch time:  14.299634456634521\n",
      "Epoch 420/500 | Train Loss: 1991.398 | Test Loss: 512.854 | Test Loss [MAPE]: 921252.978\n",
      "Epoch time:  12.481370687484741\n",
      "Epoch 421/500 | Train Loss: 2002.022 | Test Loss: 527.124 | Test Loss [MAPE]: 892208.053\n",
      "Epoch time:  12.4918692111969\n",
      "Epoch 422/500 | Train Loss: 1989.692 | Test Loss: 528.027 | Test Loss [MAPE]: 810037.539\n",
      "Epoch time:  12.508078336715698\n",
      "Epoch 423/500 | Train Loss: 1981.039 | Test Loss: 515.087 | Test Loss [MAPE]: 828388.453\n",
      "Epoch time:  12.75550365447998\n",
      "Epoch 424/500 | Train Loss: 1984.662 | Test Loss: 528.397 | Test Loss [MAPE]: 804752.098\n",
      "Epoch time:  13.402846336364746\n",
      "Epoch 425/500 | Train Loss: 1989.042 | Test Loss: 527.404 | Test Loss [MAPE]: 900185.135\n",
      "Epoch time:  12.4943528175354\n",
      "Epoch 426/500 | Train Loss: 1977.086 | Test Loss: 531.243 | Test Loss [MAPE]: 891678.650\n",
      "Epoch time:  12.49334192276001\n",
      "Epoch 427/500 | Train Loss: 1987.956 | Test Loss: 522.203 | Test Loss [MAPE]: 918335.953\n",
      "Epoch time:  12.494203329086304\n",
      "Epoch 428/500 | Train Loss: 1971.757 | Test Loss: 522.428 | Test Loss [MAPE]: 952513.711\n",
      "Epoch time:  12.495628118515015\n",
      "Epoch 429/500 | Train Loss: 1973.676 | Test Loss: 509.245 | Test Loss [MAPE]: 906013.095\n",
      "Epoch time:  13.906347751617432\n",
      "Epoch 430/500 | Train Loss: 1975.117 | Test Loss: 524.234 | Test Loss [MAPE]: 821517.987\n",
      "Epoch time:  12.48474407196045\n",
      "Epoch 431/500 | Train Loss: 1980.938 | Test Loss: 509.051 | Test Loss [MAPE]: 827996.179\n",
      "Epoch time:  14.060291051864624\n",
      "Epoch 432/500 | Train Loss: 1975.646 | Test Loss: 506.091 | Test Loss [MAPE]: 891417.395\n",
      "Epoch time:  14.03054690361023\n",
      "Epoch 433/500 | Train Loss: 1967.151 | Test Loss: 519.315 | Test Loss [MAPE]: 840413.021\n",
      "Epoch time:  12.494161128997803\n",
      "Epoch 434/500 | Train Loss: 1970.737 | Test Loss: 504.843 | Test Loss [MAPE]: 920498.482\n",
      "Epoch time:  14.098578929901123\n",
      "Epoch 435/500 | Train Loss: 1957.515 | Test Loss: 504.189 | Test Loss [MAPE]: 795907.964\n",
      "Epoch time:  13.975231647491455\n",
      "Epoch 436/500 | Train Loss: 1963.770 | Test Loss: 512.668 | Test Loss [MAPE]: 873346.395\n",
      "Epoch time:  12.481954574584961\n",
      "Epoch 437/500 | Train Loss: 1970.237 | Test Loss: 510.433 | Test Loss [MAPE]: 856875.846\n",
      "Epoch time:  12.48776626586914\n",
      "Epoch 438/500 | Train Loss: 1962.445 | Test Loss: 515.515 | Test Loss [MAPE]: 828878.413\n",
      "Epoch time:  12.489711284637451\n",
      "Epoch 439/500 | Train Loss: 1972.946 | Test Loss: 506.250 | Test Loss [MAPE]: 801431.629\n",
      "Epoch time:  12.487843036651611\n",
      "Epoch 440/500 | Train Loss: 1967.428 | Test Loss: 508.732 | Test Loss [MAPE]: 770695.450\n",
      "Epoch time:  12.488042831420898\n",
      "Epoch 441/500 | Train Loss: 1959.441 | Test Loss: 505.777 | Test Loss [MAPE]: 772919.349\n",
      "Epoch time:  12.49370527267456\n",
      "Epoch 442/500 | Train Loss: 1967.046 | Test Loss: 510.702 | Test Loss [MAPE]: 707416.377\n",
      "Epoch time:  12.492972135543823\n",
      "Epoch 443/500 | Train Loss: 1950.674 | Test Loss: 518.626 | Test Loss [MAPE]: 968917.110\n",
      "Epoch time:  12.490191459655762\n",
      "Epoch 444/500 | Train Loss: 1952.277 | Test Loss: 508.013 | Test Loss [MAPE]: 782853.127\n",
      "Epoch time:  12.488349437713623\n",
      "Epoch 445/500 | Train Loss: 1951.186 | Test Loss: 499.049 | Test Loss [MAPE]: 844997.624\n",
      "Epoch time:  14.022655725479126\n",
      "Epoch 446/500 | Train Loss: 1945.280 | Test Loss: 506.907 | Test Loss [MAPE]: 835135.424\n",
      "Epoch time:  12.481046438217163\n",
      "Epoch 447/500 | Train Loss: 1946.903 | Test Loss: 503.757 | Test Loss [MAPE]: 865938.654\n",
      "Epoch time:  12.489380359649658\n",
      "Epoch 448/500 | Train Loss: 1951.058 | Test Loss: 502.622 | Test Loss [MAPE]: 885685.731\n",
      "Epoch time:  12.486494541168213\n",
      "Epoch 449/500 | Train Loss: 1950.383 | Test Loss: 518.725 | Test Loss [MAPE]: 972085.968\n",
      "Epoch time:  12.488890647888184\n",
      "Epoch 450/500 | Train Loss: 1956.238 | Test Loss: 506.994 | Test Loss [MAPE]: 863636.118\n",
      "Epoch time:  12.491737842559814\n",
      "Epoch 451/500 | Train Loss: 1941.801 | Test Loss: 502.619 | Test Loss [MAPE]: 841071.182\n",
      "Epoch time:  12.493029356002808\n",
      "Epoch 452/500 | Train Loss: 1933.676 | Test Loss: 499.286 | Test Loss [MAPE]: 756015.030\n",
      "Epoch time:  12.490427494049072\n",
      "Epoch 453/500 | Train Loss: 1939.133 | Test Loss: 520.957 | Test Loss [MAPE]: 870773.687\n",
      "Epoch time:  12.491817951202393\n",
      "Epoch 454/500 | Train Loss: 1945.170 | Test Loss: 515.359 | Test Loss [MAPE]: 863823.629\n",
      "Epoch time:  12.49036693572998\n",
      "Epoch 455/500 | Train Loss: 1932.130 | Test Loss: 501.659 | Test Loss [MAPE]: 794179.988\n",
      "Epoch time:  12.493616342544556\n",
      "Epoch 456/500 | Train Loss: 1929.448 | Test Loss: 495.046 | Test Loss [MAPE]: 815735.105\n",
      "Epoch time:  14.086554288864136\n",
      "Epoch 457/500 | Train Loss: 1928.095 | Test Loss: 495.982 | Test Loss [MAPE]: 857930.063\n",
      "Epoch time:  12.479751110076904\n",
      "Epoch 458/500 | Train Loss: 1948.236 | Test Loss: 493.858 | Test Loss [MAPE]: 781548.313\n",
      "Epoch time:  14.046899318695068\n",
      "Epoch 459/500 | Train Loss: 1930.676 | Test Loss: 503.402 | Test Loss [MAPE]: 751775.066\n",
      "Epoch time:  12.475880146026611\n",
      "Epoch 460/500 | Train Loss: 1912.228 | Test Loss: 496.936 | Test Loss [MAPE]: 860404.081\n",
      "Epoch time:  12.48555064201355\n",
      "Epoch 461/500 | Train Loss: 1925.163 | Test Loss: 494.066 | Test Loss [MAPE]: 791563.121\n",
      "Epoch time:  12.489370346069336\n",
      "Epoch 462/500 | Train Loss: 1930.964 | Test Loss: 501.602 | Test Loss [MAPE]: 873556.051\n",
      "Epoch time:  12.488620281219482\n",
      "Epoch 463/500 | Train Loss: 1918.672 | Test Loss: 496.344 | Test Loss [MAPE]: 837127.779\n",
      "Epoch time:  12.492717027664185\n",
      "Epoch 464/500 | Train Loss: 1923.755 | Test Loss: 506.742 | Test Loss [MAPE]: 1004466.901\n",
      "Epoch time:  12.490052700042725\n",
      "Epoch 465/500 | Train Loss: 1926.878 | Test Loss: 491.644 | Test Loss [MAPE]: 823508.910\n",
      "Epoch time:  14.005069732666016\n",
      "Epoch 466/500 | Train Loss: 1898.403 | Test Loss: 509.128 | Test Loss [MAPE]: 875404.776\n",
      "Epoch time:  12.484610319137573\n",
      "Epoch 467/500 | Train Loss: 1925.768 | Test Loss: 497.244 | Test Loss [MAPE]: 895055.767\n",
      "Epoch time:  12.490904331207275\n",
      "Epoch 468/500 | Train Loss: 1905.688 | Test Loss: 498.467 | Test Loss [MAPE]: 833244.348\n",
      "Epoch time:  12.49139666557312\n",
      "Epoch 469/500 | Train Loss: 1913.717 | Test Loss: 503.296 | Test Loss [MAPE]: 977124.529\n",
      "Epoch time:  12.490683317184448\n",
      "Epoch 470/500 | Train Loss: 1907.933 | Test Loss: 494.643 | Test Loss [MAPE]: 848448.889\n",
      "Epoch time:  12.490645408630371\n",
      "Epoch 471/500 | Train Loss: 1899.385 | Test Loss: 489.154 | Test Loss [MAPE]: 795190.927\n",
      "Epoch time:  14.017244100570679\n",
      "Epoch 472/500 | Train Loss: 1899.974 | Test Loss: 490.303 | Test Loss [MAPE]: 854286.547\n",
      "Epoch time:  12.483240365982056\n",
      "Epoch 473/500 | Train Loss: 1895.873 | Test Loss: 489.767 | Test Loss [MAPE]: 926085.764\n",
      "Epoch time:  12.489617586135864\n",
      "Epoch 474/500 | Train Loss: 1908.915 | Test Loss: 504.564 | Test Loss [MAPE]: 897218.100\n",
      "Epoch time:  12.489931106567383\n",
      "Epoch 475/500 | Train Loss: 1898.577 | Test Loss: 495.914 | Test Loss [MAPE]: 917460.238\n",
      "Epoch time:  12.490278482437134\n",
      "Epoch 476/500 | Train Loss: 1897.502 | Test Loss: 490.685 | Test Loss [MAPE]: 930046.133\n",
      "Epoch time:  12.491050958633423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 477/500 | Train Loss: 1901.127 | Test Loss: 501.191 | Test Loss [MAPE]: 820007.590\n",
      "Epoch time:  12.490504503250122\n",
      "Epoch 478/500 | Train Loss: 1890.201 | Test Loss: 496.968 | Test Loss [MAPE]: 898232.505\n",
      "Epoch time:  12.489541053771973\n",
      "Epoch 479/500 | Train Loss: 1902.363 | Test Loss: 494.159 | Test Loss [MAPE]: 846794.025\n",
      "Epoch time:  12.495326280593872\n",
      "Epoch 480/500 | Train Loss: 1889.103 | Test Loss: 488.849 | Test Loss [MAPE]: 893640.847\n",
      "Epoch time:  14.006168842315674\n",
      "Epoch 481/500 | Train Loss: 1883.129 | Test Loss: 482.610 | Test Loss [MAPE]: 794870.579\n",
      "Epoch time:  14.00417685508728\n",
      "Epoch 482/500 | Train Loss: 1889.010 | Test Loss: 506.538 | Test Loss [MAPE]: 933665.872\n",
      "Epoch time:  12.47890830039978\n",
      "Epoch 483/500 | Train Loss: 1885.180 | Test Loss: 490.049 | Test Loss [MAPE]: 923336.336\n",
      "Epoch time:  12.49224328994751\n",
      "Epoch 484/500 | Train Loss: 1880.137 | Test Loss: 484.548 | Test Loss [MAPE]: 887675.140\n",
      "Epoch time:  12.493952512741089\n",
      "Epoch 485/500 | Train Loss: 1883.960 | Test Loss: 496.353 | Test Loss [MAPE]: 857900.721\n",
      "Epoch time:  12.497405052185059\n",
      "Epoch 486/500 | Train Loss: 1887.573 | Test Loss: 496.016 | Test Loss [MAPE]: 842844.660\n",
      "Epoch time:  12.491325855255127\n",
      "Epoch 487/500 | Train Loss: 1874.939 | Test Loss: 486.666 | Test Loss [MAPE]: 785645.422\n",
      "Epoch time:  12.492133855819702\n",
      "Epoch 488/500 | Train Loss: 1881.385 | Test Loss: 494.377 | Test Loss [MAPE]: 854093.764\n",
      "Epoch time:  12.49408507347107\n",
      "Epoch 489/500 | Train Loss: 1880.756 | Test Loss: 497.138 | Test Loss [MAPE]: 814227.377\n",
      "Epoch time:  12.491849184036255\n",
      "Epoch 490/500 | Train Loss: 1880.409 | Test Loss: 490.168 | Test Loss [MAPE]: 848632.094\n",
      "Epoch time:  12.495773077011108\n",
      "Epoch 491/500 | Train Loss: 1871.392 | Test Loss: 482.525 | Test Loss [MAPE]: 916287.981\n",
      "Epoch time:  14.022525072097778\n",
      "Epoch 492/500 | Train Loss: 1863.844 | Test Loss: 475.824 | Test Loss [MAPE]: 768031.308\n",
      "Epoch time:  14.015181541442871\n",
      "Epoch 493/500 | Train Loss: 1869.120 | Test Loss: 485.050 | Test Loss [MAPE]: 857156.311\n",
      "Epoch time:  12.481390714645386\n",
      "Epoch 494/500 | Train Loss: 1866.139 | Test Loss: 496.364 | Test Loss [MAPE]: 851105.633\n",
      "Epoch time:  12.488231420516968\n",
      "Epoch 495/500 | Train Loss: 1865.797 | Test Loss: 486.400 | Test Loss [MAPE]: 825248.912\n",
      "Epoch time:  12.491706609725952\n",
      "Epoch 496/500 | Train Loss: 1861.051 | Test Loss: 473.326 | Test Loss [MAPE]: 826367.893\n",
      "Epoch time:  14.050536155700684\n",
      "Epoch 497/500 | Train Loss: 1867.750 | Test Loss: 485.401 | Test Loss [MAPE]: 887238.784\n",
      "Epoch time:  12.481369733810425\n",
      "Epoch 498/500 | Train Loss: 1854.416 | Test Loss: 472.755 | Test Loss [MAPE]: 866529.427\n",
      "Epoch time:  13.99878191947937\n",
      "Epoch 499/500 | Train Loss: 1853.876 | Test Loss: 486.778 | Test Loss [MAPE]: 887497.598\n",
      "Epoch time:  12.48745322227478\n",
      "Epoch 500/500 | Train Loss: 1857.708 | Test Loss: 481.993 | Test Loss [MAPE]: 855369.280\n",
      "Epoch time:  12.487703561782837\n",
      "Execution time: 8606.65482544899 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472.755018286407"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (conv1): Conv1d(1, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=120708, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  172.69748442052224\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i].cpu().numpy()\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i].unsqueeze(0).unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "179.52  - Concentrations: 0.004999999888241291\n",
      "30.78  - Concentrations: 0.02500000037252903\n",
      "10.19  - Concentrations: 0.10000000149011612\n",
      "9.37  - Concentrations: 0.25\n",
      "7.97  - Concentrations: 0.5\n",
      "6.32  - Concentrations: 1.0\n",
      "4.88  - Concentrations: 2.5\n",
      "3.78  - Concentrations: 10.0\n",
      "3.68  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  13.925128\n",
      "--------------------\n",
      "6.71  - Min Value: 0.6783  - Mean Value: 9.2\n",
      "72.81  - Min Value: 0.0096  - Mean Value: 10.3\n",
      "10.19  - Min Value: 0.147  - Mean Value: 10.5\n",
      "10.1  - Min Value: 0.5572  - Mean Value: 8.5\n",
      "4.53  - Min Value: 1.3567  - Mean Value: 10.6\n",
      "5.35  - Min Value: 0.6332  - Mean Value: 10.9\n",
      "9.26  - Min Value: 0.7017  - Mean Value: 11.0\n",
      "10.22  - Min Value: 0.3674  - Mean Value: 8.9\n",
      "5.23  - Min Value: 0.8387  - Mean Value: 9.8\n",
      "4.86  - Min Value: 1.0913  - Mean Value: 11.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  15.661389\n",
      "--------------------\n",
      "12.52  - Min Value: 0.0111  - Mean Value: 0.1\n",
      "16.56  - Min Value: 0.0103  - Mean Value: 0.1\n",
      "12.36  - Min Value: 0.0153  - Mean Value: 0.1\n",
      "18.24  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "18.0  - Min Value: 0.0089  - Mean Value: 0.1\n",
      "17.14  - Min Value: 0.0075  - Mean Value: 0.1\n",
      "17.52  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "18.21  - Min Value: 0.0052  - Mean Value: 0.1\n",
      "12.89  - Min Value: 0.008  - Mean Value: 0.1\n",
      "13.18  - Min Value: 0.0134  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  34.30954\n",
      "--------------------\n",
      "77.74  - Min Value: 0.008  - Mean Value: 0.8\n",
      "13.14  - Min Value: 0.009  - Mean Value: 0.9\n",
      "40.89  - Min Value: 0.0138  - Mean Value: 1.5\n",
      "81.3  - Min Value: 0.0107  - Mean Value: 0.7\n",
      "34.91  - Min Value: 0.0191  - Mean Value: 0.7\n",
      "22.41  - Min Value: 0.0186  - Mean Value: 0.8\n",
      "16.84  - Min Value: 0.0175  - Mean Value: 0.8\n",
      "11.59  - Min Value: 0.0238  - Mean Value: 1.3\n",
      "17.82  - Min Value: 0.0168  - Mean Value: 0.7\n",
      "26.45  - Min Value: 0.0171  - Mean Value: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  55.403008\n",
      "--------------------\n",
      "18.82  - Min Value: 0.013  - Mean Value: 0.6\n",
      "33.36  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "24.7  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "158.43  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "41.54  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "79.51  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "87.06  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "72.74  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "18.44  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "19.43  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  418.69928\n",
      "--------------------\n",
      "122.55  - Min Value: 0.0062  - Mean Value: 2.1\n",
      "1012.64  - Min Value: 0.006  - Mean Value: 3.7\n",
      "377.19  - Min Value: 0.0066  - Mean Value: 4.3\n",
      "329.4  - Min Value: 0.0094  - Mean Value: 4.3\n",
      "306.89  - Min Value: 0.0068  - Mean Value: 4.9\n",
      "654.86  - Min Value: 0.005  - Mean Value: 3.8\n",
      "208.98  - Min Value: 0.0101  - Mean Value: 3.2\n",
      "580.95  - Min Value: 0.0062  - Mean Value: 3.2\n",
      "138.33  - Min Value: 0.0053  - Mean Value: 5.3\n",
      "455.21  - Min Value: 0.0054  - Mean Value: 2.5\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(HighDynamicRange2Conc[i].min().item(),4), \" - Mean Value:\", np.round(HighDynamicRange2Conc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  8.569571113107573\n",
      "--------------------\n",
      "8.37\n",
      "8.4\n",
      "8.49\n",
      "8.38\n",
      "8.55\n",
      "8.52\n",
      "8.57\n",
      "8.68\n",
      "8.66\n",
      "9.08\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  12.822206229223507\n",
      "--------------------\n",
      "8.38\n",
      "8.75\n",
      "8.95\n",
      "10.58\n",
      "11.3\n",
      "11.57\n",
      "11.92\n",
      "16.18\n",
      "18.18\n",
      "22.41\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a dataset with singlets added at random\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  87.2602328688426\n",
      "--------------------\n",
      "11.38\n",
      "24.48\n",
      "40.84\n",
      "57.99\n",
      "76.38\n",
      "94.98\n",
      "113.68\n",
      "132.4\n",
      "151.02\n",
      "169.45\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal Baseline 1\n",
      "tensor([0.4938, 0.5627, 0.3763, 0.5633, 0.4938, 0.5199, 0.5024, 0.6519, 0.6978,\n",
      "        0.3562, 0.4014, 0.2142, 0.5328, 0.0175, 0.1486, 0.6868, 0.6880, 0.4642,\n",
      "        0.5192, 0.3675, 0.0548, 0.9197, 0.2654, 0.4908, 0.6070, 0.4463, 0.6231,\n",
      "        0.5479, 0.5984, 0.4598, 0.4225, 0.8586, 0.5633, 0.3736, 0.4228, 0.5478,\n",
      "        0.4275, 0.4349, 0.4557, 0.3814, 0.3734, 0.4870, 0.5518, 0.4806],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Sinusoidal Baseline 2\n",
      "tensor([0.3518, 0.5440, 0.0000, 0.6040, 0.4068, 0.4461, 0.4337, 0.1631, 0.4634,\n",
      "        0.5908, 0.4198, 0.1920, 0.8816, 0.0025, 0.1194, 0.0249, 0.8781, 0.5510,\n",
      "        0.4622, 0.3985, 0.0318, 0.3681, 0.6257, 0.6062, 0.4752, 0.4208, 0.1974,\n",
      "        0.0241, 0.5905, 0.3199, 0.3847, 0.1736, 0.1487, 0.3965, 0.6294, 0.4098,\n",
      "        0.6619, 0.8012, 0.6749, 0.3272, 0.3897, 0.8832, 0.2330, 0.0814],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "HD-Range 1 - 1s and 20s\n",
      "tensor([ 0.0000, 18.8710,  0.0000, 16.5672,  0.0000, 18.1656,  0.0000, 18.6892,\n",
      "         0.0000, 18.3292,  0.0000,  6.8004,  0.0000,  5.3250,  9.3169, 18.8952,\n",
      "         0.0000, 18.9837,  0.0000, 18.0313, 16.2563, 17.3002,  0.0000, 17.9068,\n",
      "         0.0000, 16.9947,  0.0000, 19.3202,  0.0000, 20.9952,  0.8445, 17.0255,\n",
      "         0.0000, 19.2500,  0.0000, 18.1248,  0.7237, 18.3960,  0.0000, 17.7062,\n",
      "         0.0000, 16.8987,  0.0000, 18.5586], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "HD-Range 2 - 0s and 20s\n",
      "tensor([ 0.0000, 18.8699,  0.0000, 16.5693,  0.0000, 18.1657,  0.0000, 18.6904,\n",
      "         0.0000, 18.3282,  0.0000,  6.7959,  0.0000,  5.3217,  9.3107, 18.8960,\n",
      "         0.0000, 18.9831,  0.0000, 18.0310, 16.2449, 17.3022,  0.0000, 17.9075,\n",
      "         0.0000, 16.9949,  0.0000, 19.3220,  0.0000, 20.9960,  0.8342, 17.0268,\n",
      "         0.0000, 19.2508,  0.0000, 18.1231,  0.7125, 18.3992,  0.0000, 17.7081,\n",
      "         0.0000, 16.8974,  0.0000, 18.5564], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(OtherValSpectra[0].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 1s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
