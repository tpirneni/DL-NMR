{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 10\n",
    "\n",
    "# Identification part of the filenames\n",
    "model_base_name = '250000spec_RAE_ExtendedRange_MoreLeftOut_Combined1Distribution_Timing'\n",
    "base_name = '250000spec_ExtendedRange_MoreLeftOut_Combined1Distribution'    # This is the dataset base name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"MLP_44met_{model_base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra_filename = f'Dataset44_{base_name}_ForManuscript_Spec.dat'\n",
    "conc1_filename = f'Dataset44_{base_name}_ForManuscript_Conc.npy'\n",
    "\n",
    "spectra_shape = (249996, 46000)\n",
    "conc1_shape = (249996, 44)\n",
    "\n",
    "\n",
    "# Load the memmap arrays\n",
    "spectra_memmap = np.memmap(spectra_filename, dtype=np.float64, mode='r', shape=spectra_shape)\n",
    "conc1_memmap = np.load(conc1_filename)\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train_indices, X_test_indices, y_train_indices, y_test_indices = train_test_split(\n",
    "    np.arange(spectra_shape[0]), np.arange(conc1_shape[0]), test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# Create custom dataset class\n",
    "class NMRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, spectra_memmap, conc1_memmap, indices):\n",
    "        self.spectra_memmap = spectra_memmap\n",
    "        self.conc1_memmap = conc1_memmap\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        X = self.spectra_memmap[actual_idx]\n",
    "        y = self.conc1_memmap[actual_idx]\n",
    "        return torch.tensor(X).float().to(device), torch.tensor(y).float().to(device)\n",
    "    \n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NMRDataset(spectra_memmap, conc1_memmap, X_train_indices)\n",
    "test_dataset = NMRDataset(spectra_memmap, conc1_memmap, X_test_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 31  \n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "## Best params from Optuna study\n",
    "#{'n_layers': 2, \n",
    "# 'activation': 'LeakyReLU', \n",
    "# 'n_units_l0': 222, \n",
    "# 'n_units_l1': 463, \n",
    "# 'learning_rate': 0.0020767074281295714, \n",
    "# 'reg_strength': 0.009375024340091184, \n",
    "# 'batch_size': 31}\n",
    "\n",
    "# Define some model & training parameters\n",
    "size_hidden1 = 222\n",
    "size_hidden2 = 463\n",
    "size_hidden3 = 44\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(46000, size_hidden1)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n",
    "    def forward(self, input):\n",
    "        return (self.lin3(self.relu2(self.lin2(self.relu1(self.lin1(input))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = RelativeAbsoluteError()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = 0.0020767074281295714, weight_decay=0.009375024340091184)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in validation loss constitutes early stopping\n",
    "    accumulation_steps = 4\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # For timing cell run time\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        ## Training phase\n",
    "        # Instantiate the GradScaler\n",
    "        scaler = GradScaler()\n",
    "        optimizer.zero_grad()  # Only zero gradients here at the start of an epoch\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Enable autocasting for forward and backward passes\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Scale the loss to account for the accumulation steps\n",
    "                loss = loss / accumulation_steps\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Scale the loss and perform backpropagation\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                # Step the optimizer and update the scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()  # Zero gradients after accumulation_steps\n",
    "\n",
    "        # Testing phase\n",
    "        train_losses.append(train_loss)\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                # Move data to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Enable autocasting for forward passes\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "        \n",
    "        \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Epoch time: \",end-start)\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/10], Train Loss: 28213.7685, Test Loss: 28017.5197\n",
      "Epoch time:  362.61164140701294\n",
      "Epoch [2/10], Train Loss: 27659.8023, Test Loss: 27511.4675\n",
      "Epoch time:  63.01121234893799\n",
      "Epoch [3/10], Train Loss: 27100.0337, Test Loss: 26980.2631\n",
      "Epoch time:  74.04439210891724\n",
      "Epoch [4/10], Train Loss: 26642.2473, Test Loss: 26705.3691\n",
      "Epoch time:  73.93831372261047\n",
      "Epoch [5/10], Train Loss: 26342.7276, Test Loss: 26440.8414\n",
      "Epoch time:  73.77656197547913\n",
      "Epoch [6/10], Train Loss: 26219.4522, Test Loss: 26443.3865\n",
      "Epoch time:  65.80599355697632\n",
      "Epoch [7/10], Train Loss: 26105.2252, Test Loss: 26243.5925\n",
      "Epoch time:  62.38770580291748\n",
      "Epoch [8/10], Train Loss: 26042.1809, Test Loss: 26281.6352\n",
      "Epoch time:  61.85371232032776\n",
      "Epoch [9/10], Train Loss: 25973.0243, Test Loss: 26285.1512\n",
      "Epoch time:  62.67845106124878\n",
      "Epoch [10/10], Train Loss: 25907.0899, Test Loss: 26153.7861\n",
      "Epoch time:  63.698997020721436\n",
      "Execution time: 974.0280814170837 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epochs used later in training\n",
    "num_epochs = 5000\n",
    "\n",
    "# Identification part of the filenames\n",
    "model_base_name = '250000spec_RAE_ExtendedRange_MoreLeftOut_Combined1Distribution'\n",
    "\n",
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"MLP_44met_{model_base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (lin1): Linear(in_features=46000, out_features=222, bias=True)\n",
       "  (relu1): LeakyReLU(negative_slope=0.01)\n",
       "  (lin2): Linear(in_features=222, out_features=463, bias=True)\n",
       "  (relu2): LeakyReLU(negative_slope=0.01)\n",
       "  (lin3): Linear(in_features=463, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, data, device, num_repeats=10):\n",
    "    model.to(device)\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Warm-up\n",
    "    for _ in range(10):\n",
    "        _ = model(data)\n",
    "\n",
    "    torch.cuda.synchronize()  # Ensure all GPU operations are complete\n",
    "\n",
    "    times = []\n",
    "    for _ in range(num_repeats):\n",
    "        start_time = time.time()\n",
    "        _ = model(data)\n",
    "        torch.cuda.synchronize()  # Ensure all GPU operations are complete\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "    avg_time = sum(times) / num_repeats\n",
    "    print(f\"Average inference time: {avg_time:.4f} seconds\")\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time: 0.0135 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "avg_time = measure_inference_time(model_aq, spectraVal, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 10335887\n"
     ]
    }
   ],
   "source": [
    "num_params = count_parameters(model_aq)\n",
    "print(f\"Number of trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
