{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 500\n",
    "\n",
    "# Identification part of the filenames\n",
    "model_base_name = 'LogCosh_ExtendedRange_MoreLeftOut_Combined1Distribution'\n",
    "base_name = 'ExtendedRange_MoreLeftOut_Combined1Distribution'    # This is the dataset base name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"CNN_44met_{model_base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load(f'Dataset44_{base_name}_ForManuscript_Spec.npy')\n",
    "conc1 = np.load(f'Dataset44_{base_name}_ForManuscript_Conc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in datasets]\n",
    "test_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in Test_datasets]\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset_reshaped, batch_size = 64, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset_reshaped, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NMR_Model_Aq, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 42, kernel_size=6, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(120708, 200)\n",
    "        self.fc2 = nn.Linear(200, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)                  \n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogCoshLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.mean(torch.log(torch.cosh(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = LogCoshLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = 5.287243368897864e-05, weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in validation loss constitutes early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "            \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/500], Train Loss: 36768.0435, Test Loss: 8457.1024\n",
      "Epoch [2/500], Train Loss: 30330.1706, Test Loss: 7309.7295\n",
      "Epoch [3/500], Train Loss: 28856.5818, Test Loss: 7126.5032\n",
      "Epoch [4/500], Train Loss: 28003.0053, Test Loss: 6863.8628\n",
      "Epoch [5/500], Train Loss: 26518.6045, Test Loss: 6349.7190\n",
      "Epoch [6/500], Train Loss: 23512.7796, Test Loss: 5227.8563\n",
      "Epoch [7/500], Train Loss: 17156.6178, Test Loss: 3315.6528\n",
      "Epoch [8/500], Train Loss: 10680.7358, Test Loss: 2225.1578\n",
      "Epoch [9/500], Train Loss: 7642.4867, Test Loss: 1661.1649\n",
      "Epoch [10/500], Train Loss: 5702.2036, Test Loss: 1233.6135\n",
      "Epoch [11/500], Train Loss: 4282.9842, Test Loss: 964.0790\n",
      "Epoch [12/500], Train Loss: 3332.8317, Test Loss: 767.8767\n",
      "Epoch [13/500], Train Loss: 2771.7843, Test Loss: 651.3805\n",
      "Epoch [14/500], Train Loss: 2400.4548, Test Loss: 567.4459\n",
      "Epoch [15/500], Train Loss: 2161.8623, Test Loss: 517.5501\n",
      "Epoch [16/500], Train Loss: 1992.5965, Test Loss: 532.1197\n",
      "Epoch [17/500], Train Loss: 1830.3539, Test Loss: 443.9508\n",
      "Epoch [18/500], Train Loss: 1694.4821, Test Loss: 407.5897\n",
      "Epoch [19/500], Train Loss: 1579.1823, Test Loss: 444.5306\n",
      "Epoch [20/500], Train Loss: 1490.5672, Test Loss: 353.6037\n",
      "Epoch [21/500], Train Loss: 1328.5927, Test Loss: 329.0736\n",
      "Epoch [22/500], Train Loss: 1243.2015, Test Loss: 297.3105\n",
      "Epoch [23/500], Train Loss: 1139.8846, Test Loss: 276.6279\n",
      "Epoch [24/500], Train Loss: 1045.8867, Test Loss: 258.5968\n",
      "Epoch [25/500], Train Loss: 996.7831, Test Loss: 324.8810\n",
      "Epoch [26/500], Train Loss: 897.8177, Test Loss: 223.5774\n",
      "Epoch [27/500], Train Loss: 840.9546, Test Loss: 203.8757\n",
      "Epoch [28/500], Train Loss: 745.8868, Test Loss: 181.0707\n",
      "Epoch [29/500], Train Loss: 692.4225, Test Loss: 179.3885\n",
      "Epoch [30/500], Train Loss: 650.6114, Test Loss: 167.3923\n",
      "Epoch [31/500], Train Loss: 611.0485, Test Loss: 150.9668\n",
      "Epoch [32/500], Train Loss: 569.2645, Test Loss: 148.2207\n",
      "Epoch [33/500], Train Loss: 559.2103, Test Loss: 171.7347\n",
      "Epoch [34/500], Train Loss: 508.1726, Test Loss: 131.8361\n",
      "Epoch [35/500], Train Loss: 458.2765, Test Loss: 105.9696\n",
      "Epoch [36/500], Train Loss: 454.7612, Test Loss: 111.0039\n",
      "Epoch [37/500], Train Loss: 444.7575, Test Loss: 126.6161\n",
      "Epoch [38/500], Train Loss: 470.1496, Test Loss: 126.1725\n",
      "Epoch [39/500], Train Loss: 423.8733, Test Loss: 101.0051\n",
      "Epoch [40/500], Train Loss: 421.7323, Test Loss: 110.2487\n",
      "Epoch [41/500], Train Loss: 431.7311, Test Loss: 169.8708\n",
      "Epoch [42/500], Train Loss: 427.6777, Test Loss: 109.3871\n",
      "Epoch [43/500], Train Loss: 404.7129, Test Loss: 103.4704\n",
      "Epoch [44/500], Train Loss: 424.9610, Test Loss: 108.4368\n",
      "Epoch [45/500], Train Loss: 393.1264, Test Loss: 99.4961\n",
      "Epoch [46/500], Train Loss: 395.7064, Test Loss: 95.9940\n",
      "Epoch [47/500], Train Loss: 390.1387, Test Loss: 102.0275\n",
      "Epoch [48/500], Train Loss: 400.8984, Test Loss: 97.6960\n",
      "Epoch [49/500], Train Loss: 396.2082, Test Loss: 101.8038\n",
      "Epoch [50/500], Train Loss: 381.8210, Test Loss: 94.3955\n",
      "Epoch [51/500], Train Loss: 413.0584, Test Loss: 94.0380\n",
      "Epoch [52/500], Train Loss: 383.7702, Test Loss: 93.8811\n",
      "Epoch [53/500], Train Loss: 385.5618, Test Loss: 100.2968\n",
      "Epoch [54/500], Train Loss: 403.9135, Test Loss: 110.0245\n",
      "Epoch [55/500], Train Loss: 367.2527, Test Loss: 93.9300\n",
      "Epoch [56/500], Train Loss: 380.6627, Test Loss: 91.0641\n",
      "Epoch [57/500], Train Loss: 380.2581, Test Loss: 92.3613\n",
      "Epoch [58/500], Train Loss: 360.2466, Test Loss: 106.8033\n",
      "Epoch [59/500], Train Loss: 412.3234, Test Loss: 104.5076\n",
      "Epoch [60/500], Train Loss: 359.0634, Test Loss: 93.7734\n",
      "Epoch [61/500], Train Loss: 354.5416, Test Loss: 86.1430\n",
      "Epoch [62/500], Train Loss: 371.5673, Test Loss: 125.3058\n",
      "Epoch [63/500], Train Loss: 364.6736, Test Loss: 98.2350\n",
      "Epoch [64/500], Train Loss: 379.8157, Test Loss: 87.3930\n",
      "Epoch [65/500], Train Loss: 345.3248, Test Loss: 93.9817\n",
      "Epoch [66/500], Train Loss: 345.2905, Test Loss: 87.7141\n",
      "Epoch [67/500], Train Loss: 368.7631, Test Loss: 90.6896\n",
      "Epoch [68/500], Train Loss: 342.7816, Test Loss: 85.1272\n",
      "Epoch [69/500], Train Loss: 389.5097, Test Loss: 102.2981\n",
      "Epoch [70/500], Train Loss: 351.9692, Test Loss: 93.0018\n",
      "Epoch [71/500], Train Loss: 331.7543, Test Loss: 82.3110\n",
      "Epoch [72/500], Train Loss: 343.6900, Test Loss: 89.5545\n",
      "Epoch [73/500], Train Loss: 335.6866, Test Loss: 83.5579\n",
      "Epoch [74/500], Train Loss: 341.8352, Test Loss: 90.8547\n",
      "Epoch [75/500], Train Loss: 336.9592, Test Loss: 81.1903\n",
      "Epoch [76/500], Train Loss: 352.5170, Test Loss: 89.2052\n",
      "Epoch [77/500], Train Loss: 337.0500, Test Loss: 91.9866\n",
      "Epoch [78/500], Train Loss: 366.1040, Test Loss: 129.9276\n",
      "Epoch [79/500], Train Loss: 335.5971, Test Loss: 82.1539\n",
      "Epoch [80/500], Train Loss: 318.8842, Test Loss: 91.0721\n",
      "Epoch [81/500], Train Loss: 324.3981, Test Loss: 84.9458\n",
      "Epoch [82/500], Train Loss: 329.8494, Test Loss: 93.4203\n",
      "Epoch [83/500], Train Loss: 334.8286, Test Loss: 89.5397\n",
      "Epoch [84/500], Train Loss: 341.4263, Test Loss: 79.7692\n",
      "Epoch [85/500], Train Loss: 313.6001, Test Loss: 89.1780\n",
      "Epoch [86/500], Train Loss: 324.6751, Test Loss: 78.4025\n",
      "Epoch [87/500], Train Loss: 323.7973, Test Loss: 81.8513\n",
      "Epoch [88/500], Train Loss: 304.1390, Test Loss: 84.7752\n",
      "Epoch [89/500], Train Loss: 336.4858, Test Loss: 93.6756\n",
      "Epoch [90/500], Train Loss: 304.8842, Test Loss: 73.9590\n",
      "Epoch [91/500], Train Loss: 318.0266, Test Loss: 88.0908\n",
      "Epoch [92/500], Train Loss: 300.6012, Test Loss: 77.9447\n",
      "Epoch [93/500], Train Loss: 306.4013, Test Loss: 74.3793\n",
      "Epoch [94/500], Train Loss: 344.3805, Test Loss: 204.2915\n",
      "Epoch [95/500], Train Loss: 355.1702, Test Loss: 75.0723\n",
      "Epoch [96/500], Train Loss: 313.4255, Test Loss: 90.1686\n",
      "Epoch [97/500], Train Loss: 296.3058, Test Loss: 92.7671\n",
      "Epoch [98/500], Train Loss: 301.2919, Test Loss: 82.3210\n",
      "Epoch [99/500], Train Loss: 309.4757, Test Loss: 77.1700\n",
      "Epoch [100/500], Train Loss: 291.0799, Test Loss: 86.7796\n",
      "Epoch [101/500], Train Loss: 303.6142, Test Loss: 75.3345\n",
      "Epoch [102/500], Train Loss: 301.7633, Test Loss: 73.7369\n",
      "Epoch [103/500], Train Loss: 291.7226, Test Loss: 92.7187\n",
      "Epoch [104/500], Train Loss: 326.9873, Test Loss: 75.6169\n",
      "Epoch [105/500], Train Loss: 291.4008, Test Loss: 72.8805\n",
      "Epoch [106/500], Train Loss: 312.4406, Test Loss: 74.3920\n",
      "Epoch [107/500], Train Loss: 279.5315, Test Loss: 69.0580\n",
      "Epoch [108/500], Train Loss: 285.5794, Test Loss: 129.3156\n",
      "Epoch [109/500], Train Loss: 291.2686, Test Loss: 70.1720\n",
      "Epoch [110/500], Train Loss: 306.3906, Test Loss: 74.3224\n",
      "Epoch [111/500], Train Loss: 293.4978, Test Loss: 76.9597\n",
      "Epoch [112/500], Train Loss: 279.4682, Test Loss: 85.4648\n",
      "Epoch [113/500], Train Loss: 290.7878, Test Loss: 71.8212\n",
      "Epoch [114/500], Train Loss: 283.5642, Test Loss: 96.3787\n",
      "Epoch [115/500], Train Loss: 289.2391, Test Loss: 72.7115\n",
      "Epoch [116/500], Train Loss: 289.8829, Test Loss: 76.6166\n",
      "Epoch [117/500], Train Loss: 275.7538, Test Loss: 78.6165\n",
      "Epoch [118/500], Train Loss: 300.4749, Test Loss: 70.2494\n",
      "Epoch [119/500], Train Loss: 279.1426, Test Loss: 74.5719\n",
      "Epoch [120/500], Train Loss: 272.9474, Test Loss: 86.7656\n",
      "Epoch [121/500], Train Loss: 271.5630, Test Loss: 93.1516\n",
      "Epoch [122/500], Train Loss: 269.5539, Test Loss: 74.7296\n",
      "Epoch [123/500], Train Loss: 282.3946, Test Loss: 69.0942\n",
      "Epoch [124/500], Train Loss: 265.2443, Test Loss: 68.7528\n",
      "Epoch [125/500], Train Loss: 310.4177, Test Loss: 73.7048\n",
      "Epoch [126/500], Train Loss: 265.1929, Test Loss: 72.7149\n",
      "Epoch [127/500], Train Loss: 303.1930, Test Loss: 102.0843\n",
      "Epoch [128/500], Train Loss: 259.7153, Test Loss: 78.6963\n",
      "Epoch [129/500], Train Loss: 254.1756, Test Loss: 73.3877\n",
      "Epoch [130/500], Train Loss: 276.0770, Test Loss: 68.3510\n",
      "Epoch [131/500], Train Loss: 259.7093, Test Loss: 78.8514\n",
      "Epoch [132/500], Train Loss: 266.9849, Test Loss: 73.8135\n",
      "Epoch [133/500], Train Loss: 274.2206, Test Loss: 73.6322\n",
      "Epoch [134/500], Train Loss: 263.7725, Test Loss: 77.4994\n",
      "Epoch [135/500], Train Loss: 248.8583, Test Loss: 66.3760\n",
      "Epoch [136/500], Train Loss: 262.3401, Test Loss: 68.6581\n",
      "Epoch [137/500], Train Loss: 263.4340, Test Loss: 70.1778\n",
      "Epoch [138/500], Train Loss: 287.6238, Test Loss: 65.7993\n",
      "Epoch [139/500], Train Loss: 241.9561, Test Loss: 66.6570\n",
      "Epoch [140/500], Train Loss: 254.2778, Test Loss: 70.7781\n",
      "Epoch [141/500], Train Loss: 257.9920, Test Loss: 64.5746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [142/500], Train Loss: 251.3966, Test Loss: 70.4221\n",
      "Epoch [143/500], Train Loss: 248.0156, Test Loss: 89.4608\n",
      "Epoch [144/500], Train Loss: 277.0879, Test Loss: 87.0699\n",
      "Epoch [145/500], Train Loss: 243.3144, Test Loss: 66.0849\n",
      "Epoch [146/500], Train Loss: 254.1308, Test Loss: 68.8349\n",
      "Epoch [147/500], Train Loss: 235.1591, Test Loss: 61.6288\n",
      "Epoch [148/500], Train Loss: 251.3763, Test Loss: 65.9464\n",
      "Epoch [149/500], Train Loss: 260.6174, Test Loss: 67.5230\n",
      "Epoch [150/500], Train Loss: 285.3851, Test Loss: 61.6064\n",
      "Epoch [151/500], Train Loss: 245.8612, Test Loss: 60.3620\n",
      "Epoch [152/500], Train Loss: 230.5530, Test Loss: 59.5874\n",
      "Epoch [153/500], Train Loss: 242.5119, Test Loss: 65.1310\n",
      "Epoch [154/500], Train Loss: 317.4925, Test Loss: 78.4242\n",
      "Epoch [155/500], Train Loss: 227.6339, Test Loss: 67.3821\n",
      "Epoch [156/500], Train Loss: 232.9737, Test Loss: 58.7328\n",
      "Epoch [157/500], Train Loss: 223.1375, Test Loss: 62.9015\n",
      "Epoch [158/500], Train Loss: 240.9144, Test Loss: 64.4406\n",
      "Epoch [159/500], Train Loss: 233.5202, Test Loss: 60.6463\n",
      "Epoch [160/500], Train Loss: 232.5801, Test Loss: 68.0846\n",
      "Epoch [161/500], Train Loss: 232.3127, Test Loss: 63.2490\n",
      "Epoch [162/500], Train Loss: 317.5225, Test Loss: 60.0561\n",
      "Epoch [163/500], Train Loss: 219.4047, Test Loss: 61.0736\n",
      "Epoch [164/500], Train Loss: 222.6106, Test Loss: 60.8111\n",
      "Epoch [165/500], Train Loss: 223.7807, Test Loss: 70.7015\n",
      "Epoch [166/500], Train Loss: 241.2775, Test Loss: 58.1219\n",
      "Epoch [167/500], Train Loss: 229.2104, Test Loss: 60.8640\n",
      "Epoch [168/500], Train Loss: 267.7240, Test Loss: 60.7597\n",
      "Epoch [169/500], Train Loss: 256.5769, Test Loss: 100.8961\n",
      "Epoch [170/500], Train Loss: 223.1239, Test Loss: 60.4879\n",
      "Epoch [171/500], Train Loss: 207.6735, Test Loss: 67.3720\n",
      "Epoch [172/500], Train Loss: 251.0642, Test Loss: 63.1543\n",
      "Epoch [173/500], Train Loss: 215.9339, Test Loss: 62.7769\n",
      "Epoch [174/500], Train Loss: 224.1011, Test Loss: 55.7177\n",
      "Epoch [175/500], Train Loss: 223.6063, Test Loss: 58.4078\n",
      "Epoch [176/500], Train Loss: 223.9108, Test Loss: 56.9866\n",
      "Epoch [177/500], Train Loss: 224.0210, Test Loss: 94.7096\n",
      "Epoch [178/500], Train Loss: 246.0944, Test Loss: 55.2711\n",
      "Epoch [179/500], Train Loss: 210.3111, Test Loss: 67.0567\n",
      "Epoch [180/500], Train Loss: 221.4798, Test Loss: 54.2955\n",
      "Epoch [181/500], Train Loss: 220.9495, Test Loss: 53.1515\n",
      "Epoch [182/500], Train Loss: 227.0565, Test Loss: 65.7187\n",
      "Epoch [183/500], Train Loss: 226.8443, Test Loss: 71.7871\n",
      "Epoch [184/500], Train Loss: 214.2711, Test Loss: 51.1233\n",
      "Epoch [185/500], Train Loss: 214.2369, Test Loss: 61.5433\n",
      "Epoch [186/500], Train Loss: 207.2733, Test Loss: 50.8822\n",
      "Epoch [187/500], Train Loss: 235.7404, Test Loss: 57.8926\n",
      "Epoch [188/500], Train Loss: 280.5486, Test Loss: 52.4124\n",
      "Epoch [189/500], Train Loss: 189.4252, Test Loss: 55.9704\n",
      "Epoch [190/500], Train Loss: 202.8621, Test Loss: 52.2651\n",
      "Epoch [191/500], Train Loss: 200.3592, Test Loss: 59.0407\n",
      "Epoch [192/500], Train Loss: 192.6215, Test Loss: 54.0710\n",
      "Epoch [193/500], Train Loss: 207.9682, Test Loss: 61.2914\n",
      "Epoch [194/500], Train Loss: 221.7096, Test Loss: 73.2084\n",
      "Epoch [195/500], Train Loss: 208.1932, Test Loss: 65.5887\n",
      "Epoch [196/500], Train Loss: 204.4903, Test Loss: 88.9302\n",
      "Epoch [197/500], Train Loss: 198.6108, Test Loss: 50.8775\n",
      "Epoch [198/500], Train Loss: 196.3713, Test Loss: 66.5179\n",
      "Epoch [199/500], Train Loss: 297.1983, Test Loss: 196.1637\n",
      "Epoch [200/500], Train Loss: 210.5496, Test Loss: 50.4015\n",
      "Epoch [201/500], Train Loss: 180.1917, Test Loss: 51.8336\n",
      "Epoch [202/500], Train Loss: 189.8856, Test Loss: 51.1436\n",
      "Epoch [203/500], Train Loss: 191.0847, Test Loss: 51.7996\n",
      "Epoch [204/500], Train Loss: 193.4508, Test Loss: 54.6758\n",
      "Epoch [205/500], Train Loss: 207.8362, Test Loss: 160.3421\n",
      "Epoch [206/500], Train Loss: 241.1283, Test Loss: 52.3910\n",
      "Epoch [207/500], Train Loss: 184.5882, Test Loss: 50.7945\n",
      "Epoch [208/500], Train Loss: 188.8125, Test Loss: 48.6095\n",
      "Epoch [209/500], Train Loss: 187.3247, Test Loss: 50.6193\n",
      "Epoch [210/500], Train Loss: 195.5429, Test Loss: 48.4160\n",
      "Epoch [211/500], Train Loss: 199.3384, Test Loss: 52.1710\n",
      "Epoch [212/500], Train Loss: 180.4162, Test Loss: 48.3427\n",
      "Epoch [213/500], Train Loss: 199.0021, Test Loss: 55.2877\n",
      "Epoch [214/500], Train Loss: 221.1748, Test Loss: 46.8840\n",
      "Epoch [215/500], Train Loss: 180.7487, Test Loss: 49.7688\n",
      "Epoch [216/500], Train Loss: 179.5009, Test Loss: 50.1245\n",
      "Epoch [217/500], Train Loss: 194.1598, Test Loss: 51.7227\n",
      "Epoch [218/500], Train Loss: 176.0717, Test Loss: 47.1557\n",
      "Epoch [219/500], Train Loss: 185.0914, Test Loss: 119.3018\n",
      "Epoch [220/500], Train Loss: 280.2324, Test Loss: 46.0489\n",
      "Epoch [221/500], Train Loss: 171.0883, Test Loss: 47.5959\n",
      "Epoch [222/500], Train Loss: 208.2159, Test Loss: 46.6288\n",
      "Epoch [223/500], Train Loss: 176.8084, Test Loss: 44.6205\n",
      "Epoch [224/500], Train Loss: 186.6813, Test Loss: 48.2942\n",
      "Epoch [225/500], Train Loss: 174.7972, Test Loss: 67.2474\n",
      "Epoch [226/500], Train Loss: 189.8794, Test Loss: 45.8743\n",
      "Epoch [227/500], Train Loss: 175.7351, Test Loss: 48.2423\n",
      "Epoch [228/500], Train Loss: 192.2951, Test Loss: 46.7697\n",
      "Epoch [229/500], Train Loss: 206.8271, Test Loss: 50.7066\n",
      "Epoch [230/500], Train Loss: 164.6399, Test Loss: 51.1730\n",
      "Epoch [231/500], Train Loss: 170.5482, Test Loss: 58.6581\n",
      "Epoch [232/500], Train Loss: 187.0884, Test Loss: 47.7641\n",
      "Epoch [233/500], Train Loss: 193.3597, Test Loss: 45.8108\n",
      "Epoch [234/500], Train Loss: 183.0376, Test Loss: 51.0763\n",
      "Epoch [235/500], Train Loss: 178.8951, Test Loss: 51.1988\n",
      "Epoch [236/500], Train Loss: 205.1842, Test Loss: 41.9301\n",
      "Epoch [237/500], Train Loss: 164.1315, Test Loss: 45.1955\n",
      "Epoch [238/500], Train Loss: 162.9359, Test Loss: 48.9132\n",
      "Epoch [239/500], Train Loss: 175.9344, Test Loss: 46.3535\n",
      "Epoch [240/500], Train Loss: 177.6186, Test Loss: 42.3299\n",
      "Epoch [241/500], Train Loss: 166.5921, Test Loss: 55.0783\n",
      "Epoch [242/500], Train Loss: 165.7257, Test Loss: 49.4673\n",
      "Epoch [243/500], Train Loss: 190.9182, Test Loss: 49.0157\n",
      "Epoch [244/500], Train Loss: 173.2548, Test Loss: 49.0337\n",
      "Epoch [245/500], Train Loss: 162.2667, Test Loss: 47.9086\n",
      "Epoch [246/500], Train Loss: 178.0703, Test Loss: 46.0145\n",
      "Epoch [247/500], Train Loss: 166.1695, Test Loss: 72.9491\n",
      "Epoch [248/500], Train Loss: 163.3185, Test Loss: 46.1569\n",
      "Epoch [249/500], Train Loss: 168.5548, Test Loss: 45.1875\n",
      "Epoch [250/500], Train Loss: 155.7008, Test Loss: 53.7303\n",
      "Epoch [251/500], Train Loss: 195.2968, Test Loss: 47.1039\n",
      "Epoch [252/500], Train Loss: 153.7500, Test Loss: 42.9957\n",
      "Epoch [253/500], Train Loss: 174.4297, Test Loss: 42.4147\n",
      "Epoch [254/500], Train Loss: 169.8548, Test Loss: 145.1615\n",
      "Epoch [255/500], Train Loss: 194.7957, Test Loss: 45.2343\n",
      "Epoch [256/500], Train Loss: 169.3703, Test Loss: 47.1060\n",
      "Epoch [257/500], Train Loss: 157.1157, Test Loss: 44.2567\n",
      "Epoch [258/500], Train Loss: 282.4259, Test Loss: 57.2636\n",
      "Epoch [259/500], Train Loss: 157.5352, Test Loss: 42.0231\n",
      "Epoch [260/500], Train Loss: 150.6885, Test Loss: 45.9471\n",
      "Epoch [261/500], Train Loss: 154.7844, Test Loss: 41.1050\n",
      "Epoch [262/500], Train Loss: 153.5360, Test Loss: 47.4777\n",
      "Epoch [263/500], Train Loss: 160.4974, Test Loss: 50.7940\n",
      "Epoch [264/500], Train Loss: 168.7102, Test Loss: 45.1137\n",
      "Epoch [265/500], Train Loss: 175.7557, Test Loss: 97.3429\n",
      "Epoch [266/500], Train Loss: 207.1579, Test Loss: 39.6311\n",
      "Epoch [267/500], Train Loss: 158.3067, Test Loss: 58.4666\n",
      "Epoch [268/500], Train Loss: 151.7246, Test Loss: 42.8365\n",
      "Epoch [269/500], Train Loss: 154.6988, Test Loss: 74.6469\n",
      "Epoch [270/500], Train Loss: 172.4933, Test Loss: 44.9685\n",
      "Epoch [271/500], Train Loss: 175.9088, Test Loss: 42.8523\n",
      "Epoch [272/500], Train Loss: 162.8712, Test Loss: 41.8336\n",
      "Epoch [273/500], Train Loss: 144.5306, Test Loss: 42.2002\n",
      "Epoch [274/500], Train Loss: 175.2262, Test Loss: 41.2881\n",
      "Epoch [275/500], Train Loss: 150.1988, Test Loss: 45.3090\n",
      "Epoch [276/500], Train Loss: 152.5651, Test Loss: 40.9091\n",
      "Epoch [277/500], Train Loss: 180.4883, Test Loss: 42.2236\n",
      "Epoch [278/500], Train Loss: 163.4028, Test Loss: 59.6817\n",
      "Epoch [279/500], Train Loss: 170.1223, Test Loss: 42.4599\n",
      "Epoch [280/500], Train Loss: 150.6745, Test Loss: 42.1358\n",
      "Epoch [281/500], Train Loss: 149.3239, Test Loss: 37.9251\n",
      "Epoch [282/500], Train Loss: 179.2799, Test Loss: 41.5925\n",
      "Epoch [283/500], Train Loss: 150.2055, Test Loss: 37.8301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [284/500], Train Loss: 142.7135, Test Loss: 41.6673\n",
      "Epoch [285/500], Train Loss: 172.3807, Test Loss: 40.2453\n",
      "Epoch [286/500], Train Loss: 158.4038, Test Loss: 39.3716\n",
      "Epoch [287/500], Train Loss: 145.7933, Test Loss: 46.9494\n",
      "Epoch [288/500], Train Loss: 139.9543, Test Loss: 45.1476\n",
      "Epoch [289/500], Train Loss: 196.6130, Test Loss: 48.9335\n",
      "Epoch [290/500], Train Loss: 159.9482, Test Loss: 50.2637\n",
      "Epoch [291/500], Train Loss: 161.7451, Test Loss: 43.5970\n",
      "Epoch [292/500], Train Loss: 144.2455, Test Loss: 39.1941\n",
      "Epoch [293/500], Train Loss: 141.4740, Test Loss: 41.2534\n",
      "Epoch [294/500], Train Loss: 166.5776, Test Loss: 50.4302\n",
      "Epoch [295/500], Train Loss: 148.8026, Test Loss: 41.5918\n",
      "Epoch [296/500], Train Loss: 153.3512, Test Loss: 43.5470\n",
      "Epoch [297/500], Train Loss: 153.5973, Test Loss: 37.9904\n",
      "Epoch [298/500], Train Loss: 138.9838, Test Loss: 43.7864\n",
      "Epoch [299/500], Train Loss: 229.8735, Test Loss: 39.4328\n",
      "Epoch [300/500], Train Loss: 134.2912, Test Loss: 38.3485\n",
      "Epoch [301/500], Train Loss: 141.9486, Test Loss: 42.4414\n",
      "Epoch [302/500], Train Loss: 136.3992, Test Loss: 44.5603\n",
      "Epoch [303/500], Train Loss: 148.4333, Test Loss: 63.1767\n",
      "Epoch [304/500], Train Loss: 158.8195, Test Loss: 42.6036\n",
      "Epoch [305/500], Train Loss: 176.8131, Test Loss: 202.2023\n",
      "Epoch [306/500], Train Loss: 164.5030, Test Loss: 38.8200\n",
      "Epoch [307/500], Train Loss: 145.1782, Test Loss: 38.0758\n",
      "Epoch [308/500], Train Loss: 144.8252, Test Loss: 45.8049\n",
      "Epoch [309/500], Train Loss: 143.6401, Test Loss: 41.2649\n",
      "Epoch [310/500], Train Loss: 173.3730, Test Loss: 69.8026\n",
      "Epoch [311/500], Train Loss: 137.6901, Test Loss: 37.9569\n",
      "Epoch [312/500], Train Loss: 140.3085, Test Loss: 39.8464\n",
      "Epoch [313/500], Train Loss: 136.5596, Test Loss: 40.0353\n",
      "Epoch [314/500], Train Loss: 160.6562, Test Loss: 55.2090\n",
      "Epoch [315/500], Train Loss: 140.9441, Test Loss: 39.0463\n",
      "Epoch [316/500], Train Loss: 152.7064, Test Loss: 74.1348\n",
      "Epoch [317/500], Train Loss: 187.7401, Test Loss: 43.1393\n",
      "Epoch [318/500], Train Loss: 164.4517, Test Loss: 41.1997\n",
      "Epoch [319/500], Train Loss: 133.3934, Test Loss: 38.8471\n",
      "Epoch [320/500], Train Loss: 133.4548, Test Loss: 36.4450\n",
      "Epoch [321/500], Train Loss: 141.1490, Test Loss: 58.7924\n",
      "Epoch [322/500], Train Loss: 145.3853, Test Loss: 58.1121\n",
      "Epoch [323/500], Train Loss: 152.3887, Test Loss: 42.1050\n",
      "Epoch [324/500], Train Loss: 138.3759, Test Loss: 50.0293\n",
      "Epoch [325/500], Train Loss: 170.2048, Test Loss: 41.6362\n",
      "Epoch [326/500], Train Loss: 134.1589, Test Loss: 40.5632\n",
      "Epoch [327/500], Train Loss: 153.3605, Test Loss: 39.0202\n",
      "Epoch [328/500], Train Loss: 134.7671, Test Loss: 37.9127\n",
      "Epoch [329/500], Train Loss: 137.2657, Test Loss: 42.5428\n",
      "Epoch [330/500], Train Loss: 162.9731, Test Loss: 37.2410\n",
      "Epoch [331/500], Train Loss: 162.7836, Test Loss: 39.2558\n",
      "Epoch [332/500], Train Loss: 153.3793, Test Loss: 49.4118\n",
      "Epoch [333/500], Train Loss: 131.8739, Test Loss: 38.5687\n",
      "Epoch [334/500], Train Loss: 137.5775, Test Loss: 45.3206\n",
      "Epoch [335/500], Train Loss: 158.9494, Test Loss: 36.1528\n",
      "Epoch [336/500], Train Loss: 150.5441, Test Loss: 46.0107\n",
      "Epoch [337/500], Train Loss: 140.9338, Test Loss: 36.6804\n",
      "Epoch [338/500], Train Loss: 140.5134, Test Loss: 37.9829\n",
      "Epoch [339/500], Train Loss: 128.3293, Test Loss: 36.9525\n",
      "Epoch [340/500], Train Loss: 133.1248, Test Loss: 42.3053\n",
      "Epoch [341/500], Train Loss: 155.1743, Test Loss: 40.2809\n",
      "Epoch [342/500], Train Loss: 146.3950, Test Loss: 43.2723\n",
      "Epoch [343/500], Train Loss: 249.9727, Test Loss: 37.6200\n",
      "Epoch [344/500], Train Loss: 123.6757, Test Loss: 38.2237\n",
      "Epoch [345/500], Train Loss: 125.8275, Test Loss: 41.0552\n",
      "Epoch [346/500], Train Loss: 144.7867, Test Loss: 44.5738\n",
      "Epoch [347/500], Train Loss: 150.3841, Test Loss: 40.8013\n",
      "Epoch [348/500], Train Loss: 155.7647, Test Loss: 39.5373\n",
      "Epoch [349/500], Train Loss: 133.5992, Test Loss: 37.9722\n",
      "Epoch [350/500], Train Loss: 125.9166, Test Loss: 35.8072\n",
      "Epoch [351/500], Train Loss: 132.8130, Test Loss: 50.1918\n",
      "Epoch [352/500], Train Loss: 157.0126, Test Loss: 38.0465\n",
      "Epoch [353/500], Train Loss: 128.8797, Test Loss: 36.2912\n",
      "Epoch [354/500], Train Loss: 127.6548, Test Loss: 38.3457\n",
      "Epoch [355/500], Train Loss: 149.1815, Test Loss: 40.3428\n",
      "Epoch [356/500], Train Loss: 143.2620, Test Loss: 77.2246\n",
      "Epoch [357/500], Train Loss: 145.6617, Test Loss: 37.8540\n",
      "Epoch [358/500], Train Loss: 132.3420, Test Loss: 42.8696\n",
      "Epoch [359/500], Train Loss: 140.8539, Test Loss: 36.5859\n",
      "Epoch [360/500], Train Loss: 157.4776, Test Loss: 47.8044\n",
      "Epoch [361/500], Train Loss: 144.6301, Test Loss: 35.9003\n",
      "Epoch [362/500], Train Loss: 126.3026, Test Loss: 39.2031\n",
      "Epoch [363/500], Train Loss: 133.4734, Test Loss: 49.0577\n",
      "Epoch [364/500], Train Loss: 137.6671, Test Loss: 37.3103\n",
      "Epoch [365/500], Train Loss: 125.4601, Test Loss: 41.7921\n",
      "Epoch [366/500], Train Loss: 141.8938, Test Loss: 41.0405\n",
      "Epoch [367/500], Train Loss: 210.2769, Test Loss: 87.8713\n",
      "Epoch [368/500], Train Loss: 127.2353, Test Loss: 35.2095\n",
      "Epoch [369/500], Train Loss: 127.1632, Test Loss: 36.0241\n",
      "Epoch [370/500], Train Loss: 123.9522, Test Loss: 35.3099\n",
      "Epoch [371/500], Train Loss: 141.1195, Test Loss: 35.2927\n",
      "Epoch [372/500], Train Loss: 141.1758, Test Loss: 37.6207\n",
      "Epoch [373/500], Train Loss: 137.7976, Test Loss: 43.8722\n",
      "Epoch [374/500], Train Loss: 119.1559, Test Loss: 36.4980\n",
      "Epoch [375/500], Train Loss: 174.3064, Test Loss: 42.0840\n",
      "Epoch [376/500], Train Loss: 123.1899, Test Loss: 36.1266\n",
      "Epoch [377/500], Train Loss: 122.7995, Test Loss: 38.5543\n",
      "Epoch [378/500], Train Loss: 170.3691, Test Loss: 41.2338\n",
      "Epoch [379/500], Train Loss: 125.9402, Test Loss: 35.0657\n",
      "Epoch [380/500], Train Loss: 137.7989, Test Loss: 39.5785\n",
      "Epoch [381/500], Train Loss: 121.9430, Test Loss: 35.9076\n",
      "Epoch [382/500], Train Loss: 122.9008, Test Loss: 37.1141\n",
      "Epoch [383/500], Train Loss: 138.6858, Test Loss: 55.9506\n",
      "Epoch [384/500], Train Loss: 140.8623, Test Loss: 35.7780\n",
      "Epoch [385/500], Train Loss: 161.2836, Test Loss: 35.0765\n",
      "Epoch [386/500], Train Loss: 137.8140, Test Loss: 42.5222\n",
      "Epoch [387/500], Train Loss: 132.4050, Test Loss: 36.5539\n",
      "Epoch [388/500], Train Loss: 123.8336, Test Loss: 45.5819\n",
      "Epoch [389/500], Train Loss: 129.6961, Test Loss: 36.0390\n",
      "Epoch [390/500], Train Loss: 216.7562, Test Loss: 44.7535\n",
      "Epoch [391/500], Train Loss: 121.1178, Test Loss: 35.0816\n",
      "Epoch [392/500], Train Loss: 125.6618, Test Loss: 33.8535\n",
      "Epoch [393/500], Train Loss: 116.2708, Test Loss: 38.3118\n",
      "Epoch [394/500], Train Loss: 123.1409, Test Loss: 35.3215\n",
      "Epoch [395/500], Train Loss: 144.5320, Test Loss: 39.3047\n",
      "Epoch [396/500], Train Loss: 117.4960, Test Loss: 36.7526\n",
      "Epoch [397/500], Train Loss: 142.5637, Test Loss: 46.6093\n",
      "Epoch [398/500], Train Loss: 231.6592, Test Loss: 35.7891\n",
      "Epoch [399/500], Train Loss: 120.7358, Test Loss: 36.4127\n",
      "Epoch [400/500], Train Loss: 112.9811, Test Loss: 36.0331\n",
      "Epoch [401/500], Train Loss: 120.3539, Test Loss: 37.2329\n",
      "Epoch [402/500], Train Loss: 121.0324, Test Loss: 37.6789\n",
      "Epoch [403/500], Train Loss: 138.4591, Test Loss: 36.2864\n",
      "Epoch [404/500], Train Loss: 119.3665, Test Loss: 40.5429\n",
      "Epoch [405/500], Train Loss: 147.7980, Test Loss: 50.5936\n",
      "Epoch [406/500], Train Loss: 124.2133, Test Loss: 35.6301\n",
      "Epoch [407/500], Train Loss: 113.1872, Test Loss: 33.6579\n",
      "Epoch [408/500], Train Loss: 119.6871, Test Loss: 36.5888\n",
      "Epoch [409/500], Train Loss: 134.3107, Test Loss: 39.8127\n",
      "Epoch [410/500], Train Loss: 187.0762, Test Loss: 33.7344\n",
      "Epoch [411/500], Train Loss: 113.0312, Test Loss: 35.6874\n",
      "Epoch [412/500], Train Loss: 145.1604, Test Loss: 48.4157\n",
      "Epoch [413/500], Train Loss: 132.5635, Test Loss: 34.4875\n",
      "Epoch [414/500], Train Loss: 113.0763, Test Loss: 40.3667\n",
      "Epoch [415/500], Train Loss: 138.4323, Test Loss: 34.3615\n",
      "Epoch [416/500], Train Loss: 146.0746, Test Loss: 39.5530\n",
      "Epoch [417/500], Train Loss: 116.2537, Test Loss: 35.3566\n",
      "Epoch [418/500], Train Loss: 116.7061, Test Loss: 35.8193\n",
      "Epoch [419/500], Train Loss: 131.1210, Test Loss: 36.7501\n",
      "Epoch [420/500], Train Loss: 119.8337, Test Loss: 37.5465\n",
      "Epoch [421/500], Train Loss: 161.3687, Test Loss: 39.6827\n",
      "Epoch [422/500], Train Loss: 126.8293, Test Loss: 40.0761\n",
      "Epoch [423/500], Train Loss: 128.1672, Test Loss: 39.0873\n",
      "Epoch [424/500], Train Loss: 116.0826, Test Loss: 35.9702\n",
      "Epoch [425/500], Train Loss: 118.8134, Test Loss: 35.6559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [426/500], Train Loss: 129.6133, Test Loss: 66.3332\n",
      "Epoch [427/500], Train Loss: 123.4593, Test Loss: 36.6538\n",
      "Epoch [428/500], Train Loss: 119.2154, Test Loss: 37.3738\n",
      "Epoch [429/500], Train Loss: 123.1275, Test Loss: 43.9199\n",
      "Epoch [430/500], Train Loss: 121.1766, Test Loss: 37.6622\n",
      "Epoch [431/500], Train Loss: 117.4674, Test Loss: 36.6097\n",
      "Epoch [432/500], Train Loss: 193.0328, Test Loss: 38.8094\n",
      "Epoch [433/500], Train Loss: 115.9160, Test Loss: 38.1236\n",
      "Epoch [434/500], Train Loss: 110.9047, Test Loss: 35.9962\n",
      "Epoch [435/500], Train Loss: 119.7714, Test Loss: 36.3220\n",
      "Epoch [436/500], Train Loss: 162.1100, Test Loss: 33.2830\n",
      "Epoch [437/500], Train Loss: 116.2293, Test Loss: 35.5211\n",
      "Epoch [438/500], Train Loss: 119.7296, Test Loss: 33.4616\n",
      "Epoch [439/500], Train Loss: 122.7335, Test Loss: 33.3997\n",
      "Epoch [440/500], Train Loss: 112.6196, Test Loss: 34.8846\n",
      "Epoch [441/500], Train Loss: 117.7539, Test Loss: 41.2719\n",
      "Epoch [442/500], Train Loss: 124.5555, Test Loss: 34.5271\n",
      "Epoch [443/500], Train Loss: 144.9525, Test Loss: 34.6289\n",
      "Epoch [444/500], Train Loss: 115.0300, Test Loss: 38.3543\n",
      "Epoch [445/500], Train Loss: 114.2162, Test Loss: 33.8966\n",
      "Epoch [446/500], Train Loss: 124.6715, Test Loss: 33.9404\n",
      "Epoch [447/500], Train Loss: 138.2790, Test Loss: 36.9389\n",
      "Epoch [448/500], Train Loss: 124.3992, Test Loss: 39.7096\n",
      "Epoch [449/500], Train Loss: 169.0506, Test Loss: 35.4727\n",
      "Epoch [450/500], Train Loss: 105.2776, Test Loss: 35.2726\n",
      "Epoch [451/500], Train Loss: 114.9238, Test Loss: 34.0656\n",
      "Epoch [452/500], Train Loss: 120.6997, Test Loss: 39.3340\n",
      "Epoch [453/500], Train Loss: 120.0633, Test Loss: 40.5568\n",
      "Epoch [454/500], Train Loss: 138.5290, Test Loss: 33.3903\n",
      "Epoch [455/500], Train Loss: 110.8242, Test Loss: 37.8513\n",
      "Epoch [456/500], Train Loss: 118.0261, Test Loss: 39.5930\n",
      "Epoch [457/500], Train Loss: 109.6167, Test Loss: 41.3608\n",
      "Epoch [458/500], Train Loss: 128.5705, Test Loss: 35.6026\n",
      "Epoch [459/500], Train Loss: 116.0667, Test Loss: 36.9499\n",
      "Epoch [460/500], Train Loss: 117.4067, Test Loss: 39.5167\n",
      "Epoch [461/500], Train Loss: 123.1453, Test Loss: 37.3892\n",
      "Epoch [462/500], Train Loss: 114.8875, Test Loss: 36.2277\n",
      "Epoch [463/500], Train Loss: 111.1264, Test Loss: 34.3346\n",
      "Epoch [464/500], Train Loss: 120.0887, Test Loss: 36.9257\n",
      "Epoch [465/500], Train Loss: 116.6949, Test Loss: 41.8864\n",
      "Epoch [466/500], Train Loss: 133.2000, Test Loss: 36.0822\n",
      "Epoch [467/500], Train Loss: 120.0921, Test Loss: 46.1451\n",
      "Epoch [468/500], Train Loss: 125.5240, Test Loss: 34.4522\n",
      "Epoch [469/500], Train Loss: 193.1380, Test Loss: 34.8122\n",
      "Epoch [470/500], Train Loss: 104.6859, Test Loss: 32.7747\n",
      "Epoch [471/500], Train Loss: 106.9548, Test Loss: 35.7253\n",
      "Epoch [472/500], Train Loss: 103.4837, Test Loss: 33.6490\n",
      "Epoch [473/500], Train Loss: 110.8020, Test Loss: 38.9393\n",
      "Epoch [474/500], Train Loss: 122.4900, Test Loss: 38.1066\n",
      "Epoch [475/500], Train Loss: 113.6740, Test Loss: 50.4533\n",
      "Epoch [476/500], Train Loss: 144.7639, Test Loss: 35.0239\n",
      "Epoch [477/500], Train Loss: 108.7092, Test Loss: 37.3234\n",
      "Epoch [478/500], Train Loss: 109.2215, Test Loss: 32.3077\n",
      "Epoch [479/500], Train Loss: 152.1933, Test Loss: 33.8686\n",
      "Epoch [480/500], Train Loss: 105.2047, Test Loss: 34.7776\n",
      "Epoch [481/500], Train Loss: 124.1990, Test Loss: 34.1490\n",
      "Epoch [482/500], Train Loss: 112.6468, Test Loss: 33.5718\n",
      "Epoch [483/500], Train Loss: 118.6401, Test Loss: 36.2138\n",
      "Epoch [484/500], Train Loss: 124.9386, Test Loss: 33.0822\n",
      "Epoch [485/500], Train Loss: 118.8930, Test Loss: 46.1110\n",
      "Epoch [486/500], Train Loss: 124.7956, Test Loss: 53.9039\n",
      "Epoch [487/500], Train Loss: 113.4724, Test Loss: 33.6204\n",
      "Epoch [488/500], Train Loss: 104.9583, Test Loss: 38.9193\n",
      "Epoch [489/500], Train Loss: 107.1371, Test Loss: 34.9232\n",
      "Epoch [490/500], Train Loss: 132.8996, Test Loss: 38.9974\n",
      "Epoch [491/500], Train Loss: 127.4948, Test Loss: 43.3061\n",
      "Epoch [492/500], Train Loss: 111.5275, Test Loss: 37.5022\n",
      "Epoch [493/500], Train Loss: 107.5742, Test Loss: 34.9183\n",
      "Epoch [494/500], Train Loss: 121.2965, Test Loss: 39.5028\n",
      "Epoch [495/500], Train Loss: 110.7323, Test Loss: 35.4687\n",
      "Epoch [496/500], Train Loss: 107.8401, Test Loss: 36.5114\n",
      "Epoch [497/500], Train Loss: 141.4566, Test Loss: 39.7394\n",
      "Epoch [498/500], Train Loss: 110.5745, Test Loss: 32.5599\n",
      "Epoch [499/500], Train Loss: 103.8974, Test Loss: 33.6621\n",
      "Epoch [500/500], Train Loss: 105.9792, Test Loss: 40.0083\n",
      "Execution time: 24679.306550979614 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.30772264953703"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (conv1): Conv1d(1, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=120708, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  81.7677657560091\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i].cpu().numpy()\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i].unsqueeze(0).unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "218.59  - Concentrations: 0.004999999888241291\n",
      "44.58  - Concentrations: 0.02500000037252903\n",
      "11.2  - Concentrations: 0.10000000149011612\n",
      "5.5  - Concentrations: 0.25\n",
      "3.53  - Concentrations: 0.5\n",
      "1.79  - Concentrations: 1.0\n",
      "1.28  - Concentrations: 2.5\n",
      "0.64  - Concentrations: 10.0\n",
      "0.64  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  2.4639935\n",
      "--------------------\n",
      "1.31  - Min Value: 0.6783  - Mean Value: 9.2\n",
      "8.98  - Min Value: 0.0096  - Mean Value: 10.3\n",
      "2.58  - Min Value: 0.147  - Mean Value: 10.5\n",
      "2.03  - Min Value: 0.5572  - Mean Value: 8.5\n",
      "1.27  - Min Value: 1.3567  - Mean Value: 10.6\n",
      "1.76  - Min Value: 0.6332  - Mean Value: 10.9\n",
      "1.63  - Min Value: 0.7017  - Mean Value: 11.0\n",
      "2.59  - Min Value: 0.3674  - Mean Value: 8.9\n",
      "1.26  - Min Value: 0.8387  - Mean Value: 9.8\n",
      "1.23  - Min Value: 1.0913  - Mean Value: 11.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  17.430695\n",
      "--------------------\n",
      "15.88  - Min Value: 0.0111  - Mean Value: 0.1\n",
      "13.41  - Min Value: 0.0103  - Mean Value: 0.1\n",
      "10.69  - Min Value: 0.0153  - Mean Value: 0.1\n",
      "15.54  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "22.13  - Min Value: 0.0089  - Mean Value: 0.1\n",
      "23.1  - Min Value: 0.0075  - Mean Value: 0.1\n",
      "18.5  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "21.4  - Min Value: 0.0052  - Mean Value: 0.1\n",
      "20.3  - Min Value: 0.008  - Mean Value: 0.1\n",
      "13.35  - Min Value: 0.0134  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  19.08169\n",
      "--------------------\n",
      "22.01  - Min Value: 0.008  - Mean Value: 0.8\n",
      "22.62  - Min Value: 0.009  - Mean Value: 0.9\n",
      "28.88  - Min Value: 0.0138  - Mean Value: 1.5\n",
      "22.7  - Min Value: 0.0107  - Mean Value: 0.7\n",
      "12.85  - Min Value: 0.0191  - Mean Value: 0.7\n",
      "25.11  - Min Value: 0.0186  - Mean Value: 0.8\n",
      "23.44  - Min Value: 0.0175  - Mean Value: 0.8\n",
      "6.5  - Min Value: 0.0238  - Mean Value: 1.3\n",
      "10.86  - Min Value: 0.0168  - Mean Value: 0.7\n",
      "15.84  - Min Value: 0.0171  - Mean Value: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  29.519108\n",
      "--------------------\n",
      "17.36  - Min Value: 0.013  - Mean Value: 0.6\n",
      "38.21  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "33.72  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "26.51  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "19.18  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "25.48  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "53.22  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "23.27  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "13.95  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "44.28  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  137.90057\n",
      "--------------------\n",
      "102.85  - Min Value: 0.0062  - Mean Value: 2.1\n",
      "114.12  - Min Value: 0.006  - Mean Value: 3.7\n",
      "220.47  - Min Value: 0.0066  - Mean Value: 4.3\n",
      "142.99  - Min Value: 0.0094  - Mean Value: 4.3\n",
      "80.97  - Min Value: 0.0068  - Mean Value: 4.9\n",
      "173.05  - Min Value: 0.005  - Mean Value: 3.8\n",
      "106.56  - Min Value: 0.0101  - Mean Value: 3.2\n",
      "103.08  - Min Value: 0.0062  - Mean Value: 3.2\n",
      "152.67  - Min Value: 0.0053  - Mean Value: 5.3\n",
      "182.25  - Min Value: 0.0054  - Mean Value: 2.5\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(HighDynamicRange2Conc[i].min().item(),4), \" - Mean Value:\", np.round(HighDynamicRange2Conc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  4.1167151625514276\n",
      "--------------------\n",
      "3.83\n",
      "3.91\n",
      "4.0\n",
      "3.94\n",
      "4.22\n",
      "4.22\n",
      "3.99\n",
      "4.25\n",
      "4.54\n",
      "4.27\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  13.281201180897602\n",
      "--------------------\n",
      "3.94\n",
      "4.19\n",
      "4.76\n",
      "5.79\n",
      "14.37\n",
      "14.6\n",
      "15.82\n",
      "20.31\n",
      "22.29\n",
      "26.74\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a dataset with singlets added at random\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  100.19315921597135\n",
      "--------------------\n",
      "10.68\n",
      "30.71\n",
      "50.66\n",
      "70.71\n",
      "90.67\n",
      "110.37\n",
      "130.09\n",
      "149.81\n",
      "169.34\n",
      "188.89\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal Baseline 1\n",
      "tensor([0.4828, 0.5587, 0.3639, 0.5637, 0.5311, 0.4805, 0.5427, 0.6278, 0.7121,\n",
      "        0.3278, 0.4023, 0.6360, 0.5684, 0.9606, 0.6122, 0.6716, 0.7118, 0.5309,\n",
      "        0.5527, 0.4018, 0.5764, 0.9554, 0.2787, 0.5158, 0.6037, 0.4632, 0.6302,\n",
      "        0.5855, 0.5706, 0.4806, 0.4069, 0.8528, 0.5909, 0.4096, 0.4151, 0.5725,\n",
      "        0.4037, 0.4354, 0.5242, 0.3529, 0.3928, 0.5139, 0.6102, 0.4844],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Sinusoidal Baseline 2\n",
      "tensor([0.3391, 0.5329, 0.0000, 0.5409, 0.3821, 0.4322, 0.4094, 0.1385, 0.4085,\n",
      "        0.5719, 0.4534, 0.5278, 0.8178, 0.2164, 0.7824, 0.0399, 0.8381, 0.5321,\n",
      "        0.4620, 0.4158, 0.3989, 0.3375, 0.6197, 0.5513, 0.4327, 0.4501, 0.2065,\n",
      "        0.1377, 0.5611, 0.3335, 0.4151, 0.1689, 0.0870, 0.4378, 0.5969, 0.3851,\n",
      "        0.6205, 0.7646, 0.6283, 0.3091, 0.4131, 0.7863, 0.2482, 0.1088],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "HD-Range 1 - 1s and 20s\n",
      "tensor([ 0.0000, 15.9473,  0.0000, 16.5482,  0.0000, 17.3922,  0.0000, 16.2965,\n",
      "         0.0000, 18.0408,  0.0000, 18.9285,  0.0000, 18.1016,  0.0000, 17.6488,\n",
      "         0.0000, 16.4961,  0.0000, 18.1656,  0.0000, 15.3852,  0.0000, 18.7051,\n",
      "         0.0000, 18.5402,  0.0000, 18.1693,  0.0000, 19.3471,  0.2207, 16.6673,\n",
      "         0.0000, 18.1190,  0.2233, 19.2798,  0.0000, 17.5195,  0.0000, 18.2419,\n",
      "         0.0000, 17.0027,  0.0000, 17.2852], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "HD-Range 2 - 0s and 20s\n",
      "tensor([ 0.0000, 15.9466,  0.0000, 16.5517,  0.0000, 17.3913,  0.0000, 16.2974,\n",
      "         0.0000, 18.0394,  0.0000, 18.9306,  0.0000, 18.0952,  0.0000, 17.6486,\n",
      "         0.0000, 16.4959,  0.0000, 18.1655,  0.0000, 15.3887,  0.0000, 18.7065,\n",
      "         0.0000, 18.5405,  0.0000, 18.1702,  0.0000, 19.3473,  0.2118, 16.6692,\n",
      "         0.0000, 18.1196,  0.2149, 19.2772,  0.0000, 17.5233,  0.0000, 18.2427,\n",
      "         0.0000, 17.0025,  0.0000, 17.2826], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(OtherValSpectra[0].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 1s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
