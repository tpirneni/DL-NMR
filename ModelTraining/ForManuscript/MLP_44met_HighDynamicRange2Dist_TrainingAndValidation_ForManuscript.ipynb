{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 5000\n",
    "\n",
    "# Identification part of the filenames\n",
    "base_name = 'HighDynamicRange2'\n",
    "base_dir = '/path/to/base/directory'   # Set base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"MLP_44met_{base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load(f'Dataset44_{base_name}_ForManuscript_Spec.npy')\n",
    "conc1 = np.load(f'Dataset44_{base_name}_ForManuscript_Conc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 169, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 169, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "size_hidden1 = 200\n",
    "size_hidden2 = 44\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(46000, size_hidden1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "    def forward(self, input):\n",
    "        return (self.lin2(self.relu1(self.lin1(input))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = RelativeAbsoluteError()\n",
    "    optimizer = optim.AdamW(model.parameters(), weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in best validation loss constitutes early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "            \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/5000], Train Loss: 9950.8729, Test Loss: 2367.8714\n",
      "Epoch [2/5000], Train Loss: 8624.9254, Test Loss: 1963.0261\n",
      "Epoch [3/5000], Train Loss: 7044.7888, Test Loss: 1577.4776\n",
      "Epoch [4/5000], Train Loss: 5632.6824, Test Loss: 1267.1412\n",
      "Epoch [5/5000], Train Loss: 4545.3862, Test Loss: 1022.3837\n",
      "Epoch [6/5000], Train Loss: 3685.1919, Test Loss: 835.4300\n",
      "Epoch [7/5000], Train Loss: 2962.6181, Test Loss: 669.2201\n",
      "Epoch [8/5000], Train Loss: 2368.9960, Test Loss: 524.5359\n",
      "Epoch [9/5000], Train Loss: 1877.2742, Test Loss: 427.8989\n",
      "Epoch [10/5000], Train Loss: 1586.9652, Test Loss: 368.7123\n",
      "Epoch [11/5000], Train Loss: 1321.4880, Test Loss: 309.1291\n",
      "Epoch [12/5000], Train Loss: 1126.7900, Test Loss: 273.5273\n",
      "Epoch [13/5000], Train Loss: 982.3806, Test Loss: 228.7329\n",
      "Epoch [14/5000], Train Loss: 868.5210, Test Loss: 214.9019\n",
      "Epoch [15/5000], Train Loss: 757.6213, Test Loss: 178.4463\n",
      "Epoch [16/5000], Train Loss: 663.6892, Test Loss: 165.0635\n",
      "Epoch [17/5000], Train Loss: 614.9848, Test Loss: 160.7103\n",
      "Epoch [18/5000], Train Loss: 593.1134, Test Loss: 150.8773\n",
      "Epoch [19/5000], Train Loss: 584.7407, Test Loss: 154.7124\n",
      "Epoch [20/5000], Train Loss: 567.3895, Test Loss: 140.2485\n",
      "Epoch [21/5000], Train Loss: 540.2016, Test Loss: 139.3587\n",
      "Epoch [22/5000], Train Loss: 524.2770, Test Loss: 132.7890\n",
      "Epoch [23/5000], Train Loss: 516.6748, Test Loss: 134.7014\n",
      "Epoch [24/5000], Train Loss: 497.0435, Test Loss: 128.1897\n",
      "Epoch [25/5000], Train Loss: 484.0856, Test Loss: 123.9594\n",
      "Epoch [26/5000], Train Loss: 457.9135, Test Loss: 115.0405\n",
      "Epoch [27/5000], Train Loss: 445.2536, Test Loss: 124.2892\n",
      "Epoch [28/5000], Train Loss: 449.3426, Test Loss: 122.1691\n",
      "Epoch [29/5000], Train Loss: 431.5328, Test Loss: 104.6175\n",
      "Epoch [30/5000], Train Loss: 401.7140, Test Loss: 112.2038\n",
      "Epoch [31/5000], Train Loss: 396.9099, Test Loss: 97.5560\n",
      "Epoch [32/5000], Train Loss: 369.3209, Test Loss: 92.8716\n",
      "Epoch [33/5000], Train Loss: 358.5991, Test Loss: 93.5458\n",
      "Epoch [34/5000], Train Loss: 346.6133, Test Loss: 90.3568\n",
      "Epoch [35/5000], Train Loss: 349.0237, Test Loss: 90.3709\n",
      "Epoch [36/5000], Train Loss: 345.7441, Test Loss: 87.7294\n",
      "Epoch [37/5000], Train Loss: 338.7740, Test Loss: 96.7817\n",
      "Epoch [38/5000], Train Loss: 332.2462, Test Loss: 83.0726\n",
      "Epoch [39/5000], Train Loss: 326.6877, Test Loss: 86.8394\n",
      "Epoch [40/5000], Train Loss: 330.7655, Test Loss: 86.8256\n",
      "Epoch [41/5000], Train Loss: 337.8406, Test Loss: 89.4088\n",
      "Epoch [42/5000], Train Loss: 328.6313, Test Loss: 83.4979\n",
      "Epoch [43/5000], Train Loss: 316.8284, Test Loss: 88.6564\n",
      "Epoch [44/5000], Train Loss: 326.3712, Test Loss: 85.1126\n",
      "Epoch [45/5000], Train Loss: 313.9693, Test Loss: 83.2258\n",
      "Epoch [46/5000], Train Loss: 320.6008, Test Loss: 88.2121\n",
      "Epoch [47/5000], Train Loss: 311.4789, Test Loss: 84.8472\n",
      "Epoch [48/5000], Train Loss: 311.1261, Test Loss: 76.8859\n",
      "Epoch [49/5000], Train Loss: 313.3983, Test Loss: 88.3315\n",
      "Epoch [50/5000], Train Loss: 320.4737, Test Loss: 80.4310\n",
      "Epoch [51/5000], Train Loss: 309.7962, Test Loss: 79.5538\n",
      "Epoch [52/5000], Train Loss: 307.4351, Test Loss: 84.7946\n",
      "Epoch [53/5000], Train Loss: 315.9101, Test Loss: 78.5358\n",
      "Epoch [54/5000], Train Loss: 306.6319, Test Loss: 87.3365\n",
      "Epoch [55/5000], Train Loss: 309.4027, Test Loss: 87.5168\n",
      "Epoch [56/5000], Train Loss: 309.9731, Test Loss: 84.6198\n",
      "Epoch [57/5000], Train Loss: 312.9395, Test Loss: 85.1408\n",
      "Epoch [58/5000], Train Loss: 310.5979, Test Loss: 80.5181\n",
      "Epoch [59/5000], Train Loss: 304.5697, Test Loss: 80.0982\n",
      "Epoch [60/5000], Train Loss: 283.2957, Test Loss: 76.5153\n",
      "Epoch [61/5000], Train Loss: 300.5889, Test Loss: 79.0384\n",
      "Epoch [62/5000], Train Loss: 300.2171, Test Loss: 90.2593\n",
      "Epoch [63/5000], Train Loss: 304.9060, Test Loss: 86.3512\n",
      "Epoch [64/5000], Train Loss: 299.3622, Test Loss: 81.9448\n",
      "Epoch [65/5000], Train Loss: 290.5396, Test Loss: 82.2547\n",
      "Epoch [66/5000], Train Loss: 299.2503, Test Loss: 80.9962\n",
      "Epoch [67/5000], Train Loss: 292.5789, Test Loss: 79.3800\n",
      "Epoch [68/5000], Train Loss: 302.5981, Test Loss: 82.1700\n",
      "Epoch [69/5000], Train Loss: 299.3949, Test Loss: 81.1168\n",
      "Epoch [70/5000], Train Loss: 302.7007, Test Loss: 82.0154\n",
      "Epoch [71/5000], Train Loss: 302.9614, Test Loss: 75.4705\n",
      "Epoch [72/5000], Train Loss: 296.2879, Test Loss: 82.3647\n",
      "Epoch [73/5000], Train Loss: 292.2223, Test Loss: 78.4020\n",
      "Epoch [74/5000], Train Loss: 300.1421, Test Loss: 80.9084\n",
      "Epoch [75/5000], Train Loss: 294.3812, Test Loss: 82.7955\n",
      "Epoch [76/5000], Train Loss: 283.8283, Test Loss: 82.7179\n",
      "Epoch [77/5000], Train Loss: 299.9695, Test Loss: 76.5548\n",
      "Epoch [78/5000], Train Loss: 279.3502, Test Loss: 78.8207\n",
      "Epoch [79/5000], Train Loss: 288.6754, Test Loss: 77.4277\n",
      "Epoch [80/5000], Train Loss: 301.2467, Test Loss: 79.2489\n",
      "Epoch [81/5000], Train Loss: 279.3625, Test Loss: 77.1641\n",
      "Epoch [82/5000], Train Loss: 282.3597, Test Loss: 75.8590\n",
      "Epoch [83/5000], Train Loss: 287.2405, Test Loss: 83.5242\n",
      "Epoch [84/5000], Train Loss: 297.3382, Test Loss: 77.8725\n",
      "Epoch [85/5000], Train Loss: 288.6590, Test Loss: 71.8812\n",
      "Epoch [86/5000], Train Loss: 285.8034, Test Loss: 77.8845\n",
      "Epoch [87/5000], Train Loss: 276.0766, Test Loss: 76.9638\n",
      "Epoch [88/5000], Train Loss: 278.6189, Test Loss: 76.2473\n",
      "Epoch [89/5000], Train Loss: 281.9009, Test Loss: 78.7839\n",
      "Epoch [90/5000], Train Loss: 286.3262, Test Loss: 79.2466\n",
      "Epoch [91/5000], Train Loss: 287.1497, Test Loss: 78.5814\n",
      "Epoch [92/5000], Train Loss: 287.5417, Test Loss: 76.6279\n",
      "Epoch [93/5000], Train Loss: 269.4935, Test Loss: 71.7324\n",
      "Epoch [94/5000], Train Loss: 287.8028, Test Loss: 82.7656\n",
      "Epoch [95/5000], Train Loss: 295.2403, Test Loss: 78.9577\n",
      "Epoch [96/5000], Train Loss: 277.0229, Test Loss: 72.1383\n",
      "Epoch [97/5000], Train Loss: 285.9502, Test Loss: 75.1375\n",
      "Epoch [98/5000], Train Loss: 286.7234, Test Loss: 75.1120\n",
      "Epoch [99/5000], Train Loss: 273.9464, Test Loss: 76.5254\n",
      "Epoch [100/5000], Train Loss: 271.4288, Test Loss: 83.8040\n",
      "Epoch [101/5000], Train Loss: 283.2722, Test Loss: 75.7399\n",
      "Epoch [102/5000], Train Loss: 282.8891, Test Loss: 73.4985\n",
      "Epoch [103/5000], Train Loss: 274.5624, Test Loss: 80.1553\n",
      "Epoch [104/5000], Train Loss: 281.6484, Test Loss: 77.9094\n",
      "Epoch [105/5000], Train Loss: 283.0501, Test Loss: 77.8196\n",
      "Epoch [106/5000], Train Loss: 274.7530, Test Loss: 78.3218\n",
      "Epoch [107/5000], Train Loss: 284.6241, Test Loss: 73.6415\n",
      "Epoch [108/5000], Train Loss: 278.9362, Test Loss: 79.6212\n",
      "Epoch [109/5000], Train Loss: 297.5157, Test Loss: 76.4280\n",
      "Epoch [110/5000], Train Loss: 268.5048, Test Loss: 73.4728\n",
      "Epoch [111/5000], Train Loss: 284.1867, Test Loss: 82.0975\n",
      "Epoch [112/5000], Train Loss: 286.3600, Test Loss: 78.8024\n",
      "Epoch [113/5000], Train Loss: 280.0268, Test Loss: 78.9866\n",
      "Epoch [114/5000], Train Loss: 278.3928, Test Loss: 81.3937\n",
      "Epoch [115/5000], Train Loss: 278.6054, Test Loss: 76.8545\n",
      "Epoch [116/5000], Train Loss: 281.8684, Test Loss: 82.3209\n",
      "Epoch [117/5000], Train Loss: 265.2670, Test Loss: 76.6509\n",
      "Epoch [118/5000], Train Loss: 277.0275, Test Loss: 80.5342\n",
      "Epoch [119/5000], Train Loss: 285.3748, Test Loss: 77.4564\n",
      "Epoch [120/5000], Train Loss: 280.9050, Test Loss: 77.0547\n",
      "Epoch [121/5000], Train Loss: 276.2665, Test Loss: 83.6523\n",
      "Epoch [122/5000], Train Loss: 278.3915, Test Loss: 77.4327\n",
      "Epoch [123/5000], Train Loss: 268.0641, Test Loss: 74.8827\n",
      "Epoch [124/5000], Train Loss: 273.0808, Test Loss: 81.7109\n",
      "Epoch [125/5000], Train Loss: 278.5131, Test Loss: 72.7574\n",
      "Epoch [126/5000], Train Loss: 262.7770, Test Loss: 77.5618\n",
      "Epoch [127/5000], Train Loss: 266.4057, Test Loss: 80.0960\n",
      "Epoch [128/5000], Train Loss: 273.2590, Test Loss: 73.8162\n",
      "Epoch [129/5000], Train Loss: 263.1169, Test Loss: 75.0341\n",
      "Epoch [130/5000], Train Loss: 267.8170, Test Loss: 71.9212\n",
      "Epoch [131/5000], Train Loss: 262.3590, Test Loss: 76.2038\n",
      "Epoch [132/5000], Train Loss: 272.9859, Test Loss: 78.6201\n",
      "Epoch [133/5000], Train Loss: 279.6363, Test Loss: 80.3441\n",
      "Epoch [134/5000], Train Loss: 278.8944, Test Loss: 76.9796\n",
      "Epoch [135/5000], Train Loss: 289.5376, Test Loss: 81.5154\n",
      "Epoch [136/5000], Train Loss: 271.9896, Test Loss: 72.0744\n",
      "Epoch [137/5000], Train Loss: 261.6930, Test Loss: 71.2256\n",
      "Epoch [138/5000], Train Loss: 263.5470, Test Loss: 73.9962\n",
      "Epoch [139/5000], Train Loss: 268.9576, Test Loss: 74.6601\n",
      "Epoch [140/5000], Train Loss: 261.9741, Test Loss: 71.8449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [141/5000], Train Loss: 257.7490, Test Loss: 68.3451\n",
      "Epoch [142/5000], Train Loss: 268.3638, Test Loss: 75.6410\n",
      "Epoch [143/5000], Train Loss: 271.3752, Test Loss: 75.2591\n",
      "Epoch [144/5000], Train Loss: 258.3398, Test Loss: 76.7135\n",
      "Epoch [145/5000], Train Loss: 271.2280, Test Loss: 73.2601\n",
      "Epoch [146/5000], Train Loss: 273.0091, Test Loss: 75.6985\n",
      "Epoch [147/5000], Train Loss: 261.2563, Test Loss: 75.9121\n",
      "Epoch [148/5000], Train Loss: 261.2056, Test Loss: 74.2839\n",
      "Epoch [149/5000], Train Loss: 255.7684, Test Loss: 77.8900\n",
      "Epoch [150/5000], Train Loss: 275.5329, Test Loss: 77.6346\n",
      "Epoch [151/5000], Train Loss: 267.8332, Test Loss: 76.1894\n",
      "Epoch [152/5000], Train Loss: 264.2906, Test Loss: 72.0819\n",
      "Epoch [153/5000], Train Loss: 265.2049, Test Loss: 73.9996\n",
      "Epoch [154/5000], Train Loss: 278.9593, Test Loss: 77.0837\n",
      "Epoch [155/5000], Train Loss: 264.8530, Test Loss: 76.9521\n",
      "Epoch [156/5000], Train Loss: 265.8870, Test Loss: 73.5429\n",
      "Epoch [157/5000], Train Loss: 262.2256, Test Loss: 73.4383\n",
      "Epoch [158/5000], Train Loss: 264.2995, Test Loss: 75.2083\n",
      "Epoch [159/5000], Train Loss: 279.2977, Test Loss: 73.4930\n",
      "Epoch [160/5000], Train Loss: 268.8908, Test Loss: 74.4843\n",
      "Epoch [161/5000], Train Loss: 263.4763, Test Loss: 74.3208\n",
      "Epoch [162/5000], Train Loss: 252.7615, Test Loss: 73.4425\n",
      "Epoch [163/5000], Train Loss: 269.7252, Test Loss: 77.4836\n",
      "Epoch [164/5000], Train Loss: 264.1730, Test Loss: 67.6577\n",
      "Epoch [165/5000], Train Loss: 257.8302, Test Loss: 69.5753\n",
      "Epoch [166/5000], Train Loss: 259.5385, Test Loss: 79.3833\n",
      "Epoch [167/5000], Train Loss: 257.8025, Test Loss: 78.8197\n",
      "Epoch [168/5000], Train Loss: 268.5322, Test Loss: 75.3364\n",
      "Epoch [169/5000], Train Loss: 261.8660, Test Loss: 73.2939\n",
      "Epoch [170/5000], Train Loss: 267.6119, Test Loss: 75.5942\n",
      "Epoch [171/5000], Train Loss: 264.0358, Test Loss: 72.8708\n",
      "Epoch [172/5000], Train Loss: 258.5116, Test Loss: 75.5419\n",
      "Epoch [173/5000], Train Loss: 259.5287, Test Loss: 79.1672\n",
      "Epoch [174/5000], Train Loss: 273.2533, Test Loss: 75.4395\n",
      "Epoch [175/5000], Train Loss: 260.9249, Test Loss: 68.6203\n",
      "Epoch [176/5000], Train Loss: 257.8420, Test Loss: 70.5922\n",
      "Epoch [177/5000], Train Loss: 258.8872, Test Loss: 71.0537\n",
      "Epoch [178/5000], Train Loss: 263.4420, Test Loss: 78.5839\n",
      "Epoch [179/5000], Train Loss: 252.8518, Test Loss: 72.5673\n",
      "Epoch [180/5000], Train Loss: 254.9497, Test Loss: 71.4758\n",
      "Epoch [181/5000], Train Loss: 259.7488, Test Loss: 72.0799\n",
      "Epoch [182/5000], Train Loss: 263.0176, Test Loss: 80.4852\n",
      "Epoch [183/5000], Train Loss: 275.3122, Test Loss: 71.7734\n",
      "Epoch [184/5000], Train Loss: 260.3791, Test Loss: 76.5428\n",
      "Epoch [185/5000], Train Loss: 253.0674, Test Loss: 68.3170\n",
      "Epoch [186/5000], Train Loss: 259.1383, Test Loss: 72.7066\n",
      "Epoch [187/5000], Train Loss: 260.9559, Test Loss: 75.8339\n",
      "Epoch [188/5000], Train Loss: 253.0521, Test Loss: 69.8413\n",
      "Epoch [189/5000], Train Loss: 249.6410, Test Loss: 76.9394\n",
      "Epoch [190/5000], Train Loss: 257.3111, Test Loss: 72.4242\n",
      "Epoch [191/5000], Train Loss: 253.5497, Test Loss: 70.4221\n",
      "Epoch [192/5000], Train Loss: 256.3093, Test Loss: 75.2146\n",
      "Epoch [193/5000], Train Loss: 264.0573, Test Loss: 75.0373\n",
      "Epoch [194/5000], Train Loss: 257.1717, Test Loss: 72.9750\n",
      "Epoch [195/5000], Train Loss: 262.8243, Test Loss: 82.9933\n",
      "Epoch [196/5000], Train Loss: 264.4546, Test Loss: 74.3333\n",
      "Epoch [197/5000], Train Loss: 273.5244, Test Loss: 76.9793\n",
      "Epoch [198/5000], Train Loss: 272.4585, Test Loss: 69.5702\n",
      "Epoch [199/5000], Train Loss: 247.7267, Test Loss: 76.1925\n",
      "Epoch [200/5000], Train Loss: 252.4080, Test Loss: 74.3558\n",
      "Epoch [201/5000], Train Loss: 263.2047, Test Loss: 71.8446\n",
      "Epoch [202/5000], Train Loss: 251.7357, Test Loss: 72.0362\n",
      "Epoch [203/5000], Train Loss: 254.9731, Test Loss: 69.1980\n",
      "Epoch [204/5000], Train Loss: 247.9298, Test Loss: 72.2684\n",
      "Epoch [205/5000], Train Loss: 250.1007, Test Loss: 70.7329\n",
      "Epoch [206/5000], Train Loss: 256.3430, Test Loss: 75.2613\n",
      "Epoch [207/5000], Train Loss: 248.2861, Test Loss: 74.2555\n",
      "Epoch [208/5000], Train Loss: 259.2998, Test Loss: 69.0272\n",
      "Epoch [209/5000], Train Loss: 254.0119, Test Loss: 74.7538\n",
      "Epoch [210/5000], Train Loss: 264.8779, Test Loss: 74.2032\n",
      "Epoch [211/5000], Train Loss: 259.4920, Test Loss: 67.5793\n",
      "Epoch [212/5000], Train Loss: 253.2724, Test Loss: 70.6169\n",
      "Epoch [213/5000], Train Loss: 248.3561, Test Loss: 65.0415\n",
      "Epoch [214/5000], Train Loss: 248.6287, Test Loss: 68.8038\n",
      "Epoch [215/5000], Train Loss: 254.6366, Test Loss: 67.7786\n",
      "Epoch [216/5000], Train Loss: 245.2898, Test Loss: 70.9602\n",
      "Epoch [217/5000], Train Loss: 252.6393, Test Loss: 69.5955\n",
      "Epoch [218/5000], Train Loss: 252.1925, Test Loss: 72.8814\n",
      "Epoch [219/5000], Train Loss: 270.4916, Test Loss: 73.2249\n",
      "Epoch [220/5000], Train Loss: 260.4722, Test Loss: 69.8260\n",
      "Epoch [221/5000], Train Loss: 249.0378, Test Loss: 70.4892\n",
      "Epoch [222/5000], Train Loss: 250.5463, Test Loss: 71.0895\n",
      "Epoch [223/5000], Train Loss: 255.8838, Test Loss: 69.6068\n",
      "Epoch [224/5000], Train Loss: 260.9639, Test Loss: 73.6726\n",
      "Epoch [225/5000], Train Loss: 247.3665, Test Loss: 73.8493\n",
      "Epoch [226/5000], Train Loss: 256.7469, Test Loss: 71.1360\n",
      "Epoch [227/5000], Train Loss: 246.9662, Test Loss: 74.5014\n",
      "Epoch [228/5000], Train Loss: 250.1146, Test Loss: 72.4708\n",
      "Epoch [229/5000], Train Loss: 242.2322, Test Loss: 70.0440\n",
      "Epoch [230/5000], Train Loss: 245.4064, Test Loss: 75.3457\n",
      "Epoch [231/5000], Train Loss: 263.8063, Test Loss: 67.8106\n",
      "Epoch [232/5000], Train Loss: 246.6080, Test Loss: 70.9444\n",
      "Epoch [233/5000], Train Loss: 246.0909, Test Loss: 68.4259\n",
      "Epoch [234/5000], Train Loss: 249.7495, Test Loss: 71.4103\n",
      "Epoch [235/5000], Train Loss: 253.4608, Test Loss: 74.3340\n",
      "Epoch [236/5000], Train Loss: 250.4034, Test Loss: 67.2123\n",
      "Epoch [237/5000], Train Loss: 246.7059, Test Loss: 69.4415\n",
      "Epoch [238/5000], Train Loss: 237.9328, Test Loss: 66.5182\n",
      "Epoch [239/5000], Train Loss: 251.5919, Test Loss: 71.7365\n",
      "Epoch [240/5000], Train Loss: 250.3445, Test Loss: 71.4877\n",
      "Epoch [241/5000], Train Loss: 251.9302, Test Loss: 71.4040\n",
      "Epoch [242/5000], Train Loss: 255.0238, Test Loss: 73.0922\n",
      "Epoch [243/5000], Train Loss: 249.8917, Test Loss: 69.9221\n",
      "Epoch [244/5000], Train Loss: 246.8168, Test Loss: 70.6341\n",
      "Epoch [245/5000], Train Loss: 248.8395, Test Loss: 71.5004\n",
      "Epoch [246/5000], Train Loss: 242.2301, Test Loss: 68.6846\n",
      "Epoch [247/5000], Train Loss: 239.0760, Test Loss: 72.4670\n",
      "Epoch [248/5000], Train Loss: 243.1089, Test Loss: 68.1202\n",
      "Epoch [249/5000], Train Loss: 242.7922, Test Loss: 67.4698\n",
      "Epoch [250/5000], Train Loss: 242.0920, Test Loss: 78.4369\n",
      "Epoch [251/5000], Train Loss: 256.6988, Test Loss: 74.7730\n",
      "Epoch [252/5000], Train Loss: 251.2428, Test Loss: 72.5793\n",
      "Epoch [253/5000], Train Loss: 249.1178, Test Loss: 71.3089\n",
      "Epoch [254/5000], Train Loss: 246.0290, Test Loss: 68.5076\n",
      "Epoch [255/5000], Train Loss: 247.1718, Test Loss: 67.6023\n",
      "Epoch [256/5000], Train Loss: 244.6389, Test Loss: 73.2503\n",
      "Epoch [257/5000], Train Loss: 256.6477, Test Loss: 69.7931\n",
      "Epoch [258/5000], Train Loss: 240.5566, Test Loss: 66.9366\n",
      "Epoch [259/5000], Train Loss: 245.3603, Test Loss: 69.4558\n",
      "Epoch [260/5000], Train Loss: 261.0434, Test Loss: 67.6690\n",
      "Epoch [261/5000], Train Loss: 240.2994, Test Loss: 65.8320\n",
      "Epoch [262/5000], Train Loss: 238.2056, Test Loss: 68.2689\n",
      "Epoch [263/5000], Train Loss: 242.2702, Test Loss: 70.2072\n",
      "Early stopping at epoch 263\n",
      "Execution time: 296.59153747558594 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.04146546963602"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (lin1): Linear(in_features=46000, out_features=200, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (lin2): Linear(in_features=200, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  86.23191\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "60.85  - Concentrations: 0.004999999888241291\n",
      "17.71  - Concentrations: 0.02500000037252903\n",
      "4.59  - Concentrations: 0.10000000149011612\n",
      "2.55  - Concentrations: 0.25\n",
      "1.83  - Concentrations: 0.5\n",
      "1.46  - Concentrations: 1.0\n",
      "1.31  - Concentrations: 2.5\n",
      "1.22  - Concentrations: 10.0\n",
      "1.21  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  7.8682184\n",
      "--------------------\n",
      "2.51  - Min Value: 0.6783  - Mean Value: 9.2\n",
      "54.48  - Min Value: 0.0096  - Mean Value: 10.3\n",
      "2.11  - Min Value: 0.147  - Mean Value: 10.5\n",
      "2.69  - Min Value: 0.5572  - Mean Value: 8.5\n",
      "2.16  - Min Value: 1.3567  - Mean Value: 10.6\n",
      "3.63  - Min Value: 0.6332  - Mean Value: 10.9\n",
      "2.89  - Min Value: 0.7017  - Mean Value: 11.0\n",
      "3.5  - Min Value: 0.3674  - Mean Value: 8.9\n",
      "2.09  - Min Value: 0.8387  - Mean Value: 9.8\n",
      "2.63  - Min Value: 1.0913  - Mean Value: 11.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  6.47771\n",
      "--------------------\n",
      "4.96  - Min Value: 0.0111  - Mean Value: 0.1\n",
      "6.23  - Min Value: 0.0103  - Mean Value: 0.1\n",
      "5.92  - Min Value: 0.0153  - Mean Value: 0.1\n",
      "7.46  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "7.19  - Min Value: 0.0089  - Mean Value: 0.1\n",
      "6.63  - Min Value: 0.0075  - Mean Value: 0.1\n",
      "6.46  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "6.69  - Min Value: 0.0052  - Mean Value: 0.1\n",
      "7.56  - Min Value: 0.008  - Mean Value: 0.1\n",
      "5.69  - Min Value: 0.0134  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  49.53138\n",
      "--------------------\n",
      "52.51  - Min Value: 0.008  - Mean Value: 0.8\n",
      "50.17  - Min Value: 0.009  - Mean Value: 0.9\n",
      "61.89  - Min Value: 0.0138  - Mean Value: 1.5\n",
      "52.02  - Min Value: 0.0107  - Mean Value: 0.7\n",
      "46.27  - Min Value: 0.0191  - Mean Value: 0.7\n",
      "51.98  - Min Value: 0.0186  - Mean Value: 0.8\n",
      "42.28  - Min Value: 0.0175  - Mean Value: 0.8\n",
      "48.3  - Min Value: 0.0238  - Mean Value: 1.3\n",
      "45.09  - Min Value: 0.0168  - Mean Value: 0.7\n",
      "44.8  - Min Value: 0.0171  - Mean Value: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  19.873737\n",
      "--------------------\n",
      "24.77  - Min Value: 0.013  - Mean Value: 0.6\n",
      "18.37  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "12.2  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "19.17  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "15.93  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "32.92  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "24.13  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "12.67  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "10.66  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "27.93  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  162.63586\n",
      "--------------------\n",
      "136.7  - Min Value: 0.013  - Mean Value: 0.6\n",
      "121.12  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "156.1  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "158.65  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "117.31  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "267.23  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "163.99  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "117.08  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "176.83  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "211.35  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  2.1080392447888974\n",
      "--------------------\n",
      "1.92\n",
      "1.92\n",
      "2.05\n",
      "2.01\n",
      "2.02\n",
      "1.92\n",
      "2.25\n",
      "2.51\n",
      "2.08\n",
      "2.39\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  13.580643125744754\n",
      "--------------------\n",
      "2.08\n",
      "2.06\n",
      "2.82\n",
      "5.24\n",
      "14.18\n",
      "15.01\n",
      "16.35\n",
      "22.45\n",
      "24.57\n",
      "31.04\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"Singlet_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"Singlet_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  26.976076840850318\n",
      "--------------------\n",
      "2.85\n",
      "7.91\n",
      "13.28\n",
      "18.58\n",
      "23.92\n",
      "29.54\n",
      "35.13\n",
      "40.67\n",
      "46.1\n",
      "51.76\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal Baseline 1\n",
      "tensor([0.5036, 0.5994, 0.3945, 0.6268, 0.5148, 0.5178, 0.4873, 0.6364, 0.7411,\n",
      "        0.3925, 0.4176, 0.6267, 0.5211, 0.9800, 0.5112, 0.6810, 0.7056, 0.4855,\n",
      "        0.5170, 0.4145, 0.4796, 0.8953, 0.2958, 0.4595, 0.6209, 0.4692, 0.6459,\n",
      "        0.5883, 0.6039, 0.4472, 0.4191, 0.8709, 0.4971, 0.3775, 0.3855, 0.5082,\n",
      "        0.4220, 0.4533, 0.4492, 0.4322, 0.3824, 0.5182, 0.5407, 0.4970],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Sinusoidal Baseline 2\n",
      "tensor([0.3366, 0.5847, 0.0000, 0.6392, 0.3271, 0.4811, 0.3918, 0.2184, 0.4235,\n",
      "        0.5458, 0.4205, 0.5897, 0.8206, 0.1795, 0.6910, 0.1103, 0.7599, 0.5742,\n",
      "        0.4738, 0.4382, 0.3868, 0.3272, 0.6284, 0.4670, 0.4357, 0.4262, 0.1496,\n",
      "        0.0967, 0.5967, 0.3568, 0.3810, 0.2548, 0.2803, 0.4891, 0.5432, 0.4442,\n",
      "        0.6066, 0.6891, 0.5532, 0.3481, 0.4069, 0.7714, 0.2321, 0.1299],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "HD-Range 1 - 0.01s and 20s\n",
      "tensor([ 0.5100, 20.8677,  0.6002, 20.3224,  0.4479, 20.7605,  1.0521, 21.0418,\n",
      "         0.1790, 20.6835,  0.7644, 20.7036,  0.4987, 20.6545,  0.0000, 20.8260,\n",
      "         0.8669, 20.9699,  0.6373, 20.2839,  0.9416, 20.4828,  0.1514, 20.1728,\n",
      "         0.2230, 20.4880,  0.4062, 20.6591,  0.5436, 20.7791,  0.4961, 20.9936,\n",
      "         0.6472, 20.4885,  0.9374, 21.2058,  0.2457, 20.8236,  0.5365, 20.3000,\n",
      "         0.8396, 20.5370,  0.0000, 21.2661], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "HD-Range 2 - 0s and 20s\n",
      "tensor([ 0.4986, 20.8659,  0.5926, 20.3264,  0.4416, 20.7590,  1.0426, 21.0417,\n",
      "         0.1726, 20.6827,  0.7559, 20.7044,  0.4911, 20.6529,  0.0000, 20.8261,\n",
      "         0.8566, 20.9703,  0.6280, 20.2857,  0.9283, 20.4859,  0.1448, 20.1726,\n",
      "         0.2156, 20.4882,  0.3938, 20.6631,  0.5327, 20.7792,  0.4851, 20.9946,\n",
      "         0.6354, 20.4887,  0.9284, 21.2014,  0.2362, 20.8265,  0.5285, 20.3038,\n",
      "         0.8289, 20.5370,  0.0000, 21.2647], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(OtherValSpectra[0])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 0.01s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
