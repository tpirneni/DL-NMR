{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 500\n",
    "\n",
    "# Identification part of the filenames\n",
    "model_base_name = '250000spec_8Metabolites_RAE_ExtendedRange_MoreLeftOut_Combined1Distribution_OptParams'\n",
    "base_name = '250000spec_ExtendedRange_MoreLeftOut_Combined1Distribution'    # This is the dataset base name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN on dataset of 8 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"CNN_44met_{model_base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra_filename = f'Dataset8_{base_name}_ForManuscript_Spec.dat'\n",
    "conc1_filename = f'Dataset8_{base_name}_ForManuscript_Conc.npy'\n",
    "\n",
    "spectra_shape = (249996, 46000)\n",
    "conc1_shape = (249996, 8)\n",
    "\n",
    "\n",
    "# Load the memmap arrays\n",
    "spectra_memmap = np.memmap(spectra_filename, dtype=np.float64, mode='r', shape=spectra_shape)\n",
    "conc1_memmap = np.load(conc1_filename)\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train_indices, X_test_indices, y_train_indices, y_test_indices = train_test_split(\n",
    "    np.arange(spectra_shape[0]), np.arange(conc1_shape[0]), test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# Create custom dataset class\n",
    "class NMRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, spectra_memmap, conc1_memmap, indices):\n",
    "        self.spectra_memmap = spectra_memmap\n",
    "        self.conc1_memmap = conc1_memmap\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        X = self.spectra_memmap[actual_idx]\n",
    "        y = self.conc1_memmap[actual_idx]\n",
    "        return torch.tensor(X).float().to(device), torch.tensor(y).float().to(device)\n",
    "    \n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NMRDataset(spectra_memmap, conc1_memmap, X_train_indices)\n",
    "test_dataset = NMRDataset(spectra_memmap, conc1_memmap, X_test_indices)\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16  \n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "## Best parameters determined in Optuna study\n",
    "#{'num_conv_layers': 3, \n",
    "# 'kernel_size': 10, \n",
    "# 'num_channels': 35, \n",
    "# 'pooling_type': 'none', \n",
    "# 'conv_stride': 4, \n",
    "# 'feedforward_size': 233, \n",
    "# 'learning_rate': 2.067749078763321e-05, \n",
    "# 'reg_strength': 0.004990448395463014, \n",
    "# 'bs': 16}\n",
    "\n",
    "\n",
    "# Define some model & training parameters\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NMR_Model_Aq, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 35, kernel_size=10, stride=4, padding=1)\n",
    "        self.conv2 = nn.Conv1d(35, 35, kernel_size=10, stride=4, padding=1)\n",
    "        self.conv3 = nn.Conv1d(35, 35, kernel_size=10, stride=4, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(25095, 233)\n",
    "        self.fc2 = nn.Linear(233, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)                  \n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f512ba29e90>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(spectra_memmap[200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = RelativeAbsoluteError()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = 2.067749078763321e-05, weight_decay=0.004990448395463014)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in validation loss constitutes early stopping\n",
    "    accumulation_steps = 4\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # For timing cell run time\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        ## Training phase\n",
    "        # Instantiate the GradScaler\n",
    "        scaler = GradScaler()\n",
    "        optimizer.zero_grad()  # Only zero gradients here at the start of an epoch\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Enable autocasting for forward and backward passes\n",
    "            with autocast():\n",
    "                outputs = model(inputs.unsqueeze(2))\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Scale the loss to account for the accumulation steps\n",
    "                loss = loss / accumulation_steps\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Scale the loss and perform backpropagation\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                # Step the optimizer and update the scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()  # Zero gradients after accumulation_steps\n",
    "\n",
    "        # Testing phase\n",
    "        train_losses.append(train_loss)\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                # Move data to GPU\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Enable autocasting for forward passes\n",
    "                with autocast():\n",
    "                    outputs = model(inputs.unsqueeze(2))\n",
    "                    loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "        \n",
    "        \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Epoch time: \",end-start)\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.AdamW(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/500], Train Loss: 22687.1910, Test Loss: 10369.5284\n",
      "Epoch time:  316.41800236701965\n",
      "Epoch [2/500], Train Loss: 6480.0790, Test Loss: 2657.8458\n",
      "Epoch time:  276.33479619026184\n",
      "Epoch [3/500], Train Loss: 1389.0022, Test Loss: 1025.6821\n",
      "Epoch time:  313.8678822517395\n",
      "Epoch [4/500], Train Loss: 1082.2447, Test Loss: 990.0864\n",
      "Epoch time:  273.1220865249634\n",
      "Epoch [5/500], Train Loss: 994.9839, Test Loss: 972.2735\n",
      "Epoch time:  286.30682921409607\n",
      "Epoch [6/500], Train Loss: 847.9388, Test Loss: 876.1393\n",
      "Epoch time:  294.9248561859131\n",
      "Epoch [7/500], Train Loss: 763.5150, Test Loss: 1126.1877\n",
      "Epoch time:  278.37423634529114\n",
      "Epoch [8/500], Train Loss: 731.4343, Test Loss: 864.4674\n",
      "Epoch time:  233.9039855003357\n",
      "Epoch [9/500], Train Loss: 670.8442, Test Loss: 627.9522\n",
      "Epoch time:  114.51777338981628\n",
      "Epoch [10/500], Train Loss: 620.5799, Test Loss: 564.0813\n",
      "Epoch time:  115.60127186775208\n",
      "Epoch [11/500], Train Loss: 595.1836, Test Loss: 625.0390\n",
      "Epoch time:  114.64877128601074\n",
      "Epoch [12/500], Train Loss: 559.4365, Test Loss: 623.2705\n",
      "Epoch time:  114.277503490448\n",
      "Epoch [13/500], Train Loss: 532.1727, Test Loss: 554.3852\n",
      "Epoch time:  114.50600552558899\n",
      "Epoch [14/500], Train Loss: 514.4420, Test Loss: 530.7334\n",
      "Epoch time:  114.72066164016724\n",
      "Epoch [15/500], Train Loss: 492.2104, Test Loss: 479.0124\n",
      "Epoch time:  114.74474239349365\n",
      "Epoch [16/500], Train Loss: 483.1165, Test Loss: 467.2027\n",
      "Epoch time:  115.26583361625671\n",
      "Epoch [17/500], Train Loss: 467.0173, Test Loss: 478.8087\n",
      "Epoch time:  113.35554194450378\n",
      "Epoch [18/500], Train Loss: 471.5753, Test Loss: 430.3520\n",
      "Epoch time:  113.6194224357605\n",
      "Epoch [19/500], Train Loss: 437.5642, Test Loss: 406.6414\n",
      "Epoch time:  114.00470995903015\n",
      "Epoch [20/500], Train Loss: 430.7225, Test Loss: 492.3627\n",
      "Epoch time:  113.35443997383118\n",
      "Epoch [21/500], Train Loss: 412.7602, Test Loss: 428.8469\n",
      "Epoch time:  113.4262101650238\n",
      "Epoch [22/500], Train Loss: 404.4211, Test Loss: 368.8880\n",
      "Epoch time:  113.83261847496033\n",
      "Epoch [23/500], Train Loss: 399.8485, Test Loss: 445.0539\n",
      "Epoch time:  113.47148489952087\n",
      "Epoch [24/500], Train Loss: 387.8898, Test Loss: 474.3699\n",
      "Epoch time:  113.50762367248535\n",
      "Epoch [25/500], Train Loss: 376.3444, Test Loss: 417.8534\n",
      "Epoch time:  113.52591395378113\n",
      "Epoch [26/500], Train Loss: 368.0603, Test Loss: 344.5017\n",
      "Epoch time:  114.06110143661499\n",
      "Epoch [27/500], Train Loss: 361.4426, Test Loss: 338.2620\n",
      "Epoch time:  114.00708270072937\n",
      "Epoch [28/500], Train Loss: 359.1675, Test Loss: 357.4805\n",
      "Epoch time:  113.60459637641907\n",
      "Epoch [29/500], Train Loss: 354.8646, Test Loss: 337.8327\n",
      "Epoch time:  114.13801193237305\n",
      "Epoch [30/500], Train Loss: 348.0704, Test Loss: 386.5104\n",
      "Epoch time:  113.76103949546814\n",
      "Epoch [31/500], Train Loss: 342.6862, Test Loss: 352.2707\n",
      "Epoch time:  113.68973016738892\n",
      "Epoch [32/500], Train Loss: 339.2083, Test Loss: 349.7342\n",
      "Epoch time:  113.48384070396423\n",
      "Epoch [33/500], Train Loss: 331.2879, Test Loss: 312.3719\n",
      "Epoch time:  113.68629479408264\n",
      "Epoch [34/500], Train Loss: 329.9137, Test Loss: 309.2438\n",
      "Epoch time:  113.75019478797913\n",
      "Epoch [35/500], Train Loss: 321.9596, Test Loss: 307.7642\n",
      "Epoch time:  115.82772636413574\n",
      "Epoch [36/500], Train Loss: 322.2446, Test Loss: 334.7224\n",
      "Epoch time:  114.85091352462769\n",
      "Epoch [37/500], Train Loss: 315.4810, Test Loss: 294.3671\n",
      "Epoch time:  114.97743654251099\n",
      "Epoch [38/500], Train Loss: 311.5627, Test Loss: 304.0322\n",
      "Epoch time:  114.56654119491577\n",
      "Epoch [39/500], Train Loss: 302.8765, Test Loss: 307.1661\n",
      "Epoch time:  114.49527907371521\n",
      "Epoch [40/500], Train Loss: 301.0505, Test Loss: 293.9112\n",
      "Epoch time:  114.86204433441162\n",
      "Epoch [41/500], Train Loss: 301.3564, Test Loss: 385.5712\n",
      "Epoch time:  114.39381718635559\n",
      "Epoch [42/500], Train Loss: 296.7204, Test Loss: 276.8929\n",
      "Epoch time:  114.63978552818298\n",
      "Epoch [43/500], Train Loss: 289.9443, Test Loss: 322.4032\n",
      "Epoch time:  114.52447080612183\n",
      "Epoch [44/500], Train Loss: 294.8672, Test Loss: 281.5831\n",
      "Epoch time:  114.33170557022095\n",
      "Epoch [45/500], Train Loss: 287.1404, Test Loss: 289.1632\n",
      "Epoch time:  114.27123808860779\n",
      "Epoch [46/500], Train Loss: 284.4388, Test Loss: 268.5364\n",
      "Epoch time:  114.77868437767029\n",
      "Epoch [47/500], Train Loss: 283.6859, Test Loss: 288.9142\n",
      "Epoch time:  114.36638236045837\n",
      "Epoch [48/500], Train Loss: 276.9286, Test Loss: 270.6329\n",
      "Epoch time:  114.22822570800781\n",
      "Epoch [49/500], Train Loss: 273.3670, Test Loss: 277.4706\n",
      "Epoch time:  114.12932872772217\n",
      "Epoch [50/500], Train Loss: 271.0226, Test Loss: 307.9516\n",
      "Epoch time:  114.20544052124023\n",
      "Epoch [51/500], Train Loss: 271.2054, Test Loss: 277.0070\n",
      "Epoch time:  114.27905058860779\n",
      "Epoch [52/500], Train Loss: 266.9172, Test Loss: 261.5361\n",
      "Epoch time:  114.83303380012512\n",
      "Epoch [53/500], Train Loss: 263.0686, Test Loss: 265.2617\n",
      "Epoch time:  114.16846561431885\n",
      "Epoch [54/500], Train Loss: 259.5035, Test Loss: 253.1116\n",
      "Epoch time:  114.69555258750916\n",
      "Epoch [55/500], Train Loss: 258.6007, Test Loss: 261.6027\n",
      "Epoch time:  114.40436792373657\n",
      "Epoch [56/500], Train Loss: 252.7483, Test Loss: 232.4427\n",
      "Epoch time:  114.86571717262268\n",
      "Epoch [57/500], Train Loss: 253.6050, Test Loss: 241.7422\n",
      "Epoch time:  114.52994275093079\n",
      "Epoch [58/500], Train Loss: 255.9997, Test Loss: 233.5183\n",
      "Epoch time:  114.42565608024597\n",
      "Epoch [59/500], Train Loss: 253.9557, Test Loss: 249.5033\n",
      "Epoch time:  114.45755338668823\n",
      "Epoch [60/500], Train Loss: 253.7010, Test Loss: 249.5611\n",
      "Epoch time:  114.44226479530334\n",
      "Epoch [61/500], Train Loss: 247.5728, Test Loss: 241.3217\n",
      "Epoch time:  114.1808979511261\n",
      "Epoch [62/500], Train Loss: 244.7821, Test Loss: 238.6462\n",
      "Epoch time:  114.20531034469604\n",
      "Epoch [63/500], Train Loss: 242.6360, Test Loss: 285.1299\n",
      "Epoch time:  114.17741131782532\n",
      "Epoch [64/500], Train Loss: 241.0171, Test Loss: 263.7799\n",
      "Epoch time:  114.19961762428284\n",
      "Epoch [65/500], Train Loss: 240.1884, Test Loss: 241.8981\n",
      "Epoch time:  114.2127034664154\n",
      "Epoch [66/500], Train Loss: 238.9621, Test Loss: 237.6563\n",
      "Epoch time:  117.18951320648193\n",
      "Epoch [67/500], Train Loss: 239.3703, Test Loss: 246.5501\n",
      "Epoch time:  125.80612778663635\n",
      "Epoch [68/500], Train Loss: 237.1929, Test Loss: 246.8812\n",
      "Epoch time:  128.6272633075714\n",
      "Epoch [69/500], Train Loss: 235.5431, Test Loss: 230.2919\n",
      "Epoch time:  129.01210713386536\n",
      "Epoch [70/500], Train Loss: 233.5152, Test Loss: 232.4690\n",
      "Epoch time:  114.19617104530334\n",
      "Epoch [71/500], Train Loss: 229.5993, Test Loss: 231.8828\n",
      "Epoch time:  114.23272323608398\n",
      "Epoch [72/500], Train Loss: 230.5548, Test Loss: 236.1677\n",
      "Epoch time:  114.29923605918884\n",
      "Epoch [73/500], Train Loss: 229.4059, Test Loss: 231.5903\n",
      "Epoch time:  118.75073647499084\n",
      "Epoch [74/500], Train Loss: 227.7695, Test Loss: 214.6372\n",
      "Epoch time:  129.0097415447235\n",
      "Epoch [75/500], Train Loss: 227.2271, Test Loss: 258.4002\n",
      "Epoch time:  114.47829675674438\n",
      "Epoch [76/500], Train Loss: 229.3482, Test Loss: 230.1778\n",
      "Epoch time:  114.29104804992676\n",
      "Epoch [77/500], Train Loss: 226.9696, Test Loss: 215.1193\n",
      "Epoch time:  114.55390524864197\n",
      "Epoch [78/500], Train Loss: 221.9780, Test Loss: 223.8576\n",
      "Epoch time:  114.59326505661011\n",
      "Epoch [79/500], Train Loss: 226.0711, Test Loss: 212.5913\n",
      "Epoch time:  115.04868054389954\n",
      "Epoch [80/500], Train Loss: 222.9990, Test Loss: 227.6378\n",
      "Epoch time:  114.82741451263428\n",
      "Epoch [81/500], Train Loss: 222.4241, Test Loss: 220.4243\n",
      "Epoch time:  114.84101581573486\n",
      "Epoch [82/500], Train Loss: 222.7819, Test Loss: 227.7626\n",
      "Epoch time:  114.78277683258057\n",
      "Epoch [83/500], Train Loss: 222.5341, Test Loss: 271.4634\n",
      "Epoch time:  114.84074378013611\n",
      "Epoch [84/500], Train Loss: 217.5324, Test Loss: 228.7042\n",
      "Epoch time:  114.96619963645935\n",
      "Epoch [85/500], Train Loss: 218.5868, Test Loss: 225.7755\n",
      "Epoch time:  114.762610912323\n",
      "Epoch [86/500], Train Loss: 218.4146, Test Loss: 219.6916\n",
      "Epoch time:  114.73102927207947\n",
      "Epoch [87/500], Train Loss: 215.8829, Test Loss: 231.1585\n",
      "Epoch time:  114.71163940429688\n",
      "Epoch [88/500], Train Loss: 216.3332, Test Loss: 226.2866\n",
      "Epoch time:  114.70913934707642\n",
      "Epoch [89/500], Train Loss: 212.0860, Test Loss: 217.7978\n",
      "Epoch time:  114.82574796676636\n",
      "Epoch [90/500], Train Loss: 211.6470, Test Loss: 198.7553\n",
      "Epoch time:  115.27178311347961\n",
      "Epoch [91/500], Train Loss: 215.1158, Test Loss: 202.6247\n",
      "Epoch time:  114.94009566307068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/500], Train Loss: 205.9428, Test Loss: 198.9275\n",
      "Epoch time:  114.74519896507263\n",
      "Epoch [93/500], Train Loss: 209.9261, Test Loss: 212.9022\n",
      "Epoch time:  114.63426303863525\n",
      "Epoch [94/500], Train Loss: 211.5050, Test Loss: 246.5413\n",
      "Epoch time:  114.64651560783386\n",
      "Epoch [95/500], Train Loss: 211.5613, Test Loss: 225.3989\n",
      "Epoch time:  114.76028490066528\n",
      "Epoch [96/500], Train Loss: 205.9596, Test Loss: 210.0422\n",
      "Epoch time:  114.7157130241394\n",
      "Epoch [97/500], Train Loss: 208.0499, Test Loss: 201.2425\n",
      "Epoch time:  114.73249316215515\n",
      "Epoch [98/500], Train Loss: 204.7073, Test Loss: 226.5880\n",
      "Epoch time:  123.2004325389862\n",
      "Epoch [99/500], Train Loss: 206.8458, Test Loss: 230.1486\n",
      "Epoch time:  117.14843082427979\n",
      "Epoch [100/500], Train Loss: 206.1554, Test Loss: 197.7500\n",
      "Epoch time:  115.22432231903076\n",
      "Epoch [101/500], Train Loss: 206.7166, Test Loss: 242.2177\n",
      "Epoch time:  114.71469497680664\n",
      "Epoch [102/500], Train Loss: 204.5056, Test Loss: 208.3646\n",
      "Epoch time:  114.80062747001648\n",
      "Epoch [103/500], Train Loss: 208.1585, Test Loss: 206.4553\n",
      "Epoch time:  114.861576795578\n",
      "Epoch [104/500], Train Loss: 203.3810, Test Loss: 234.8111\n",
      "Epoch time:  114.69075965881348\n",
      "Epoch [105/500], Train Loss: 203.8681, Test Loss: 202.9945\n",
      "Epoch time:  114.59272933006287\n",
      "Epoch [106/500], Train Loss: 201.5990, Test Loss: 202.9089\n",
      "Epoch time:  114.6744954586029\n",
      "Epoch [107/500], Train Loss: 201.8959, Test Loss: 186.4731\n",
      "Epoch time:  115.28918385505676\n",
      "Epoch [108/500], Train Loss: 195.5351, Test Loss: 186.0729\n",
      "Epoch time:  114.93248128890991\n",
      "Epoch [109/500], Train Loss: 201.2988, Test Loss: 190.5215\n",
      "Epoch time:  114.73365998268127\n",
      "Epoch [110/500], Train Loss: 200.7611, Test Loss: 194.1045\n",
      "Epoch time:  114.61170101165771\n",
      "Epoch [111/500], Train Loss: 197.1967, Test Loss: 188.6801\n",
      "Epoch time:  114.73496770858765\n",
      "Epoch [112/500], Train Loss: 198.6757, Test Loss: 186.2908\n",
      "Epoch time:  114.69127869606018\n",
      "Epoch [113/500], Train Loss: 196.1390, Test Loss: 182.3704\n",
      "Epoch time:  115.0460422039032\n",
      "Epoch [114/500], Train Loss: 198.4099, Test Loss: 190.4608\n",
      "Epoch time:  114.82027578353882\n",
      "Epoch [115/500], Train Loss: 197.0051, Test Loss: 188.4021\n",
      "Epoch time:  114.51685070991516\n",
      "Epoch [116/500], Train Loss: 199.0504, Test Loss: 190.7464\n",
      "Epoch time:  114.33657908439636\n",
      "Epoch [117/500], Train Loss: 192.5473, Test Loss: 186.3308\n",
      "Epoch time:  114.24334907531738\n",
      "Epoch [118/500], Train Loss: 196.8282, Test Loss: 206.2867\n",
      "Epoch time:  114.51647067070007\n",
      "Epoch [119/500], Train Loss: 192.3917, Test Loss: 188.2170\n",
      "Epoch time:  114.55283117294312\n",
      "Epoch [120/500], Train Loss: 196.9340, Test Loss: 185.2466\n",
      "Epoch time:  114.3026340007782\n",
      "Epoch [121/500], Train Loss: 193.1887, Test Loss: 198.2030\n",
      "Epoch time:  114.30762767791748\n",
      "Epoch [122/500], Train Loss: 193.4050, Test Loss: 182.7153\n",
      "Epoch time:  114.21953701972961\n",
      "Epoch [123/500], Train Loss: 192.9398, Test Loss: 214.9433\n",
      "Epoch time:  114.25692439079285\n",
      "Epoch [124/500], Train Loss: 190.4870, Test Loss: 193.9061\n",
      "Epoch time:  114.2063980102539\n",
      "Epoch [125/500], Train Loss: 195.7765, Test Loss: 199.3562\n",
      "Epoch time:  114.16865396499634\n",
      "Epoch [126/500], Train Loss: 191.8485, Test Loss: 205.3031\n",
      "Epoch time:  114.3108561038971\n",
      "Epoch [127/500], Train Loss: 188.0492, Test Loss: 205.7253\n",
      "Epoch time:  114.14477252960205\n",
      "Epoch [128/500], Train Loss: 189.3223, Test Loss: 219.7082\n",
      "Epoch time:  114.3000841140747\n",
      "Epoch [129/500], Train Loss: 187.1327, Test Loss: 181.9898\n",
      "Epoch time:  114.495534658432\n",
      "Epoch [130/500], Train Loss: 189.0059, Test Loss: 182.7542\n",
      "Epoch time:  114.40068364143372\n",
      "Epoch [131/500], Train Loss: 187.3716, Test Loss: 185.7648\n",
      "Epoch time:  114.46912956237793\n",
      "Epoch [132/500], Train Loss: 186.8325, Test Loss: 183.9877\n",
      "Epoch time:  114.65348982810974\n",
      "Epoch [133/500], Train Loss: 186.7619, Test Loss: 184.9098\n",
      "Epoch time:  118.32109999656677\n",
      "Epoch [134/500], Train Loss: 187.1900, Test Loss: 228.5823\n",
      "Epoch time:  115.60435819625854\n",
      "Epoch [135/500], Train Loss: 185.8583, Test Loss: 182.0336\n",
      "Epoch time:  200.88646507263184\n",
      "Epoch [136/500], Train Loss: 185.5543, Test Loss: 181.8019\n",
      "Epoch time:  225.09077262878418\n",
      "Epoch [137/500], Train Loss: 183.2209, Test Loss: 176.3558\n",
      "Epoch time:  282.0344455242157\n",
      "Epoch [138/500], Train Loss: 185.5841, Test Loss: 184.2340\n",
      "Epoch time:  244.7838580608368\n",
      "Epoch [139/500], Train Loss: 186.3595, Test Loss: 171.8179\n",
      "Epoch time:  256.3264718055725\n",
      "Epoch [140/500], Train Loss: 183.5022, Test Loss: 198.0766\n",
      "Epoch time:  295.5519962310791\n",
      "Epoch [141/500], Train Loss: 180.9607, Test Loss: 196.0880\n",
      "Epoch time:  411.36891198158264\n",
      "Epoch [142/500], Train Loss: 185.0653, Test Loss: 168.0490\n",
      "Epoch time:  370.8429102897644\n",
      "Epoch [143/500], Train Loss: 180.7181, Test Loss: 183.5804\n",
      "Epoch time:  377.2366256713867\n",
      "Epoch [144/500], Train Loss: 182.1675, Test Loss: 179.9272\n",
      "Epoch time:  389.2810137271881\n",
      "Epoch [145/500], Train Loss: 179.1139, Test Loss: 178.3104\n",
      "Epoch time:  352.3308072090149\n",
      "Epoch [146/500], Train Loss: 183.0667, Test Loss: 191.7838\n",
      "Epoch time:  346.8380615711212\n",
      "Epoch [147/500], Train Loss: 181.2235, Test Loss: 192.7016\n",
      "Epoch time:  340.0022156238556\n",
      "Epoch [148/500], Train Loss: 179.5545, Test Loss: 196.1550\n",
      "Epoch time:  369.5063192844391\n",
      "Epoch [149/500], Train Loss: 182.3632, Test Loss: 178.6985\n",
      "Epoch time:  403.75801157951355\n",
      "Epoch [150/500], Train Loss: 179.0939, Test Loss: 183.6863\n",
      "Epoch time:  400.9309575557709\n",
      "Epoch [151/500], Train Loss: 179.5335, Test Loss: 180.5116\n",
      "Epoch time:  365.7430467605591\n",
      "Epoch [152/500], Train Loss: 178.5272, Test Loss: 187.9078\n",
      "Epoch time:  354.2756953239441\n",
      "Epoch [153/500], Train Loss: 178.4073, Test Loss: 195.0273\n",
      "Epoch time:  374.94693779945374\n",
      "Epoch [154/500], Train Loss: 179.2244, Test Loss: 174.8367\n",
      "Epoch time:  391.4575252532959\n",
      "Epoch [155/500], Train Loss: 178.2313, Test Loss: 182.2301\n",
      "Epoch time:  381.8473656177521\n",
      "Epoch [156/500], Train Loss: 175.1955, Test Loss: 170.0237\n",
      "Epoch time:  360.2877449989319\n",
      "Epoch [157/500], Train Loss: 178.6314, Test Loss: 179.2771\n",
      "Epoch time:  385.54326462745667\n",
      "Epoch [158/500], Train Loss: 176.5839, Test Loss: 179.8045\n",
      "Epoch time:  414.83875465393066\n",
      "Epoch [159/500], Train Loss: 175.6508, Test Loss: 165.1186\n",
      "Epoch time:  354.52304577827454\n",
      "Epoch [160/500], Train Loss: 179.0441, Test Loss: 174.5959\n",
      "Epoch time:  395.33668065071106\n",
      "Epoch [161/500], Train Loss: 174.3363, Test Loss: 187.4167\n",
      "Epoch time:  402.320029258728\n",
      "Epoch [162/500], Train Loss: 176.0803, Test Loss: 188.2839\n",
      "Epoch time:  413.9625771045685\n",
      "Epoch [163/500], Train Loss: 172.1087, Test Loss: 196.7618\n",
      "Epoch time:  359.9886865615845\n",
      "Epoch [164/500], Train Loss: 175.4173, Test Loss: 172.3220\n",
      "Epoch time:  389.40733575820923\n",
      "Epoch [165/500], Train Loss: 173.8213, Test Loss: 173.8616\n",
      "Epoch time:  356.40709137916565\n",
      "Epoch [166/500], Train Loss: 172.4938, Test Loss: 171.6881\n",
      "Epoch time:  357.0900056362152\n",
      "Epoch [167/500], Train Loss: 170.9056, Test Loss: 160.6437\n",
      "Epoch time:  346.096150636673\n",
      "Epoch [168/500], Train Loss: 172.4479, Test Loss: 166.5506\n",
      "Epoch time:  319.52338099479675\n",
      "Epoch [169/500], Train Loss: 172.7051, Test Loss: 161.9717\n",
      "Epoch time:  368.2086982727051\n",
      "Epoch [170/500], Train Loss: 172.5836, Test Loss: 199.4816\n",
      "Epoch time:  367.1592848300934\n",
      "Epoch [171/500], Train Loss: 171.1211, Test Loss: 167.6883\n",
      "Epoch time:  351.06820821762085\n",
      "Epoch [172/500], Train Loss: 170.6386, Test Loss: 182.1053\n",
      "Epoch time:  354.65425848960876\n",
      "Epoch [173/500], Train Loss: 170.5101, Test Loss: 173.2884\n",
      "Epoch time:  385.14163732528687\n",
      "Epoch [174/500], Train Loss: 171.3066, Test Loss: 206.2157\n",
      "Epoch time:  402.2110056877136\n",
      "Epoch [175/500], Train Loss: 172.5643, Test Loss: 161.0590\n",
      "Epoch time:  374.95272755622864\n",
      "Epoch [176/500], Train Loss: 171.7506, Test Loss: 164.6846\n",
      "Epoch time:  379.49620246887207\n",
      "Epoch [177/500], Train Loss: 169.8529, Test Loss: 172.4668\n",
      "Epoch time:  390.5805826187134\n",
      "Epoch [178/500], Train Loss: 172.1944, Test Loss: 210.3948\n",
      "Epoch time:  402.6013705730438\n",
      "Epoch [179/500], Train Loss: 168.8515, Test Loss: 159.8621\n",
      "Epoch time:  387.62377190589905\n",
      "Epoch [180/500], Train Loss: 169.0851, Test Loss: 180.0460\n",
      "Epoch time:  362.0380868911743\n",
      "Epoch [181/500], Train Loss: 176.6265, Test Loss: 174.5054\n",
      "Epoch time:  380.9422709941864\n",
      "Epoch [182/500], Train Loss: 166.0336, Test Loss: 165.1307\n",
      "Epoch time:  389.57071566581726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [183/500], Train Loss: 165.3627, Test Loss: 167.6069\n",
      "Epoch time:  341.7514977455139\n",
      "Epoch [184/500], Train Loss: 166.4148, Test Loss: 170.3659\n",
      "Epoch time:  367.5832118988037\n",
      "Epoch [185/500], Train Loss: 169.6761, Test Loss: 180.5981\n",
      "Epoch time:  388.9622104167938\n",
      "Epoch [186/500], Train Loss: 168.4802, Test Loss: 181.1735\n",
      "Epoch time:  385.8068473339081\n",
      "Epoch [187/500], Train Loss: 165.4792, Test Loss: 159.4353\n",
      "Epoch time:  380.4220254421234\n",
      "Epoch [188/500], Train Loss: 167.5605, Test Loss: 161.9843\n",
      "Epoch time:  403.3245368003845\n",
      "Epoch [189/500], Train Loss: 167.9769, Test Loss: 163.2777\n",
      "Epoch time:  384.6775019168854\n",
      "Epoch [190/500], Train Loss: 166.0382, Test Loss: 160.5259\n",
      "Epoch time:  390.6225574016571\n",
      "Epoch [191/500], Train Loss: 171.0516, Test Loss: 158.7195\n",
      "Epoch time:  388.5043625831604\n",
      "Epoch [192/500], Train Loss: 166.1921, Test Loss: 173.9340\n",
      "Epoch time:  383.7001099586487\n",
      "Epoch [193/500], Train Loss: 166.3260, Test Loss: 154.6751\n",
      "Epoch time:  434.7759301662445\n",
      "Epoch [194/500], Train Loss: 165.2932, Test Loss: 173.1761\n",
      "Epoch time:  368.8121511936188\n",
      "Epoch [195/500], Train Loss: 165.8426, Test Loss: 176.5302\n",
      "Epoch time:  366.7778990268707\n",
      "Epoch [196/500], Train Loss: 164.5556, Test Loss: 161.2359\n",
      "Epoch time:  399.296156167984\n",
      "Epoch [197/500], Train Loss: 165.0266, Test Loss: 155.3564\n",
      "Epoch time:  369.6552152633667\n",
      "Epoch [198/500], Train Loss: 163.1314, Test Loss: 185.8311\n",
      "Epoch time:  384.64339089393616\n",
      "Epoch [199/500], Train Loss: 163.5257, Test Loss: 181.0665\n",
      "Epoch time:  352.73405146598816\n",
      "Epoch [200/500], Train Loss: 162.7012, Test Loss: 168.9770\n",
      "Epoch time:  397.4727056026459\n",
      "Epoch [201/500], Train Loss: 164.5423, Test Loss: 166.8470\n",
      "Epoch time:  388.25879526138306\n",
      "Epoch [202/500], Train Loss: 164.9080, Test Loss: 177.7991\n",
      "Epoch time:  377.7676420211792\n",
      "Epoch [203/500], Train Loss: 161.4875, Test Loss: 159.8677\n",
      "Epoch time:  335.9838306903839\n",
      "Epoch [204/500], Train Loss: 163.5205, Test Loss: 169.0605\n",
      "Epoch time:  348.48636054992676\n",
      "Epoch [205/500], Train Loss: 163.0722, Test Loss: 157.8238\n",
      "Epoch time:  390.2330605983734\n",
      "Epoch [206/500], Train Loss: 162.8629, Test Loss: 155.4473\n",
      "Epoch time:  371.23914980888367\n",
      "Epoch [207/500], Train Loss: 160.8258, Test Loss: 163.2635\n",
      "Epoch time:  365.48280692100525\n",
      "Epoch [208/500], Train Loss: 161.9386, Test Loss: 174.6307\n",
      "Epoch time:  367.4952154159546\n",
      "Epoch [209/500], Train Loss: 160.4651, Test Loss: 185.3881\n",
      "Epoch time:  402.093754529953\n",
      "Epoch [210/500], Train Loss: 169.7300, Test Loss: 182.0361\n",
      "Epoch time:  395.07309341430664\n",
      "Epoch [211/500], Train Loss: 159.9753, Test Loss: 151.6341\n",
      "Epoch time:  429.8286271095276\n",
      "Epoch [212/500], Train Loss: 160.3880, Test Loss: 175.2297\n",
      "Epoch time:  363.9524190425873\n",
      "Epoch [213/500], Train Loss: 163.2431, Test Loss: 152.2639\n",
      "Epoch time:  391.97559809684753\n",
      "Epoch [214/500], Train Loss: 159.5136, Test Loss: 156.1946\n",
      "Epoch time:  361.4309084415436\n",
      "Epoch [215/500], Train Loss: 163.3239, Test Loss: 239.8681\n",
      "Epoch time:  392.3497381210327\n",
      "Epoch [216/500], Train Loss: 162.3286, Test Loss: 169.8805\n",
      "Epoch time:  392.3670976161957\n",
      "Epoch [217/500], Train Loss: 158.2484, Test Loss: 165.9739\n",
      "Epoch time:  360.73658990859985\n",
      "Epoch [218/500], Train Loss: 159.3162, Test Loss: 152.7217\n",
      "Epoch time:  407.34483003616333\n",
      "Epoch [219/500], Train Loss: 160.1601, Test Loss: 162.2940\n",
      "Epoch time:  352.48306465148926\n",
      "Epoch [220/500], Train Loss: 158.7371, Test Loss: 172.2525\n",
      "Epoch time:  356.83602476119995\n",
      "Epoch [221/500], Train Loss: 160.1560, Test Loss: 158.6463\n",
      "Epoch time:  383.4131257534027\n",
      "Epoch [222/500], Train Loss: 158.7570, Test Loss: 164.5207\n",
      "Epoch time:  416.86458945274353\n",
      "Epoch [223/500], Train Loss: 159.4365, Test Loss: 151.2207\n",
      "Epoch time:  392.50235509872437\n",
      "Epoch [224/500], Train Loss: 159.4829, Test Loss: 154.0235\n",
      "Epoch time:  378.1768867969513\n",
      "Epoch [225/500], Train Loss: 155.5868, Test Loss: 160.7775\n",
      "Epoch time:  423.11202812194824\n",
      "Epoch [226/500], Train Loss: 157.3962, Test Loss: 171.9965\n",
      "Epoch time:  375.09320402145386\n",
      "Epoch [227/500], Train Loss: 159.8309, Test Loss: 146.3381\n",
      "Epoch time:  337.6472578048706\n",
      "Epoch [228/500], Train Loss: 156.3613, Test Loss: 159.4655\n",
      "Epoch time:  350.48519349098206\n",
      "Epoch [229/500], Train Loss: 155.6290, Test Loss: 184.0790\n",
      "Epoch time:  379.08754420280457\n",
      "Epoch [230/500], Train Loss: 156.8020, Test Loss: 218.7300\n",
      "Epoch time:  397.34527611732483\n",
      "Epoch [231/500], Train Loss: 158.4741, Test Loss: 157.1970\n",
      "Epoch time:  387.51845812797546\n",
      "Epoch [232/500], Train Loss: 157.6075, Test Loss: 150.9165\n",
      "Epoch time:  342.64073944091797\n",
      "Epoch [233/500], Train Loss: 155.4253, Test Loss: 184.3976\n",
      "Epoch time:  348.0644974708557\n",
      "Epoch [234/500], Train Loss: 154.2895, Test Loss: 149.2574\n",
      "Epoch time:  369.5514748096466\n",
      "Epoch [235/500], Train Loss: 157.2735, Test Loss: 169.6053\n",
      "Epoch time:  398.28038573265076\n",
      "Epoch [236/500], Train Loss: 154.7883, Test Loss: 164.8371\n",
      "Epoch time:  362.1895046234131\n",
      "Epoch [237/500], Train Loss: 154.3356, Test Loss: 173.6610\n",
      "Epoch time:  386.4192039966583\n",
      "Epoch [238/500], Train Loss: 155.7859, Test Loss: 153.8220\n",
      "Epoch time:  367.3153133392334\n",
      "Epoch [239/500], Train Loss: 155.1063, Test Loss: 151.0558\n",
      "Epoch time:  364.0911362171173\n",
      "Epoch [240/500], Train Loss: 155.3451, Test Loss: 161.0295\n",
      "Epoch time:  388.3618404865265\n",
      "Epoch [241/500], Train Loss: 154.5169, Test Loss: 149.0298\n",
      "Epoch time:  399.52165937423706\n",
      "Epoch [242/500], Train Loss: 155.4764, Test Loss: 144.6888\n",
      "Epoch time:  383.61874437332153\n",
      "Epoch [243/500], Train Loss: 155.0979, Test Loss: 148.3496\n",
      "Epoch time:  400.7007281780243\n",
      "Epoch [244/500], Train Loss: 152.1909, Test Loss: 149.9085\n",
      "Epoch time:  398.46589636802673\n",
      "Epoch [245/500], Train Loss: 155.6772, Test Loss: 151.8867\n",
      "Epoch time:  411.9173345565796\n",
      "Epoch [246/500], Train Loss: 152.4259, Test Loss: 168.1877\n",
      "Epoch time:  412.9482433795929\n",
      "Epoch [247/500], Train Loss: 153.0014, Test Loss: 154.5746\n",
      "Epoch time:  412.28042125701904\n",
      "Epoch [248/500], Train Loss: 153.9799, Test Loss: 155.5969\n",
      "Epoch time:  395.1729803085327\n",
      "Epoch [249/500], Train Loss: 151.7972, Test Loss: 142.7222\n",
      "Epoch time:  387.68375301361084\n",
      "Epoch [250/500], Train Loss: 155.5872, Test Loss: 156.2121\n",
      "Epoch time:  412.60075664520264\n",
      "Epoch [251/500], Train Loss: 152.6860, Test Loss: 141.7218\n",
      "Epoch time:  424.01839303970337\n",
      "Epoch [252/500], Train Loss: 151.1108, Test Loss: 144.6469\n",
      "Epoch time:  405.3379635810852\n",
      "Epoch [253/500], Train Loss: 152.4715, Test Loss: 151.2414\n",
      "Epoch time:  377.18724966049194\n",
      "Epoch [254/500], Train Loss: 153.5064, Test Loss: 159.7169\n",
      "Epoch time:  364.29639410972595\n",
      "Epoch [255/500], Train Loss: 150.9525, Test Loss: 184.5444\n",
      "Epoch time:  358.9887418746948\n",
      "Epoch [256/500], Train Loss: 151.5402, Test Loss: 146.6658\n",
      "Epoch time:  382.0281171798706\n",
      "Epoch [257/500], Train Loss: 149.8275, Test Loss: 140.7452\n",
      "Epoch time:  356.92173051834106\n",
      "Epoch [258/500], Train Loss: 154.5927, Test Loss: 151.8927\n",
      "Epoch time:  361.38300347328186\n",
      "Epoch [259/500], Train Loss: 152.7798, Test Loss: 145.4218\n",
      "Epoch time:  375.1119792461395\n",
      "Epoch [260/500], Train Loss: 151.1081, Test Loss: 165.0787\n",
      "Epoch time:  395.6045687198639\n",
      "Epoch [261/500], Train Loss: 154.1582, Test Loss: 142.7243\n",
      "Epoch time:  360.2069969177246\n",
      "Epoch [262/500], Train Loss: 149.7912, Test Loss: 192.2714\n",
      "Epoch time:  360.9788658618927\n",
      "Epoch [263/500], Train Loss: 149.5722, Test Loss: 153.0994\n",
      "Epoch time:  371.3647050857544\n",
      "Epoch [264/500], Train Loss: 157.1893, Test Loss: 152.6778\n",
      "Epoch time:  397.92222905158997\n",
      "Epoch [265/500], Train Loss: 148.8954, Test Loss: 148.9828\n",
      "Epoch time:  412.24610114097595\n",
      "Epoch [266/500], Train Loss: 148.9808, Test Loss: 158.3709\n",
      "Epoch time:  365.3825297355652\n",
      "Epoch [267/500], Train Loss: 149.2057, Test Loss: 166.2588\n",
      "Epoch time:  387.1754734516144\n",
      "Epoch [268/500], Train Loss: 149.9957, Test Loss: 156.0232\n",
      "Epoch time:  380.9549357891083\n",
      "Epoch [269/500], Train Loss: 149.4905, Test Loss: 138.9447\n",
      "Epoch time:  379.77891635894775\n",
      "Epoch [270/500], Train Loss: 148.8245, Test Loss: 161.1421\n",
      "Epoch time:  371.9767563343048\n",
      "Epoch [271/500], Train Loss: 152.3360, Test Loss: 145.4719\n",
      "Epoch time:  374.20652508735657\n",
      "Epoch [272/500], Train Loss: 147.1333, Test Loss: 153.8246\n",
      "Epoch time:  508.6960606575012\n",
      "Epoch [273/500], Train Loss: 150.8152, Test Loss: 160.6700\n",
      "Epoch time:  1063.6921544075012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [274/500], Train Loss: 148.9895, Test Loss: 140.6965\n",
      "Epoch time:  817.6061763763428\n",
      "Epoch [275/500], Train Loss: 150.9448, Test Loss: 170.2856\n",
      "Epoch time:  823.1899199485779\n",
      "Epoch [276/500], Train Loss: 146.4193, Test Loss: 149.7584\n",
      "Epoch time:  772.0420145988464\n",
      "Epoch [277/500], Train Loss: 148.5678, Test Loss: 146.2019\n",
      "Epoch time:  741.3812208175659\n",
      "Epoch [278/500], Train Loss: 148.7175, Test Loss: 139.2811\n",
      "Epoch time:  769.9577763080597\n",
      "Epoch [279/500], Train Loss: 147.2816, Test Loss: 155.1349\n",
      "Epoch time:  789.9351093769073\n",
      "Epoch [280/500], Train Loss: 147.3973, Test Loss: 136.1744\n",
      "Epoch time:  729.5194661617279\n",
      "Epoch [281/500], Train Loss: 144.6550, Test Loss: 149.0621\n",
      "Epoch time:  680.5453956127167\n",
      "Epoch [282/500], Train Loss: 146.9371, Test Loss: 165.6005\n",
      "Epoch time:  667.7661581039429\n",
      "Epoch [283/500], Train Loss: 146.2896, Test Loss: 167.1103\n",
      "Epoch time:  671.915956735611\n",
      "Epoch [284/500], Train Loss: 146.5905, Test Loss: 133.6830\n",
      "Epoch time:  665.7099876403809\n",
      "Epoch [285/500], Train Loss: 144.7625, Test Loss: 151.1501\n",
      "Epoch time:  692.2875738143921\n",
      "Epoch [286/500], Train Loss: 146.2719, Test Loss: 146.0989\n",
      "Epoch time:  671.5494966506958\n",
      "Epoch [287/500], Train Loss: 143.9545, Test Loss: 145.6082\n",
      "Epoch time:  987.7819483280182\n",
      "Epoch [288/500], Train Loss: 144.3173, Test Loss: 148.2282\n",
      "Epoch time:  1057.373948097229\n",
      "Epoch [289/500], Train Loss: 147.0708, Test Loss: 148.8014\n",
      "Epoch time:  1018.3658814430237\n",
      "Epoch [290/500], Train Loss: 142.7126, Test Loss: 134.0701\n",
      "Epoch time:  965.4620213508606\n",
      "Epoch [291/500], Train Loss: 145.5777, Test Loss: 138.0546\n",
      "Epoch time:  1050.5581159591675\n",
      "Epoch [292/500], Train Loss: 143.4064, Test Loss: 136.7147\n",
      "Epoch time:  931.7594633102417\n",
      "Epoch [293/500], Train Loss: 145.0573, Test Loss: 140.4207\n",
      "Epoch time:  962.8165023326874\n",
      "Epoch [294/500], Train Loss: 144.5820, Test Loss: 153.6989\n",
      "Epoch time:  913.4751031398773\n",
      "Epoch [295/500], Train Loss: 145.6380, Test Loss: 155.3059\n",
      "Epoch time:  864.7731173038483\n",
      "Epoch [296/500], Train Loss: 144.7214, Test Loss: 136.8649\n",
      "Epoch time:  1007.638044834137\n",
      "Epoch [297/500], Train Loss: 143.1435, Test Loss: 142.0414\n",
      "Epoch time:  926.21400141716\n",
      "Epoch [298/500], Train Loss: 144.1897, Test Loss: 142.2236\n",
      "Epoch time:  954.3108508586884\n",
      "Epoch [299/500], Train Loss: 143.9570, Test Loss: 152.1262\n",
      "Epoch time:  929.9091866016388\n",
      "Epoch [300/500], Train Loss: 145.2235, Test Loss: 150.3781\n",
      "Epoch time:  992.0785491466522\n",
      "Epoch [301/500], Train Loss: 144.8417, Test Loss: 137.9993\n",
      "Epoch time:  912.4339756965637\n",
      "Epoch [302/500], Train Loss: 142.3223, Test Loss: 152.7013\n",
      "Epoch time:  968.0614376068115\n",
      "Epoch [303/500], Train Loss: 139.9689, Test Loss: 150.7069\n",
      "Epoch time:  979.3845658302307\n",
      "Epoch [304/500], Train Loss: 143.2788, Test Loss: 141.7582\n",
      "Epoch time:  902.1365172863007\n",
      "Epoch [305/500], Train Loss: 141.7430, Test Loss: 145.2957\n",
      "Epoch time:  945.548691034317\n",
      "Epoch [306/500], Train Loss: 142.8163, Test Loss: 141.5694\n",
      "Epoch time:  943.4630539417267\n",
      "Epoch [307/500], Train Loss: 142.5430, Test Loss: 147.5418\n",
      "Epoch time:  908.399650812149\n",
      "Epoch [308/500], Train Loss: 143.0908, Test Loss: 139.2604\n",
      "Epoch time:  913.1101987361908\n",
      "Epoch [309/500], Train Loss: 141.0051, Test Loss: 151.9083\n",
      "Epoch time:  876.8498437404633\n",
      "Epoch [310/500], Train Loss: 143.3257, Test Loss: 275.2521\n",
      "Epoch time:  957.3554034233093\n",
      "Epoch [311/500], Train Loss: 141.3834, Test Loss: 144.5894\n",
      "Epoch time:  986.4153680801392\n",
      "Epoch [312/500], Train Loss: 140.8185, Test Loss: 145.0360\n",
      "Epoch time:  953.2673654556274\n",
      "Epoch [313/500], Train Loss: 146.2664, Test Loss: 155.7001\n",
      "Epoch time:  973.6827259063721\n",
      "Epoch [314/500], Train Loss: 142.7101, Test Loss: 129.7625\n",
      "Epoch time:  897.5823986530304\n",
      "Epoch [315/500], Train Loss: 143.6976, Test Loss: 139.2629\n",
      "Epoch time:  953.5097060203552\n",
      "Epoch [316/500], Train Loss: 138.7745, Test Loss: 135.9125\n",
      "Epoch time:  613.3240485191345\n",
      "Epoch [317/500], Train Loss: 140.4723, Test Loss: 154.2226\n",
      "Epoch time:  594.4541666507721\n",
      "Epoch [318/500], Train Loss: 140.7424, Test Loss: 146.0749\n",
      "Epoch time:  577.6707046031952\n",
      "Epoch [319/500], Train Loss: 140.3474, Test Loss: 142.2240\n",
      "Epoch time:  587.4649901390076\n",
      "Epoch [320/500], Train Loss: 142.2608, Test Loss: 143.8235\n",
      "Epoch time:  538.9175274372101\n",
      "Epoch [321/500], Train Loss: 140.3234, Test Loss: 136.5894\n",
      "Epoch time:  574.1853914260864\n",
      "Epoch [322/500], Train Loss: 137.9657, Test Loss: 134.0179\n",
      "Epoch time:  569.0280964374542\n",
      "Epoch [323/500], Train Loss: 138.6279, Test Loss: 130.5049\n",
      "Epoch time:  568.5641326904297\n",
      "Epoch [324/500], Train Loss: 139.9317, Test Loss: 159.9184\n",
      "Epoch time:  542.0789976119995\n",
      "Epoch [325/500], Train Loss: 143.5879, Test Loss: 137.1477\n",
      "Epoch time:  538.1780204772949\n",
      "Epoch [326/500], Train Loss: 137.9865, Test Loss: 148.4210\n",
      "Epoch time:  551.4358429908752\n",
      "Epoch [327/500], Train Loss: 139.1036, Test Loss: 177.6798\n",
      "Epoch time:  557.3388411998749\n",
      "Epoch [328/500], Train Loss: 140.0424, Test Loss: 133.3364\n",
      "Epoch time:  557.2396545410156\n",
      "Epoch [329/500], Train Loss: 140.0687, Test Loss: 147.9789\n",
      "Epoch time:  533.1830010414124\n",
      "Epoch [330/500], Train Loss: 138.6449, Test Loss: 154.3034\n",
      "Epoch time:  542.6209893226624\n",
      "Epoch [331/500], Train Loss: 137.9031, Test Loss: 151.2524\n",
      "Epoch time:  564.9969208240509\n",
      "Epoch [332/500], Train Loss: 137.7207, Test Loss: 135.4814\n",
      "Epoch time:  535.4494686126709\n",
      "Epoch [333/500], Train Loss: 138.8429, Test Loss: 154.4429\n",
      "Epoch time:  542.173773765564\n",
      "Epoch [334/500], Train Loss: 149.0630, Test Loss: 152.0657\n",
      "Epoch time:  546.3522880077362\n",
      "Epoch [335/500], Train Loss: 139.5005, Test Loss: 134.7198\n",
      "Epoch time:  535.2026214599609\n",
      "Epoch [336/500], Train Loss: 135.4849, Test Loss: 130.0498\n",
      "Epoch time:  542.2249343395233\n",
      "Epoch [337/500], Train Loss: 135.8711, Test Loss: 136.4357\n",
      "Epoch time:  526.8515076637268\n",
      "Epoch [338/500], Train Loss: 136.0606, Test Loss: 124.1821\n",
      "Epoch time:  541.3484659194946\n",
      "Epoch [339/500], Train Loss: 136.8188, Test Loss: 135.9868\n",
      "Epoch time:  516.4496684074402\n",
      "Epoch [340/500], Train Loss: 134.4391, Test Loss: 131.2597\n",
      "Epoch time:  540.9519171714783\n",
      "Epoch [341/500], Train Loss: 137.0991, Test Loss: 128.5469\n",
      "Epoch time:  530.8336508274078\n",
      "Epoch [342/500], Train Loss: 140.9084, Test Loss: 152.9974\n",
      "Epoch time:  514.2259831428528\n",
      "Epoch [343/500], Train Loss: 136.3006, Test Loss: 198.3981\n",
      "Epoch time:  502.91633200645447\n",
      "Epoch [344/500], Train Loss: 137.6153, Test Loss: 142.7306\n",
      "Epoch time:  549.0437707901001\n",
      "Epoch [345/500], Train Loss: 134.7689, Test Loss: 128.6291\n",
      "Epoch time:  524.8545727729797\n",
      "Epoch [346/500], Train Loss: 137.3097, Test Loss: 130.1262\n",
      "Epoch time:  550.141387462616\n",
      "Epoch [347/500], Train Loss: 141.8540, Test Loss: 137.1875\n",
      "Epoch time:  538.3453922271729\n",
      "Epoch [348/500], Train Loss: 136.4680, Test Loss: 134.7499\n",
      "Epoch time:  540.5546147823334\n",
      "Epoch [349/500], Train Loss: 135.4061, Test Loss: 155.4159\n",
      "Epoch time:  532.1175785064697\n",
      "Epoch [350/500], Train Loss: 138.1065, Test Loss: 124.2489\n",
      "Epoch time:  553.4858741760254\n",
      "Epoch [351/500], Train Loss: 133.7625, Test Loss: 141.6956\n",
      "Epoch time:  572.5309062004089\n",
      "Epoch [352/500], Train Loss: 135.3728, Test Loss: 178.4970\n",
      "Epoch time:  559.1175167560577\n",
      "Epoch [353/500], Train Loss: 136.0555, Test Loss: 137.9405\n",
      "Epoch time:  551.1626079082489\n",
      "Epoch [354/500], Train Loss: 133.0541, Test Loss: 161.3289\n",
      "Epoch time:  557.6138260364532\n",
      "Epoch [355/500], Train Loss: 133.9832, Test Loss: 136.4003\n",
      "Epoch time:  575.8448343276978\n",
      "Epoch [356/500], Train Loss: 134.3434, Test Loss: 137.8867\n",
      "Epoch time:  552.6605067253113\n",
      "Epoch [357/500], Train Loss: 134.8055, Test Loss: 143.8934\n",
      "Epoch time:  529.7630734443665\n",
      "Epoch [358/500], Train Loss: 133.7655, Test Loss: 138.0735\n",
      "Epoch time:  562.1016023159027\n",
      "Epoch [359/500], Train Loss: 134.1409, Test Loss: 131.5514\n",
      "Epoch time:  552.7618610858917\n",
      "Epoch [360/500], Train Loss: 133.9967, Test Loss: 132.0196\n",
      "Epoch time:  546.1382012367249\n",
      "Epoch [361/500], Train Loss: 132.2222, Test Loss: 155.5398\n",
      "Epoch time:  522.106392621994\n",
      "Epoch [362/500], Train Loss: 137.7294, Test Loss: 154.1061\n",
      "Epoch time:  536.07253241539\n",
      "Epoch [363/500], Train Loss: 134.2910, Test Loss: 132.4746\n",
      "Epoch time:  612.5843758583069\n",
      "Epoch [364/500], Train Loss: 133.4845, Test Loss: 130.8773\n",
      "Epoch time:  890.2364933490753\n",
      "Epoch [365/500], Train Loss: 135.4009, Test Loss: 134.6223\n",
      "Epoch time:  877.0718483924866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [366/500], Train Loss: 135.3598, Test Loss: 128.7783\n",
      "Epoch time:  879.6847143173218\n",
      "Epoch [367/500], Train Loss: 134.1131, Test Loss: 135.8996\n",
      "Epoch time:  873.8160061836243\n",
      "Epoch [368/500], Train Loss: 130.1807, Test Loss: 134.4412\n",
      "Epoch time:  952.2930920124054\n",
      "Epoch [369/500], Train Loss: 132.5647, Test Loss: 126.9600\n",
      "Epoch time:  725.0651819705963\n",
      "Epoch [370/500], Train Loss: 133.4244, Test Loss: 123.3477\n",
      "Epoch time:  816.5076355934143\n",
      "Epoch [371/500], Train Loss: 131.5808, Test Loss: 148.7344\n",
      "Epoch time:  777.4826083183289\n",
      "Epoch [372/500], Train Loss: 135.9249, Test Loss: 126.6407\n",
      "Epoch time:  715.624272108078\n",
      "Epoch [373/500], Train Loss: 134.4462, Test Loss: 130.8891\n",
      "Epoch time:  691.243257522583\n",
      "Epoch [374/500], Train Loss: 132.5830, Test Loss: 136.6392\n",
      "Epoch time:  771.978039264679\n",
      "Epoch [375/500], Train Loss: 134.1177, Test Loss: 163.1273\n",
      "Epoch time:  876.5650222301483\n",
      "Epoch [376/500], Train Loss: 131.3073, Test Loss: 148.1325\n",
      "Epoch time:  778.963098526001\n",
      "Epoch [377/500], Train Loss: 133.7367, Test Loss: 135.7205\n",
      "Epoch time:  699.5425901412964\n",
      "Epoch [378/500], Train Loss: 134.4439, Test Loss: 125.1135\n",
      "Epoch time:  700.4711418151855\n",
      "Epoch [379/500], Train Loss: 131.7921, Test Loss: 147.0709\n",
      "Epoch time:  732.4870254993439\n",
      "Epoch [380/500], Train Loss: 130.3928, Test Loss: 127.2047\n",
      "Epoch time:  699.5889420509338\n",
      "Epoch [381/500], Train Loss: 131.4099, Test Loss: 142.1858\n",
      "Epoch time:  766.5802569389343\n",
      "Epoch [382/500], Train Loss: 129.1443, Test Loss: 127.2864\n",
      "Epoch time:  828.8960666656494\n",
      "Epoch [383/500], Train Loss: 131.6268, Test Loss: 155.9591\n",
      "Epoch time:  550.0134444236755\n",
      "Epoch [384/500], Train Loss: 130.6094, Test Loss: 143.7548\n",
      "Epoch time:  286.7284381389618\n",
      "Epoch [385/500], Train Loss: 131.2430, Test Loss: 131.2043\n",
      "Epoch time:  274.59806513786316\n",
      "Epoch [386/500], Train Loss: 129.6603, Test Loss: 157.9917\n",
      "Epoch time:  292.0260090827942\n",
      "Epoch [387/500], Train Loss: 130.3813, Test Loss: 120.6029\n",
      "Epoch time:  289.37550592422485\n",
      "Epoch [388/500], Train Loss: 128.9023, Test Loss: 126.9359\n",
      "Epoch time:  271.9667925834656\n",
      "Epoch [389/500], Train Loss: 129.7994, Test Loss: 121.8231\n",
      "Epoch time:  272.9825234413147\n",
      "Epoch [390/500], Train Loss: 128.7511, Test Loss: 136.9546\n",
      "Epoch time:  292.3466453552246\n",
      "Epoch [391/500], Train Loss: 129.6174, Test Loss: 122.6247\n",
      "Epoch time:  291.46352338790894\n",
      "Epoch [392/500], Train Loss: 129.5236, Test Loss: 127.2284\n",
      "Epoch time:  306.63763666152954\n",
      "Epoch [393/500], Train Loss: 129.6866, Test Loss: 131.1477\n",
      "Epoch time:  1981.0257267951965\n",
      "Epoch [394/500], Train Loss: 129.8762, Test Loss: 132.6951\n",
      "Epoch time:  1451.1892459392548\n",
      "Epoch [395/500], Train Loss: 128.3494, Test Loss: 130.2362\n",
      "Epoch time:  917.8992648124695\n",
      "Epoch [396/500], Train Loss: 129.9916, Test Loss: 129.7956\n",
      "Epoch time:  1059.2813458442688\n",
      "Epoch [397/500], Train Loss: 128.9649, Test Loss: 129.8427\n",
      "Epoch time:  877.2168800830841\n",
      "Epoch [398/500], Train Loss: 131.3606, Test Loss: 142.2019\n",
      "Epoch time:  863.9080040454865\n",
      "Epoch [399/500], Train Loss: 131.8033, Test Loss: 143.3702\n",
      "Epoch time:  793.7796466350555\n",
      "Epoch [400/500], Train Loss: 132.0892, Test Loss: 145.0440\n",
      "Epoch time:  825.3006522655487\n",
      "Epoch [401/500], Train Loss: 127.4245, Test Loss: 120.0453\n",
      "Epoch time:  860.0114283561707\n",
      "Epoch [402/500], Train Loss: 129.6873, Test Loss: 129.2872\n",
      "Epoch time:  799.4828848838806\n",
      "Epoch [403/500], Train Loss: 128.1450, Test Loss: 146.5241\n",
      "Epoch time:  828.5854158401489\n",
      "Epoch [404/500], Train Loss: 127.9253, Test Loss: 136.0704\n",
      "Epoch time:  869.5879430770874\n",
      "Epoch [405/500], Train Loss: 128.6235, Test Loss: 123.8647\n",
      "Epoch time:  850.4223589897156\n",
      "Epoch [406/500], Train Loss: 129.3435, Test Loss: 141.1545\n",
      "Epoch time:  414.4069094657898\n",
      "Epoch [407/500], Train Loss: 130.6307, Test Loss: 121.6580\n",
      "Epoch time:  307.71823930740356\n",
      "Epoch [408/500], Train Loss: 127.9273, Test Loss: 130.1966\n",
      "Epoch time:  305.3840911388397\n",
      "Epoch [409/500], Train Loss: 128.7409, Test Loss: 137.4398\n",
      "Epoch time:  298.2772445678711\n",
      "Epoch [410/500], Train Loss: 127.3612, Test Loss: 127.8136\n",
      "Epoch time:  297.1094148159027\n",
      "Epoch [411/500], Train Loss: 126.9882, Test Loss: 162.4134\n",
      "Epoch time:  262.85137581825256\n",
      "Epoch [412/500], Train Loss: 128.4601, Test Loss: 127.6419\n",
      "Epoch time:  296.4642028808594\n",
      "Epoch [413/500], Train Loss: 126.8049, Test Loss: 132.3811\n",
      "Epoch time:  265.4653332233429\n",
      "Epoch [414/500], Train Loss: 128.2359, Test Loss: 124.5872\n",
      "Epoch time:  287.25323605537415\n",
      "Epoch [415/500], Train Loss: 128.7793, Test Loss: 133.6103\n",
      "Epoch time:  308.10404562950134\n",
      "Epoch [416/500], Train Loss: 129.0473, Test Loss: 140.1639\n",
      "Epoch time:  302.5519964694977\n",
      "Epoch [417/500], Train Loss: 127.3933, Test Loss: 127.0587\n",
      "Epoch time:  291.1063928604126\n",
      "Epoch [418/500], Train Loss: 127.9339, Test Loss: 123.1490\n",
      "Epoch time:  295.77083945274353\n",
      "Epoch [419/500], Train Loss: 127.0247, Test Loss: 119.8212\n",
      "Epoch time:  296.976927280426\n",
      "Epoch [420/500], Train Loss: 126.0617, Test Loss: 146.5720\n",
      "Epoch time:  293.65599632263184\n",
      "Epoch [421/500], Train Loss: 127.5949, Test Loss: 130.0686\n",
      "Epoch time:  268.45411920547485\n",
      "Epoch [422/500], Train Loss: 130.6958, Test Loss: 124.8629\n",
      "Epoch time:  255.73979020118713\n",
      "Epoch [423/500], Train Loss: 125.6229, Test Loss: 145.8318\n",
      "Epoch time:  292.80273246765137\n",
      "Epoch [424/500], Train Loss: 126.8256, Test Loss: 125.7651\n",
      "Epoch time:  318.551025390625\n",
      "Epoch [425/500], Train Loss: 126.2011, Test Loss: 130.5193\n",
      "Epoch time:  272.81949377059937\n",
      "Epoch [426/500], Train Loss: 125.6247, Test Loss: 123.0144\n",
      "Epoch time:  262.9405343532562\n",
      "Epoch [427/500], Train Loss: 126.9838, Test Loss: 163.9337\n",
      "Epoch time:  291.53472542762756\n",
      "Epoch [428/500], Train Loss: 125.8676, Test Loss: 120.9980\n",
      "Epoch time:  302.3499674797058\n",
      "Epoch [429/500], Train Loss: 130.4427, Test Loss: 118.7596\n",
      "Epoch time:  294.1077501773834\n",
      "Epoch [430/500], Train Loss: 127.7258, Test Loss: 117.6443\n",
      "Epoch time:  301.0796694755554\n",
      "Epoch [431/500], Train Loss: 124.8111, Test Loss: 132.5202\n",
      "Epoch time:  286.98037457466125\n",
      "Epoch [432/500], Train Loss: 124.3100, Test Loss: 123.4993\n",
      "Epoch time:  281.6201500892639\n",
      "Epoch [433/500], Train Loss: 125.7857, Test Loss: 123.0648\n",
      "Epoch time:  286.8881697654724\n",
      "Epoch [434/500], Train Loss: 123.8257, Test Loss: 130.8348\n",
      "Epoch time:  288.34377694129944\n",
      "Epoch [435/500], Train Loss: 123.8727, Test Loss: 130.0963\n",
      "Epoch time:  292.8693251609802\n",
      "Epoch [436/500], Train Loss: 124.1084, Test Loss: 126.9497\n",
      "Epoch time:  309.7641532421112\n",
      "Epoch [437/500], Train Loss: 123.9182, Test Loss: 128.8287\n",
      "Epoch time:  293.51429080963135\n",
      "Epoch [438/500], Train Loss: 125.3645, Test Loss: 122.9592\n",
      "Epoch time:  288.15525937080383\n",
      "Epoch [439/500], Train Loss: 122.0061, Test Loss: 112.7465\n",
      "Epoch time:  292.9954159259796\n",
      "Epoch [440/500], Train Loss: 123.1435, Test Loss: 124.4653\n",
      "Epoch time:  310.5662171840668\n",
      "Epoch [441/500], Train Loss: 125.7133, Test Loss: 114.3856\n",
      "Epoch time:  282.83963418006897\n",
      "Epoch [442/500], Train Loss: 122.2262, Test Loss: 122.5839\n",
      "Epoch time:  306.9634780883789\n",
      "Epoch [443/500], Train Loss: 122.7653, Test Loss: 120.2206\n",
      "Epoch time:  282.24838423728943\n",
      "Epoch [444/500], Train Loss: 127.4676, Test Loss: 127.0050\n",
      "Epoch time:  289.8734540939331\n",
      "Epoch [445/500], Train Loss: 123.1242, Test Loss: 136.4954\n",
      "Epoch time:  281.11511993408203\n",
      "Epoch [446/500], Train Loss: 123.0259, Test Loss: 114.1225\n",
      "Epoch time:  276.6270864009857\n",
      "Epoch [447/500], Train Loss: 125.3356, Test Loss: 143.0218\n",
      "Epoch time:  261.00368070602417\n",
      "Epoch [448/500], Train Loss: 122.1387, Test Loss: 126.0226\n",
      "Epoch time:  258.668336391449\n",
      "Epoch [449/500], Train Loss: 121.3181, Test Loss: 124.1643\n",
      "Epoch time:  291.89956998825073\n",
      "Epoch [450/500], Train Loss: 122.0511, Test Loss: 113.0402\n",
      "Epoch time:  313.12321400642395\n",
      "Epoch [451/500], Train Loss: 120.5484, Test Loss: 133.9285\n",
      "Epoch time:  299.91835355758667\n",
      "Epoch [452/500], Train Loss: 121.1260, Test Loss: 123.3929\n",
      "Epoch time:  302.8107342720032\n",
      "Epoch [453/500], Train Loss: 122.3005, Test Loss: 129.9251\n",
      "Epoch time:  293.52499628067017\n",
      "Epoch [454/500], Train Loss: 121.8423, Test Loss: 117.6556\n",
      "Epoch time:  299.07300424575806\n",
      "Epoch [455/500], Train Loss: 121.1749, Test Loss: 127.6420\n",
      "Epoch time:  275.56076979637146\n",
      "Epoch [456/500], Train Loss: 120.3733, Test Loss: 119.0985\n",
      "Epoch time:  287.63386607170105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [457/500], Train Loss: 120.0207, Test Loss: 132.0764\n",
      "Epoch time:  265.0945608615875\n",
      "Epoch [458/500], Train Loss: 120.3892, Test Loss: 122.1022\n",
      "Epoch time:  294.0668442249298\n",
      "Epoch [459/500], Train Loss: 120.4343, Test Loss: 118.1757\n",
      "Epoch time:  294.3512170314789\n",
      "Epoch [460/500], Train Loss: 122.0919, Test Loss: 111.5492\n",
      "Epoch time:  284.2815783023834\n",
      "Epoch [461/500], Train Loss: 121.9311, Test Loss: 115.7607\n",
      "Epoch time:  293.1828191280365\n",
      "Epoch [462/500], Train Loss: 119.4121, Test Loss: 121.7531\n",
      "Epoch time:  259.54112362861633\n",
      "Epoch [463/500], Train Loss: 121.3991, Test Loss: 115.0013\n",
      "Epoch time:  274.80488777160645\n",
      "Epoch [464/500], Train Loss: 122.0897, Test Loss: 112.0867\n",
      "Epoch time:  307.47100043296814\n",
      "Epoch [465/500], Train Loss: 120.7588, Test Loss: 116.2001\n",
      "Epoch time:  313.5331358909607\n",
      "Epoch [466/500], Train Loss: 118.9081, Test Loss: 154.0788\n",
      "Epoch time:  287.7577254772186\n",
      "Epoch [467/500], Train Loss: 120.8858, Test Loss: 119.2117\n",
      "Epoch time:  296.3318061828613\n",
      "Epoch [468/500], Train Loss: 119.8953, Test Loss: 124.6644\n",
      "Epoch time:  286.4155263900757\n",
      "Epoch [469/500], Train Loss: 119.0151, Test Loss: 116.7923\n",
      "Epoch time:  294.73744654655457\n",
      "Epoch [470/500], Train Loss: 120.8048, Test Loss: 118.2456\n",
      "Epoch time:  304.37442541122437\n",
      "Epoch [471/500], Train Loss: 119.7233, Test Loss: 185.5036\n",
      "Epoch time:  295.193678855896\n",
      "Epoch [472/500], Train Loss: 118.2539, Test Loss: 134.2457\n",
      "Epoch time:  289.29720735549927\n",
      "Epoch [473/500], Train Loss: 118.1739, Test Loss: 125.7660\n",
      "Epoch time:  280.6844162940979\n",
      "Epoch [474/500], Train Loss: 119.7778, Test Loss: 125.0970\n",
      "Epoch time:  290.9473509788513\n",
      "Epoch [475/500], Train Loss: 124.3196, Test Loss: 127.7032\n",
      "Epoch time:  303.94614720344543\n",
      "Epoch [476/500], Train Loss: 118.0611, Test Loss: 131.6954\n",
      "Epoch time:  293.3336169719696\n",
      "Epoch [477/500], Train Loss: 118.1002, Test Loss: 120.7693\n",
      "Epoch time:  290.83938694000244\n",
      "Epoch [478/500], Train Loss: 118.3169, Test Loss: 118.9660\n",
      "Epoch time:  288.5862627029419\n",
      "Epoch [479/500], Train Loss: 118.0029, Test Loss: 115.1670\n",
      "Epoch time:  284.4236855506897\n",
      "Epoch [480/500], Train Loss: 118.9716, Test Loss: 117.6319\n",
      "Epoch time:  284.65007996559143\n",
      "Epoch [481/500], Train Loss: 119.6489, Test Loss: 110.3851\n",
      "Epoch time:  265.94199872016907\n",
      "Epoch [482/500], Train Loss: 118.0857, Test Loss: 177.8636\n",
      "Epoch time:  296.8012316226959\n",
      "Epoch [483/500], Train Loss: 116.5394, Test Loss: 110.0010\n",
      "Epoch time:  290.90153455734253\n",
      "Epoch [484/500], Train Loss: 116.4615, Test Loss: 112.7465\n",
      "Epoch time:  303.7187056541443\n",
      "Epoch [485/500], Train Loss: 118.3441, Test Loss: 116.8377\n",
      "Epoch time:  280.3216133117676\n",
      "Epoch [486/500], Train Loss: 115.3548, Test Loss: 138.5416\n",
      "Epoch time:  291.4253137111664\n",
      "Epoch [487/500], Train Loss: 118.6620, Test Loss: 142.5146\n",
      "Epoch time:  290.0981333255768\n",
      "Epoch [488/500], Train Loss: 117.2698, Test Loss: 127.5202\n",
      "Epoch time:  282.4595260620117\n",
      "Epoch [489/500], Train Loss: 118.3690, Test Loss: 111.6211\n",
      "Epoch time:  295.1009364128113\n",
      "Epoch [490/500], Train Loss: 117.6776, Test Loss: 118.2352\n",
      "Epoch time:  287.6412959098816\n",
      "Epoch [491/500], Train Loss: 118.9446, Test Loss: 120.3943\n",
      "Epoch time:  293.015828371048\n",
      "Epoch [492/500], Train Loss: 119.0870, Test Loss: 111.0268\n",
      "Epoch time:  299.88798904418945\n",
      "Epoch [493/500], Train Loss: 115.7466, Test Loss: 116.7253\n",
      "Epoch time:  153.4739592075348\n",
      "Epoch [494/500], Train Loss: 116.3156, Test Loss: 115.8748\n",
      "Epoch time:  116.37768745422363\n",
      "Epoch [495/500], Train Loss: 117.5388, Test Loss: 141.9581\n",
      "Epoch time:  113.23124432563782\n",
      "Epoch [496/500], Train Loss: 118.0773, Test Loss: 110.5851\n",
      "Epoch time:  112.82099938392639\n",
      "Epoch [497/500], Train Loss: 116.5635, Test Loss: 115.5642\n",
      "Epoch time:  113.32485246658325\n",
      "Epoch [498/500], Train Loss: 114.5051, Test Loss: 124.3279\n",
      "Epoch time:  119.46691417694092\n",
      "Epoch [499/500], Train Loss: 117.1331, Test Loss: 115.1225\n",
      "Epoch time:  112.76045560836792\n",
      "Epoch [500/500], Train Loss: 117.4280, Test Loss: 119.6292\n",
      "Epoch time:  113.33802127838135\n",
      "Execution time: 190602.56845593452 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.00104810576886"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset8_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset8_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_8met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_8met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_8met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_8met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_8met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_8met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_8met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_8met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_8met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_8met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_8met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_8met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_8met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_8met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_8met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_8met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_8met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_8met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_8met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_8met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (conv1): Conv1d(1, 35, kernel_size=(10,), stride=(4,), padding=(1,))\n",
       "  (conv2): Conv1d(35, 35, kernel_size=(10,), stride=(4,), padding=(1,))\n",
       "  (conv3): Conv1d(35, 35, kernel_size=(10,), stride=(4,), padding=(1,))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=25095, out_features=233, bias=True)\n",
       "  (fc2): Linear(in_features=233, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  10.760277064488275\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i].cpu().numpy()\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i].unsqueeze(0).unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "86.46  - Concentrations: 0.004999999888241291\n",
      "95.5  - Concentrations: 0.02500000037252903\n",
      "87.4  - Concentrations: 0.10000000149011612\n",
      "84.4  - Concentrations: 0.25\n",
      "84.62  - Concentrations: 0.5\n",
      "84.6  - Concentrations: 1.0\n",
      "84.5  - Concentrations: 2.5\n",
      "84.36  - Concentrations: 10.0\n",
      "84.34  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  0.14583758\n",
      "--------------------\n",
      "0.1  - Min Value: 1.8406  - Mean Value: 10.2\n",
      "0.54  - Min Value: 0.3771  - Mean Value: 11.0\n",
      "0.1  - Min Value: 3.4576  - Mean Value: 11.6\n",
      "0.12  - Min Value: 1.4371  - Mean Value: 6.6\n",
      "0.17  - Min Value: 1.6178  - Mean Value: 11.4\n",
      "0.08  - Min Value: 4.1236  - Mean Value: 12.4\n",
      "0.1  - Min Value: 1.3767  - Mean Value: 10.2\n",
      "0.11  - Min Value: 4.7076  - Mean Value: 12.2\n",
      "0.07  - Min Value: 3.7959  - Mean Value: 11.8\n",
      "0.08  - Min Value: 2.6395  - Mean Value: 10.5\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  2.94983\n",
      "--------------------\n",
      "3.01  - Min Value: 0.0228  - Mean Value: 0.1\n",
      "7.94  - Min Value: 0.0054  - Mean Value: 0.1\n",
      "2.5  - Min Value: 0.0267  - Mean Value: 0.1\n",
      "1.4  - Min Value: 0.0371  - Mean Value: 0.1\n",
      "1.59  - Min Value: 0.0507  - Mean Value: 0.1\n",
      "4.07  - Min Value: 0.0052  - Mean Value: 0.1\n",
      "2.58  - Min Value: 0.0341  - Mean Value: 0.1\n",
      "3.39  - Min Value: 0.0149  - Mean Value: 0.1\n",
      "0.87  - Min Value: 0.0726  - Mean Value: 0.2\n",
      "2.15  - Min Value: 0.0367  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  2.482538\n",
      "--------------------\n",
      "1.98  - Min Value: 0.0179  - Mean Value: 0.5\n",
      "1.8  - Min Value: 0.0264  - Mean Value: 1.2\n",
      "1.79  - Min Value: 0.0488  - Mean Value: 0.7\n",
      "5.84  - Min Value: 0.0103  - Mean Value: 0.2\n",
      "3.62  - Min Value: 0.0237  - Mean Value: 0.6\n",
      "6.77  - Min Value: 0.006  - Mean Value: 0.8\n",
      "0.59  - Min Value: 0.0609  - Mean Value: 1.2\n",
      "1.33  - Min Value: 0.0462  - Mean Value: 0.5\n",
      "0.39  - Min Value: 0.0353  - Mean Value: 2.7\n",
      "0.71  - Min Value: 0.1399  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  367.73386\n",
      "--------------------\n",
      "489.9  - Min Value: 0.0323  - Mean Value: 0.7\n",
      "403.01  - Min Value: 0.0323  - Mean Value: 0.7\n",
      "487.39  - Min Value: 0.0323  - Mean Value: 0.9\n",
      "635.56  - Min Value: 0.0323  - Mean Value: 0.6\n",
      "253.71  - Min Value: 0.0323  - Mean Value: 0.8\n",
      "447.56  - Min Value: 0.0323  - Mean Value: 1.3\n",
      "347.51  - Min Value: 0.0323  - Mean Value: 0.9\n",
      "273.0  - Min Value: 0.0323  - Mean Value: 0.6\n",
      "90.08  - Min Value: 0.0323  - Mean Value: 0.9\n",
      "249.63  - Min Value: 0.0324  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  8.212171\n",
      "--------------------\n",
      "12.05  - Min Value: 0.0067  - Mean Value: 2.5\n",
      "5.95  - Min Value: 0.0095  - Mean Value: 3.6\n",
      "5.83  - Min Value: 0.0109  - Mean Value: 0.1\n",
      "13.28  - Min Value: 0.0078  - Mean Value: 8.7\n",
      "3.48  - Min Value: 0.029  - Mean Value: 6.1\n",
      "2.93  - Min Value: 0.021  - Mean Value: 8.6\n",
      "4.69  - Min Value: 0.0473  - Mean Value: 6.9\n",
      "17.12  - Min Value: 0.0055  - Mean Value: 9.5\n",
      "13.04  - Min Value: 0.02  - Mean Value: 6.0\n",
      "3.76  - Min Value: 0.0415  - Mean Value: 5.5\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(HighDynamicRange2Conc[i].min().item(),4), \" - Mean Value:\", np.round(HighDynamicRange2Conc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x29190 and 25095x233)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m GroundTruth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.43\u001b[39m\n\u001b[1;32m      8\u001b[0m model_aq\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 9\u001b[0m Prediction \u001b[38;5;241m=\u001b[39m model_aq(SNR_Spec[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Move Prediction tensor to CPU and detach from computation graph\u001b[39;00m\n\u001b[1;32m     12\u001b[0m Prediction_cpu \u001b[38;5;241m=\u001b[39m Prediction\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m, in \u001b[0;36mNMR_Model_Aq.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[0;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x29190 and 25095x233)"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  85.8453055973663\n",
      "--------------------\n",
      "84.86\n",
      "84.99\n",
      "85.01\n",
      "84.99\n",
      "85.76\n",
      "85.85\n",
      "85.92\n",
      "86.97\n",
      "87.44\n",
      "86.68\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a dataset with singlets added at random\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  73.10043324564778\n",
      "--------------------\n",
      "82.49\n",
      "76.42\n",
      "70.51\n",
      "66.67\n",
      "65.68\n",
      "67.39\n",
      "70.26\n",
      "73.15\n",
      "76.96\n",
      "81.46\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal Baseline 1\n",
      "tensor([0.7191, 1.1480, 0.3818, 1.6846, 0.8900, 0.8150, 0.7971, 1.0269],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Sinusoidal Baseline 2\n",
      "tensor([0.5799, 0.8592, 0.0000, 1.8067, 0.7731, 0.8847, 0.6577, 0.4835],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "HD-Range 1 - 1s and 20s\n",
      "tensor([ 0.0000, 19.7999,  0.0000, 19.8230,  0.0730, 19.7280,  0.0000, 19.5708],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "HD-Range 2 - 0s and 20s\n",
      "tensor([ 0.0000, 19.8016,  0.0000, 19.8239,  0.0650, 19.7267,  0.0000, 19.5728],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(OtherValSpectra[0].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 1s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
