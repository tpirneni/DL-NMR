{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 500\n",
    "\n",
    "# Identification part of the filenames\n",
    "model_base_name = 'Quantile_q0pt1_ExtendedRange_MoreLeftOut_Combined1Distribution'\n",
    "base_name = 'ExtendedRange_MoreLeftOut_Combined1Distribution'    # This is the dataset base name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"CNN_44met_{model_base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load(f'Dataset44_{base_name}_ForManuscript_Spec.npy')\n",
    "conc1 = np.load(f'Dataset44_{base_name}_ForManuscript_Conc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in datasets]\n",
    "test_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in Test_datasets]\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset_reshaped, batch_size = 64, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset_reshaped, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NMR_Model_Aq, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 42, kernel_size=6, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(120708, 200)\n",
    "        self.fc2 = nn.Linear(200, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)                  \n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile=0.1):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        error = y_true - y_pred\n",
    "        loss = torch.mean(torch.max(self.quantile * error, (self.quantile - 1) * error))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = QuantileLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = 5.287243368897864e-05, weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in validation loss constitutes early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "            \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/500], Train Loss: 4100.8387, Test Loss: 1033.7054\n",
      "Epoch [2/500], Train Loss: 4091.3226, Test Loss: 1033.7852\n",
      "Epoch [3/500], Train Loss: 4090.5876, Test Loss: 1033.7518\n",
      "Epoch [4/500], Train Loss: 4090.1752, Test Loss: 1034.3810\n",
      "Epoch [5/500], Train Loss: 4090.2691, Test Loss: 1034.7880\n",
      "Epoch [6/500], Train Loss: 4089.9468, Test Loss: 1033.4788\n",
      "Epoch [7/500], Train Loss: 4089.6040, Test Loss: 1033.3291\n",
      "Epoch [8/500], Train Loss: 4089.3187, Test Loss: 1033.2715\n",
      "Epoch [9/500], Train Loss: 4088.9981, Test Loss: 1033.2033\n",
      "Epoch [10/500], Train Loss: 4088.3657, Test Loss: 1032.9826\n",
      "Epoch [11/500], Train Loss: 4087.6136, Test Loss: 1032.6433\n",
      "Epoch [12/500], Train Loss: 4085.4923, Test Loss: 1031.0185\n",
      "Epoch [13/500], Train Loss: 4068.5614, Test Loss: 1022.5153\n",
      "Epoch [14/500], Train Loss: 4028.7828, Test Loss: 1011.4544\n",
      "Epoch [15/500], Train Loss: 3982.6199, Test Loss: 1001.2208\n",
      "Epoch [16/500], Train Loss: 3959.2806, Test Loss: 998.5958\n",
      "Epoch [17/500], Train Loss: 3947.5283, Test Loss: 995.1281\n",
      "Epoch [18/500], Train Loss: 3940.2793, Test Loss: 994.3111\n",
      "Epoch [19/500], Train Loss: 3936.6615, Test Loss: 992.3861\n",
      "Epoch [20/500], Train Loss: 3932.0261, Test Loss: 992.2085\n",
      "Epoch [21/500], Train Loss: 3928.6708, Test Loss: 991.9693\n",
      "Epoch [22/500], Train Loss: 3926.7910, Test Loss: 990.9292\n",
      "Epoch [23/500], Train Loss: 3923.5654, Test Loss: 989.8084\n",
      "Epoch [24/500], Train Loss: 3922.4192, Test Loss: 991.2099\n",
      "Epoch [25/500], Train Loss: 3918.0912, Test Loss: 988.9039\n",
      "Epoch [26/500], Train Loss: 3916.3192, Test Loss: 989.3743\n",
      "Epoch [27/500], Train Loss: 3913.8872, Test Loss: 987.6293\n",
      "Epoch [28/500], Train Loss: 3911.1220, Test Loss: 987.2551\n",
      "Epoch [29/500], Train Loss: 3908.3028, Test Loss: 986.7542\n",
      "Epoch [30/500], Train Loss: 3905.5289, Test Loss: 986.3402\n",
      "Epoch [31/500], Train Loss: 3903.2223, Test Loss: 985.4405\n",
      "Epoch [32/500], Train Loss: 3901.6676, Test Loss: 985.3737\n",
      "Epoch [33/500], Train Loss: 3897.5185, Test Loss: 986.4836\n",
      "Epoch [34/500], Train Loss: 3894.2005, Test Loss: 984.0625\n",
      "Epoch [35/500], Train Loss: 3891.5123, Test Loss: 983.2047\n",
      "Epoch [36/500], Train Loss: 3887.4044, Test Loss: 982.5838\n",
      "Epoch [37/500], Train Loss: 3885.2120, Test Loss: 982.2090\n",
      "Epoch [38/500], Train Loss: 3879.9922, Test Loss: 982.7192\n",
      "Epoch [39/500], Train Loss: 3878.1158, Test Loss: 981.6449\n",
      "Epoch [40/500], Train Loss: 3873.3859, Test Loss: 980.3712\n",
      "Epoch [41/500], Train Loss: 3869.8745, Test Loss: 980.0999\n",
      "Epoch [42/500], Train Loss: 3867.3515, Test Loss: 977.9147\n",
      "Epoch [43/500], Train Loss: 3862.9109, Test Loss: 977.2950\n",
      "Epoch [44/500], Train Loss: 3860.2000, Test Loss: 977.7736\n",
      "Epoch [45/500], Train Loss: 3854.8648, Test Loss: 978.7013\n",
      "Epoch [46/500], Train Loss: 3852.2195, Test Loss: 975.8085\n",
      "Epoch [47/500], Train Loss: 3849.1540, Test Loss: 975.4953\n",
      "Epoch [48/500], Train Loss: 3844.7674, Test Loss: 974.5747\n",
      "Epoch [49/500], Train Loss: 3842.0059, Test Loss: 975.1147\n",
      "Epoch [50/500], Train Loss: 3838.4161, Test Loss: 973.6406\n",
      "Epoch [51/500], Train Loss: 3835.4220, Test Loss: 973.2635\n",
      "Epoch [52/500], Train Loss: 3831.6265, Test Loss: 973.0442\n",
      "Epoch [53/500], Train Loss: 3827.7086, Test Loss: 973.7077\n",
      "Epoch [54/500], Train Loss: 3823.3847, Test Loss: 973.8860\n",
      "Epoch [55/500], Train Loss: 3822.1334, Test Loss: 971.5135\n",
      "Epoch [56/500], Train Loss: 3816.0614, Test Loss: 969.3652\n",
      "Epoch [57/500], Train Loss: 3812.7325, Test Loss: 968.9585\n",
      "Epoch [58/500], Train Loss: 3806.8592, Test Loss: 967.6511\n",
      "Epoch [59/500], Train Loss: 3800.0658, Test Loss: 968.0706\n",
      "Epoch [60/500], Train Loss: 3792.6143, Test Loss: 965.6611\n",
      "Epoch [61/500], Train Loss: 3781.1209, Test Loss: 962.8190\n",
      "Epoch [62/500], Train Loss: 3770.5709, Test Loss: 958.2160\n",
      "Epoch [63/500], Train Loss: 3758.7184, Test Loss: 959.6201\n",
      "Epoch [64/500], Train Loss: 3751.2072, Test Loss: 955.9550\n",
      "Epoch [65/500], Train Loss: 3743.6827, Test Loss: 953.3850\n",
      "Epoch [66/500], Train Loss: 3734.3355, Test Loss: 950.9547\n",
      "Epoch [67/500], Train Loss: 3729.2489, Test Loss: 948.1663\n",
      "Epoch [68/500], Train Loss: 3723.8152, Test Loss: 948.6263\n",
      "Epoch [69/500], Train Loss: 3718.3354, Test Loss: 946.3687\n",
      "Epoch [70/500], Train Loss: 3711.2526, Test Loss: 945.8783\n",
      "Epoch [71/500], Train Loss: 3708.2240, Test Loss: 944.4243\n",
      "Epoch [72/500], Train Loss: 3700.7634, Test Loss: 943.1646\n",
      "Epoch [73/500], Train Loss: 3696.6182, Test Loss: 941.2951\n",
      "Epoch [74/500], Train Loss: 3691.9228, Test Loss: 945.2913\n",
      "Epoch [75/500], Train Loss: 3687.2182, Test Loss: 942.2156\n",
      "Epoch [76/500], Train Loss: 3682.1927, Test Loss: 939.1627\n",
      "Epoch [77/500], Train Loss: 3676.2946, Test Loss: 937.4077\n",
      "Epoch [78/500], Train Loss: 3672.9133, Test Loss: 935.9511\n",
      "Epoch [79/500], Train Loss: 3666.8929, Test Loss: 936.5031\n",
      "Epoch [80/500], Train Loss: 3661.6526, Test Loss: 934.7478\n",
      "Epoch [81/500], Train Loss: 3657.7321, Test Loss: 933.8551\n",
      "Epoch [82/500], Train Loss: 3652.9361, Test Loss: 933.4624\n",
      "Epoch [83/500], Train Loss: 3646.2153, Test Loss: 932.8030\n",
      "Epoch [84/500], Train Loss: 3643.6847, Test Loss: 932.2561\n",
      "Epoch [85/500], Train Loss: 3639.8295, Test Loss: 931.6313\n",
      "Epoch [86/500], Train Loss: 3634.5783, Test Loss: 928.8424\n",
      "Epoch [87/500], Train Loss: 3629.5516, Test Loss: 929.4335\n",
      "Epoch [88/500], Train Loss: 3624.9040, Test Loss: 926.6212\n",
      "Epoch [89/500], Train Loss: 3621.0667, Test Loss: 925.0808\n",
      "Epoch [90/500], Train Loss: 3614.7846, Test Loss: 925.6579\n",
      "Epoch [91/500], Train Loss: 3611.7854, Test Loss: 924.4772\n",
      "Epoch [92/500], Train Loss: 3608.8461, Test Loss: 924.9233\n",
      "Epoch [93/500], Train Loss: 3607.7078, Test Loss: 927.1979\n",
      "Epoch [94/500], Train Loss: 3602.0785, Test Loss: 921.3248\n",
      "Epoch [95/500], Train Loss: 3600.4294, Test Loss: 921.0102\n",
      "Epoch [96/500], Train Loss: 3594.0067, Test Loss: 921.4580\n",
      "Epoch [97/500], Train Loss: 3591.3688, Test Loss: 919.2310\n",
      "Epoch [98/500], Train Loss: 3584.9155, Test Loss: 920.2889\n",
      "Epoch [99/500], Train Loss: 3583.9524, Test Loss: 919.6692\n",
      "Epoch [100/500], Train Loss: 3579.3998, Test Loss: 915.7913\n",
      "Epoch [101/500], Train Loss: 3576.0885, Test Loss: 915.8928\n",
      "Epoch [102/500], Train Loss: 3571.5130, Test Loss: 917.4544\n",
      "Epoch [103/500], Train Loss: 3568.8232, Test Loss: 915.2577\n",
      "Epoch [104/500], Train Loss: 3566.6463, Test Loss: 914.6255\n",
      "Epoch [105/500], Train Loss: 3562.5477, Test Loss: 912.8007\n",
      "Epoch [106/500], Train Loss: 3559.1233, Test Loss: 913.9410\n",
      "Epoch [107/500], Train Loss: 3558.0015, Test Loss: 912.2214\n",
      "Epoch [108/500], Train Loss: 3553.3551, Test Loss: 912.1302\n",
      "Epoch [109/500], Train Loss: 3551.2999, Test Loss: 914.3445\n",
      "Epoch [110/500], Train Loss: 3547.5542, Test Loss: 910.3095\n",
      "Epoch [111/500], Train Loss: 3544.8188, Test Loss: 910.6624\n",
      "Epoch [112/500], Train Loss: 3540.2312, Test Loss: 909.0493\n",
      "Epoch [113/500], Train Loss: 3541.3354, Test Loss: 908.3832\n",
      "Epoch [114/500], Train Loss: 3536.7064, Test Loss: 907.1330\n",
      "Epoch [115/500], Train Loss: 3532.8609, Test Loss: 906.4545\n",
      "Epoch [116/500], Train Loss: 3529.8142, Test Loss: 905.6538\n",
      "Epoch [117/500], Train Loss: 3527.1661, Test Loss: 906.0959\n",
      "Epoch [118/500], Train Loss: 3526.5331, Test Loss: 904.6622\n",
      "Epoch [119/500], Train Loss: 3522.1941, Test Loss: 906.2538\n",
      "Epoch [120/500], Train Loss: 3520.7728, Test Loss: 903.8310\n",
      "Epoch [121/500], Train Loss: 3517.1702, Test Loss: 904.7509\n",
      "Epoch [122/500], Train Loss: 3514.8965, Test Loss: 903.3919\n",
      "Epoch [123/500], Train Loss: 3512.3730, Test Loss: 903.3930\n",
      "Epoch [124/500], Train Loss: 3508.0320, Test Loss: 900.7354\n",
      "Epoch [125/500], Train Loss: 3504.8572, Test Loss: 900.7538\n",
      "Epoch [126/500], Train Loss: 3502.8520, Test Loss: 901.0257\n",
      "Epoch [127/500], Train Loss: 3498.8680, Test Loss: 902.1996\n",
      "Epoch [128/500], Train Loss: 3497.0621, Test Loss: 902.3451\n",
      "Epoch [129/500], Train Loss: 3493.6451, Test Loss: 896.4004\n",
      "Epoch [130/500], Train Loss: 3489.9646, Test Loss: 898.1579\n",
      "Epoch [131/500], Train Loss: 3485.3510, Test Loss: 897.4290\n",
      "Epoch [132/500], Train Loss: 3479.0708, Test Loss: 893.9597\n",
      "Epoch [133/500], Train Loss: 3476.2895, Test Loss: 895.9465\n",
      "Epoch [134/500], Train Loss: 3469.5154, Test Loss: 893.0334\n",
      "Epoch [135/500], Train Loss: 3465.7462, Test Loss: 888.9051\n",
      "Epoch [136/500], Train Loss: 3461.1551, Test Loss: 889.9321\n",
      "Epoch [137/500], Train Loss: 3455.3172, Test Loss: 887.7329\n",
      "Epoch [138/500], Train Loss: 3450.5447, Test Loss: 885.5141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [139/500], Train Loss: 3446.2281, Test Loss: 885.3318\n",
      "Epoch [140/500], Train Loss: 3440.3588, Test Loss: 884.7546\n",
      "Epoch [141/500], Train Loss: 3436.0759, Test Loss: 883.4620\n",
      "Epoch [142/500], Train Loss: 3433.2768, Test Loss: 882.8990\n",
      "Epoch [143/500], Train Loss: 3428.6426, Test Loss: 884.7784\n",
      "Epoch [144/500], Train Loss: 3422.8667, Test Loss: 881.0198\n",
      "Epoch [145/500], Train Loss: 3419.5237, Test Loss: 880.3850\n",
      "Epoch [146/500], Train Loss: 3416.7081, Test Loss: 883.8762\n",
      "Epoch [147/500], Train Loss: 3411.5071, Test Loss: 880.5472\n",
      "Epoch [148/500], Train Loss: 3411.1528, Test Loss: 877.8004\n",
      "Epoch [149/500], Train Loss: 3406.5472, Test Loss: 882.4770\n",
      "Epoch [150/500], Train Loss: 3402.4265, Test Loss: 875.6728\n",
      "Epoch [151/500], Train Loss: 3399.1232, Test Loss: 875.4415\n",
      "Epoch [152/500], Train Loss: 3394.7862, Test Loss: 873.2690\n",
      "Epoch [153/500], Train Loss: 3392.6053, Test Loss: 874.2075\n",
      "Epoch [154/500], Train Loss: 3392.6842, Test Loss: 873.8536\n",
      "Epoch [155/500], Train Loss: 3387.9012, Test Loss: 872.1518\n",
      "Epoch [156/500], Train Loss: 3385.2231, Test Loss: 870.9333\n",
      "Epoch [157/500], Train Loss: 3381.4121, Test Loss: 870.5574\n",
      "Epoch [158/500], Train Loss: 3377.7210, Test Loss: 870.2424\n",
      "Epoch [159/500], Train Loss: 3375.2109, Test Loss: 867.8454\n",
      "Epoch [160/500], Train Loss: 3370.0969, Test Loss: 867.6206\n",
      "Epoch [161/500], Train Loss: 3369.1634, Test Loss: 866.3346\n",
      "Epoch [162/500], Train Loss: 3364.4944, Test Loss: 868.0230\n",
      "Epoch [163/500], Train Loss: 3362.5731, Test Loss: 865.6462\n",
      "Epoch [164/500], Train Loss: 3358.4511, Test Loss: 865.8255\n",
      "Epoch [165/500], Train Loss: 3356.9819, Test Loss: 865.4814\n",
      "Epoch [166/500], Train Loss: 3355.0901, Test Loss: 864.6016\n",
      "Epoch [167/500], Train Loss: 3349.7146, Test Loss: 864.7924\n",
      "Epoch [168/500], Train Loss: 3346.7833, Test Loss: 863.1486\n",
      "Epoch [169/500], Train Loss: 3346.6022, Test Loss: 862.5625\n",
      "Epoch [170/500], Train Loss: 3343.1122, Test Loss: 860.8136\n",
      "Epoch [171/500], Train Loss: 3339.4206, Test Loss: 860.1469\n",
      "Epoch [172/500], Train Loss: 3338.2829, Test Loss: 859.0801\n",
      "Epoch [173/500], Train Loss: 3334.2727, Test Loss: 861.1491\n",
      "Epoch [174/500], Train Loss: 3331.7410, Test Loss: 857.7522\n",
      "Epoch [175/500], Train Loss: 3326.3878, Test Loss: 860.5302\n",
      "Epoch [176/500], Train Loss: 3324.3861, Test Loss: 858.9179\n",
      "Epoch [177/500], Train Loss: 3321.5412, Test Loss: 864.3096\n",
      "Epoch [178/500], Train Loss: 3320.6880, Test Loss: 856.7801\n",
      "Epoch [179/500], Train Loss: 3317.5683, Test Loss: 855.3814\n",
      "Epoch [180/500], Train Loss: 3315.4202, Test Loss: 854.6347\n",
      "Epoch [181/500], Train Loss: 3313.0858, Test Loss: 854.2256\n",
      "Epoch [182/500], Train Loss: 3309.4545, Test Loss: 854.7625\n",
      "Epoch [183/500], Train Loss: 3309.1555, Test Loss: 853.8643\n",
      "Epoch [184/500], Train Loss: 3306.7433, Test Loss: 855.4616\n",
      "Epoch [185/500], Train Loss: 3301.5273, Test Loss: 851.0594\n",
      "Epoch [186/500], Train Loss: 3301.3906, Test Loss: 851.3158\n",
      "Epoch [187/500], Train Loss: 3299.6389, Test Loss: 850.4536\n",
      "Epoch [188/500], Train Loss: 3297.2348, Test Loss: 852.7051\n",
      "Epoch [189/500], Train Loss: 3292.5270, Test Loss: 851.8943\n",
      "Epoch [190/500], Train Loss: 3292.5893, Test Loss: 848.8592\n",
      "Epoch [191/500], Train Loss: 3293.5652, Test Loss: 850.9871\n",
      "Epoch [192/500], Train Loss: 3289.5920, Test Loss: 851.1786\n",
      "Epoch [193/500], Train Loss: 3288.8139, Test Loss: 848.8994\n",
      "Epoch [194/500], Train Loss: 3289.1794, Test Loss: 848.4769\n",
      "Epoch [195/500], Train Loss: 3284.3535, Test Loss: 848.8995\n",
      "Epoch [196/500], Train Loss: 3281.7531, Test Loss: 847.0939\n",
      "Epoch [197/500], Train Loss: 3282.5617, Test Loss: 848.4626\n",
      "Epoch [198/500], Train Loss: 3278.8105, Test Loss: 846.2680\n",
      "Epoch [199/500], Train Loss: 3278.3701, Test Loss: 848.1267\n",
      "Epoch [200/500], Train Loss: 3276.6152, Test Loss: 849.1554\n",
      "Epoch [201/500], Train Loss: 3273.5944, Test Loss: 848.9044\n",
      "Epoch [202/500], Train Loss: 3272.0105, Test Loss: 848.2401\n",
      "Epoch [203/500], Train Loss: 3273.1735, Test Loss: 846.5392\n",
      "Epoch [204/500], Train Loss: 3270.3377, Test Loss: 850.5037\n",
      "Epoch [205/500], Train Loss: 3268.4060, Test Loss: 845.9513\n",
      "Epoch [206/500], Train Loss: 3267.8000, Test Loss: 847.9589\n",
      "Epoch [207/500], Train Loss: 3267.1315, Test Loss: 845.2251\n",
      "Epoch [208/500], Train Loss: 3263.6917, Test Loss: 846.2615\n",
      "Epoch [209/500], Train Loss: 3265.1176, Test Loss: 844.5185\n",
      "Epoch [210/500], Train Loss: 3262.9364, Test Loss: 844.6426\n",
      "Epoch [211/500], Train Loss: 3260.7636, Test Loss: 843.8192\n",
      "Epoch [212/500], Train Loss: 3258.5819, Test Loss: 846.0129\n",
      "Epoch [213/500], Train Loss: 3256.8706, Test Loss: 844.5805\n",
      "Epoch [214/500], Train Loss: 3257.5636, Test Loss: 848.5225\n",
      "Epoch [215/500], Train Loss: 3255.9549, Test Loss: 843.3267\n",
      "Epoch [216/500], Train Loss: 3256.6586, Test Loss: 845.7810\n",
      "Epoch [217/500], Train Loss: 3253.1876, Test Loss: 842.1772\n",
      "Epoch [218/500], Train Loss: 3250.7424, Test Loss: 841.7401\n",
      "Epoch [219/500], Train Loss: 3247.5006, Test Loss: 841.1776\n",
      "Epoch [220/500], Train Loss: 3251.9055, Test Loss: 843.9453\n",
      "Epoch [221/500], Train Loss: 3248.9150, Test Loss: 839.9710\n",
      "Epoch [222/500], Train Loss: 3247.3764, Test Loss: 846.9229\n",
      "Epoch [223/500], Train Loss: 3246.4447, Test Loss: 841.0437\n",
      "Epoch [224/500], Train Loss: 3248.0588, Test Loss: 840.5886\n",
      "Epoch [225/500], Train Loss: 3243.9895, Test Loss: 840.2955\n",
      "Epoch [226/500], Train Loss: 3242.4432, Test Loss: 839.2910\n",
      "Epoch [227/500], Train Loss: 3242.4435, Test Loss: 839.9410\n",
      "Epoch [228/500], Train Loss: 3239.7892, Test Loss: 839.3805\n",
      "Epoch [229/500], Train Loss: 3238.6770, Test Loss: 842.7199\n",
      "Epoch [230/500], Train Loss: 3240.2613, Test Loss: 839.6109\n",
      "Epoch [231/500], Train Loss: 3236.6774, Test Loss: 839.8878\n",
      "Epoch [232/500], Train Loss: 3237.2918, Test Loss: 839.5314\n",
      "Epoch [233/500], Train Loss: 3233.4862, Test Loss: 837.9224\n",
      "Epoch [234/500], Train Loss: 3232.2175, Test Loss: 838.0060\n",
      "Epoch [235/500], Train Loss: 3230.8437, Test Loss: 837.0641\n",
      "Epoch [236/500], Train Loss: 3232.9086, Test Loss: 845.6867\n",
      "Epoch [237/500], Train Loss: 3231.4066, Test Loss: 837.1322\n",
      "Epoch [238/500], Train Loss: 3233.3108, Test Loss: 837.9959\n",
      "Epoch [239/500], Train Loss: 3230.3946, Test Loss: 838.8325\n",
      "Epoch [240/500], Train Loss: 3227.6022, Test Loss: 840.3182\n",
      "Epoch [241/500], Train Loss: 3226.9332, Test Loss: 838.3695\n",
      "Epoch [242/500], Train Loss: 3224.3454, Test Loss: 837.2422\n",
      "Epoch [243/500], Train Loss: 3224.6054, Test Loss: 838.0690\n",
      "Epoch [244/500], Train Loss: 3221.8983, Test Loss: 839.5489\n",
      "Epoch [245/500], Train Loss: 3221.7744, Test Loss: 836.6346\n",
      "Epoch [246/500], Train Loss: 3222.0177, Test Loss: 836.1155\n",
      "Epoch [247/500], Train Loss: 3220.7243, Test Loss: 837.4557\n",
      "Epoch [248/500], Train Loss: 3218.7215, Test Loss: 835.9608\n",
      "Epoch [249/500], Train Loss: 3218.6002, Test Loss: 836.2413\n",
      "Epoch [250/500], Train Loss: 3217.1420, Test Loss: 838.6984\n",
      "Epoch [251/500], Train Loss: 3215.0096, Test Loss: 834.9108\n",
      "Epoch [252/500], Train Loss: 3214.6696, Test Loss: 835.8436\n",
      "Epoch [253/500], Train Loss: 3214.2598, Test Loss: 837.2527\n",
      "Epoch [254/500], Train Loss: 3214.3287, Test Loss: 835.7673\n",
      "Epoch [255/500], Train Loss: 3212.3525, Test Loss: 839.2376\n",
      "Epoch [256/500], Train Loss: 3210.7064, Test Loss: 834.4331\n",
      "Epoch [257/500], Train Loss: 3209.5459, Test Loss: 834.1055\n",
      "Epoch [258/500], Train Loss: 3210.7055, Test Loss: 833.3259\n",
      "Epoch [259/500], Train Loss: 3208.9072, Test Loss: 832.3994\n",
      "Epoch [260/500], Train Loss: 3205.2809, Test Loss: 835.2637\n",
      "Epoch [261/500], Train Loss: 3205.1603, Test Loss: 833.0786\n",
      "Epoch [262/500], Train Loss: 3205.3261, Test Loss: 834.8869\n",
      "Epoch [263/500], Train Loss: 3200.3431, Test Loss: 832.9276\n",
      "Epoch [264/500], Train Loss: 3202.2491, Test Loss: 833.8866\n",
      "Epoch [265/500], Train Loss: 3201.1357, Test Loss: 831.2349\n",
      "Epoch [266/500], Train Loss: 3197.6609, Test Loss: 834.3666\n",
      "Epoch [267/500], Train Loss: 3199.8244, Test Loss: 830.6467\n",
      "Epoch [268/500], Train Loss: 3198.3365, Test Loss: 833.0397\n",
      "Epoch [269/500], Train Loss: 3198.3224, Test Loss: 832.7037\n",
      "Epoch [270/500], Train Loss: 3194.6109, Test Loss: 829.6241\n",
      "Epoch [271/500], Train Loss: 3192.0366, Test Loss: 834.4533\n",
      "Epoch [272/500], Train Loss: 3192.3305, Test Loss: 832.2079\n",
      "Epoch [273/500], Train Loss: 3193.5612, Test Loss: 831.0546\n",
      "Epoch [274/500], Train Loss: 3190.5392, Test Loss: 831.8384\n",
      "Epoch [275/500], Train Loss: 3189.2306, Test Loss: 830.4239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [276/500], Train Loss: 3185.8158, Test Loss: 831.6996\n",
      "Epoch [277/500], Train Loss: 3186.7595, Test Loss: 830.2917\n",
      "Epoch [278/500], Train Loss: 3185.7347, Test Loss: 830.7092\n",
      "Epoch [279/500], Train Loss: 3185.4416, Test Loss: 826.5162\n",
      "Epoch [280/500], Train Loss: 3184.1553, Test Loss: 830.5461\n",
      "Epoch [281/500], Train Loss: 3181.8719, Test Loss: 830.5298\n",
      "Epoch [282/500], Train Loss: 3179.5619, Test Loss: 826.7090\n",
      "Epoch [283/500], Train Loss: 3177.6778, Test Loss: 831.1131\n",
      "Epoch [284/500], Train Loss: 3178.7345, Test Loss: 828.0702\n",
      "Epoch [285/500], Train Loss: 3176.8785, Test Loss: 828.5324\n",
      "Epoch [286/500], Train Loss: 3175.3459, Test Loss: 826.4913\n",
      "Epoch [287/500], Train Loss: 3174.2945, Test Loss: 829.8364\n",
      "Epoch [288/500], Train Loss: 3174.4438, Test Loss: 826.2800\n",
      "Epoch [289/500], Train Loss: 3171.7577, Test Loss: 825.7004\n",
      "Epoch [290/500], Train Loss: 3170.5507, Test Loss: 824.2208\n",
      "Epoch [291/500], Train Loss: 3167.9644, Test Loss: 825.2375\n",
      "Epoch [292/500], Train Loss: 3168.3956, Test Loss: 824.9638\n",
      "Epoch [293/500], Train Loss: 3167.8211, Test Loss: 824.8509\n",
      "Epoch [294/500], Train Loss: 3166.0934, Test Loss: 824.3387\n",
      "Epoch [295/500], Train Loss: 3163.3581, Test Loss: 824.7692\n",
      "Epoch [296/500], Train Loss: 3163.1700, Test Loss: 823.3791\n",
      "Epoch [297/500], Train Loss: 3162.5448, Test Loss: 829.3121\n",
      "Epoch [298/500], Train Loss: 3161.8838, Test Loss: 821.9156\n",
      "Epoch [299/500], Train Loss: 3159.8088, Test Loss: 826.1381\n",
      "Epoch [300/500], Train Loss: 3160.5918, Test Loss: 822.2247\n",
      "Epoch [301/500], Train Loss: 3154.1068, Test Loss: 821.1263\n",
      "Epoch [302/500], Train Loss: 3154.9046, Test Loss: 821.0171\n",
      "Epoch [303/500], Train Loss: 3152.2797, Test Loss: 822.8574\n",
      "Epoch [304/500], Train Loss: 3152.1258, Test Loss: 823.1449\n",
      "Epoch [305/500], Train Loss: 3148.3977, Test Loss: 820.1650\n",
      "Epoch [306/500], Train Loss: 3146.7649, Test Loss: 822.6592\n",
      "Epoch [307/500], Train Loss: 3148.4568, Test Loss: 819.2476\n",
      "Epoch [308/500], Train Loss: 3144.0991, Test Loss: 819.0499\n",
      "Epoch [309/500], Train Loss: 3142.7278, Test Loss: 819.5871\n",
      "Epoch [310/500], Train Loss: 3140.7577, Test Loss: 817.3785\n",
      "Epoch [311/500], Train Loss: 3139.3054, Test Loss: 818.7946\n",
      "Epoch [312/500], Train Loss: 3135.9172, Test Loss: 817.6960\n",
      "Epoch [313/500], Train Loss: 3137.3612, Test Loss: 816.7195\n",
      "Epoch [314/500], Train Loss: 3136.5007, Test Loss: 819.3894\n",
      "Epoch [315/500], Train Loss: 3134.6421, Test Loss: 817.8955\n",
      "Epoch [316/500], Train Loss: 3131.8149, Test Loss: 815.0601\n",
      "Epoch [317/500], Train Loss: 3129.8238, Test Loss: 817.5756\n",
      "Epoch [318/500], Train Loss: 3130.7717, Test Loss: 814.6719\n",
      "Epoch [319/500], Train Loss: 3125.8197, Test Loss: 817.5707\n",
      "Epoch [320/500], Train Loss: 3123.4591, Test Loss: 813.5565\n",
      "Epoch [321/500], Train Loss: 3122.5466, Test Loss: 814.9407\n",
      "Epoch [322/500], Train Loss: 3122.1037, Test Loss: 815.4453\n",
      "Epoch [323/500], Train Loss: 3118.9901, Test Loss: 815.4757\n",
      "Epoch [324/500], Train Loss: 3117.3251, Test Loss: 814.9381\n",
      "Epoch [325/500], Train Loss: 3116.0216, Test Loss: 812.5896\n",
      "Epoch [326/500], Train Loss: 3116.9183, Test Loss: 811.3782\n",
      "Epoch [327/500], Train Loss: 3113.0755, Test Loss: 811.2540\n",
      "Epoch [328/500], Train Loss: 3112.5569, Test Loss: 811.6513\n",
      "Epoch [329/500], Train Loss: 3108.7320, Test Loss: 809.9734\n",
      "Epoch [330/500], Train Loss: 3106.7734, Test Loss: 809.5355\n",
      "Epoch [331/500], Train Loss: 3102.6675, Test Loss: 811.2760\n",
      "Epoch [332/500], Train Loss: 3103.3919, Test Loss: 808.1516\n",
      "Epoch [333/500], Train Loss: 3099.4545, Test Loss: 807.8732\n",
      "Epoch [334/500], Train Loss: 3096.0529, Test Loss: 808.0625\n",
      "Epoch [335/500], Train Loss: 3095.0472, Test Loss: 809.5910\n",
      "Epoch [336/500], Train Loss: 3092.4920, Test Loss: 804.7448\n",
      "Epoch [337/500], Train Loss: 3089.8671, Test Loss: 806.2071\n",
      "Epoch [338/500], Train Loss: 3087.9814, Test Loss: 807.0866\n",
      "Epoch [339/500], Train Loss: 3086.2359, Test Loss: 805.7221\n",
      "Epoch [340/500], Train Loss: 3083.9431, Test Loss: 805.1321\n",
      "Epoch [341/500], Train Loss: 3082.9505, Test Loss: 803.9140\n",
      "Epoch [342/500], Train Loss: 3078.4279, Test Loss: 801.2641\n",
      "Epoch [343/500], Train Loss: 3076.0538, Test Loss: 801.1778\n",
      "Epoch [344/500], Train Loss: 3075.0991, Test Loss: 803.9553\n",
      "Epoch [345/500], Train Loss: 3070.8523, Test Loss: 801.1221\n",
      "Epoch [346/500], Train Loss: 3067.7617, Test Loss: 798.8923\n",
      "Epoch [347/500], Train Loss: 3063.9305, Test Loss: 799.6313\n",
      "Epoch [348/500], Train Loss: 3059.9128, Test Loss: 797.4057\n",
      "Epoch [349/500], Train Loss: 3057.7475, Test Loss: 795.7584\n",
      "Epoch [350/500], Train Loss: 3053.5860, Test Loss: 795.0744\n",
      "Epoch [351/500], Train Loss: 3049.6790, Test Loss: 793.5678\n",
      "Epoch [352/500], Train Loss: 3044.3473, Test Loss: 792.4843\n",
      "Epoch [353/500], Train Loss: 3038.5923, Test Loss: 789.1709\n",
      "Epoch [354/500], Train Loss: 3034.7098, Test Loss: 789.6874\n",
      "Epoch [355/500], Train Loss: 3030.7990, Test Loss: 789.7714\n",
      "Epoch [356/500], Train Loss: 3024.4249, Test Loss: 786.2223\n",
      "Epoch [357/500], Train Loss: 3017.3088, Test Loss: 785.8171\n",
      "Epoch [358/500], Train Loss: 3011.5347, Test Loss: 785.5788\n",
      "Epoch [359/500], Train Loss: 3005.9291, Test Loss: 782.5942\n",
      "Epoch [360/500], Train Loss: 3002.6212, Test Loss: 780.9204\n",
      "Epoch [361/500], Train Loss: 2998.1174, Test Loss: 780.0859\n",
      "Epoch [362/500], Train Loss: 2989.0647, Test Loss: 776.0291\n",
      "Epoch [363/500], Train Loss: 2984.8562, Test Loss: 775.9438\n",
      "Epoch [364/500], Train Loss: 2981.3303, Test Loss: 775.8876\n",
      "Epoch [365/500], Train Loss: 2977.6189, Test Loss: 775.1515\n",
      "Epoch [366/500], Train Loss: 2973.2225, Test Loss: 772.8654\n",
      "Epoch [367/500], Train Loss: 2969.2037, Test Loss: 771.2356\n",
      "Epoch [368/500], Train Loss: 2962.6397, Test Loss: 769.9954\n",
      "Epoch [369/500], Train Loss: 2959.9336, Test Loss: 768.6485\n",
      "Epoch [370/500], Train Loss: 2955.9224, Test Loss: 772.2980\n",
      "Epoch [371/500], Train Loss: 2952.3613, Test Loss: 769.1395\n",
      "Epoch [372/500], Train Loss: 2950.9940, Test Loss: 765.9767\n",
      "Epoch [373/500], Train Loss: 2947.7557, Test Loss: 765.4748\n",
      "Epoch [374/500], Train Loss: 2944.7421, Test Loss: 768.3392\n",
      "Epoch [375/500], Train Loss: 2937.8078, Test Loss: 764.6664\n",
      "Epoch [376/500], Train Loss: 2935.4708, Test Loss: 765.5182\n",
      "Epoch [377/500], Train Loss: 2933.1985, Test Loss: 762.3235\n",
      "Epoch [378/500], Train Loss: 2928.9331, Test Loss: 762.1572\n",
      "Epoch [379/500], Train Loss: 2925.6639, Test Loss: 759.5758\n",
      "Epoch [380/500], Train Loss: 2923.4198, Test Loss: 762.0356\n",
      "Epoch [381/500], Train Loss: 2924.7545, Test Loss: 759.2038\n",
      "Epoch [382/500], Train Loss: 2919.0635, Test Loss: 761.2408\n",
      "Epoch [383/500], Train Loss: 2916.0766, Test Loss: 759.5622\n",
      "Epoch [384/500], Train Loss: 2913.9091, Test Loss: 756.6476\n",
      "Epoch [385/500], Train Loss: 2913.0813, Test Loss: 756.7057\n",
      "Epoch [386/500], Train Loss: 2912.7521, Test Loss: 757.6207\n",
      "Epoch [387/500], Train Loss: 2909.5724, Test Loss: 757.4178\n",
      "Epoch [388/500], Train Loss: 2906.4658, Test Loss: 756.8027\n",
      "Epoch [389/500], Train Loss: 2904.9278, Test Loss: 760.0146\n",
      "Epoch [390/500], Train Loss: 2899.9848, Test Loss: 755.0925\n",
      "Epoch [391/500], Train Loss: 2898.4240, Test Loss: 755.0579\n",
      "Epoch [392/500], Train Loss: 2900.2537, Test Loss: 755.2685\n",
      "Epoch [393/500], Train Loss: 2897.4628, Test Loss: 755.2245\n",
      "Epoch [394/500], Train Loss: 2897.4241, Test Loss: 754.4759\n",
      "Epoch [395/500], Train Loss: 2891.6793, Test Loss: 751.1307\n",
      "Epoch [396/500], Train Loss: 2891.6505, Test Loss: 752.3579\n",
      "Epoch [397/500], Train Loss: 2889.3569, Test Loss: 751.8873\n",
      "Epoch [398/500], Train Loss: 2888.5571, Test Loss: 751.6384\n",
      "Epoch [399/500], Train Loss: 2886.4421, Test Loss: 751.8071\n",
      "Epoch [400/500], Train Loss: 2885.1173, Test Loss: 749.5615\n",
      "Epoch [401/500], Train Loss: 2880.1263, Test Loss: 749.4835\n",
      "Epoch [402/500], Train Loss: 2881.2745, Test Loss: 749.8850\n",
      "Epoch [403/500], Train Loss: 2879.6717, Test Loss: 748.6290\n",
      "Epoch [404/500], Train Loss: 2879.0441, Test Loss: 750.4256\n",
      "Epoch [405/500], Train Loss: 2875.3188, Test Loss: 749.6119\n",
      "Epoch [406/500], Train Loss: 2874.7097, Test Loss: 748.1062\n",
      "Epoch [407/500], Train Loss: 2874.8823, Test Loss: 746.7350\n",
      "Epoch [408/500], Train Loss: 2874.9582, Test Loss: 749.5115\n",
      "Epoch [409/500], Train Loss: 2871.2536, Test Loss: 745.6322\n",
      "Epoch [410/500], Train Loss: 2869.4401, Test Loss: 747.0770\n",
      "Epoch [411/500], Train Loss: 2869.9293, Test Loss: 747.1974\n",
      "Epoch [412/500], Train Loss: 2866.9299, Test Loss: 746.6055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [413/500], Train Loss: 2864.5403, Test Loss: 744.2862\n",
      "Epoch [414/500], Train Loss: 2863.8354, Test Loss: 744.1393\n",
      "Epoch [415/500], Train Loss: 2862.3112, Test Loss: 747.9249\n",
      "Epoch [416/500], Train Loss: 2858.6856, Test Loss: 742.5424\n",
      "Epoch [417/500], Train Loss: 2858.1598, Test Loss: 746.1092\n",
      "Epoch [418/500], Train Loss: 2857.0004, Test Loss: 744.5038\n",
      "Epoch [419/500], Train Loss: 2855.8048, Test Loss: 746.7262\n",
      "Epoch [420/500], Train Loss: 2857.0888, Test Loss: 743.0047\n",
      "Epoch [421/500], Train Loss: 2852.1920, Test Loss: 740.8132\n",
      "Epoch [422/500], Train Loss: 2850.7994, Test Loss: 741.0516\n",
      "Epoch [423/500], Train Loss: 2849.8839, Test Loss: 742.1155\n",
      "Epoch [424/500], Train Loss: 2844.5290, Test Loss: 742.5508\n",
      "Epoch [425/500], Train Loss: 2846.5048, Test Loss: 740.0974\n",
      "Epoch [426/500], Train Loss: 2843.4269, Test Loss: 740.9654\n",
      "Epoch [427/500], Train Loss: 2842.8085, Test Loss: 741.0449\n",
      "Epoch [428/500], Train Loss: 2840.0442, Test Loss: 739.8278\n",
      "Epoch [429/500], Train Loss: 2836.9703, Test Loss: 738.1483\n",
      "Epoch [430/500], Train Loss: 2839.5266, Test Loss: 737.5027\n",
      "Epoch [431/500], Train Loss: 2835.8578, Test Loss: 737.5543\n",
      "Epoch [432/500], Train Loss: 2834.7916, Test Loss: 738.4980\n",
      "Epoch [433/500], Train Loss: 2833.2839, Test Loss: 738.9932\n",
      "Epoch [434/500], Train Loss: 2828.5390, Test Loss: 736.7257\n",
      "Epoch [435/500], Train Loss: 2828.0188, Test Loss: 735.9948\n",
      "Epoch [436/500], Train Loss: 2830.6398, Test Loss: 735.8559\n",
      "Epoch [437/500], Train Loss: 2826.8908, Test Loss: 738.1148\n",
      "Epoch [438/500], Train Loss: 2826.7229, Test Loss: 734.6549\n",
      "Epoch [439/500], Train Loss: 2821.9592, Test Loss: 735.5439\n",
      "Epoch [440/500], Train Loss: 2822.3906, Test Loss: 734.4692\n",
      "Epoch [441/500], Train Loss: 2819.8105, Test Loss: 734.4848\n",
      "Epoch [442/500], Train Loss: 2815.9987, Test Loss: 733.7792\n",
      "Epoch [443/500], Train Loss: 2813.8384, Test Loss: 736.4926\n",
      "Epoch [444/500], Train Loss: 2814.1492, Test Loss: 732.6096\n",
      "Epoch [445/500], Train Loss: 2814.9453, Test Loss: 731.5990\n",
      "Epoch [446/500], Train Loss: 2814.5656, Test Loss: 732.4927\n",
      "Epoch [447/500], Train Loss: 2809.8930, Test Loss: 730.5715\n",
      "Epoch [448/500], Train Loss: 2807.4292, Test Loss: 731.0907\n",
      "Epoch [449/500], Train Loss: 2809.6259, Test Loss: 730.0543\n",
      "Epoch [450/500], Train Loss: 2807.0192, Test Loss: 729.9035\n",
      "Epoch [451/500], Train Loss: 2802.9532, Test Loss: 731.3727\n",
      "Epoch [452/500], Train Loss: 2804.4851, Test Loss: 728.8027\n",
      "Epoch [453/500], Train Loss: 2800.7703, Test Loss: 728.7499\n",
      "Epoch [454/500], Train Loss: 2802.3435, Test Loss: 728.2654\n",
      "Epoch [455/500], Train Loss: 2798.0937, Test Loss: 728.2751\n",
      "Epoch [456/500], Train Loss: 2798.7109, Test Loss: 728.2595\n",
      "Epoch [457/500], Train Loss: 2795.2266, Test Loss: 726.9664\n",
      "Epoch [458/500], Train Loss: 2796.6354, Test Loss: 728.2519\n",
      "Epoch [459/500], Train Loss: 2793.2331, Test Loss: 728.6798\n",
      "Epoch [460/500], Train Loss: 2791.4466, Test Loss: 727.4271\n",
      "Epoch [461/500], Train Loss: 2791.5431, Test Loss: 726.2077\n",
      "Epoch [462/500], Train Loss: 2788.6070, Test Loss: 726.9925\n",
      "Epoch [463/500], Train Loss: 2791.9342, Test Loss: 727.9258\n",
      "Epoch [464/500], Train Loss: 2789.8287, Test Loss: 728.0045\n",
      "Epoch [465/500], Train Loss: 2789.3031, Test Loss: 726.8995\n",
      "Epoch [466/500], Train Loss: 2787.2996, Test Loss: 726.3365\n",
      "Epoch [467/500], Train Loss: 2782.0315, Test Loss: 724.4082\n",
      "Epoch [468/500], Train Loss: 2784.5180, Test Loss: 725.3611\n",
      "Epoch [469/500], Train Loss: 2782.0406, Test Loss: 724.4267\n",
      "Epoch [470/500], Train Loss: 2784.1431, Test Loss: 725.4685\n",
      "Epoch [471/500], Train Loss: 2786.5516, Test Loss: 723.9600\n",
      "Epoch [472/500], Train Loss: 2778.9794, Test Loss: 724.3951\n",
      "Epoch [473/500], Train Loss: 2779.1035, Test Loss: 725.4097\n",
      "Epoch [474/500], Train Loss: 2778.6443, Test Loss: 723.1773\n",
      "Epoch [475/500], Train Loss: 2776.3990, Test Loss: 723.7968\n",
      "Epoch [476/500], Train Loss: 2778.3791, Test Loss: 722.5497\n",
      "Epoch [477/500], Train Loss: 2774.8964, Test Loss: 722.8262\n",
      "Epoch [478/500], Train Loss: 2774.7975, Test Loss: 723.8532\n",
      "Epoch [479/500], Train Loss: 2772.3612, Test Loss: 723.7342\n",
      "Epoch [480/500], Train Loss: 2771.4954, Test Loss: 722.1918\n",
      "Epoch [481/500], Train Loss: 2772.2879, Test Loss: 723.3088\n",
      "Epoch [482/500], Train Loss: 2771.2062, Test Loss: 723.1444\n",
      "Epoch [483/500], Train Loss: 2769.8752, Test Loss: 725.3632\n",
      "Epoch [484/500], Train Loss: 2770.2923, Test Loss: 721.5078\n",
      "Epoch [485/500], Train Loss: 2768.6202, Test Loss: 720.4403\n",
      "Epoch [486/500], Train Loss: 2767.1411, Test Loss: 721.6815\n",
      "Epoch [487/500], Train Loss: 2765.4511, Test Loss: 721.5681\n",
      "Epoch [488/500], Train Loss: 2766.8283, Test Loss: 721.6287\n",
      "Epoch [489/500], Train Loss: 2763.2037, Test Loss: 719.8270\n",
      "Epoch [490/500], Train Loss: 2764.9307, Test Loss: 724.3086\n",
      "Epoch [491/500], Train Loss: 2761.4046, Test Loss: 724.0926\n",
      "Epoch [492/500], Train Loss: 2761.9926, Test Loss: 718.3845\n",
      "Epoch [493/500], Train Loss: 2760.4476, Test Loss: 719.8994\n",
      "Epoch [494/500], Train Loss: 2758.2641, Test Loss: 719.9753\n",
      "Epoch [495/500], Train Loss: 2758.6880, Test Loss: 723.7698\n",
      "Epoch [496/500], Train Loss: 2754.6280, Test Loss: 719.0221\n",
      "Epoch [497/500], Train Loss: 2755.5823, Test Loss: 717.2262\n",
      "Epoch [498/500], Train Loss: 2753.4741, Test Loss: 716.5116\n",
      "Epoch [499/500], Train Loss: 2757.6299, Test Loss: 719.6803\n",
      "Epoch [500/500], Train Loss: 2751.8204, Test Loss: 718.7987\n",
      "Execution time: 22795.605699300766 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "716.5115691572428"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (conv1): Conv1d(1, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=120708, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  128.90230473689329\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i].cpu().numpy()\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i].unsqueeze(0).unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "102.09  - Concentrations: 0.004999999888241291\n",
      "100.42  - Concentrations: 0.02500000037252903\n",
      "100.1  - Concentrations: 0.10000000149011612\n",
      "84.48  - Concentrations: 0.25\n",
      "77.69  - Concentrations: 0.5\n",
      "75.94  - Concentrations: 1.0\n",
      "75.69  - Concentrations: 2.5\n",
      "32.69  - Concentrations: 10.0\n",
      "42.61  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  107.93618\n",
      "--------------------\n",
      "51.2  - Min Value: 0.6783  - Mean Value: 9.2\n",
      "632.25  - Min Value: 0.0096  - Mean Value: 10.3\n",
      "39.87  - Min Value: 0.147  - Mean Value: 10.5\n",
      "79.6  - Min Value: 0.5572  - Mean Value: 8.5\n",
      "40.91  - Min Value: 1.3567  - Mean Value: 10.6\n",
      "43.42  - Min Value: 0.6332  - Mean Value: 10.9\n",
      "43.34  - Min Value: 0.7017  - Mean Value: 11.0\n",
      "67.14  - Min Value: 0.3674  - Mean Value: 8.9\n",
      "41.43  - Min Value: 0.8387  - Mean Value: 9.8\n",
      "40.21  - Min Value: 1.0913  - Mean Value: 11.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  100.779945\n",
      "--------------------\n",
      "98.27  - Min Value: 0.0111  - Mean Value: 0.1\n",
      "100.15  - Min Value: 0.0103  - Mean Value: 0.1\n",
      "100.71  - Min Value: 0.0153  - Mean Value: 0.1\n",
      "97.42  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "107.37  - Min Value: 0.0089  - Mean Value: 0.1\n",
      "98.12  - Min Value: 0.0075  - Mean Value: 0.1\n",
      "99.29  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "97.88  - Min Value: 0.0052  - Mean Value: 0.1\n",
      "100.1  - Min Value: 0.008  - Mean Value: 0.1\n",
      "108.48  - Min Value: 0.0134  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  89.09007\n",
      "--------------------\n",
      "100.72  - Min Value: 0.008  - Mean Value: 0.8\n",
      "86.76  - Min Value: 0.009  - Mean Value: 0.9\n",
      "90.53  - Min Value: 0.0138  - Mean Value: 1.5\n",
      "81.37  - Min Value: 0.0107  - Mean Value: 0.7\n",
      "91.17  - Min Value: 0.0191  - Mean Value: 0.7\n",
      "93.54  - Min Value: 0.0186  - Mean Value: 0.8\n",
      "85.37  - Min Value: 0.0175  - Mean Value: 0.8\n",
      "86.46  - Min Value: 0.0238  - Mean Value: 1.3\n",
      "85.54  - Min Value: 0.0168  - Mean Value: 0.7\n",
      "89.46  - Min Value: 0.0171  - Mean Value: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  111.1442\n",
      "--------------------\n",
      "112.24  - Min Value: 0.013  - Mean Value: 0.6\n",
      "155.41  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "114.65  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "115.39  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "106.89  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "121.82  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "95.73  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "88.02  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "90.31  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "110.99  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  220.36856\n",
      "--------------------\n",
      "138.79  - Min Value: 0.0062  - Mean Value: 2.1\n",
      "298.34  - Min Value: 0.006  - Mean Value: 3.7\n",
      "116.03  - Min Value: 0.0066  - Mean Value: 4.3\n",
      "215.86  - Min Value: 0.0094  - Mean Value: 4.3\n",
      "419.96  - Min Value: 0.0068  - Mean Value: 4.9\n",
      "186.96  - Min Value: 0.005  - Mean Value: 3.8\n",
      "142.22  - Min Value: 0.0101  - Mean Value: 3.2\n",
      "176.35  - Min Value: 0.0062  - Mean Value: 3.2\n",
      "303.25  - Min Value: 0.0053  - Mean Value: 5.3\n",
      "205.93  - Min Value: 0.0054  - Mean Value: 2.5\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(HighDynamicRange2Conc[i].min().item(),4), \" - Mean Value:\", np.round(HighDynamicRange2Conc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  78.63974990787787\n",
      "--------------------\n",
      "78.56\n",
      "78.54\n",
      "78.55\n",
      "78.64\n",
      "78.58\n",
      "78.68\n",
      "78.65\n",
      "78.61\n",
      "78.79\n",
      "78.79\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  78.52498259470673\n",
      "--------------------\n",
      "78.56\n",
      "79.1\n",
      "78.6\n",
      "78.67\n",
      "78.37\n",
      "78.01\n",
      "77.88\n",
      "78.25\n",
      "78.57\n",
      "79.22\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a dataset with singlets added at random\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  83.44275185187229\n",
      "--------------------\n",
      "79.11\n",
      "80.37\n",
      "81.4\n",
      "82.44\n",
      "83.3\n",
      "84.15\n",
      "84.88\n",
      "85.59\n",
      "86.28\n",
      "86.93\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal Baseline 1\n",
      "tensor([4.5609e-01, 3.9399e-01, 3.0220e-01, 0.0000e+00, 1.7475e-03, 0.0000e+00,\n",
      "        4.4890e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        4.3002e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 4.7024e-02, 0.0000e+00, 0.0000e+00, 2.0594e-04, 3.6937e-01,\n",
      "        0.0000e+00, 0.0000e+00, 5.0628e-04, 4.0034e-01, 4.6665e-01, 4.2338e-01,\n",
      "        0.0000e+00, 6.5660e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2688e-01,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3984e-05, 0.0000e+00,\n",
      "        0.0000e+00, 3.6917e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Sinusoidal Baseline 2\n",
      "tensor([4.2321e-01, 3.3235e-01, 0.0000e+00, 0.0000e+00, 1.3940e-03, 0.0000e+00,\n",
      "        3.1666e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        6.6732e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 2.6159e-02, 0.0000e+00, 0.0000e+00, 1.2906e-04, 4.2845e-01,\n",
      "        0.0000e+00, 0.0000e+00, 3.3237e-04, 1.7471e-01, 4.6332e-01, 3.6971e-01,\n",
      "        0.0000e+00, 2.6435e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2343e-01,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 7.8255e-02], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "HD-Range 1 - 1s and 20s\n",
      "tensor([0.0000e+00, 1.9704e+01, 0.0000e+00, 3.5955e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8607e+00, 1.2219e-01, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 4.2541e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 5.7352e+00, 5.0580e-03, 0.0000e+00, 0.0000e+00, 1.9098e+01,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8812e+01, 0.0000e+00, 1.9706e+01,\n",
      "        0.0000e+00, 1.9281e+01, 0.0000e+00, 4.4151e+00, 4.1194e-01, 1.9526e+01,\n",
      "        0.0000e+00, 7.6424e+00, 0.0000e+00, 1.8246e-01, 0.0000e+00, 2.4387e-01,\n",
      "        1.9609e-01, 1.9822e+01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "HD-Range 2 - 0s and 20s\n",
      "tensor([0.0000e+00, 1.9702e+01, 0.0000e+00, 3.5513e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8016e+00, 1.0566e-01, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 4.1596e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 5.7243e+00, 3.2045e-03, 0.0000e+00, 0.0000e+00, 1.9097e+01,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8810e+01, 0.0000e+00, 1.9706e+01,\n",
      "        0.0000e+00, 1.9276e+01, 0.0000e+00, 4.3716e+00, 4.4144e-01, 1.9523e+01,\n",
      "        0.0000e+00, 7.6205e+00, 0.0000e+00, 1.3564e-01, 0.0000e+00, 2.3790e-01,\n",
      "        1.8706e-01, 1.9813e+01], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(OtherValSpectra[0].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 1s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
