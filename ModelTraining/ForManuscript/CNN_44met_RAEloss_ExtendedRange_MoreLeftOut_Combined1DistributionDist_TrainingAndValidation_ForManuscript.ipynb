{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 500\n",
    "\n",
    "# Identification part of the filenames\n",
    "model_base_name = 'RAE_ExtendedRange_MoreLeftOut_Combined1Distribution'\n",
    "base_name = 'ExtendedRange_MoreLeftOut_Combined1Distribution'    # This is the dataset base name\n",
    "base_dir = '/path/to/base/directory'   # Set base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = f\"CNN_44met_{model_base_name}Dist_TrainingAndValidation_ForManuscript_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load(f'Dataset44_{base_name}_ForManuscript_Spec.npy')\n",
    "conc1 = np.load(f'Dataset44_{base_name}_ForManuscript_Conc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in datasets]\n",
    "test_dataset_reshaped = [(data.unsqueeze(1), label) for data, label in Test_datasets]\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset_reshaped, batch_size = 64, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset_reshaped, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NMR_Model_Aq, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 42, kernel_size=6, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(42, 42, kernel_size=6, padding=1)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(120708, 200)\n",
    "        self.fc2 = nn.Linear(200, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)                  \n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAbsoluteError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeAbsoluteError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the mean of the true values\n",
    "        y_mean = torch.mean(y_true)\n",
    "        \n",
    "        # Compute the absolute differences\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mean_absolute_errors = torch.abs(y_true - y_mean)\n",
    "        \n",
    "        # Compute RAE\n",
    "        rae = torch.sum(absolute_errors) / torch.sum(mean_absolute_errors)\n",
    "        return rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = RelativeAbsoluteError()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = 5.287243368897864e-05, weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Set how many epochs without improvement in validation loss constitutes early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            \n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "    \n",
    "            \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/500], Train Loss: 10248.0126, Test Loss: 2525.8778\n",
      "Epoch [2/500], Train Loss: 8908.5994, Test Loss: 2072.7581\n",
      "Epoch [3/500], Train Loss: 8287.8134, Test Loss: 2040.2225\n",
      "Epoch [4/500], Train Loss: 8173.3331, Test Loss: 2019.9041\n",
      "Epoch [5/500], Train Loss: 8087.9983, Test Loss: 2001.9088\n",
      "Epoch [6/500], Train Loss: 7949.1971, Test Loss: 1963.1719\n",
      "Epoch [7/500], Train Loss: 7757.6898, Test Loss: 1901.7805\n",
      "Epoch [8/500], Train Loss: 7534.8335, Test Loss: 1846.1049\n",
      "Epoch [9/500], Train Loss: 7303.8797, Test Loss: 1775.0618\n",
      "Epoch [10/500], Train Loss: 6978.4827, Test Loss: 1694.8876\n",
      "Epoch [11/500], Train Loss: 6631.7261, Test Loss: 1602.5676\n",
      "Epoch [12/500], Train Loss: 6279.1586, Test Loss: 1519.4529\n",
      "Epoch [13/500], Train Loss: 5915.0872, Test Loss: 1439.1713\n",
      "Epoch [14/500], Train Loss: 5646.3689, Test Loss: 1371.3321\n",
      "Epoch [15/500], Train Loss: 5429.0222, Test Loss: 1316.6687\n",
      "Epoch [16/500], Train Loss: 5019.5309, Test Loss: 1189.2756\n",
      "Epoch [17/500], Train Loss: 4543.4370, Test Loss: 1089.9578\n",
      "Epoch [18/500], Train Loss: 4162.7983, Test Loss: 1003.9075\n",
      "Epoch [19/500], Train Loss: 3804.6745, Test Loss: 911.0881\n",
      "Epoch [20/500], Train Loss: 3495.9647, Test Loss: 845.3836\n",
      "Epoch [21/500], Train Loss: 3297.4447, Test Loss: 803.7172\n",
      "Epoch [22/500], Train Loss: 3154.4130, Test Loss: 774.0663\n",
      "Epoch [23/500], Train Loss: 3025.1726, Test Loss: 755.5596\n",
      "Epoch [24/500], Train Loss: 2934.7669, Test Loss: 733.4201\n",
      "Epoch [25/500], Train Loss: 2862.8194, Test Loss: 702.0940\n",
      "Epoch [26/500], Train Loss: 2781.2568, Test Loss: 688.1796\n",
      "Epoch [27/500], Train Loss: 2729.0201, Test Loss: 676.8840\n",
      "Epoch [28/500], Train Loss: 2664.0171, Test Loss: 658.8910\n",
      "Epoch [29/500], Train Loss: 2611.2664, Test Loss: 656.2933\n",
      "Epoch [30/500], Train Loss: 2574.8274, Test Loss: 644.5084\n",
      "Epoch [31/500], Train Loss: 2521.9025, Test Loss: 631.4815\n",
      "Epoch [32/500], Train Loss: 2486.5209, Test Loss: 617.7887\n",
      "Epoch [33/500], Train Loss: 2448.7185, Test Loss: 609.1386\n",
      "Epoch [34/500], Train Loss: 2406.1136, Test Loss: 602.1680\n",
      "Epoch [35/500], Train Loss: 2376.9552, Test Loss: 593.7196\n",
      "Epoch [36/500], Train Loss: 2351.9280, Test Loss: 588.9802\n",
      "Epoch [37/500], Train Loss: 2307.6564, Test Loss: 577.7970\n",
      "Epoch [38/500], Train Loss: 2281.3305, Test Loss: 573.2221\n",
      "Epoch [39/500], Train Loss: 2262.2389, Test Loss: 569.4621\n",
      "Epoch [40/500], Train Loss: 2245.1686, Test Loss: 564.6171\n",
      "Epoch [41/500], Train Loss: 2221.1577, Test Loss: 559.1271\n",
      "Epoch [42/500], Train Loss: 2208.7315, Test Loss: 560.3779\n",
      "Epoch [43/500], Train Loss: 2197.9431, Test Loss: 548.1939\n",
      "Epoch [44/500], Train Loss: 2179.2050, Test Loss: 547.2369\n",
      "Epoch [45/500], Train Loss: 2168.1881, Test Loss: 548.9482\n",
      "Epoch [46/500], Train Loss: 2145.9895, Test Loss: 541.4100\n",
      "Epoch [47/500], Train Loss: 2142.3471, Test Loss: 535.1233\n",
      "Epoch [48/500], Train Loss: 2126.3420, Test Loss: 539.9775\n",
      "Epoch [49/500], Train Loss: 2122.6650, Test Loss: 527.4518\n",
      "Epoch [50/500], Train Loss: 2100.7347, Test Loss: 524.5945\n",
      "Epoch [51/500], Train Loss: 2088.4189, Test Loss: 523.9729\n",
      "Epoch [52/500], Train Loss: 2085.2007, Test Loss: 523.6074\n",
      "Epoch [53/500], Train Loss: 2070.9049, Test Loss: 522.1181\n",
      "Epoch [54/500], Train Loss: 2062.6227, Test Loss: 522.8233\n",
      "Epoch [55/500], Train Loss: 2058.2203, Test Loss: 518.2979\n",
      "Epoch [56/500], Train Loss: 2050.5334, Test Loss: 517.2974\n",
      "Epoch [57/500], Train Loss: 2039.0969, Test Loss: 514.5227\n",
      "Epoch [58/500], Train Loss: 2031.8316, Test Loss: 509.8138\n",
      "Epoch [59/500], Train Loss: 2032.3707, Test Loss: 512.9571\n",
      "Epoch [60/500], Train Loss: 2020.1880, Test Loss: 504.4359\n",
      "Epoch [61/500], Train Loss: 2008.8840, Test Loss: 505.6229\n",
      "Epoch [62/500], Train Loss: 2005.1972, Test Loss: 508.2987\n",
      "Epoch [63/500], Train Loss: 1996.9648, Test Loss: 501.0219\n",
      "Epoch [64/500], Train Loss: 1990.7458, Test Loss: 500.8846\n",
      "Epoch [65/500], Train Loss: 1996.9470, Test Loss: 497.9899\n",
      "Epoch [66/500], Train Loss: 1981.0927, Test Loss: 500.0487\n",
      "Epoch [67/500], Train Loss: 1976.9651, Test Loss: 499.0342\n",
      "Epoch [68/500], Train Loss: 1970.8890, Test Loss: 497.0672\n",
      "Epoch [69/500], Train Loss: 1965.8421, Test Loss: 497.0369\n",
      "Epoch [70/500], Train Loss: 1960.3739, Test Loss: 492.9234\n",
      "Epoch [71/500], Train Loss: 1958.1220, Test Loss: 494.6490\n",
      "Epoch [72/500], Train Loss: 1943.9845, Test Loss: 490.8176\n",
      "Epoch [73/500], Train Loss: 1943.3483, Test Loss: 489.4691\n",
      "Epoch [74/500], Train Loss: 1943.4572, Test Loss: 489.9538\n",
      "Epoch [75/500], Train Loss: 1934.0247, Test Loss: 486.7334\n",
      "Epoch [76/500], Train Loss: 1933.1803, Test Loss: 489.6006\n",
      "Epoch [77/500], Train Loss: 1923.8430, Test Loss: 482.6982\n",
      "Epoch [78/500], Train Loss: 1921.0824, Test Loss: 486.1657\n",
      "Epoch [79/500], Train Loss: 1923.5316, Test Loss: 486.2200\n",
      "Epoch [80/500], Train Loss: 1915.8916, Test Loss: 486.1333\n",
      "Epoch [81/500], Train Loss: 1910.9789, Test Loss: 480.2972\n",
      "Epoch [82/500], Train Loss: 1905.9678, Test Loss: 478.0020\n",
      "Epoch [83/500], Train Loss: 1899.1402, Test Loss: 476.8676\n",
      "Epoch [84/500], Train Loss: 1901.4500, Test Loss: 479.8671\n",
      "Epoch [85/500], Train Loss: 1893.4055, Test Loss: 479.4116\n",
      "Epoch [86/500], Train Loss: 1896.8096, Test Loss: 476.3601\n",
      "Epoch [87/500], Train Loss: 1888.1177, Test Loss: 477.3001\n",
      "Epoch [88/500], Train Loss: 1880.7540, Test Loss: 480.3861\n",
      "Epoch [89/500], Train Loss: 1884.0998, Test Loss: 476.2468\n",
      "Epoch [90/500], Train Loss: 1881.7986, Test Loss: 476.6633\n",
      "Epoch [91/500], Train Loss: 1876.5106, Test Loss: 478.9738\n",
      "Epoch [92/500], Train Loss: 1874.3279, Test Loss: 469.5666\n",
      "Epoch [93/500], Train Loss: 1871.9320, Test Loss: 474.8627\n",
      "Epoch [94/500], Train Loss: 1872.8670, Test Loss: 471.5465\n",
      "Epoch [95/500], Train Loss: 1864.6215, Test Loss: 469.6985\n",
      "Epoch [96/500], Train Loss: 1870.5222, Test Loss: 470.8109\n",
      "Epoch [97/500], Train Loss: 1863.4989, Test Loss: 468.2604\n",
      "Epoch [98/500], Train Loss: 1860.5858, Test Loss: 475.4106\n",
      "Epoch [99/500], Train Loss: 1854.0987, Test Loss: 469.5236\n",
      "Epoch [100/500], Train Loss: 1857.3823, Test Loss: 467.4130\n",
      "Epoch [101/500], Train Loss: 1854.5198, Test Loss: 465.7825\n",
      "Epoch [102/500], Train Loss: 1852.2255, Test Loss: 466.9128\n",
      "Epoch [103/500], Train Loss: 1852.6285, Test Loss: 469.3118\n",
      "Epoch [104/500], Train Loss: 1844.8573, Test Loss: 468.3894\n",
      "Epoch [105/500], Train Loss: 1845.2468, Test Loss: 466.7232\n",
      "Epoch [106/500], Train Loss: 1841.3864, Test Loss: 470.3301\n",
      "Epoch [107/500], Train Loss: 1839.9592, Test Loss: 466.2132\n",
      "Epoch [108/500], Train Loss: 1842.2483, Test Loss: 462.1094\n",
      "Epoch [109/500], Train Loss: 1835.0865, Test Loss: 468.7911\n",
      "Epoch [110/500], Train Loss: 1840.4446, Test Loss: 463.7485\n",
      "Epoch [111/500], Train Loss: 1832.3682, Test Loss: 464.0219\n",
      "Epoch [112/500], Train Loss: 1835.9443, Test Loss: 466.5066\n",
      "Epoch [113/500], Train Loss: 1833.6095, Test Loss: 461.4781\n",
      "Epoch [114/500], Train Loss: 1834.3273, Test Loss: 466.4004\n",
      "Epoch [115/500], Train Loss: 1832.4079, Test Loss: 468.9529\n",
      "Epoch [116/500], Train Loss: 1830.2775, Test Loss: 461.6341\n",
      "Epoch [117/500], Train Loss: 1828.8286, Test Loss: 462.2985\n",
      "Epoch [118/500], Train Loss: 1822.0075, Test Loss: 465.6686\n",
      "Epoch [119/500], Train Loss: 1826.6142, Test Loss: 461.6031\n",
      "Epoch [120/500], Train Loss: 1824.6429, Test Loss: 464.3422\n",
      "Epoch [121/500], Train Loss: 1816.6156, Test Loss: 465.2916\n",
      "Epoch [122/500], Train Loss: 1823.2642, Test Loss: 461.2324\n",
      "Epoch [123/500], Train Loss: 1821.0946, Test Loss: 459.3827\n",
      "Epoch [124/500], Train Loss: 1818.9135, Test Loss: 460.5356\n",
      "Epoch [125/500], Train Loss: 1814.5396, Test Loss: 464.5343\n",
      "Epoch [126/500], Train Loss: 1818.6235, Test Loss: 458.2014\n",
      "Epoch [127/500], Train Loss: 1811.2201, Test Loss: 460.4430\n",
      "Epoch [128/500], Train Loss: 1818.9210, Test Loss: 464.5021\n",
      "Epoch [129/500], Train Loss: 1808.5674, Test Loss: 456.1938\n",
      "Epoch [130/500], Train Loss: 1812.8339, Test Loss: 456.2410\n",
      "Epoch [131/500], Train Loss: 1809.4524, Test Loss: 467.6572\n",
      "Epoch [132/500], Train Loss: 1809.3760, Test Loss: 460.0165\n",
      "Epoch [133/500], Train Loss: 1811.5727, Test Loss: 455.9460\n",
      "Epoch [134/500], Train Loss: 1802.0582, Test Loss: 457.4876\n",
      "Epoch [135/500], Train Loss: 1802.5143, Test Loss: 460.5100\n",
      "Epoch [136/500], Train Loss: 1805.1155, Test Loss: 459.5133\n",
      "Epoch [137/500], Train Loss: 1805.6500, Test Loss: 460.1375\n",
      "Epoch [138/500], Train Loss: 1802.9356, Test Loss: 454.8362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [139/500], Train Loss: 1802.9068, Test Loss: 456.1869\n",
      "Epoch [140/500], Train Loss: 1799.6582, Test Loss: 457.2315\n",
      "Epoch [141/500], Train Loss: 1800.1157, Test Loss: 455.4405\n",
      "Epoch [142/500], Train Loss: 1797.9389, Test Loss: 455.7024\n",
      "Epoch [143/500], Train Loss: 1794.8077, Test Loss: 454.5385\n",
      "Epoch [144/500], Train Loss: 1799.5193, Test Loss: 460.5026\n",
      "Epoch [145/500], Train Loss: 1794.0693, Test Loss: 452.3324\n",
      "Epoch [146/500], Train Loss: 1794.9547, Test Loss: 456.5920\n",
      "Epoch [147/500], Train Loss: 1793.9080, Test Loss: 456.6587\n",
      "Epoch [148/500], Train Loss: 1792.4557, Test Loss: 452.4852\n",
      "Epoch [149/500], Train Loss: 1791.9804, Test Loss: 453.4438\n",
      "Epoch [150/500], Train Loss: 1795.1875, Test Loss: 459.3133\n",
      "Epoch [151/500], Train Loss: 1788.9448, Test Loss: 451.8994\n",
      "Epoch [152/500], Train Loss: 1791.5347, Test Loss: 455.0063\n",
      "Epoch [153/500], Train Loss: 1784.0092, Test Loss: 454.3275\n",
      "Epoch [154/500], Train Loss: 1791.8354, Test Loss: 452.5178\n",
      "Epoch [155/500], Train Loss: 1789.7711, Test Loss: 455.9648\n",
      "Epoch [156/500], Train Loss: 1787.9830, Test Loss: 452.1043\n",
      "Epoch [157/500], Train Loss: 1787.7994, Test Loss: 450.6751\n",
      "Epoch [158/500], Train Loss: 1790.8071, Test Loss: 449.1779\n",
      "Epoch [159/500], Train Loss: 1785.3217, Test Loss: 460.6049\n",
      "Epoch [160/500], Train Loss: 1785.7685, Test Loss: 449.5171\n",
      "Epoch [161/500], Train Loss: 1789.8524, Test Loss: 453.6935\n",
      "Epoch [162/500], Train Loss: 1780.0962, Test Loss: 453.5526\n",
      "Epoch [163/500], Train Loss: 1784.4992, Test Loss: 447.2680\n",
      "Epoch [164/500], Train Loss: 1777.8370, Test Loss: 453.6252\n",
      "Epoch [165/500], Train Loss: 1781.3480, Test Loss: 452.8512\n",
      "Epoch [166/500], Train Loss: 1782.9696, Test Loss: 453.1429\n",
      "Epoch [167/500], Train Loss: 1780.0115, Test Loss: 451.1408\n",
      "Epoch [168/500], Train Loss: 1781.2983, Test Loss: 456.7671\n",
      "Epoch [169/500], Train Loss: 1778.2535, Test Loss: 452.2112\n",
      "Epoch [170/500], Train Loss: 1779.2661, Test Loss: 449.8729\n",
      "Epoch [171/500], Train Loss: 1778.7837, Test Loss: 450.9912\n",
      "Epoch [172/500], Train Loss: 1783.0013, Test Loss: 451.2287\n",
      "Epoch [173/500], Train Loss: 1776.9721, Test Loss: 448.2771\n",
      "Epoch [174/500], Train Loss: 1775.8809, Test Loss: 452.7134\n",
      "Epoch [175/500], Train Loss: 1773.1182, Test Loss: 446.5159\n",
      "Epoch [176/500], Train Loss: 1774.4565, Test Loss: 451.4068\n",
      "Epoch [177/500], Train Loss: 1774.4930, Test Loss: 450.1027\n",
      "Epoch [178/500], Train Loss: 1776.6055, Test Loss: 453.0714\n",
      "Epoch [179/500], Train Loss: 1779.8146, Test Loss: 449.2577\n",
      "Epoch [180/500], Train Loss: 1769.1586, Test Loss: 455.5958\n",
      "Epoch [181/500], Train Loss: 1775.8532, Test Loss: 456.5084\n",
      "Epoch [182/500], Train Loss: 1774.5044, Test Loss: 448.8545\n",
      "Epoch [183/500], Train Loss: 1775.9829, Test Loss: 446.8060\n",
      "Epoch [184/500], Train Loss: 1772.1518, Test Loss: 453.6342\n",
      "Epoch [185/500], Train Loss: 1777.0454, Test Loss: 449.4541\n",
      "Epoch [186/500], Train Loss: 1766.8564, Test Loss: 447.6881\n",
      "Epoch [187/500], Train Loss: 1769.5225, Test Loss: 449.1382\n",
      "Epoch [188/500], Train Loss: 1765.6612, Test Loss: 451.5587\n",
      "Epoch [189/500], Train Loss: 1778.2120, Test Loss: 448.0205\n",
      "Epoch [190/500], Train Loss: 1767.0314, Test Loss: 447.9998\n",
      "Epoch [191/500], Train Loss: 1766.7613, Test Loss: 447.6623\n",
      "Epoch [192/500], Train Loss: 1772.0401, Test Loss: 449.2907\n",
      "Epoch [193/500], Train Loss: 1765.9407, Test Loss: 450.9547\n",
      "Epoch [194/500], Train Loss: 1767.7446, Test Loss: 456.6611\n",
      "Epoch [195/500], Train Loss: 1771.6439, Test Loss: 451.6164\n",
      "Epoch [196/500], Train Loss: 1770.1909, Test Loss: 449.0717\n",
      "Epoch [197/500], Train Loss: 1765.4993, Test Loss: 443.6835\n",
      "Epoch [198/500], Train Loss: 1765.5469, Test Loss: 452.1814\n",
      "Epoch [199/500], Train Loss: 1758.3635, Test Loss: 451.9831\n",
      "Epoch [200/500], Train Loss: 1768.3577, Test Loss: 446.9131\n",
      "Epoch [201/500], Train Loss: 1766.3373, Test Loss: 452.2595\n",
      "Epoch [202/500], Train Loss: 1762.8779, Test Loss: 451.1484\n",
      "Epoch [203/500], Train Loss: 1768.7562, Test Loss: 445.8583\n",
      "Epoch [204/500], Train Loss: 1764.4602, Test Loss: 450.5281\n",
      "Epoch [205/500], Train Loss: 1761.4157, Test Loss: 442.7590\n",
      "Epoch [206/500], Train Loss: 1772.6304, Test Loss: 450.8991\n",
      "Epoch [207/500], Train Loss: 1764.5292, Test Loss: 447.4584\n",
      "Epoch [208/500], Train Loss: 1758.2532, Test Loss: 449.1079\n",
      "Epoch [209/500], Train Loss: 1764.7402, Test Loss: 449.3871\n",
      "Epoch [210/500], Train Loss: 1761.6537, Test Loss: 444.6446\n",
      "Epoch [211/500], Train Loss: 1760.3484, Test Loss: 448.5341\n",
      "Epoch [212/500], Train Loss: 1770.7534, Test Loss: 453.1000\n",
      "Epoch [213/500], Train Loss: 1761.8434, Test Loss: 452.1463\n",
      "Epoch [214/500], Train Loss: 1765.7288, Test Loss: 447.5009\n",
      "Epoch [215/500], Train Loss: 1760.6431, Test Loss: 444.9814\n",
      "Epoch [216/500], Train Loss: 1757.1523, Test Loss: 449.0699\n",
      "Epoch [217/500], Train Loss: 1761.4657, Test Loss: 446.9579\n",
      "Epoch [218/500], Train Loss: 1760.9668, Test Loss: 453.5717\n",
      "Epoch [219/500], Train Loss: 1761.2333, Test Loss: 443.5581\n",
      "Epoch [220/500], Train Loss: 1760.0872, Test Loss: 451.3194\n",
      "Epoch [221/500], Train Loss: 1757.0311, Test Loss: 446.2243\n",
      "Epoch [222/500], Train Loss: 1762.3737, Test Loss: 446.1538\n",
      "Epoch [223/500], Train Loss: 1756.1816, Test Loss: 447.0019\n",
      "Epoch [224/500], Train Loss: 1760.7422, Test Loss: 445.2473\n",
      "Epoch [225/500], Train Loss: 1763.3987, Test Loss: 445.2129\n",
      "Epoch [226/500], Train Loss: 1762.5826, Test Loss: 446.2220\n",
      "Epoch [227/500], Train Loss: 1756.1537, Test Loss: 446.5750\n",
      "Epoch [228/500], Train Loss: 1754.1107, Test Loss: 443.5772\n",
      "Epoch [229/500], Train Loss: 1755.1571, Test Loss: 447.0954\n",
      "Epoch [230/500], Train Loss: 1756.7522, Test Loss: 445.9200\n",
      "Epoch [231/500], Train Loss: 1752.6008, Test Loss: 445.9114\n",
      "Epoch [232/500], Train Loss: 1755.0326, Test Loss: 444.5623\n",
      "Epoch [233/500], Train Loss: 1758.2336, Test Loss: 448.0342\n",
      "Epoch [234/500], Train Loss: 1751.2970, Test Loss: 454.3481\n",
      "Epoch [235/500], Train Loss: 1759.4402, Test Loss: 445.2346\n",
      "Epoch [236/500], Train Loss: 1753.4125, Test Loss: 442.8853\n",
      "Epoch [237/500], Train Loss: 1751.0676, Test Loss: 448.0135\n",
      "Epoch [238/500], Train Loss: 1750.7510, Test Loss: 444.8734\n",
      "Epoch [239/500], Train Loss: 1750.8340, Test Loss: 442.4538\n",
      "Epoch [240/500], Train Loss: 1756.6869, Test Loss: 446.5743\n",
      "Epoch [241/500], Train Loss: 1748.4535, Test Loss: 443.8280\n",
      "Epoch [242/500], Train Loss: 1749.5238, Test Loss: 449.1005\n",
      "Epoch [243/500], Train Loss: 1753.7556, Test Loss: 444.0432\n",
      "Epoch [244/500], Train Loss: 1746.7262, Test Loss: 443.1081\n",
      "Epoch [245/500], Train Loss: 1750.0087, Test Loss: 443.0548\n",
      "Epoch [246/500], Train Loss: 1755.1038, Test Loss: 443.7041\n",
      "Epoch [247/500], Train Loss: 1752.2371, Test Loss: 447.2953\n",
      "Epoch [248/500], Train Loss: 1753.7165, Test Loss: 445.4533\n",
      "Epoch [249/500], Train Loss: 1751.9595, Test Loss: 447.7574\n",
      "Epoch [250/500], Train Loss: 1745.3303, Test Loss: 445.9414\n",
      "Epoch [251/500], Train Loss: 1749.4343, Test Loss: 449.4035\n",
      "Epoch [252/500], Train Loss: 1745.8326, Test Loss: 447.3439\n",
      "Epoch [253/500], Train Loss: 1750.6155, Test Loss: 445.6512\n",
      "Epoch [254/500], Train Loss: 1749.8937, Test Loss: 446.5362\n",
      "Epoch [255/500], Train Loss: 1752.5865, Test Loss: 444.8787\n",
      "Epoch [256/500], Train Loss: 1753.4088, Test Loss: 445.9163\n",
      "Epoch [257/500], Train Loss: 1752.0851, Test Loss: 443.9651\n",
      "Epoch [258/500], Train Loss: 1750.1336, Test Loss: 448.6209\n",
      "Epoch [259/500], Train Loss: 1749.3849, Test Loss: 443.1392\n",
      "Epoch [260/500], Train Loss: 1747.5790, Test Loss: 442.6684\n",
      "Epoch [261/500], Train Loss: 1748.8834, Test Loss: 448.6824\n",
      "Epoch [262/500], Train Loss: 1744.2455, Test Loss: 445.9453\n",
      "Epoch [263/500], Train Loss: 1747.0901, Test Loss: 448.3582\n",
      "Epoch [264/500], Train Loss: 1751.8498, Test Loss: 446.4588\n",
      "Epoch [265/500], Train Loss: 1754.2061, Test Loss: 443.5840\n",
      "Epoch [266/500], Train Loss: 1741.7810, Test Loss: 445.1232\n",
      "Epoch [267/500], Train Loss: 1743.5240, Test Loss: 444.6436\n",
      "Epoch [268/500], Train Loss: 1745.9076, Test Loss: 447.5084\n",
      "Epoch [269/500], Train Loss: 1748.1443, Test Loss: 444.1422\n",
      "Epoch [270/500], Train Loss: 1750.1918, Test Loss: 446.3937\n",
      "Epoch [271/500], Train Loss: 1745.2753, Test Loss: 441.8045\n",
      "Epoch [272/500], Train Loss: 1743.6157, Test Loss: 441.5054\n",
      "Epoch [273/500], Train Loss: 1742.9443, Test Loss: 444.2598\n",
      "Epoch [274/500], Train Loss: 1741.6095, Test Loss: 443.8584\n",
      "Epoch [275/500], Train Loss: 1748.0676, Test Loss: 442.2262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [276/500], Train Loss: 1743.6693, Test Loss: 446.4327\n",
      "Epoch [277/500], Train Loss: 1744.1808, Test Loss: 442.6547\n",
      "Epoch [278/500], Train Loss: 1742.0488, Test Loss: 444.6008\n",
      "Epoch [279/500], Train Loss: 1745.6910, Test Loss: 440.6937\n",
      "Epoch [280/500], Train Loss: 1744.6958, Test Loss: 445.3391\n",
      "Epoch [281/500], Train Loss: 1744.6695, Test Loss: 444.2431\n",
      "Epoch [282/500], Train Loss: 1745.1739, Test Loss: 445.0496\n",
      "Epoch [283/500], Train Loss: 1741.5923, Test Loss: 444.5986\n",
      "Epoch [284/500], Train Loss: 1745.9546, Test Loss: 448.7299\n",
      "Epoch [285/500], Train Loss: 1744.5812, Test Loss: 448.8438\n",
      "Epoch [286/500], Train Loss: 1744.3269, Test Loss: 443.3853\n",
      "Epoch [287/500], Train Loss: 1739.0830, Test Loss: 442.4208\n",
      "Epoch [288/500], Train Loss: 1736.3620, Test Loss: 444.0099\n",
      "Epoch [289/500], Train Loss: 1739.8086, Test Loss: 446.4980\n",
      "Epoch [290/500], Train Loss: 1742.8728, Test Loss: 445.2061\n",
      "Epoch [291/500], Train Loss: 1736.9913, Test Loss: 444.1022\n",
      "Epoch [292/500], Train Loss: 1743.8494, Test Loss: 448.2783\n",
      "Epoch [293/500], Train Loss: 1739.6015, Test Loss: 438.1893\n",
      "Epoch [294/500], Train Loss: 1740.5194, Test Loss: 443.7591\n",
      "Epoch [295/500], Train Loss: 1742.4049, Test Loss: 448.3366\n",
      "Epoch [296/500], Train Loss: 1738.2862, Test Loss: 441.0551\n",
      "Epoch [297/500], Train Loss: 1741.1042, Test Loss: 444.7191\n",
      "Epoch [298/500], Train Loss: 1737.0429, Test Loss: 445.8538\n",
      "Epoch [299/500], Train Loss: 1739.7598, Test Loss: 443.4099\n",
      "Epoch [300/500], Train Loss: 1745.1526, Test Loss: 441.7061\n",
      "Epoch [301/500], Train Loss: 1737.6201, Test Loss: 439.7603\n",
      "Epoch [302/500], Train Loss: 1738.0475, Test Loss: 444.2256\n",
      "Epoch [303/500], Train Loss: 1737.9152, Test Loss: 445.4621\n",
      "Epoch [304/500], Train Loss: 1739.5294, Test Loss: 441.6023\n",
      "Epoch [305/500], Train Loss: 1737.4071, Test Loss: 443.0524\n",
      "Epoch [306/500], Train Loss: 1742.9738, Test Loss: 442.1004\n",
      "Epoch [307/500], Train Loss: 1738.4565, Test Loss: 441.8445\n",
      "Epoch [308/500], Train Loss: 1736.7155, Test Loss: 441.0783\n",
      "Epoch [309/500], Train Loss: 1736.9020, Test Loss: 445.2838\n",
      "Epoch [310/500], Train Loss: 1739.2100, Test Loss: 441.4009\n",
      "Epoch [311/500], Train Loss: 1733.8821, Test Loss: 445.5732\n",
      "Epoch [312/500], Train Loss: 1739.4511, Test Loss: 442.1662\n",
      "Epoch [313/500], Train Loss: 1735.7263, Test Loss: 441.4586\n",
      "Epoch [314/500], Train Loss: 1737.3829, Test Loss: 449.4009\n",
      "Epoch [315/500], Train Loss: 1732.7818, Test Loss: 446.0993\n",
      "Epoch [316/500], Train Loss: 1732.9303, Test Loss: 444.2972\n",
      "Epoch [317/500], Train Loss: 1739.7657, Test Loss: 450.3896\n",
      "Epoch [318/500], Train Loss: 1736.6487, Test Loss: 444.3685\n",
      "Epoch [319/500], Train Loss: 1736.6272, Test Loss: 442.4136\n",
      "Epoch [320/500], Train Loss: 1731.8078, Test Loss: 443.3100\n",
      "Epoch [321/500], Train Loss: 1740.6656, Test Loss: 440.6763\n",
      "Epoch [322/500], Train Loss: 1732.1298, Test Loss: 440.7149\n",
      "Epoch [323/500], Train Loss: 1731.0480, Test Loss: 441.7155\n",
      "Epoch [324/500], Train Loss: 1734.3315, Test Loss: 442.9501\n",
      "Epoch [325/500], Train Loss: 1733.2781, Test Loss: 441.5913\n",
      "Epoch [326/500], Train Loss: 1735.9561, Test Loss: 440.7044\n",
      "Epoch [327/500], Train Loss: 1734.1830, Test Loss: 441.9882\n",
      "Epoch [328/500], Train Loss: 1736.2394, Test Loss: 442.4358\n",
      "Epoch [329/500], Train Loss: 1731.7780, Test Loss: 445.2521\n",
      "Epoch [330/500], Train Loss: 1740.8445, Test Loss: 439.5067\n",
      "Epoch [331/500], Train Loss: 1731.1169, Test Loss: 437.8486\n",
      "Epoch [332/500], Train Loss: 1731.7960, Test Loss: 443.2943\n",
      "Epoch [333/500], Train Loss: 1735.2621, Test Loss: 440.2526\n",
      "Epoch [334/500], Train Loss: 1730.7112, Test Loss: 441.4504\n",
      "Epoch [335/500], Train Loss: 1729.6301, Test Loss: 441.9015\n",
      "Epoch [336/500], Train Loss: 1732.8491, Test Loss: 438.1412\n",
      "Epoch [337/500], Train Loss: 1730.7858, Test Loss: 439.5882\n",
      "Epoch [338/500], Train Loss: 1729.0618, Test Loss: 442.2337\n",
      "Epoch [339/500], Train Loss: 1733.7425, Test Loss: 442.5745\n",
      "Epoch [340/500], Train Loss: 1731.4675, Test Loss: 443.3661\n",
      "Epoch [341/500], Train Loss: 1730.8426, Test Loss: 441.7192\n",
      "Epoch [342/500], Train Loss: 1731.3736, Test Loss: 437.0905\n",
      "Epoch [343/500], Train Loss: 1732.7963, Test Loss: 441.0810\n",
      "Epoch [344/500], Train Loss: 1732.4202, Test Loss: 437.8029\n",
      "Epoch [345/500], Train Loss: 1728.6604, Test Loss: 438.4303\n",
      "Epoch [346/500], Train Loss: 1728.9931, Test Loss: 441.5797\n",
      "Epoch [347/500], Train Loss: 1731.6717, Test Loss: 439.3100\n",
      "Epoch [348/500], Train Loss: 1727.6229, Test Loss: 439.1483\n",
      "Epoch [349/500], Train Loss: 1729.5731, Test Loss: 440.3775\n",
      "Epoch [350/500], Train Loss: 1728.9360, Test Loss: 442.2201\n",
      "Epoch [351/500], Train Loss: 1731.7853, Test Loss: 443.8641\n",
      "Epoch [352/500], Train Loss: 1727.4353, Test Loss: 439.7607\n",
      "Epoch [353/500], Train Loss: 1726.5526, Test Loss: 438.2314\n",
      "Epoch [354/500], Train Loss: 1729.7333, Test Loss: 443.1865\n",
      "Epoch [355/500], Train Loss: 1730.7745, Test Loss: 441.7840\n",
      "Epoch [356/500], Train Loss: 1734.9174, Test Loss: 439.3527\n",
      "Epoch [357/500], Train Loss: 1726.2703, Test Loss: 442.3694\n",
      "Epoch [358/500], Train Loss: 1727.9393, Test Loss: 440.8358\n",
      "Epoch [359/500], Train Loss: 1726.5096, Test Loss: 440.6402\n",
      "Epoch [360/500], Train Loss: 1724.7795, Test Loss: 442.9744\n",
      "Epoch [361/500], Train Loss: 1728.0150, Test Loss: 438.7642\n",
      "Epoch [362/500], Train Loss: 1732.0873, Test Loss: 445.3005\n",
      "Epoch [363/500], Train Loss: 1728.9233, Test Loss: 439.7351\n",
      "Epoch [364/500], Train Loss: 1728.1822, Test Loss: 440.3461\n",
      "Epoch [365/500], Train Loss: 1726.9164, Test Loss: 439.0904\n",
      "Epoch [366/500], Train Loss: 1727.3864, Test Loss: 439.6293\n",
      "Epoch [367/500], Train Loss: 1733.0786, Test Loss: 441.9592\n",
      "Epoch [368/500], Train Loss: 1730.0014, Test Loss: 438.9484\n",
      "Epoch [369/500], Train Loss: 1722.5230, Test Loss: 442.6503\n",
      "Epoch [370/500], Train Loss: 1726.3188, Test Loss: 437.4954\n",
      "Epoch [371/500], Train Loss: 1721.8967, Test Loss: 438.8068\n",
      "Epoch [372/500], Train Loss: 1729.7252, Test Loss: 444.2718\n",
      "Epoch [373/500], Train Loss: 1726.5262, Test Loss: 441.1082\n",
      "Epoch [374/500], Train Loss: 1723.5744, Test Loss: 442.4610\n",
      "Epoch [375/500], Train Loss: 1731.2493, Test Loss: 439.3917\n",
      "Epoch [376/500], Train Loss: 1729.6192, Test Loss: 441.1882\n",
      "Epoch [377/500], Train Loss: 1720.7592, Test Loss: 438.1695\n",
      "Epoch [378/500], Train Loss: 1725.6268, Test Loss: 441.1517\n",
      "Epoch [379/500], Train Loss: 1724.6578, Test Loss: 443.8370\n",
      "Epoch [380/500], Train Loss: 1723.2045, Test Loss: 441.1118\n",
      "Epoch [381/500], Train Loss: 1723.0015, Test Loss: 440.0435\n",
      "Epoch [382/500], Train Loss: 1725.7809, Test Loss: 438.9942\n",
      "Epoch [383/500], Train Loss: 1724.9557, Test Loss: 437.4775\n",
      "Epoch [384/500], Train Loss: 1724.8991, Test Loss: 437.0357\n",
      "Epoch [385/500], Train Loss: 1722.3412, Test Loss: 438.8925\n",
      "Epoch [386/500], Train Loss: 1727.5103, Test Loss: 435.5406\n",
      "Epoch [387/500], Train Loss: 1725.6401, Test Loss: 437.7585\n",
      "Epoch [388/500], Train Loss: 1723.2177, Test Loss: 438.6124\n",
      "Epoch [389/500], Train Loss: 1723.2128, Test Loss: 441.2990\n",
      "Epoch [390/500], Train Loss: 1728.6065, Test Loss: 440.9257\n",
      "Epoch [391/500], Train Loss: 1718.3200, Test Loss: 436.5434\n",
      "Epoch [392/500], Train Loss: 1725.4626, Test Loss: 436.2026\n",
      "Epoch [393/500], Train Loss: 1725.4310, Test Loss: 440.1649\n",
      "Epoch [394/500], Train Loss: 1719.0619, Test Loss: 436.8463\n",
      "Epoch [395/500], Train Loss: 1722.0908, Test Loss: 442.9789\n",
      "Epoch [396/500], Train Loss: 1723.6903, Test Loss: 444.2389\n",
      "Epoch [397/500], Train Loss: 1721.6130, Test Loss: 438.2145\n",
      "Epoch [398/500], Train Loss: 1725.2453, Test Loss: 438.6522\n",
      "Epoch [399/500], Train Loss: 1719.4289, Test Loss: 437.8413\n",
      "Epoch [400/500], Train Loss: 1715.3963, Test Loss: 435.0288\n",
      "Epoch [401/500], Train Loss: 1719.7081, Test Loss: 436.6137\n",
      "Epoch [402/500], Train Loss: 1725.6327, Test Loss: 437.3781\n",
      "Epoch [403/500], Train Loss: 1722.6838, Test Loss: 435.0716\n",
      "Epoch [404/500], Train Loss: 1719.8524, Test Loss: 440.3954\n",
      "Epoch [405/500], Train Loss: 1722.9769, Test Loss: 434.7011\n",
      "Epoch [406/500], Train Loss: 1716.7636, Test Loss: 437.2858\n",
      "Epoch [407/500], Train Loss: 1720.5430, Test Loss: 435.9392\n",
      "Epoch [408/500], Train Loss: 1727.9532, Test Loss: 441.2821\n",
      "Epoch [409/500], Train Loss: 1720.7544, Test Loss: 441.7092\n",
      "Epoch [410/500], Train Loss: 1722.5345, Test Loss: 438.1942\n",
      "Epoch [411/500], Train Loss: 1722.9065, Test Loss: 437.4784\n",
      "Epoch [412/500], Train Loss: 1723.3621, Test Loss: 440.2745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [413/500], Train Loss: 1717.4048, Test Loss: 436.6836\n",
      "Epoch [414/500], Train Loss: 1718.2113, Test Loss: 439.7869\n",
      "Epoch [415/500], Train Loss: 1723.3993, Test Loss: 438.5868\n",
      "Epoch [416/500], Train Loss: 1722.5005, Test Loss: 440.6940\n",
      "Epoch [417/500], Train Loss: 1721.4955, Test Loss: 438.5352\n",
      "Epoch [418/500], Train Loss: 1721.1958, Test Loss: 442.0239\n",
      "Epoch [419/500], Train Loss: 1721.6241, Test Loss: 435.1269\n",
      "Epoch [420/500], Train Loss: 1714.7889, Test Loss: 437.4427\n",
      "Epoch [421/500], Train Loss: 1718.6115, Test Loss: 438.1869\n",
      "Epoch [422/500], Train Loss: 1720.1892, Test Loss: 441.4779\n",
      "Epoch [423/500], Train Loss: 1715.4275, Test Loss: 438.7488\n",
      "Epoch [424/500], Train Loss: 1719.2405, Test Loss: 436.2846\n",
      "Epoch [425/500], Train Loss: 1714.0717, Test Loss: 438.0358\n",
      "Epoch [426/500], Train Loss: 1714.7222, Test Loss: 438.9873\n",
      "Epoch [427/500], Train Loss: 1721.3078, Test Loss: 439.9630\n",
      "Epoch [428/500], Train Loss: 1719.8608, Test Loss: 440.2201\n",
      "Epoch [429/500], Train Loss: 1719.4356, Test Loss: 436.5314\n",
      "Epoch [430/500], Train Loss: 1716.8269, Test Loss: 440.6821\n",
      "Epoch [431/500], Train Loss: 1719.4821, Test Loss: 438.4314\n",
      "Epoch [432/500], Train Loss: 1718.7818, Test Loss: 438.0273\n",
      "Epoch [433/500], Train Loss: 1713.0649, Test Loss: 440.7546\n",
      "Epoch [434/500], Train Loss: 1720.0393, Test Loss: 437.3188\n",
      "Epoch [435/500], Train Loss: 1716.8201, Test Loss: 438.3358\n",
      "Epoch [436/500], Train Loss: 1718.8647, Test Loss: 436.7836\n",
      "Epoch [437/500], Train Loss: 1716.0132, Test Loss: 437.9801\n",
      "Epoch [438/500], Train Loss: 1718.3491, Test Loss: 439.1939\n",
      "Epoch [439/500], Train Loss: 1716.7182, Test Loss: 434.6933\n",
      "Epoch [440/500], Train Loss: 1719.1358, Test Loss: 438.5214\n",
      "Epoch [441/500], Train Loss: 1714.0312, Test Loss: 436.2063\n",
      "Epoch [442/500], Train Loss: 1715.0417, Test Loss: 438.7227\n",
      "Epoch [443/500], Train Loss: 1716.7318, Test Loss: 436.9678\n",
      "Epoch [444/500], Train Loss: 1719.7900, Test Loss: 435.0843\n",
      "Epoch [445/500], Train Loss: 1712.9564, Test Loss: 436.3486\n",
      "Epoch [446/500], Train Loss: 1714.2954, Test Loss: 440.9812\n",
      "Epoch [447/500], Train Loss: 1713.5468, Test Loss: 441.5740\n",
      "Epoch [448/500], Train Loss: 1713.9417, Test Loss: 436.1840\n",
      "Epoch [449/500], Train Loss: 1715.2011, Test Loss: 436.1130\n",
      "Epoch [450/500], Train Loss: 1714.9562, Test Loss: 437.2320\n",
      "Epoch [451/500], Train Loss: 1712.2413, Test Loss: 435.9287\n",
      "Epoch [452/500], Train Loss: 1717.4790, Test Loss: 435.3885\n",
      "Epoch [453/500], Train Loss: 1718.0899, Test Loss: 439.5636\n",
      "Epoch [454/500], Train Loss: 1716.4972, Test Loss: 437.4063\n",
      "Epoch [455/500], Train Loss: 1711.6116, Test Loss: 435.6157\n",
      "Epoch [456/500], Train Loss: 1714.7451, Test Loss: 434.3982\n",
      "Epoch [457/500], Train Loss: 1707.7997, Test Loss: 435.6503\n",
      "Epoch [458/500], Train Loss: 1714.1524, Test Loss: 437.0664\n",
      "Epoch [459/500], Train Loss: 1710.2800, Test Loss: 441.0707\n",
      "Epoch [460/500], Train Loss: 1714.8016, Test Loss: 438.6311\n",
      "Epoch [461/500], Train Loss: 1712.0446, Test Loss: 443.3875\n",
      "Epoch [462/500], Train Loss: 1712.6472, Test Loss: 438.2281\n",
      "Epoch [463/500], Train Loss: 1711.6421, Test Loss: 436.2608\n",
      "Epoch [464/500], Train Loss: 1709.4100, Test Loss: 436.1954\n",
      "Epoch [465/500], Train Loss: 1711.4552, Test Loss: 436.4712\n",
      "Epoch [466/500], Train Loss: 1710.4270, Test Loss: 433.4710\n",
      "Epoch [467/500], Train Loss: 1710.8092, Test Loss: 440.3441\n",
      "Epoch [468/500], Train Loss: 1710.5092, Test Loss: 435.7745\n",
      "Epoch [469/500], Train Loss: 1711.6485, Test Loss: 434.5518\n",
      "Epoch [470/500], Train Loss: 1709.7388, Test Loss: 437.5064\n",
      "Epoch [471/500], Train Loss: 1714.7257, Test Loss: 436.3115\n",
      "Epoch [472/500], Train Loss: 1712.0101, Test Loss: 437.3186\n",
      "Epoch [473/500], Train Loss: 1707.5373, Test Loss: 441.0229\n",
      "Epoch [474/500], Train Loss: 1707.9354, Test Loss: 434.9891\n",
      "Epoch [475/500], Train Loss: 1713.3754, Test Loss: 435.5911\n",
      "Epoch [476/500], Train Loss: 1709.8059, Test Loss: 436.5443\n",
      "Epoch [477/500], Train Loss: 1712.2959, Test Loss: 435.3953\n",
      "Epoch [478/500], Train Loss: 1707.4528, Test Loss: 437.1687\n",
      "Epoch [479/500], Train Loss: 1713.1006, Test Loss: 437.9655\n",
      "Epoch [480/500], Train Loss: 1710.5059, Test Loss: 436.5250\n",
      "Epoch [481/500], Train Loss: 1710.1474, Test Loss: 437.1742\n",
      "Epoch [482/500], Train Loss: 1708.9633, Test Loss: 442.0860\n",
      "Epoch [483/500], Train Loss: 1708.5758, Test Loss: 439.0737\n",
      "Epoch [484/500], Train Loss: 1709.8085, Test Loss: 434.4238\n",
      "Epoch [485/500], Train Loss: 1709.1590, Test Loss: 434.2439\n",
      "Epoch [486/500], Train Loss: 1705.1916, Test Loss: 435.0204\n",
      "Epoch [487/500], Train Loss: 1706.3238, Test Loss: 436.6498\n",
      "Epoch [488/500], Train Loss: 1706.2263, Test Loss: 433.0031\n",
      "Epoch [489/500], Train Loss: 1706.9010, Test Loss: 434.1193\n",
      "Epoch [490/500], Train Loss: 1713.2464, Test Loss: 437.0272\n",
      "Epoch [491/500], Train Loss: 1709.8373, Test Loss: 434.2676\n",
      "Epoch [492/500], Train Loss: 1706.8654, Test Loss: 435.3695\n",
      "Epoch [493/500], Train Loss: 1706.0370, Test Loss: 434.7294\n",
      "Epoch [494/500], Train Loss: 1709.6322, Test Loss: 436.1390\n",
      "Epoch [495/500], Train Loss: 1707.0608, Test Loss: 438.0173\n",
      "Epoch [496/500], Train Loss: 1701.9307, Test Loss: 435.0678\n",
      "Epoch [497/500], Train Loss: 1708.3977, Test Loss: 438.8025\n",
      "Epoch [498/500], Train Loss: 1704.5373, Test Loss: 433.0951\n",
      "Epoch [499/500], Train Loss: 1707.9281, Test Loss: 437.6152\n",
      "Epoch [500/500], Train Loss: 1709.8139, Test Loss: 436.5628\n",
      "Execution time: 27631.63863325119 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433.0030723810196"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Spec.npy')\n",
    "concVal = np.load(f'Dataset44_{base_name}_ForManuscript_Val_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Load representative validation spectra and concentrations\n",
    "# Load spectra of varied concentrations (all metabolites at X-mM from 0.005mm to 20mM)\n",
    "ConcSpec = np.load(f'Concentration_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "ConcConc = np.load(f'Concentration_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load uniform concentration distribution validation spectra\n",
    "UniformSpec = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "UniformConc = np.load(f'UniformDist_44met_{base_name}_ForManuscript_Conc.npy')  \n",
    "#  Load low concentration uniform concentration distribution validation spectra\n",
    "LowUniformSpec = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "LowUniformConc = np.load(f'LowUniformDist_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load tissue mimicking concentration distribution validation spectra\n",
    "MimicTissueRangeSpec = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeConc = np.load(f'MimicTissueRange_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load liver tissue mimicking concentration distribution (high relative glucose) validation spectra\n",
    "MimicTissueRangeGlucSpec = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "MimicTissueRangeGlucConc = np.load(f'MimicTissueRangeGluc_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load high dynamic range #2 validation spectra\n",
    "HighDynamicRange2Spec = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "HighDynamicRange2Conc = np.load(f'HighDynRange2_44met_{base_name}_ForManuscript_Conc.npy') \n",
    "#  Load varied SNR validation spectra\n",
    "SNR_Spec = np.load(f'SNR_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "SNR_Conc = np.load(f'SNR_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random singlet validation spectra\n",
    "Singlet_Spec = np.load(f'Singlet_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "Singlet_Conc = np.load(f'Singlet_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load random qref checker validation spectra\n",
    "QrefSensSpec = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "QrefSensConc = np.load(f'QrefSensitivity_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "#  Load other validation spectra\n",
    "OtherValSpectra = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Spec.npy')\n",
    "OtherValConc = np.load(f'OtherVal_44met_{base_name}_ForManuscript_Conc.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   \n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ConcSpec = torch.tensor(ConcSpec).float().to(device)   \n",
    "ConcConc = torch.tensor(ConcConc).float().to(device)\n",
    "UniformSpec = torch.tensor(UniformSpec).float().to(device)   \n",
    "UniformConc = torch.tensor(UniformConc).float().to(device)\n",
    "LowUniformSpec = torch.tensor(LowUniformSpec).float().to(device)   \n",
    "LowUniformConc = torch.tensor(LowUniformConc).float().to(device)\n",
    "MimicTissueRangeSpec = torch.tensor(MimicTissueRangeSpec).float().to(device)   \n",
    "MimicTissueRangeConc = torch.tensor(MimicTissueRangeConc).float().to(device)\n",
    "MimicTissueRangeGlucSpec = torch.tensor(MimicTissueRangeGlucSpec).float().to(device)   \n",
    "MimicTissueRangeGlucConc = torch.tensor(MimicTissueRangeGlucConc).float().to(device)\n",
    "HighDynamicRange2Spec = torch.tensor(HighDynamicRange2Spec).float().to(device)   \n",
    "HighDynamicRange2Conc = torch.tensor(HighDynamicRange2Conc).float().to(device)\n",
    "SNR_Spec = torch.tensor(SNR_Spec).float().to(device)   \n",
    "SNR_Conc = torch.tensor(SNR_Conc).float().to(device)\n",
    "Singlet_Spec = torch.tensor(Singlet_Spec).float().to(device)   \n",
    "Singlet_Conc = torch.tensor(Singlet_Conc).float().to(device)\n",
    "QrefSensSpec = torch.tensor(QrefSensSpec).float().to(device)   \n",
    "QrefSensConc = torch.tensor(QrefSensConc).float().to(device)\n",
    "OtherValSpectra = torch.tensor(OtherValSpectra).float().to(device)   \n",
    "OtherValConc = torch.tensor(OtherValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (conv1): Conv1d(1, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv1d(42, 42, kernel_size=(6,), stride=(1,), padding=(1,))\n",
       "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=120708, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir(base_dir+'/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  224.09715965685808\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on validation set\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(5000):\n",
    "    GroundTruth = concVal[i].cpu().numpy()\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(spectraVal[i].unsqueeze(0).unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  inf\n",
      "--------------------\n",
      "inf  - Concentrations: 0.0\n",
      "167.7  - Concentrations: 0.004999999888241291\n",
      "41.05  - Concentrations: 0.02500000037252903\n",
      "18.33  - Concentrations: 0.10000000149011612\n",
      "14.06  - Concentrations: 0.25\n",
      "12.9  - Concentrations: 0.5\n",
      "12.27  - Concentrations: 1.0\n",
      "11.73  - Concentrations: 2.5\n",
      "10.47  - Concentrations: 10.0\n",
      "10.23  - Concentrations: 20.0\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on concentration varied validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ConcConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(ConcSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ConcExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Concentrations:\",ConcConc[i][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  20.700483\n",
      "--------------------\n",
      "9.83  - Min Value: 0.6783  - Mean Value: 9.2\n",
      "64.22  - Min Value: 0.0096  - Mean Value: 10.3\n",
      "11.51  - Min Value: 0.147  - Mean Value: 10.5\n",
      "20.54  - Min Value: 0.5572  - Mean Value: 8.5\n",
      "15.78  - Min Value: 1.3567  - Mean Value: 10.6\n",
      "8.81  - Min Value: 0.6332  - Mean Value: 10.9\n",
      "30.26  - Min Value: 0.7017  - Mean Value: 11.0\n",
      "25.82  - Min Value: 0.3674  - Mean Value: 8.9\n",
      "9.18  - Min Value: 0.8387  - Mean Value: 9.8\n",
      "11.06  - Min Value: 1.0913  - Mean Value: 11.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = UniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(UniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"UniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(UniformConc[i].min().item(),4), \" - Mean Value:\", np.round(UniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  19.468258\n",
      "--------------------\n",
      "17.28  - Min Value: 0.0111  - Mean Value: 0.1\n",
      "17.85  - Min Value: 0.0103  - Mean Value: 0.1\n",
      "18.66  - Min Value: 0.0153  - Mean Value: 0.1\n",
      "22.08  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "19.4  - Min Value: 0.0089  - Mean Value: 0.1\n",
      "20.49  - Min Value: 0.0075  - Mean Value: 0.1\n",
      "18.73  - Min Value: 0.0117  - Mean Value: 0.1\n",
      "21.84  - Min Value: 0.0052  - Mean Value: 0.1\n",
      "19.5  - Min Value: 0.008  - Mean Value: 0.1\n",
      "18.83  - Min Value: 0.0134  - Mean Value: 0.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on low concentration uniform distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = LowUniformConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(LowUniformSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"LowUniformExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(LowUniformConc[i].min().item(),4), \" - Mean Value:\", np.round(LowUniformConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  55.82603\n",
      "--------------------\n",
      "87.34  - Min Value: 0.008  - Mean Value: 0.8\n",
      "46.15  - Min Value: 0.009  - Mean Value: 0.9\n",
      "116.23  - Min Value: 0.0138  - Mean Value: 1.5\n",
      "60.95  - Min Value: 0.0107  - Mean Value: 0.7\n",
      "47.32  - Min Value: 0.0191  - Mean Value: 0.7\n",
      "39.58  - Min Value: 0.0186  - Mean Value: 0.8\n",
      "37.77  - Min Value: 0.0175  - Mean Value: 0.8\n",
      "22.31  - Min Value: 0.0238  - Mean Value: 1.3\n",
      "70.74  - Min Value: 0.0168  - Mean Value: 0.7\n",
      "29.85  - Min Value: 0.0171  - Mean Value: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  78.603714\n",
      "--------------------\n",
      "82.57  - Min Value: 0.013  - Mean Value: 0.6\n",
      "40.44  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "47.56  - Min Value: 0.0115  - Mean Value: 0.4\n",
      "54.11  - Min Value: 0.0115  - Mean Value: 0.6\n",
      "78.55  - Min Value: 0.0115  - Mean Value: 1.0\n",
      "134.77  - Min Value: 0.0115  - Mean Value: 1.1\n",
      "164.92  - Min Value: 0.0115  - Mean Value: 0.8\n",
      "50.25  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "18.94  - Min Value: 0.0115  - Mean Value: 0.5\n",
      "113.92  - Min Value: 0.0115  - Mean Value: 1.1\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on tissue mimicking distribution validation spectra (high relative glucose concentration)\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = MimicTissueRangeGlucConc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(MimicTissueRangeGlucSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"MimicTissueRangeGlucExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(MimicTissueRangeGlucConc[i].min().item(),4), \" - Mean Value:\", np.round(MimicTissueRangeGlucConc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  561.7414\n",
      "--------------------\n",
      "377.17  - Min Value: 0.0062  - Mean Value: 2.1\n",
      "701.46  - Min Value: 0.006  - Mean Value: 3.7\n",
      "144.88  - Min Value: 0.0066  - Mean Value: 4.3\n",
      "966.42  - Min Value: 0.0094  - Mean Value: 4.3\n",
      "1050.12  - Min Value: 0.0068  - Mean Value: 4.9\n",
      "402.47  - Min Value: 0.005  - Mean Value: 3.8\n",
      "291.22  - Min Value: 0.0101  - Mean Value: 3.2\n",
      "508.44  - Min Value: 0.0062  - Mean Value: 3.2\n",
      "724.79  - Min Value: 0.0053  - Mean Value: 5.3\n",
      "450.44  - Min Value: 0.0054  - Mean Value: 2.5\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a further high dynamic range dataset\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = HighDynamicRange2Conc[i]\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(HighDynamicRange2Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"HighDynamicRange2Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - Min Value:\", np.round(HighDynamicRange2Conc[i].min().item(),4), \" - Mean Value:\", np.round(HighDynamicRange2Conc[i].mean().item(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  13.124164752740942\n",
      "--------------------\n",
      "13.18\n",
      "13.14\n",
      "13.04\n",
      "13.07\n",
      "13.16\n",
      "13.13\n",
      "13.16\n",
      "13.04\n",
      "13.17\n",
      "13.15\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(SNR_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SNR_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  18.23302566658619\n",
      "--------------------\n",
      "13.13\n",
      "13.21\n",
      "13.12\n",
      "14.91\n",
      "17.22\n",
      "17.49\n",
      "17.72\n",
      "22.13\n",
      "24.88\n",
      "28.53\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a dataset with singlets added at random\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(Singlet_Spec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"SingletExamples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAPE:  60.77456037661092\n",
      "--------------------\n",
      "14.92\n",
      "24.94\n",
      "34.98\n",
      "45.18\n",
      "55.29\n",
      "65.65\n",
      "76.05\n",
      "86.5\n",
      "97.03\n",
      "107.22\n"
     ]
    }
   ],
   "source": [
    "## Compute absolute percent error statistics on a examples of varying SNR\n",
    "\n",
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = 0.43\n",
    "    model_aq.eval()\n",
    "    Prediction = model_aq(QrefSensSpec[i].unsqueeze(2))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth - Prediction_cpu[0][metabolite]) / GroundTruth\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"QrefSensitivity_Examples_MAPEs.npy\", np.array(MAPEs))\n",
    "\n",
    "\n",
    "\n",
    "## Output metrics\n",
    "print('Overall MAPE: ',np.array(MAPEs).mean())\n",
    "print(\"--------------------\")\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoidal Baseline 1\n",
      "tensor([0.4841, 0.5979, 0.3888, 0.6302, 0.5077, 0.5162, 0.5130, 0.1544, 0.7401,\n",
      "        0.3847, 0.4279, 0.1306, 0.5428, 0.1342, 0.6400, 0.7099, 0.7089, 0.5126,\n",
      "        0.5350, 0.3870, 0.5263, 0.9419, 0.2557, 0.5199, 0.6329, 0.4814, 0.6324,\n",
      "        0.6156, 0.5790, 0.4576, 0.4585, 0.2017, 0.1676, 0.4220, 0.3852, 0.5262,\n",
      "        0.2044, 0.4511, 0.2093, 0.4070, 0.4084, 0.5164, 0.5930, 0.4660],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Sinusoidal Baseline 2\n",
      "tensor([0.3686, 0.5959, 0.0000, 0.5439, 0.4358, 0.4699, 0.4293, 0.1146, 0.4630,\n",
      "        0.6340, 0.4248, 0.1012, 0.8592, 0.0938, 0.8210, 0.0588, 0.9193, 0.5821,\n",
      "        0.4988, 0.3934, 0.4084, 0.3685, 0.6352, 0.5960, 0.4644, 0.4871, 0.2086,\n",
      "        0.0135, 0.5891, 0.3625, 0.4304, 0.1459, 0.1243, 0.4923, 0.6057, 0.3878,\n",
      "        0.1984, 0.8046, 0.1676, 0.3379, 0.4287, 0.8587, 0.2526, 0.1073],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "HD-Range 1 - 1s and 20s\n",
      "tensor([ 0.0000, 17.6314,  0.0000, 16.0923,  1.0210, 19.0833,  0.0000,  2.8091,\n",
      "         0.0000, 19.0322,  0.0000,  2.5084,  0.0000,  2.3243,  0.0000, 18.0621,\n",
      "         0.0000, 19.2929,  0.0000, 19.7045,  0.0000, 18.5735,  0.0000, 19.1522,\n",
      "         0.0000, 18.8591,  0.0000, 18.2655,  0.0000, 20.3431,  0.0000,  3.4280,\n",
      "         2.7318, 19.1140,  0.0000, 20.0639,  5.3895, 19.4645,  4.6618, 18.6588,\n",
      "         0.0000, 18.2117,  0.0000, 18.6474], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "HD-Range 2 - 0s and 20s\n",
      "tensor([ 0.0000, 17.6304,  0.0000, 16.0978,  1.0144, 19.0836,  0.0000,  2.8079,\n",
      "         0.0000, 19.0308,  0.0000,  2.5068,  0.0000,  2.3233,  0.0000, 18.0623,\n",
      "         0.0000, 19.2928,  0.0000, 19.7047,  0.0000, 18.5769,  0.0000, 19.1521,\n",
      "         0.0000, 18.8590,  0.0000, 18.2678,  0.0000, 20.3432,  0.0000,  3.4266,\n",
      "         2.7305, 19.1154,  0.0000, 20.0618,  5.3874, 19.4668,  4.6605, 18.6609,\n",
      "         0.0000, 18.2113,  0.0000, 18.6457], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(OtherValSpectra[0].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 1\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[1].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Sinusoidal Baseline 2\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[2].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 1 - 1s and 20s\")\n",
    "print(Pred[0])\n",
    "\n",
    "Pred = model_aq(OtherValSpectra[3].unsqueeze(2))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"HD-Range 2 - 0s and 20s\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
