{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer Encoder on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"Transformer_44met_TrainingAndValidation_1000bin_NoDropout_Dist2_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('Dataset44_Dist2_Spec.npy')\n",
    "conc1 = np.load('Dataset44_Dist2_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "#spectraVal = np.load('Dataset44_Dist2_Val_Spec.npy')\n",
    "#concVal = np.load('Dataset44_Dist2_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "#ValSpectra = np.load(\"Dataset44_Dist2_RepresentativeExamples_Spectra.npy\")\n",
    "#ValConc = np.load(\"Dataset44_Dist2_RepresentativeExamples_Concentrations.npy\")\n",
    "#ValSpecNames = np.load(\"Dataset44_Dist2_RepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 32, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.Linear(23552, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Binning\n",
    "        batch_size, seq_length = x.size()\n",
    "        num_bins = seq_length // self.input_dim\n",
    "        x = x.view(batch_size, num_bins, self.input_dim)  # (batch_size, num_bins, input_dim)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = x.permute(1, 0, 2)  # (num_bins, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)  # (num_bins, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Reconstruct original sequence\n",
    "        x = x.reshape(batch_size, num_bins * d_model)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder(x)  # (batch_size, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "input_dim = 1000  # Size of each bin\n",
    "d_model = 512     # Embedding dimension\n",
    "nhead = 1         # Number of attention heads\n",
    "num_encoder_layers = 1  # Number of transformer encoder layers\n",
    "dim_feedforward = 2048  # Feedforward dimension\n",
    "dropout = 0.0     # Dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "       \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/1000], Train Loss: 2501301.8691, Test Loss: 201939.5441\n",
      "Epoch [2/1000], Train Loss: 323645.6729, Test Loss: 45647.7993\n",
      "Epoch [3/1000], Train Loss: 131670.3552, Test Loss: 26790.8142\n",
      "Epoch [4/1000], Train Loss: 77587.7232, Test Loss: 16627.7122\n",
      "Epoch [5/1000], Train Loss: 55130.6130, Test Loss: 13039.8351\n",
      "Epoch [6/1000], Train Loss: 43308.4757, Test Loss: 12138.5606\n",
      "Epoch [7/1000], Train Loss: 35742.2276, Test Loss: 8848.8823\n",
      "Epoch [8/1000], Train Loss: 31388.0544, Test Loss: 8118.9027\n",
      "Epoch [9/1000], Train Loss: 28861.7700, Test Loss: 7630.0240\n",
      "Epoch [10/1000], Train Loss: 26049.7003, Test Loss: 7610.7641\n",
      "Epoch [11/1000], Train Loss: 23984.9459, Test Loss: 7047.6977\n",
      "Epoch [12/1000], Train Loss: 22397.2841, Test Loss: 6239.0748\n",
      "Epoch [13/1000], Train Loss: 23923.7772, Test Loss: 5872.7401\n",
      "Epoch [14/1000], Train Loss: 19784.9365, Test Loss: 5430.7344\n",
      "Epoch [15/1000], Train Loss: 19135.4040, Test Loss: 5219.4023\n",
      "Epoch [16/1000], Train Loss: 19159.6478, Test Loss: 5312.7105\n",
      "Epoch [17/1000], Train Loss: 17423.0121, Test Loss: 4799.1194\n",
      "Epoch [18/1000], Train Loss: 18152.3573, Test Loss: 5344.1045\n",
      "Epoch [19/1000], Train Loss: 15741.1314, Test Loss: 4412.5893\n",
      "Epoch [20/1000], Train Loss: 15622.5829, Test Loss: 5534.6893\n",
      "Epoch [21/1000], Train Loss: 16208.5671, Test Loss: 4674.0959\n",
      "Epoch [22/1000], Train Loss: 17506.9831, Test Loss: 4245.7275\n",
      "Epoch [23/1000], Train Loss: 13219.6551, Test Loss: 4864.7695\n",
      "Epoch [24/1000], Train Loss: 1612432.0727, Test Loss: 115025.6204\n",
      "Epoch [25/1000], Train Loss: 102212.8524, Test Loss: 9198.0181\n",
      "Epoch [26/1000], Train Loss: 27573.1634, Test Loss: 7311.6286\n",
      "Epoch [27/1000], Train Loss: 20155.8421, Test Loss: 5913.1897\n",
      "Epoch [28/1000], Train Loss: 16963.5571, Test Loss: 4935.3239\n",
      "Epoch [29/1000], Train Loss: 15102.5760, Test Loss: 4420.9305\n",
      "Epoch [30/1000], Train Loss: 14446.1097, Test Loss: 4380.4438\n",
      "Epoch [31/1000], Train Loss: 12679.0675, Test Loss: 4103.9986\n",
      "Epoch [32/1000], Train Loss: 12775.6577, Test Loss: 3856.7041\n",
      "Epoch [33/1000], Train Loss: 11963.2735, Test Loss: 4000.8309\n",
      "Epoch [34/1000], Train Loss: 12388.7914, Test Loss: 3835.0696\n",
      "Epoch [35/1000], Train Loss: 11945.3863, Test Loss: 4618.5544\n",
      "Epoch [36/1000], Train Loss: 11913.0726, Test Loss: 3906.5365\n",
      "Epoch [37/1000], Train Loss: 12771.0885, Test Loss: 4138.4662\n",
      "Epoch [38/1000], Train Loss: 12126.3252, Test Loss: 6904.6323\n",
      "Epoch [39/1000], Train Loss: 14689.0020, Test Loss: 4055.1542\n",
      "Epoch [40/1000], Train Loss: 11562.0028, Test Loss: 4020.9741\n",
      "Epoch [41/1000], Train Loss: 11470.4875, Test Loss: 3385.2104\n",
      "Epoch [42/1000], Train Loss: 11095.1777, Test Loss: 3445.4456\n",
      "Epoch [43/1000], Train Loss: 11450.8986, Test Loss: 3515.1645\n",
      "Epoch [44/1000], Train Loss: 10744.5981, Test Loss: 3401.7708\n",
      "Epoch [45/1000], Train Loss: 10561.4337, Test Loss: 3598.3706\n",
      "Epoch [46/1000], Train Loss: 10498.5938, Test Loss: 3433.4715\n",
      "Epoch [47/1000], Train Loss: 10442.0353, Test Loss: 3367.1009\n",
      "Epoch [48/1000], Train Loss: 9676.9970, Test Loss: 3331.0932\n",
      "Epoch [49/1000], Train Loss: 9859.2557, Test Loss: 3257.3466\n",
      "Epoch [50/1000], Train Loss: 716092.2467, Test Loss: 27957.0465\n",
      "Epoch [51/1000], Train Loss: 41570.3568, Test Loss: 6175.3542\n",
      "Epoch [52/1000], Train Loss: 18596.2163, Test Loss: 5467.7006\n",
      "Epoch [53/1000], Train Loss: 13705.2859, Test Loss: 3926.6004\n",
      "Epoch [54/1000], Train Loss: 12647.6029, Test Loss: 3331.5828\n",
      "Epoch [55/1000], Train Loss: 10685.1425, Test Loss: 3678.4912\n",
      "Epoch [56/1000], Train Loss: 9725.5915, Test Loss: 3763.3921\n",
      "Epoch [57/1000], Train Loss: 9137.9476, Test Loss: 3100.3696\n",
      "Epoch [58/1000], Train Loss: 8793.3117, Test Loss: 3087.0289\n",
      "Epoch [59/1000], Train Loss: 8440.7321, Test Loss: 3457.5232\n",
      "Epoch [60/1000], Train Loss: 8392.5498, Test Loss: 2790.8566\n",
      "Epoch [61/1000], Train Loss: 8651.9522, Test Loss: 4525.7768\n",
      "Epoch [62/1000], Train Loss: 26119.8205, Test Loss: 2817.2289\n",
      "Epoch [63/1000], Train Loss: 7501.1301, Test Loss: 2690.2595\n",
      "Epoch [64/1000], Train Loss: 7353.9806, Test Loss: 2620.3032\n",
      "Epoch [65/1000], Train Loss: 7762.3331, Test Loss: 2692.5312\n",
      "Epoch [66/1000], Train Loss: 2008732.7895, Test Loss: 830592.2993\n",
      "Epoch [67/1000], Train Loss: 3208063.1870, Test Loss: 791379.7231\n",
      "Epoch [68/1000], Train Loss: 3140812.7856, Test Loss: 787909.4487\n",
      "Epoch [69/1000], Train Loss: 3129316.2666, Test Loss: 784760.4595\n",
      "Epoch [70/1000], Train Loss: 1634717.0232, Test Loss: 28536.2801\n",
      "Epoch [71/1000], Train Loss: 66759.1632, Test Loss: 10539.0717\n",
      "Epoch [72/1000], Train Loss: 28355.4558, Test Loss: 7618.8599\n",
      "Epoch [73/1000], Train Loss: 21010.3685, Test Loss: 5152.7099\n",
      "Epoch [74/1000], Train Loss: 16844.3226, Test Loss: 4810.4984\n",
      "Epoch [75/1000], Train Loss: 14817.8053, Test Loss: 4481.8184\n",
      "Epoch [76/1000], Train Loss: 14209.6584, Test Loss: 3867.9149\n",
      "Epoch [77/1000], Train Loss: 12810.9065, Test Loss: 3821.4359\n",
      "Epoch [78/1000], Train Loss: 11232.6982, Test Loss: 4263.2970\n",
      "Epoch [79/1000], Train Loss: 264680.1681, Test Loss: 28354.8437\n",
      "Epoch [80/1000], Train Loss: 27726.8881, Test Loss: 4250.9483\n",
      "Epoch [81/1000], Train Loss: 13237.2939, Test Loss: 3790.3411\n",
      "Epoch [82/1000], Train Loss: 10172.2901, Test Loss: 3245.7654\n",
      "Epoch [83/1000], Train Loss: 9596.0386, Test Loss: 3067.0788\n",
      "Epoch [84/1000], Train Loss: 9405.7387, Test Loss: 3263.7446\n",
      "Epoch [85/1000], Train Loss: 8997.6488, Test Loss: 2976.9499\n",
      "Epoch [86/1000], Train Loss: 8753.8593, Test Loss: 2941.8833\n",
      "Epoch [87/1000], Train Loss: 8582.4725, Test Loss: 2937.9352\n",
      "Epoch [88/1000], Train Loss: 9023.7944, Test Loss: 3440.8602\n",
      "Epoch [89/1000], Train Loss: 20738.2436, Test Loss: 2828.6897\n",
      "Epoch [90/1000], Train Loss: 8705.2051, Test Loss: 2819.3296\n",
      "Epoch [91/1000], Train Loss: 7927.9043, Test Loss: 2952.5664\n",
      "Epoch [92/1000], Train Loss: 8071.1373, Test Loss: 2965.5949\n",
      "Epoch [93/1000], Train Loss: 8233.7611, Test Loss: 2598.1906\n",
      "Epoch [94/1000], Train Loss: 8777.7565, Test Loss: 2849.8747\n",
      "Epoch [95/1000], Train Loss: 8713.8462, Test Loss: 2832.9239\n",
      "Epoch [96/1000], Train Loss: 8167.3375, Test Loss: 2567.2253\n",
      "Epoch [97/1000], Train Loss: 100857.5438, Test Loss: 6867.5391\n",
      "Epoch [98/1000], Train Loss: 15514.5985, Test Loss: 3271.6702\n",
      "Epoch [99/1000], Train Loss: 8818.7790, Test Loss: 2774.5796\n",
      "Epoch [100/1000], Train Loss: 7435.2153, Test Loss: 2604.8329\n",
      "Epoch [101/1000], Train Loss: 7145.0613, Test Loss: 2615.2534\n",
      "Epoch [102/1000], Train Loss: 6967.1833, Test Loss: 2682.2647\n",
      "Epoch [103/1000], Train Loss: 6767.3870, Test Loss: 2689.1811\n",
      "Epoch [104/1000], Train Loss: 6740.1058, Test Loss: 2420.8069\n",
      "Epoch [105/1000], Train Loss: 6880.2744, Test Loss: 2409.3645\n",
      "Epoch [106/1000], Train Loss: 6831.6041, Test Loss: 2431.9879\n",
      "Epoch [107/1000], Train Loss: 6661.2191, Test Loss: 2542.0797\n",
      "Epoch [108/1000], Train Loss: 6724.9067, Test Loss: 2490.9791\n",
      "Epoch [109/1000], Train Loss: 6679.5884, Test Loss: 2512.3215\n",
      "Epoch [110/1000], Train Loss: 7286.8696, Test Loss: 2791.5299\n",
      "Epoch [111/1000], Train Loss: 7327.5587, Test Loss: 2476.8509\n",
      "Epoch [112/1000], Train Loss: 7531.0799, Test Loss: 2713.3959\n",
      "Epoch [113/1000], Train Loss: 7607.5131, Test Loss: 2593.2266\n",
      "Epoch [114/1000], Train Loss: 10148.5664, Test Loss: 2547.1223\n",
      "Epoch [115/1000], Train Loss: 6746.0733, Test Loss: 2382.3038\n",
      "Epoch [116/1000], Train Loss: 7321.3137, Test Loss: 4568.6943\n",
      "Epoch [117/1000], Train Loss: 7561.7944, Test Loss: 2343.8082\n",
      "Epoch [118/1000], Train Loss: 6180.1491, Test Loss: 2349.3165\n",
      "Epoch [119/1000], Train Loss: 6518.3344, Test Loss: 2560.7921\n",
      "Epoch [120/1000], Train Loss: 6626.8721, Test Loss: 2401.2871\n",
      "Epoch [121/1000], Train Loss: 6519.9819, Test Loss: 2719.5998\n",
      "Epoch [122/1000], Train Loss: 6572.0104, Test Loss: 2722.9901\n",
      "Epoch [123/1000], Train Loss: 6428.4624, Test Loss: 2854.8497\n",
      "Epoch [124/1000], Train Loss: 6277.2169, Test Loss: 2436.2800\n",
      "Epoch [125/1000], Train Loss: 6559.6976, Test Loss: 2831.3855\n",
      "Epoch [126/1000], Train Loss: 6559.3781, Test Loss: 2296.4891\n",
      "Epoch [127/1000], Train Loss: 8656.6644, Test Loss: 2154.6224\n",
      "Epoch [128/1000], Train Loss: 5335.0945, Test Loss: 2225.1674\n",
      "Epoch [129/1000], Train Loss: 5540.7474, Test Loss: 2294.7556\n",
      "Epoch [130/1000], Train Loss: 5681.9093, Test Loss: 2297.4801\n",
      "Epoch [131/1000], Train Loss: 6745.2931, Test Loss: 2134.9435\n",
      "Epoch [132/1000], Train Loss: 5568.3499, Test Loss: 2166.9451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [133/1000], Train Loss: 11310.2466, Test Loss: 2323.2029\n",
      "Epoch [134/1000], Train Loss: 5534.9705, Test Loss: 2299.9572\n",
      "Epoch [135/1000], Train Loss: 6121.2775, Test Loss: 2200.7209\n",
      "Epoch [136/1000], Train Loss: 5137.4124, Test Loss: 2601.7102\n",
      "Epoch [137/1000], Train Loss: 5334.2364, Test Loss: 2343.5009\n",
      "Epoch [138/1000], Train Loss: 6378.7819, Test Loss: 2161.3324\n",
      "Epoch [139/1000], Train Loss: 5387.0544, Test Loss: 2087.9267\n",
      "Epoch [140/1000], Train Loss: 4937.4984, Test Loss: 2110.3470\n",
      "Epoch [141/1000], Train Loss: 5115.0554, Test Loss: 2224.7428\n",
      "Epoch [142/1000], Train Loss: 5536.6491, Test Loss: 2792.3576\n",
      "Epoch [143/1000], Train Loss: 8697.7681, Test Loss: 2113.5787\n",
      "Epoch [144/1000], Train Loss: 4894.5195, Test Loss: 1985.5926\n",
      "Epoch [145/1000], Train Loss: 4660.4420, Test Loss: 2130.3045\n",
      "Epoch [146/1000], Train Loss: 5203.1326, Test Loss: 2583.4164\n",
      "Epoch [147/1000], Train Loss: 5876.7357, Test Loss: 2060.7555\n",
      "Epoch [148/1000], Train Loss: 6475.9103, Test Loss: 2442.5105\n",
      "Epoch [149/1000], Train Loss: 5058.2404, Test Loss: 2386.9521\n",
      "Epoch [150/1000], Train Loss: 5024.0207, Test Loss: 2165.9006\n",
      "Epoch [151/1000], Train Loss: 4826.8859, Test Loss: 2902.1339\n",
      "Epoch [152/1000], Train Loss: 7882.6302, Test Loss: 7758.9239\n",
      "Epoch [153/1000], Train Loss: 10992.4186, Test Loss: 2496.5810\n",
      "Epoch [154/1000], Train Loss: 5808.9467, Test Loss: 2301.3187\n",
      "Epoch [155/1000], Train Loss: 5091.8527, Test Loss: 2274.6397\n",
      "Epoch [156/1000], Train Loss: 4844.0615, Test Loss: 2257.0544\n",
      "Epoch [157/1000], Train Loss: 4541.6470, Test Loss: 2160.4828\n",
      "Epoch [158/1000], Train Loss: 4743.2348, Test Loss: 1993.8972\n",
      "Epoch [159/1000], Train Loss: 4837.3248, Test Loss: 2493.4889\n",
      "Epoch [160/1000], Train Loss: 5598.5916, Test Loss: 2525.6354\n",
      "Epoch [161/1000], Train Loss: 6668.3551, Test Loss: 2180.9626\n",
      "Epoch [162/1000], Train Loss: 4596.7164, Test Loss: 2094.9780\n",
      "Epoch [163/1000], Train Loss: 4586.6661, Test Loss: 2229.6117\n",
      "Epoch [164/1000], Train Loss: 4902.3477, Test Loss: 2270.3705\n",
      "Epoch [165/1000], Train Loss: 5162.8487, Test Loss: 2074.1173\n",
      "Epoch [166/1000], Train Loss: 5041.6262, Test Loss: 2922.0063\n",
      "Epoch [167/1000], Train Loss: 6644.0906, Test Loss: 2344.3931\n",
      "Epoch [168/1000], Train Loss: 6823.1942, Test Loss: 2386.5058\n",
      "Epoch [169/1000], Train Loss: 4820.0031, Test Loss: 1995.9197\n",
      "Epoch [170/1000], Train Loss: 4093.7378, Test Loss: 2054.6748\n",
      "Epoch [171/1000], Train Loss: 7816.0458, Test Loss: 5044.9914\n",
      "Epoch [172/1000], Train Loss: 16392.5857, Test Loss: 2264.5208\n",
      "Epoch [173/1000], Train Loss: 4517.3164, Test Loss: 2094.4412\n",
      "Epoch [174/1000], Train Loss: 4665.2054, Test Loss: 2821.0146\n",
      "Epoch [175/1000], Train Loss: 5765.1191, Test Loss: 2385.1681\n",
      "Epoch [176/1000], Train Loss: 5788.4652, Test Loss: 2372.0926\n",
      "Epoch [177/1000], Train Loss: 6317.3246, Test Loss: 3175.1396\n",
      "Epoch [178/1000], Train Loss: 10853.7673, Test Loss: 2269.2195\n",
      "Epoch [179/1000], Train Loss: 12898.6963, Test Loss: 3574.0830\n",
      "Epoch [180/1000], Train Loss: 8950.7865, Test Loss: 2312.6249\n",
      "Epoch [181/1000], Train Loss: 4513.5049, Test Loss: 1960.9910\n",
      "Epoch [182/1000], Train Loss: 7623.9898, Test Loss: 3487.9462\n",
      "Epoch [183/1000], Train Loss: 6645.9309, Test Loss: 2222.6992\n",
      "Epoch [184/1000], Train Loss: 5722.2126, Test Loss: 2094.0472\n",
      "Epoch [185/1000], Train Loss: 4249.6014, Test Loss: 2424.8728\n",
      "Epoch [186/1000], Train Loss: 4616.1211, Test Loss: 2151.8426\n",
      "Epoch [187/1000], Train Loss: 4507.6614, Test Loss: 2107.3590\n",
      "Epoch [188/1000], Train Loss: 5290.2129, Test Loss: 2403.6480\n",
      "Epoch [189/1000], Train Loss: 12034.6896, Test Loss: 3024.2199\n",
      "Epoch [190/1000], Train Loss: 7486.8277, Test Loss: 2550.1478\n",
      "Epoch [191/1000], Train Loss: 4471.8969, Test Loss: 2120.4988\n",
      "Epoch [192/1000], Train Loss: 3883.4079, Test Loss: 1881.1301\n",
      "Epoch [193/1000], Train Loss: 3659.2682, Test Loss: 1885.8616\n",
      "Epoch [194/1000], Train Loss: 3871.7876, Test Loss: 2067.0216\n",
      "Epoch [195/1000], Train Loss: 5437.7663, Test Loss: 2269.7807\n",
      "Epoch [196/1000], Train Loss: 5050.3828, Test Loss: 2068.7406\n",
      "Epoch [197/1000], Train Loss: 4287.9000, Test Loss: 1950.5194\n",
      "Epoch [198/1000], Train Loss: 4661.3239, Test Loss: 1916.4022\n",
      "Epoch [199/1000], Train Loss: 4016.0516, Test Loss: 2043.3674\n",
      "Epoch [200/1000], Train Loss: 4170.5232, Test Loss: 1916.7400\n",
      "Epoch [201/1000], Train Loss: 4183.8896, Test Loss: 2226.5449\n",
      "Epoch [202/1000], Train Loss: 4301.6703, Test Loss: 3380.7451\n",
      "Epoch [203/1000], Train Loss: 20132.6334, Test Loss: 2233.9278\n",
      "Epoch [204/1000], Train Loss: 4624.9352, Test Loss: 1925.8406\n",
      "Epoch [205/1000], Train Loss: 4571.4864, Test Loss: 2557.8094\n",
      "Epoch [206/1000], Train Loss: 5550.1760, Test Loss: 5383.2835\n",
      "Epoch [207/1000], Train Loss: 5539.0620, Test Loss: 2011.3211\n",
      "Epoch [208/1000], Train Loss: 3881.7997, Test Loss: 1835.7443\n",
      "Epoch [209/1000], Train Loss: 3447.9326, Test Loss: 1902.8716\n",
      "Epoch [210/1000], Train Loss: 3907.5855, Test Loss: 1965.4456\n",
      "Epoch [211/1000], Train Loss: 7786.1206, Test Loss: 1899.6042\n",
      "Epoch [212/1000], Train Loss: 5998.1589, Test Loss: 1987.1546\n",
      "Epoch [213/1000], Train Loss: 4237.5303, Test Loss: 2368.4974\n",
      "Epoch [214/1000], Train Loss: 6101.9106, Test Loss: 2050.9682\n",
      "Epoch [215/1000], Train Loss: 4796.2590, Test Loss: 1796.8610\n",
      "Epoch [216/1000], Train Loss: 3509.0534, Test Loss: 1977.7680\n",
      "Epoch [217/1000], Train Loss: 5590.7420, Test Loss: 3396.4468\n",
      "Epoch [218/1000], Train Loss: 8663.1618, Test Loss: 3085.9738\n",
      "Epoch [219/1000], Train Loss: 5977.5786, Test Loss: 2972.6726\n",
      "Epoch [220/1000], Train Loss: 4354.3580, Test Loss: 1827.0592\n",
      "Epoch [221/1000], Train Loss: 3278.4190, Test Loss: 1809.5317\n",
      "Epoch [222/1000], Train Loss: 3488.4829, Test Loss: 1804.6260\n",
      "Epoch [223/1000], Train Loss: 4042.8256, Test Loss: 1913.0277\n",
      "Epoch [224/1000], Train Loss: 4185.0549, Test Loss: 2050.0937\n",
      "Epoch [225/1000], Train Loss: 4031.0230, Test Loss: 1914.5745\n",
      "Epoch [226/1000], Train Loss: 4100.5835, Test Loss: 2109.1745\n",
      "Epoch [227/1000], Train Loss: 4333.5912, Test Loss: 2082.8164\n",
      "Epoch [228/1000], Train Loss: 4646.4594, Test Loss: 2323.6552\n",
      "Epoch [229/1000], Train Loss: 4660.2001, Test Loss: 2066.4908\n",
      "Epoch [230/1000], Train Loss: 3748.3751, Test Loss: 2033.1515\n",
      "Epoch [231/1000], Train Loss: 4607.1753, Test Loss: 2032.4520\n",
      "Epoch [232/1000], Train Loss: 4489.7632, Test Loss: 2953.6231\n",
      "Epoch [233/1000], Train Loss: 4137.3233, Test Loss: 1931.6955\n",
      "Epoch [234/1000], Train Loss: 3848.9780, Test Loss: 1949.6148\n",
      "Epoch [235/1000], Train Loss: 3720.8687, Test Loss: 2024.2978\n",
      "Epoch [236/1000], Train Loss: 4517.5928, Test Loss: 2190.9151\n",
      "Epoch [237/1000], Train Loss: 4539.4795, Test Loss: 1870.9960\n",
      "Epoch [238/1000], Train Loss: 3581.5348, Test Loss: 1928.2624\n",
      "Epoch [239/1000], Train Loss: 8460.3404, Test Loss: 1874.9759\n",
      "Epoch [240/1000], Train Loss: 3484.7860, Test Loss: 1900.9328\n",
      "Epoch [241/1000], Train Loss: 3299.5791, Test Loss: 1829.9360\n",
      "Epoch [242/1000], Train Loss: 3365.8517, Test Loss: 1844.8420\n",
      "Epoch [243/1000], Train Loss: 3407.0834, Test Loss: 1922.4049\n",
      "Epoch [244/1000], Train Loss: 3553.6317, Test Loss: 1793.7075\n",
      "Epoch [245/1000], Train Loss: 3458.7263, Test Loss: 1987.6254\n",
      "Epoch [246/1000], Train Loss: 3669.0996, Test Loss: 2002.4724\n",
      "Epoch [247/1000], Train Loss: 4235.7603, Test Loss: 2002.2105\n",
      "Epoch [248/1000], Train Loss: 4155.7746, Test Loss: 1956.5122\n",
      "Epoch [249/1000], Train Loss: 3522.3031, Test Loss: 1877.7261\n",
      "Epoch [250/1000], Train Loss: 3503.0270, Test Loss: 1751.0231\n",
      "Epoch [251/1000], Train Loss: 3930.9661, Test Loss: 1944.1372\n",
      "Epoch [252/1000], Train Loss: 3993.7286, Test Loss: 2082.0176\n",
      "Epoch [253/1000], Train Loss: 3438.1885, Test Loss: 1948.6145\n",
      "Epoch [254/1000], Train Loss: 3896.6741, Test Loss: 2301.5268\n",
      "Epoch [255/1000], Train Loss: 4575.9855, Test Loss: 1895.1883\n",
      "Epoch [256/1000], Train Loss: 3205.8103, Test Loss: 1828.0974\n",
      "Epoch [257/1000], Train Loss: 3307.7627, Test Loss: 1856.8246\n",
      "Epoch [258/1000], Train Loss: 3389.8189, Test Loss: 2040.1369\n",
      "Epoch [259/1000], Train Loss: 3476.1184, Test Loss: 1817.7153\n",
      "Epoch [260/1000], Train Loss: 4037.3438, Test Loss: 1926.4596\n",
      "Epoch [261/1000], Train Loss: 3569.6909, Test Loss: 2041.3218\n",
      "Epoch [262/1000], Train Loss: 3431.3655, Test Loss: 1780.7925\n",
      "Epoch [263/1000], Train Loss: 3462.5940, Test Loss: 1903.8727\n",
      "Epoch [264/1000], Train Loss: 3347.7062, Test Loss: 1764.3628\n",
      "Epoch [265/1000], Train Loss: 3308.4395, Test Loss: 2027.1713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [266/1000], Train Loss: 3547.1579, Test Loss: 1845.8026\n",
      "Epoch [267/1000], Train Loss: 3711.0413, Test Loss: 1808.3753\n",
      "Epoch [268/1000], Train Loss: 3427.3395, Test Loss: 1845.2663\n",
      "Epoch [269/1000], Train Loss: 3743.5359, Test Loss: 1820.6923\n",
      "Epoch [270/1000], Train Loss: 3415.3262, Test Loss: 1840.8813\n",
      "Epoch [271/1000], Train Loss: 3318.3302, Test Loss: 1896.9539\n",
      "Epoch [272/1000], Train Loss: 3426.5460, Test Loss: 1892.7043\n",
      "Epoch [273/1000], Train Loss: 3184.7185, Test Loss: 2366.7871\n",
      "Epoch [274/1000], Train Loss: 3500.2890, Test Loss: 1814.9453\n",
      "Epoch [275/1000], Train Loss: 3451.2501, Test Loss: 1937.4057\n",
      "Epoch [276/1000], Train Loss: 3166.6818, Test Loss: 1845.2139\n",
      "Epoch [277/1000], Train Loss: 3198.9744, Test Loss: 2083.8420\n",
      "Epoch [278/1000], Train Loss: 3292.2827, Test Loss: 1775.4558\n",
      "Epoch [279/1000], Train Loss: 3055.0736, Test Loss: 1990.7668\n",
      "Epoch [280/1000], Train Loss: 3323.7825, Test Loss: 1925.0512\n",
      "Epoch [281/1000], Train Loss: 5910.7855, Test Loss: 1927.5221\n",
      "Epoch [282/1000], Train Loss: 3231.9081, Test Loss: 1828.9561\n",
      "Epoch [283/1000], Train Loss: 2796.0587, Test Loss: 1785.2565\n",
      "Epoch [284/1000], Train Loss: 2869.8086, Test Loss: 1819.5113\n",
      "Epoch [285/1000], Train Loss: 2819.7104, Test Loss: 1880.4063\n",
      "Epoch [286/1000], Train Loss: 2995.5510, Test Loss: 1819.4371\n",
      "Epoch [287/1000], Train Loss: 4747.8153, Test Loss: 2199.1909\n",
      "Epoch [288/1000], Train Loss: 3162.9945, Test Loss: 1769.4312\n",
      "Epoch [289/1000], Train Loss: 2639.7997, Test Loss: 1805.5139\n",
      "Epoch [290/1000], Train Loss: 2877.5353, Test Loss: 1936.3799\n",
      "Epoch [291/1000], Train Loss: 3138.7317, Test Loss: 1713.6160\n",
      "Epoch [292/1000], Train Loss: 3103.2775, Test Loss: 2062.8733\n",
      "Epoch [293/1000], Train Loss: 3720.4844, Test Loss: 1814.9832\n",
      "Epoch [294/1000], Train Loss: 2816.0122, Test Loss: 1798.1828\n",
      "Epoch [295/1000], Train Loss: 3017.2427, Test Loss: 1706.7620\n",
      "Epoch [296/1000], Train Loss: 3466.2394, Test Loss: 1735.1233\n",
      "Epoch [297/1000], Train Loss: 2774.6202, Test Loss: 1821.3132\n",
      "Epoch [298/1000], Train Loss: 2971.8154, Test Loss: 1945.9932\n",
      "Epoch [299/1000], Train Loss: 3025.5837, Test Loss: 1827.3123\n",
      "Epoch [300/1000], Train Loss: 3597.0719, Test Loss: 1937.8775\n",
      "Epoch [301/1000], Train Loss: 3057.3358, Test Loss: 1737.2918\n",
      "Epoch [302/1000], Train Loss: 3895.5397, Test Loss: 2138.4330\n",
      "Epoch [303/1000], Train Loss: 2916.4508, Test Loss: 1737.0493\n",
      "Epoch [304/1000], Train Loss: 2682.5540, Test Loss: 1758.2698\n",
      "Epoch [305/1000], Train Loss: 2640.8361, Test Loss: 1890.4137\n",
      "Epoch [306/1000], Train Loss: 3725.4723, Test Loss: 1915.8193\n",
      "Epoch [307/1000], Train Loss: 2800.5558, Test Loss: 1907.2450\n",
      "Epoch [308/1000], Train Loss: 2717.6373, Test Loss: 1792.9447\n",
      "Epoch [309/1000], Train Loss: 2660.9283, Test Loss: 1721.9585\n",
      "Epoch [310/1000], Train Loss: 3143.9561, Test Loss: 1867.3373\n",
      "Epoch [311/1000], Train Loss: 2763.8352, Test Loss: 1993.2698\n",
      "Epoch [312/1000], Train Loss: 3163.8398, Test Loss: 1689.0478\n",
      "Epoch [313/1000], Train Loss: 2804.1734, Test Loss: 2069.1173\n",
      "Epoch [314/1000], Train Loss: 3275.3992, Test Loss: 1724.0345\n",
      "Epoch [315/1000], Train Loss: 2686.9606, Test Loss: 1700.9075\n",
      "Epoch [316/1000], Train Loss: 3757.7087, Test Loss: 2316.4658\n",
      "Epoch [317/1000], Train Loss: 2637.1728, Test Loss: 1719.9984\n",
      "Epoch [318/1000], Train Loss: 2540.6352, Test Loss: 1798.0350\n",
      "Epoch [319/1000], Train Loss: 3037.7095, Test Loss: 1728.9621\n",
      "Epoch [320/1000], Train Loss: 2520.0333, Test Loss: 1773.1008\n",
      "Epoch [321/1000], Train Loss: 2763.6917, Test Loss: 1961.6474\n",
      "Epoch [322/1000], Train Loss: 2851.5391, Test Loss: 1912.5617\n",
      "Epoch [323/1000], Train Loss: 3067.2359, Test Loss: 3144.4453\n",
      "Epoch [324/1000], Train Loss: 10407.0310, Test Loss: 1682.3462\n",
      "Epoch [325/1000], Train Loss: 5941.6229, Test Loss: 1728.6433\n",
      "Epoch [326/1000], Train Loss: 2247.9205, Test Loss: 1656.6804\n",
      "Epoch [327/1000], Train Loss: 2306.0677, Test Loss: 1752.6954\n",
      "Epoch [328/1000], Train Loss: 2275.0943, Test Loss: 1754.4513\n",
      "Epoch [329/1000], Train Loss: 2317.6099, Test Loss: 1741.8595\n",
      "Epoch [330/1000], Train Loss: 2449.1286, Test Loss: 1746.0467\n",
      "Epoch [331/1000], Train Loss: 2477.5118, Test Loss: 1827.8904\n",
      "Epoch [332/1000], Train Loss: 2701.5565, Test Loss: 2099.4042\n",
      "Epoch [333/1000], Train Loss: 3334.4377, Test Loss: 1784.4945\n",
      "Epoch [334/1000], Train Loss: 2427.1398, Test Loss: 1678.3184\n",
      "Epoch [335/1000], Train Loss: 2654.3116, Test Loss: 1883.7622\n",
      "Epoch [336/1000], Train Loss: 2609.8405, Test Loss: 1743.2248\n",
      "Epoch [337/1000], Train Loss: 2629.5751, Test Loss: 1730.0081\n",
      "Epoch [338/1000], Train Loss: 2576.1800, Test Loss: 2056.0169\n",
      "Epoch [339/1000], Train Loss: 2836.5230, Test Loss: 1846.0369\n",
      "Epoch [340/1000], Train Loss: 2594.6820, Test Loss: 1786.0174\n",
      "Epoch [341/1000], Train Loss: 2809.9983, Test Loss: 1810.6193\n",
      "Epoch [342/1000], Train Loss: 2716.8414, Test Loss: 1801.6526\n",
      "Epoch [343/1000], Train Loss: 2530.2475, Test Loss: 1808.7358\n",
      "Epoch [344/1000], Train Loss: 2554.1264, Test Loss: 1860.6928\n",
      "Epoch [345/1000], Train Loss: 2756.0659, Test Loss: 1780.5712\n",
      "Epoch [346/1000], Train Loss: 2458.9410, Test Loss: 1736.5265\n",
      "Epoch [347/1000], Train Loss: 2878.6601, Test Loss: 1807.3753\n",
      "Epoch [348/1000], Train Loss: 2556.7785, Test Loss: 1777.5575\n",
      "Epoch [349/1000], Train Loss: 2574.4199, Test Loss: 1769.9328\n",
      "Epoch [350/1000], Train Loss: 2475.3854, Test Loss: 1906.3420\n",
      "Epoch [351/1000], Train Loss: 2470.7743, Test Loss: 1874.0464\n",
      "Epoch [352/1000], Train Loss: 2703.3712, Test Loss: 1792.5120\n",
      "Epoch [353/1000], Train Loss: 2663.5851, Test Loss: 1759.8724\n",
      "Epoch [354/1000], Train Loss: 3097.1091, Test Loss: 2280.0682\n",
      "Epoch [355/1000], Train Loss: 2983.0227, Test Loss: 1707.8392\n",
      "Epoch [356/1000], Train Loss: 2146.6651, Test Loss: 1669.9733\n",
      "Epoch [357/1000], Train Loss: 2229.7485, Test Loss: 1847.3158\n",
      "Epoch [358/1000], Train Loss: 2392.3728, Test Loss: 1830.6515\n",
      "Epoch [359/1000], Train Loss: 2763.8305, Test Loss: 1731.8416\n",
      "Epoch [360/1000], Train Loss: 2546.8285, Test Loss: 1840.1506\n",
      "Epoch [361/1000], Train Loss: 2803.4452, Test Loss: 1743.3115\n",
      "Epoch [362/1000], Train Loss: 2504.5848, Test Loss: 1716.2542\n",
      "Epoch [363/1000], Train Loss: 2191.5076, Test Loss: 1679.8693\n",
      "Epoch [364/1000], Train Loss: 2250.7035, Test Loss: 1833.8869\n",
      "Epoch [365/1000], Train Loss: 4153.9334, Test Loss: 1769.7638\n",
      "Epoch [366/1000], Train Loss: 2184.4066, Test Loss: 1799.0115\n",
      "Epoch [367/1000], Train Loss: 2104.0950, Test Loss: 1703.4274\n",
      "Epoch [368/1000], Train Loss: 2347.2508, Test Loss: 1743.2093\n",
      "Epoch [369/1000], Train Loss: 2510.8910, Test Loss: 1762.5608\n",
      "Epoch [370/1000], Train Loss: 2203.5481, Test Loss: 1731.8874\n",
      "Epoch [371/1000], Train Loss: 3056.9724, Test Loss: 1856.3266\n",
      "Epoch [372/1000], Train Loss: 2239.3380, Test Loss: 1681.6398\n",
      "Epoch [373/1000], Train Loss: 2065.7340, Test Loss: 1834.7079\n",
      "Epoch [374/1000], Train Loss: 2271.8336, Test Loss: 1729.6764\n",
      "Epoch [375/1000], Train Loss: 2563.2562, Test Loss: 1755.4636\n",
      "Epoch [376/1000], Train Loss: 3408.0110, Test Loss: 2252.1580\n",
      "Epoch [377/1000], Train Loss: 2457.2023, Test Loss: 1725.4878\n",
      "Epoch [378/1000], Train Loss: 2066.8024, Test Loss: 1703.2399\n",
      "Epoch [379/1000], Train Loss: 2070.7952, Test Loss: 1747.4324\n",
      "Epoch [380/1000], Train Loss: 2148.7604, Test Loss: 1766.3485\n",
      "Epoch [381/1000], Train Loss: 2333.1877, Test Loss: 2143.9438\n",
      "Epoch [382/1000], Train Loss: 3014.0406, Test Loss: 1679.9610\n",
      "Epoch [383/1000], Train Loss: 2013.4115, Test Loss: 1741.4222\n",
      "Epoch [384/1000], Train Loss: 2056.2088, Test Loss: 1684.3435\n",
      "Epoch [385/1000], Train Loss: 2296.6077, Test Loss: 1757.9473\n",
      "Epoch [386/1000], Train Loss: 2396.1438, Test Loss: 2299.5915\n",
      "Epoch [387/1000], Train Loss: 3427.1372, Test Loss: 2658.7902\n",
      "Epoch [388/1000], Train Loss: 2700.4728, Test Loss: 1724.7645\n",
      "Epoch [389/1000], Train Loss: 2027.5650, Test Loss: 1706.6247\n",
      "Epoch [390/1000], Train Loss: 1878.4825, Test Loss: 1730.9802\n",
      "Epoch [391/1000], Train Loss: 2526.0208, Test Loss: 1840.0172\n",
      "Epoch [392/1000], Train Loss: 2300.5747, Test Loss: 1777.8173\n",
      "Epoch [393/1000], Train Loss: 2122.3419, Test Loss: 1727.3081\n",
      "Epoch [394/1000], Train Loss: 2523.3863, Test Loss: 1702.8803\n",
      "Epoch [395/1000], Train Loss: 2051.0903, Test Loss: 1671.9590\n",
      "Epoch [396/1000], Train Loss: 2050.9614, Test Loss: 2038.7929\n",
      "Epoch [397/1000], Train Loss: 3064.2861, Test Loss: 1764.3503\n",
      "Epoch [398/1000], Train Loss: 2412.0245, Test Loss: 2017.6225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [399/1000], Train Loss: 2033.4170, Test Loss: 1696.3643\n",
      "Epoch [400/1000], Train Loss: 1926.2298, Test Loss: 1689.1967\n",
      "Epoch [401/1000], Train Loss: 1961.0799, Test Loss: 1754.6361\n",
      "Epoch [402/1000], Train Loss: 1986.0562, Test Loss: 1750.5343\n",
      "Epoch [403/1000], Train Loss: 2821.3689, Test Loss: 1866.8430\n",
      "Epoch [404/1000], Train Loss: 2104.3975, Test Loss: 1721.1667\n",
      "Epoch [405/1000], Train Loss: 2095.6095, Test Loss: 1780.8906\n",
      "Epoch [406/1000], Train Loss: 2204.7414, Test Loss: 1768.1805\n",
      "Epoch [407/1000], Train Loss: 1989.8663, Test Loss: 1762.3646\n",
      "Epoch [408/1000], Train Loss: 2487.6855, Test Loss: 1814.3467\n",
      "Epoch [409/1000], Train Loss: 1953.6172, Test Loss: 2027.8706\n",
      "Epoch [410/1000], Train Loss: 2936.3230, Test Loss: 1785.5784\n",
      "Epoch [411/1000], Train Loss: 1965.3008, Test Loss: 1755.0063\n",
      "Epoch [412/1000], Train Loss: 1852.4122, Test Loss: 1697.0717\n",
      "Epoch [413/1000], Train Loss: 1982.7002, Test Loss: 1829.4472\n",
      "Epoch [414/1000], Train Loss: 2115.4189, Test Loss: 1778.4684\n",
      "Epoch [415/1000], Train Loss: 2077.3590, Test Loss: 2094.5834\n",
      "Epoch [416/1000], Train Loss: 2180.6919, Test Loss: 1850.3301\n",
      "Epoch [417/1000], Train Loss: 2097.0619, Test Loss: 2020.1571\n",
      "Epoch [418/1000], Train Loss: 2162.9733, Test Loss: 2022.0437\n",
      "Epoch [419/1000], Train Loss: 2322.4657, Test Loss: 1798.3624\n",
      "Epoch [420/1000], Train Loss: 1925.5731, Test Loss: 1890.9150\n",
      "Epoch [421/1000], Train Loss: 2277.9957, Test Loss: 2244.0911\n",
      "Epoch [422/1000], Train Loss: 2264.1830, Test Loss: 1792.7028\n",
      "Epoch [423/1000], Train Loss: 1985.5767, Test Loss: 1915.7517\n",
      "Epoch [424/1000], Train Loss: 3169.0752, Test Loss: 2306.9000\n",
      "Epoch [425/1000], Train Loss: 3661.8817, Test Loss: 2102.7579\n",
      "Epoch [426/1000], Train Loss: 3973.7190, Test Loss: 2123.0730\n",
      "Epoch [427/1000], Train Loss: 2602.8391, Test Loss: 1897.3257\n",
      "Epoch [428/1000], Train Loss: 2428.3569, Test Loss: 1974.6903\n",
      "Epoch [429/1000], Train Loss: 2684.3912, Test Loss: 2089.2521\n",
      "Epoch [430/1000], Train Loss: 4125.1399, Test Loss: 2504.7935\n",
      "Epoch [431/1000], Train Loss: 3665.8086, Test Loss: 2306.0673\n",
      "Epoch [432/1000], Train Loss: 2634.9789, Test Loss: 1730.7009\n",
      "Epoch [433/1000], Train Loss: 1875.8610, Test Loss: 1714.3890\n",
      "Epoch [434/1000], Train Loss: 1860.4778, Test Loss: 1759.6999\n",
      "Epoch [435/1000], Train Loss: 2010.5223, Test Loss: 1817.3872\n",
      "Epoch [436/1000], Train Loss: 2343.2786, Test Loss: 1906.1031\n",
      "Epoch [437/1000], Train Loss: 1993.7068, Test Loss: 1855.3891\n",
      "Epoch [438/1000], Train Loss: 2186.5162, Test Loss: 1783.3185\n",
      "Epoch [439/1000], Train Loss: 1881.6068, Test Loss: 1711.3342\n",
      "Epoch [440/1000], Train Loss: 1937.6982, Test Loss: 2160.9034\n",
      "Epoch [441/1000], Train Loss: 2893.8498, Test Loss: 1750.7736\n",
      "Epoch [442/1000], Train Loss: 1910.2234, Test Loss: 1716.8401\n",
      "Epoch [443/1000], Train Loss: 2196.6775, Test Loss: 1727.9981\n",
      "Epoch [444/1000], Train Loss: 1688.0580, Test Loss: 1717.5081\n",
      "Epoch [445/1000], Train Loss: 1836.7180, Test Loss: 1817.2817\n",
      "Epoch [446/1000], Train Loss: 1897.9739, Test Loss: 1736.3088\n",
      "Epoch [447/1000], Train Loss: 1767.2080, Test Loss: 1855.7992\n",
      "Epoch [448/1000], Train Loss: 2048.1711, Test Loss: 1846.9503\n",
      "Epoch [449/1000], Train Loss: 2627.2015, Test Loss: 1766.3644\n",
      "Epoch [450/1000], Train Loss: 2192.0118, Test Loss: 1888.9745\n",
      "Epoch [451/1000], Train Loss: 1812.9392, Test Loss: 1773.8588\n",
      "Epoch [452/1000], Train Loss: 1803.3750, Test Loss: 1772.6716\n",
      "Epoch [453/1000], Train Loss: 1756.9837, Test Loss: 1769.9162\n",
      "Epoch [454/1000], Train Loss: 1842.0974, Test Loss: 1810.8273\n",
      "Epoch [455/1000], Train Loss: 3796.8218, Test Loss: 1836.5505\n",
      "Epoch [456/1000], Train Loss: 1887.7929, Test Loss: 1773.6804\n",
      "Epoch [457/1000], Train Loss: 1601.3148, Test Loss: 1693.2612\n",
      "Epoch [458/1000], Train Loss: 1592.7551, Test Loss: 1882.3489\n",
      "Epoch [459/1000], Train Loss: 1641.4314, Test Loss: 1746.9518\n",
      "Epoch [460/1000], Train Loss: 1829.5698, Test Loss: 1798.1194\n",
      "Epoch [461/1000], Train Loss: 1919.8643, Test Loss: 2476.5500\n",
      "Epoch [462/1000], Train Loss: 2430.3391, Test Loss: 1757.3543\n",
      "Epoch [463/1000], Train Loss: 1686.7552, Test Loss: 1726.8736\n",
      "Epoch [464/1000], Train Loss: 1612.0043, Test Loss: 2054.2679\n",
      "Epoch [465/1000], Train Loss: 2002.0304, Test Loss: 1775.9686\n",
      "Epoch [466/1000], Train Loss: 2084.1870, Test Loss: 1771.2920\n",
      "Epoch [467/1000], Train Loss: 1607.6247, Test Loss: 1933.7583\n",
      "Epoch [468/1000], Train Loss: 1855.4052, Test Loss: 2095.6863\n",
      "Epoch [469/1000], Train Loss: 2197.3422, Test Loss: 1942.6068\n",
      "Epoch [470/1000], Train Loss: 1964.5247, Test Loss: 1757.5612\n",
      "Epoch [471/1000], Train Loss: 1915.3838, Test Loss: 2882.2244\n",
      "Epoch [472/1000], Train Loss: 2992.7703, Test Loss: 1780.1880\n",
      "Epoch [473/1000], Train Loss: 1562.8890, Test Loss: 1713.0113\n",
      "Epoch [474/1000], Train Loss: 1513.1489, Test Loss: 1683.2576\n",
      "Epoch [475/1000], Train Loss: 1681.2918, Test Loss: 1756.3123\n",
      "Epoch [476/1000], Train Loss: 1803.4581, Test Loss: 1746.6487\n",
      "Epoch [477/1000], Train Loss: 2119.5777, Test Loss: 2035.9328\n",
      "Epoch [478/1000], Train Loss: 2098.3756, Test Loss: 1800.8639\n",
      "Epoch [479/1000], Train Loss: 1658.7500, Test Loss: 1820.6774\n",
      "Epoch [480/1000], Train Loss: 1701.5369, Test Loss: 1743.2669\n",
      "Epoch [481/1000], Train Loss: 1574.9069, Test Loss: 1726.6617\n",
      "Epoch [482/1000], Train Loss: 1749.9998, Test Loss: 8041.7471\n",
      "Epoch [483/1000], Train Loss: 3046.3400, Test Loss: 1931.3479\n",
      "Epoch [484/1000], Train Loss: 1764.0097, Test Loss: 1746.3627\n",
      "Epoch [485/1000], Train Loss: 1444.9490, Test Loss: 1794.0698\n",
      "Epoch [486/1000], Train Loss: 1505.6032, Test Loss: 1726.6231\n",
      "Epoch [487/1000], Train Loss: 1643.7446, Test Loss: 1798.2558\n",
      "Epoch [488/1000], Train Loss: 3124.0665, Test Loss: 3138.0863\n",
      "Epoch [489/1000], Train Loss: 2389.0734, Test Loss: 1734.0100\n",
      "Epoch [490/1000], Train Loss: 1421.5995, Test Loss: 1737.4395\n",
      "Epoch [491/1000], Train Loss: 1440.3952, Test Loss: 1701.9836\n",
      "Epoch [492/1000], Train Loss: 1477.8409, Test Loss: 1737.0282\n",
      "Epoch [493/1000], Train Loss: 1602.7399, Test Loss: 1804.3434\n",
      "Epoch [494/1000], Train Loss: 1995.0124, Test Loss: 1860.4484\n",
      "Epoch [495/1000], Train Loss: 1945.7685, Test Loss: 2367.3771\n",
      "Epoch [496/1000], Train Loss: 2235.2312, Test Loss: 1899.4786\n",
      "Epoch [497/1000], Train Loss: 1642.9853, Test Loss: 1729.1882\n",
      "Epoch [498/1000], Train Loss: 1557.1404, Test Loss: 1737.4737\n",
      "Epoch [499/1000], Train Loss: 1644.8545, Test Loss: 1812.9171\n",
      "Epoch [500/1000], Train Loss: 1639.0307, Test Loss: 1781.8731\n",
      "Epoch [501/1000], Train Loss: 1577.4760, Test Loss: 1789.6432\n",
      "Epoch [502/1000], Train Loss: 1651.2769, Test Loss: 1826.3497\n",
      "Epoch [503/1000], Train Loss: 1666.5516, Test Loss: 1853.5111\n",
      "Epoch [504/1000], Train Loss: 2070.1101, Test Loss: 1772.7551\n",
      "Epoch [505/1000], Train Loss: 2750.6742, Test Loss: 2646.7463\n",
      "Epoch [506/1000], Train Loss: 2109.1260, Test Loss: 1813.2378\n",
      "Epoch [507/1000], Train Loss: 1439.5151, Test Loss: 1739.8026\n",
      "Epoch [508/1000], Train Loss: 1326.6624, Test Loss: 1739.6649\n",
      "Epoch [509/1000], Train Loss: 1511.7709, Test Loss: 1759.0975\n",
      "Epoch [510/1000], Train Loss: 1996.7956, Test Loss: 1785.1412\n",
      "Epoch [511/1000], Train Loss: 1601.6227, Test Loss: 1847.8294\n",
      "Epoch [512/1000], Train Loss: 1541.4656, Test Loss: 1797.2786\n",
      "Epoch [513/1000], Train Loss: 1862.4512, Test Loss: 1782.8634\n",
      "Epoch [514/1000], Train Loss: 1608.2862, Test Loss: 1767.7655\n",
      "Epoch [515/1000], Train Loss: 1848.1801, Test Loss: 1739.4248\n",
      "Epoch [516/1000], Train Loss: 1709.2276, Test Loss: 1787.9691\n",
      "Epoch [517/1000], Train Loss: 1480.1318, Test Loss: 1934.0775\n",
      "Epoch [518/1000], Train Loss: 1784.2174, Test Loss: 1972.0319\n",
      "Epoch [519/1000], Train Loss: 1644.9422, Test Loss: 1904.6827\n",
      "Epoch [520/1000], Train Loss: 1647.6411, Test Loss: 1830.2297\n",
      "Epoch [521/1000], Train Loss: 2033.6260, Test Loss: 2646.5819\n",
      "Epoch [522/1000], Train Loss: 1883.7001, Test Loss: 1894.9515\n",
      "Epoch [523/1000], Train Loss: 1507.8281, Test Loss: 1759.6512\n",
      "Epoch [524/1000], Train Loss: 1339.3952, Test Loss: 1781.3339\n",
      "Epoch [525/1000], Train Loss: 1705.9136, Test Loss: 1869.4635\n",
      "Epoch [526/1000], Train Loss: 1617.8137, Test Loss: 1824.2817\n",
      "Epoch [527/1000], Train Loss: 1492.9131, Test Loss: 1837.5551\n",
      "Epoch [528/1000], Train Loss: 2235.8476, Test Loss: 1761.7699\n",
      "Epoch [529/1000], Train Loss: 1476.3013, Test Loss: 1796.4370\n",
      "Epoch [530/1000], Train Loss: 1405.7946, Test Loss: 1796.7532\n",
      "Epoch [531/1000], Train Loss: 1447.7693, Test Loss: 1847.2421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [532/1000], Train Loss: 1590.4152, Test Loss: 1818.7609\n",
      "Epoch [533/1000], Train Loss: 1515.8063, Test Loss: 1763.8762\n",
      "Epoch [534/1000], Train Loss: 1422.2170, Test Loss: 1859.2764\n",
      "Epoch [535/1000], Train Loss: 1888.4003, Test Loss: 1790.9330\n",
      "Epoch [536/1000], Train Loss: 1934.9531, Test Loss: 1833.9357\n",
      "Epoch [537/1000], Train Loss: 1507.8819, Test Loss: 1818.0309\n",
      "Epoch [538/1000], Train Loss: 1485.8467, Test Loss: 1730.2133\n",
      "Epoch [539/1000], Train Loss: 1569.4616, Test Loss: 1829.1602\n",
      "Epoch [540/1000], Train Loss: 1947.8415, Test Loss: 1819.0941\n",
      "Epoch [541/1000], Train Loss: 1477.9607, Test Loss: 1816.0201\n",
      "Epoch [542/1000], Train Loss: 1546.2392, Test Loss: 1826.5293\n",
      "Epoch [543/1000], Train Loss: 1620.6034, Test Loss: 1880.1225\n",
      "Epoch [544/1000], Train Loss: 2468.4354, Test Loss: 1720.3134\n",
      "Epoch [545/1000], Train Loss: 1383.5867, Test Loss: 1831.8734\n",
      "Epoch [546/1000], Train Loss: 1310.9466, Test Loss: 1819.5475\n",
      "Epoch [547/1000], Train Loss: 1334.7936, Test Loss: 1745.9916\n",
      "Epoch [548/1000], Train Loss: 1426.2592, Test Loss: 1790.2327\n",
      "Epoch [549/1000], Train Loss: 3597.6021, Test Loss: 2257.7683\n",
      "Epoch [550/1000], Train Loss: 1674.7593, Test Loss: 1733.2980\n",
      "Epoch [551/1000], Train Loss: 1247.4208, Test Loss: 1745.8443\n",
      "Epoch [552/1000], Train Loss: 1421.2920, Test Loss: 1743.3315\n",
      "Epoch [553/1000], Train Loss: 2398.0675, Test Loss: 1769.9375\n",
      "Epoch [554/1000], Train Loss: 1266.6735, Test Loss: 1733.6271\n",
      "Epoch [555/1000], Train Loss: 1271.5132, Test Loss: 1767.4910\n",
      "Epoch [556/1000], Train Loss: 1318.5319, Test Loss: 1776.6185\n",
      "Epoch [557/1000], Train Loss: 1465.2229, Test Loss: 1838.5315\n",
      "Epoch [558/1000], Train Loss: 1430.8468, Test Loss: 1779.4134\n",
      "Epoch [559/1000], Train Loss: 1520.5086, Test Loss: 1832.8539\n",
      "Epoch [560/1000], Train Loss: 1458.5470, Test Loss: 1876.6202\n",
      "Epoch [561/1000], Train Loss: 1458.6337, Test Loss: 1756.1601\n",
      "Epoch [562/1000], Train Loss: 1812.9909, Test Loss: 1888.7349\n",
      "Epoch [563/1000], Train Loss: 1664.8117, Test Loss: 1867.2655\n",
      "Epoch [564/1000], Train Loss: 1578.5320, Test Loss: 1806.3577\n",
      "Epoch [565/1000], Train Loss: 1502.5300, Test Loss: 1792.8361\n",
      "Epoch [566/1000], Train Loss: 1387.4947, Test Loss: 1806.0513\n",
      "Epoch [567/1000], Train Loss: 1663.5681, Test Loss: 1818.2905\n",
      "Epoch [568/1000], Train Loss: 1327.8664, Test Loss: 1841.9832\n",
      "Epoch [569/1000], Train Loss: 2239.4551, Test Loss: 1837.8293\n",
      "Epoch [570/1000], Train Loss: 1932.2049, Test Loss: 1824.9167\n",
      "Epoch [571/1000], Train Loss: 1349.5167, Test Loss: 1725.4865\n",
      "Epoch [572/1000], Train Loss: 1153.8357, Test Loss: 1704.3896\n",
      "Epoch [573/1000], Train Loss: 1305.4225, Test Loss: 1829.7439\n",
      "Epoch [574/1000], Train Loss: 1345.2501, Test Loss: 1763.1159\n",
      "Epoch [575/1000], Train Loss: 1255.9228, Test Loss: 1832.1122\n",
      "Epoch [576/1000], Train Loss: 1381.9559, Test Loss: 1816.8983\n",
      "Epoch [577/1000], Train Loss: 1471.2483, Test Loss: 1797.4029\n",
      "Epoch [578/1000], Train Loss: 1845.1230, Test Loss: 1866.5562\n",
      "Epoch [579/1000], Train Loss: 1463.7913, Test Loss: 1818.9138\n",
      "Epoch [580/1000], Train Loss: 1407.5655, Test Loss: 2048.1207\n",
      "Epoch [581/1000], Train Loss: 1523.2166, Test Loss: 1849.3935\n",
      "Epoch [582/1000], Train Loss: 1364.6545, Test Loss: 1811.3910\n",
      "Epoch [583/1000], Train Loss: 1422.0144, Test Loss: 1854.8713\n",
      "Epoch [584/1000], Train Loss: 2115.3799, Test Loss: 1938.1284\n",
      "Epoch [585/1000], Train Loss: 1699.6069, Test Loss: 1769.0489\n",
      "Epoch [586/1000], Train Loss: 1261.8876, Test Loss: 1797.9119\n",
      "Epoch [587/1000], Train Loss: 1193.2224, Test Loss: 1753.4250\n",
      "Epoch [588/1000], Train Loss: 1214.5748, Test Loss: 1789.3875\n",
      "Epoch [589/1000], Train Loss: 1226.8518, Test Loss: 1782.6802\n",
      "Epoch [590/1000], Train Loss: 1755.4510, Test Loss: 1800.8378\n",
      "Epoch [591/1000], Train Loss: 1747.6149, Test Loss: 1756.4832\n",
      "Epoch [592/1000], Train Loss: 1249.6247, Test Loss: 1784.3606\n",
      "Epoch [593/1000], Train Loss: 1275.8190, Test Loss: 1734.9249\n",
      "Epoch [594/1000], Train Loss: 1799.8428, Test Loss: 1860.1353\n",
      "Epoch [595/1000], Train Loss: 1344.5135, Test Loss: 1788.1331\n",
      "Epoch [596/1000], Train Loss: 1134.0791, Test Loss: 1800.7374\n",
      "Epoch [597/1000], Train Loss: 2046.4236, Test Loss: 2060.9960\n",
      "Epoch [598/1000], Train Loss: 1422.5306, Test Loss: 1761.2385\n",
      "Epoch [599/1000], Train Loss: 1247.3478, Test Loss: 1813.2693\n",
      "Epoch [600/1000], Train Loss: 1235.6230, Test Loss: 1813.9641\n",
      "Epoch [601/1000], Train Loss: 1162.6157, Test Loss: 1798.9565\n",
      "Epoch [602/1000], Train Loss: 1354.8148, Test Loss: 1783.3342\n",
      "Epoch [603/1000], Train Loss: 1330.5429, Test Loss: 1830.1923\n",
      "Epoch [604/1000], Train Loss: 1809.6835, Test Loss: 1859.5957\n",
      "Epoch [605/1000], Train Loss: 1251.3808, Test Loss: 1769.0478\n",
      "Epoch [606/1000], Train Loss: 1442.7649, Test Loss: 1774.1432\n",
      "Epoch [607/1000], Train Loss: 1294.0599, Test Loss: 1804.2919\n",
      "Epoch [608/1000], Train Loss: 2410.4042, Test Loss: 1728.9279\n",
      "Epoch [609/1000], Train Loss: 1174.7332, Test Loss: 1813.9746\n",
      "Epoch [610/1000], Train Loss: 1091.9058, Test Loss: 1800.9210\n",
      "Epoch [611/1000], Train Loss: 1202.8749, Test Loss: 1813.9575\n",
      "Epoch [612/1000], Train Loss: 1172.7246, Test Loss: 1812.4895\n",
      "Epoch [613/1000], Train Loss: 1449.4569, Test Loss: 1862.2924\n",
      "Epoch [614/1000], Train Loss: 2055.6169, Test Loss: 2824.5470\n",
      "Epoch [615/1000], Train Loss: 1452.6828, Test Loss: 1815.3613\n",
      "Epoch [616/1000], Train Loss: 1104.6114, Test Loss: 1806.0971\n",
      "Epoch [617/1000], Train Loss: 1090.6592, Test Loss: 1786.0801\n",
      "Epoch [618/1000], Train Loss: 1420.7338, Test Loss: 1855.6914\n",
      "Epoch [619/1000], Train Loss: 1533.3687, Test Loss: 1908.4201\n",
      "Epoch [620/1000], Train Loss: 1390.6314, Test Loss: 1865.3926\n",
      "Epoch [621/1000], Train Loss: 1459.0304, Test Loss: 1836.0417\n",
      "Epoch [622/1000], Train Loss: 1311.5915, Test Loss: 1723.9047\n",
      "Epoch [623/1000], Train Loss: 1346.5043, Test Loss: 1975.1759\n",
      "Epoch [624/1000], Train Loss: 1370.5266, Test Loss: 1816.6240\n",
      "Epoch [625/1000], Train Loss: 1201.5484, Test Loss: 1744.9905\n",
      "Epoch [626/1000], Train Loss: 1324.2711, Test Loss: 1980.2291\n",
      "Epoch [627/1000], Train Loss: 1861.1946, Test Loss: 1959.5364\n",
      "Epoch [628/1000], Train Loss: 1380.2165, Test Loss: 1761.1552\n",
      "Epoch [629/1000], Train Loss: 1758.5664, Test Loss: 1808.9389\n",
      "Epoch [630/1000], Train Loss: 1445.1926, Test Loss: 1818.1665\n",
      "Epoch [631/1000], Train Loss: 1084.1708, Test Loss: 1765.8528\n",
      "Epoch [632/1000], Train Loss: 1060.7350, Test Loss: 1776.5663\n",
      "Epoch [633/1000], Train Loss: 1164.1500, Test Loss: 1803.4143\n",
      "Epoch [634/1000], Train Loss: 1226.1769, Test Loss: 1820.8499\n",
      "Epoch [635/1000], Train Loss: 1540.4283, Test Loss: 1922.1048\n",
      "Epoch [636/1000], Train Loss: 2714.8194, Test Loss: 2145.7211\n",
      "Epoch [637/1000], Train Loss: 1634.5197, Test Loss: 1761.0175\n",
      "Epoch [638/1000], Train Loss: 1002.0766, Test Loss: 1754.9956\n",
      "Epoch [639/1000], Train Loss: 1020.9707, Test Loss: 1783.2911\n",
      "Epoch [640/1000], Train Loss: 1060.3612, Test Loss: 1766.1406\n",
      "Epoch [641/1000], Train Loss: 1109.4479, Test Loss: 1833.7180\n",
      "Epoch [642/1000], Train Loss: 1376.2091, Test Loss: 1851.9227\n",
      "Epoch [643/1000], Train Loss: 1617.8874, Test Loss: 1826.1058\n",
      "Epoch [644/1000], Train Loss: 1502.0912, Test Loss: 2034.3476\n",
      "Epoch [645/1000], Train Loss: 1308.3571, Test Loss: 1797.5414\n",
      "Epoch [646/1000], Train Loss: 1294.1666, Test Loss: 1755.6523\n",
      "Epoch [647/1000], Train Loss: 1214.8967, Test Loss: 1838.6767\n",
      "Epoch [648/1000], Train Loss: 1093.8445, Test Loss: 1872.9625\n",
      "Epoch [649/1000], Train Loss: 1299.8582, Test Loss: 1892.4676\n",
      "Epoch [650/1000], Train Loss: 1456.2886, Test Loss: 1786.7552\n",
      "Epoch [651/1000], Train Loss: 1220.2026, Test Loss: 1855.0458\n",
      "Epoch [652/1000], Train Loss: 1184.0254, Test Loss: 1788.6732\n",
      "Epoch [653/1000], Train Loss: 1127.1891, Test Loss: 1766.0329\n",
      "Epoch [654/1000], Train Loss: 1586.1755, Test Loss: 1882.6530\n",
      "Epoch [655/1000], Train Loss: 1769.2076, Test Loss: 1822.6067\n",
      "Epoch [656/1000], Train Loss: 1154.2305, Test Loss: 1771.5170\n",
      "Epoch [657/1000], Train Loss: 1108.2406, Test Loss: 1828.4162\n",
      "Epoch [658/1000], Train Loss: 1044.8527, Test Loss: 1758.2633\n",
      "Epoch [659/1000], Train Loss: 1411.7461, Test Loss: 1831.8492\n",
      "Epoch [660/1000], Train Loss: 1424.7614, Test Loss: 3084.4491\n",
      "Epoch [661/1000], Train Loss: 1872.0337, Test Loss: 1843.4674\n",
      "Epoch [662/1000], Train Loss: 1147.9665, Test Loss: 1864.9122\n",
      "Epoch [663/1000], Train Loss: 1098.4138, Test Loss: 1804.5283\n",
      "Epoch [664/1000], Train Loss: 1060.4926, Test Loss: 1857.8452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [665/1000], Train Loss: 1066.5576, Test Loss: 1844.0202\n",
      "Epoch [666/1000], Train Loss: 1168.0090, Test Loss: 1806.8232\n",
      "Epoch [667/1000], Train Loss: 1372.9051, Test Loss: 2020.4157\n",
      "Epoch [668/1000], Train Loss: 2222.9677, Test Loss: 3051.5187\n",
      "Epoch [669/1000], Train Loss: 1756.3532, Test Loss: 1810.5610\n",
      "Epoch [670/1000], Train Loss: 1028.9767, Test Loss: 1775.7457\n",
      "Epoch [671/1000], Train Loss: 997.8516, Test Loss: 1779.3504\n",
      "Epoch [672/1000], Train Loss: 1066.4765, Test Loss: 1859.4468\n",
      "Epoch [673/1000], Train Loss: 1830.6904, Test Loss: 1971.4632\n",
      "Epoch [674/1000], Train Loss: 1523.7506, Test Loss: 1826.0967\n",
      "Epoch [675/1000], Train Loss: 1415.5519, Test Loss: 1833.0305\n",
      "Epoch [676/1000], Train Loss: 1350.8205, Test Loss: 1772.6357\n",
      "Epoch [677/1000], Train Loss: 1059.9593, Test Loss: 1777.2294\n",
      "Epoch [678/1000], Train Loss: 1037.1100, Test Loss: 1771.2790\n",
      "Epoch [679/1000], Train Loss: 1149.3352, Test Loss: 1902.3996\n",
      "Epoch [680/1000], Train Loss: 2448.1365, Test Loss: 1854.1987\n",
      "Epoch [681/1000], Train Loss: 1323.6023, Test Loss: 1794.1113\n",
      "Epoch [682/1000], Train Loss: 994.2078, Test Loss: 1836.5737\n",
      "Epoch [683/1000], Train Loss: 957.9810, Test Loss: 1833.4299\n",
      "Epoch [684/1000], Train Loss: 1267.8661, Test Loss: 1788.6292\n",
      "Epoch [685/1000], Train Loss: 1094.7591, Test Loss: 1850.9553\n",
      "Epoch [686/1000], Train Loss: 1049.1317, Test Loss: 1799.2499\n",
      "Epoch [687/1000], Train Loss: 1104.8203, Test Loss: 1779.8650\n",
      "Epoch [688/1000], Train Loss: 1185.2245, Test Loss: 1880.1850\n",
      "Epoch [689/1000], Train Loss: 1181.1691, Test Loss: 1783.1140\n",
      "Epoch [690/1000], Train Loss: 1152.9118, Test Loss: 1772.8509\n",
      "Epoch [691/1000], Train Loss: 1629.9967, Test Loss: 1866.6038\n",
      "Epoch [692/1000], Train Loss: 1436.3711, Test Loss: 1825.3336\n",
      "Epoch [693/1000], Train Loss: 1088.3840, Test Loss: 1813.5991\n",
      "Epoch [694/1000], Train Loss: 1023.9424, Test Loss: 1800.3543\n",
      "Epoch [695/1000], Train Loss: 1096.2518, Test Loss: 1804.8329\n",
      "Epoch [696/1000], Train Loss: 1102.6283, Test Loss: 1822.0093\n",
      "Epoch [697/1000], Train Loss: 1708.4590, Test Loss: 1889.6373\n",
      "Epoch [698/1000], Train Loss: 1203.6935, Test Loss: 1790.5309\n",
      "Epoch [699/1000], Train Loss: 1032.7733, Test Loss: 1783.3443\n",
      "Epoch [700/1000], Train Loss: 1014.6340, Test Loss: 1826.6538\n",
      "Epoch [701/1000], Train Loss: 1987.0698, Test Loss: 2389.0582\n",
      "Epoch [702/1000], Train Loss: 1432.8240, Test Loss: 1862.3143\n",
      "Epoch [703/1000], Train Loss: 960.7747, Test Loss: 1772.6072\n",
      "Epoch [704/1000], Train Loss: 905.1247, Test Loss: 1805.3735\n",
      "Epoch [705/1000], Train Loss: 995.2688, Test Loss: 1790.9451\n",
      "Epoch [706/1000], Train Loss: 1016.8502, Test Loss: 1818.2666\n",
      "Epoch [707/1000], Train Loss: 1625.9001, Test Loss: 1872.1021\n",
      "Epoch [708/1000], Train Loss: 1145.0037, Test Loss: 1813.5535\n",
      "Epoch [709/1000], Train Loss: 1013.2925, Test Loss: 1863.1648\n",
      "Epoch [710/1000], Train Loss: 1021.3130, Test Loss: 1844.6389\n",
      "Epoch [711/1000], Train Loss: 1025.0210, Test Loss: 1796.8640\n",
      "Epoch [712/1000], Train Loss: 1052.6511, Test Loss: 1825.0091\n",
      "Epoch [713/1000], Train Loss: 1444.9915, Test Loss: 2142.6520\n",
      "Epoch [714/1000], Train Loss: 2508.9879, Test Loss: 1813.9056\n",
      "Epoch [715/1000], Train Loss: 952.5265, Test Loss: 1806.3572\n",
      "Epoch [716/1000], Train Loss: 984.4655, Test Loss: 1796.8782\n",
      "Epoch [717/1000], Train Loss: 956.2475, Test Loss: 1798.7388\n",
      "Epoch [718/1000], Train Loss: 953.3051, Test Loss: 1868.1036\n",
      "Epoch [719/1000], Train Loss: 1033.7381, Test Loss: 1769.8414\n",
      "Epoch [720/1000], Train Loss: 1307.3144, Test Loss: 1837.8583\n",
      "Epoch [721/1000], Train Loss: 1061.5670, Test Loss: 1806.4558\n",
      "Epoch [722/1000], Train Loss: 1076.3803, Test Loss: 1861.8460\n",
      "Epoch [723/1000], Train Loss: 1084.9740, Test Loss: 1909.1340\n",
      "Epoch [724/1000], Train Loss: 1262.3983, Test Loss: 1842.4274\n",
      "Epoch [725/1000], Train Loss: 1164.3106, Test Loss: 2005.5081\n",
      "Epoch [726/1000], Train Loss: 1100.6519, Test Loss: 1992.1646\n",
      "Epoch [727/1000], Train Loss: 1715.7386, Test Loss: 2193.2522\n",
      "Epoch [728/1000], Train Loss: 1285.0350, Test Loss: 1828.4405\n",
      "Epoch [729/1000], Train Loss: 918.9114, Test Loss: 1791.0573\n",
      "Epoch [730/1000], Train Loss: 933.8971, Test Loss: 1845.0275\n",
      "Epoch [731/1000], Train Loss: 1052.8489, Test Loss: 1851.3961\n",
      "Epoch [732/1000], Train Loss: 1051.0243, Test Loss: 1882.5756\n",
      "Epoch [733/1000], Train Loss: 1137.7775, Test Loss: 1810.9310\n",
      "Epoch [734/1000], Train Loss: 1138.7008, Test Loss: 1793.1349\n",
      "Epoch [735/1000], Train Loss: 1009.3158, Test Loss: 1869.9055\n",
      "Epoch [736/1000], Train Loss: 2475.6363, Test Loss: 1866.0253\n",
      "Epoch [737/1000], Train Loss: 1582.2423, Test Loss: 1933.7521\n",
      "Epoch [738/1000], Train Loss: 2067.3307, Test Loss: 2112.0873\n",
      "Epoch [739/1000], Train Loss: 1699.1411, Test Loss: 1927.9432\n",
      "Epoch [740/1000], Train Loss: 1163.7416, Test Loss: 1861.1967\n",
      "Epoch [741/1000], Train Loss: 1102.1418, Test Loss: 1883.7652\n",
      "Epoch [742/1000], Train Loss: 1127.0525, Test Loss: 1855.7401\n",
      "Epoch [743/1000], Train Loss: 1146.9957, Test Loss: 1913.2850\n",
      "Epoch [744/1000], Train Loss: 1207.3686, Test Loss: 1862.2482\n",
      "Epoch [745/1000], Train Loss: 1398.1882, Test Loss: 1863.0129\n",
      "Epoch [746/1000], Train Loss: 2615.9275, Test Loss: 1849.0682\n",
      "Epoch [747/1000], Train Loss: 1056.4476, Test Loss: 1789.4877\n",
      "Epoch [748/1000], Train Loss: 890.4402, Test Loss: 1829.9359\n",
      "Epoch [749/1000], Train Loss: 891.3781, Test Loss: 1870.8989\n",
      "Epoch [750/1000], Train Loss: 991.7696, Test Loss: 1832.3858\n",
      "Epoch [751/1000], Train Loss: 1025.4142, Test Loss: 1818.4164\n",
      "Epoch [752/1000], Train Loss: 1313.6366, Test Loss: 1881.6039\n",
      "Epoch [753/1000], Train Loss: 1156.1608, Test Loss: 1869.0767\n",
      "Epoch [754/1000], Train Loss: 1199.4801, Test Loss: 2327.1722\n",
      "Epoch [755/1000], Train Loss: 1424.4176, Test Loss: 2006.8357\n",
      "Epoch [756/1000], Train Loss: 983.1070, Test Loss: 1828.8540\n",
      "Epoch [757/1000], Train Loss: 939.7376, Test Loss: 1868.1857\n",
      "Epoch [758/1000], Train Loss: 953.9284, Test Loss: 1882.6138\n",
      "Epoch [759/1000], Train Loss: 1029.1375, Test Loss: 1910.3140\n",
      "Epoch [760/1000], Train Loss: 1166.5139, Test Loss: 1816.7339\n",
      "Epoch [761/1000], Train Loss: 1319.9873, Test Loss: 1876.4258\n",
      "Epoch [762/1000], Train Loss: 1054.9589, Test Loss: 1871.9170\n",
      "Epoch [763/1000], Train Loss: 962.8842, Test Loss: 1846.2431\n",
      "Epoch [764/1000], Train Loss: 1826.8787, Test Loss: 1838.7510\n",
      "Epoch [765/1000], Train Loss: 865.8610, Test Loss: 1866.4477\n",
      "Epoch [766/1000], Train Loss: 886.5363, Test Loss: 1872.6512\n",
      "Epoch [767/1000], Train Loss: 939.9555, Test Loss: 1843.1720\n",
      "Epoch [768/1000], Train Loss: 1107.6572, Test Loss: 2041.4376\n",
      "Epoch [769/1000], Train Loss: 1451.3881, Test Loss: 1826.0963\n",
      "Epoch [770/1000], Train Loss: 1183.6079, Test Loss: 1879.6820\n",
      "Epoch [771/1000], Train Loss: 890.6279, Test Loss: 1829.3364\n",
      "Epoch [772/1000], Train Loss: 860.3035, Test Loss: 2037.3939\n",
      "Epoch [773/1000], Train Loss: 1694.2073, Test Loss: 2368.1090\n",
      "Epoch [774/1000], Train Loss: 1291.6139, Test Loss: 1826.2522\n",
      "Epoch [775/1000], Train Loss: 835.5041, Test Loss: 1871.1019\n",
      "Epoch [776/1000], Train Loss: 870.4993, Test Loss: 1865.3257\n",
      "Epoch [777/1000], Train Loss: 1007.4238, Test Loss: 1884.9927\n",
      "Epoch [778/1000], Train Loss: 1070.9409, Test Loss: 1852.6844\n",
      "Epoch [779/1000], Train Loss: 1140.9052, Test Loss: 1866.0289\n",
      "Epoch [780/1000], Train Loss: 998.2126, Test Loss: 1883.9817\n",
      "Epoch [781/1000], Train Loss: 1034.2400, Test Loss: 1915.6503\n",
      "Epoch [782/1000], Train Loss: 992.3870, Test Loss: 1892.8784\n",
      "Epoch [783/1000], Train Loss: 996.9518, Test Loss: 1932.1007\n",
      "Epoch [784/1000], Train Loss: 1858.5317, Test Loss: 2196.5450\n",
      "Epoch [785/1000], Train Loss: 1088.1854, Test Loss: 1891.3533\n",
      "Epoch [786/1000], Train Loss: 824.5951, Test Loss: 1890.9466\n",
      "Epoch [787/1000], Train Loss: 861.9676, Test Loss: 1826.6409\n",
      "Epoch [788/1000], Train Loss: 1165.5971, Test Loss: 1901.0114\n",
      "Epoch [789/1000], Train Loss: 965.5013, Test Loss: 1949.4662\n",
      "Epoch [790/1000], Train Loss: 1449.5369, Test Loss: 1933.2305\n",
      "Epoch [791/1000], Train Loss: 1016.2808, Test Loss: 1881.7549\n",
      "Epoch [792/1000], Train Loss: 2370.3726, Test Loss: 1938.5105\n",
      "Epoch [793/1000], Train Loss: 953.6361, Test Loss: 1822.7374\n",
      "Epoch [794/1000], Train Loss: 846.0170, Test Loss: 1871.7526\n",
      "Epoch [795/1000], Train Loss: 866.7565, Test Loss: 1800.6648\n",
      "Epoch [796/1000], Train Loss: 938.2798, Test Loss: 1820.8906\n",
      "Epoch [797/1000], Train Loss: 942.3731, Test Loss: 1855.0292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [798/1000], Train Loss: 990.6181, Test Loss: 1955.3054\n",
      "Epoch [799/1000], Train Loss: 1258.3392, Test Loss: 1857.6017\n",
      "Epoch [800/1000], Train Loss: 902.6484, Test Loss: 1851.7015\n",
      "Epoch [801/1000], Train Loss: 894.4030, Test Loss: 1871.1952\n",
      "Epoch [802/1000], Train Loss: 967.8347, Test Loss: 1870.3789\n",
      "Epoch [803/1000], Train Loss: 937.0239, Test Loss: 1924.0603\n",
      "Epoch [804/1000], Train Loss: 1398.1709, Test Loss: 1869.2823\n",
      "Epoch [805/1000], Train Loss: 1126.0594, Test Loss: 1908.8568\n",
      "Epoch [806/1000], Train Loss: 1161.7728, Test Loss: 1876.7334\n",
      "Epoch [807/1000], Train Loss: 1201.0533, Test Loss: 1892.6893\n",
      "Epoch [808/1000], Train Loss: 904.2651, Test Loss: 1869.1733\n",
      "Epoch [809/1000], Train Loss: 836.0570, Test Loss: 1863.7397\n",
      "Epoch [810/1000], Train Loss: 804.2653, Test Loss: 1848.1164\n",
      "Epoch [811/1000], Train Loss: 916.0711, Test Loss: 1934.2481\n",
      "Epoch [812/1000], Train Loss: 1124.0333, Test Loss: 1954.5199\n",
      "Epoch [813/1000], Train Loss: 1102.9938, Test Loss: 1875.0453\n",
      "Epoch [814/1000], Train Loss: 981.0307, Test Loss: 1885.2788\n",
      "Epoch [815/1000], Train Loss: 1103.2764, Test Loss: 1973.4711\n",
      "Epoch [816/1000], Train Loss: 1232.9739, Test Loss: 1845.9539\n",
      "Epoch [817/1000], Train Loss: 836.7622, Test Loss: 1871.5124\n",
      "Epoch [818/1000], Train Loss: 821.6078, Test Loss: 1827.6935\n",
      "Epoch [819/1000], Train Loss: 920.8802, Test Loss: 1887.4455\n",
      "Epoch [820/1000], Train Loss: 1072.9724, Test Loss: 1978.3842\n",
      "Epoch [821/1000], Train Loss: 1155.5295, Test Loss: 1881.6101\n",
      "Epoch [822/1000], Train Loss: 1060.8589, Test Loss: 1899.1233\n",
      "Epoch [823/1000], Train Loss: 998.6536, Test Loss: 1998.9063\n",
      "Epoch [824/1000], Train Loss: 2549.8565, Test Loss: 2019.0213\n",
      "Epoch [825/1000], Train Loss: 1052.6704, Test Loss: 1829.0227\n",
      "Epoch [826/1000], Train Loss: 818.9438, Test Loss: 1850.9923\n",
      "Epoch [827/1000], Train Loss: 726.0269, Test Loss: 1833.9185\n",
      "Epoch [828/1000], Train Loss: 760.4409, Test Loss: 1857.5581\n",
      "Epoch [829/1000], Train Loss: 796.0265, Test Loss: 1886.7936\n",
      "Epoch [830/1000], Train Loss: 878.0464, Test Loss: 1963.1741\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m save_path \u001b[38;5;241m=\u001b[39m ModelName \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Params.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m train_losses, test_losses, is_model_trained \u001b[38;5;241m=\u001b[39m train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Finish timing cell run time\u001b[39;00m\n\u001b[1;32m     25\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[8], line 63\u001b[0m, in \u001b[0;36mtrain_or_load_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pretrained model found. Training from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#optimizer = optim.Adam(model.parameters())  \u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n\u001b[1;32m     64\u001b[0m is_model_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set flag to True after training\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Save losses per epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mtrain_and_save_best_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m---> 25\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     26\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     27\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(batch_size, num_bins \u001b[38;5;241m*\u001b[39m d_model)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Decoding\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)  \u001b[38;5;66;03m# (batch_size, output_dim)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load('Dataset44_Dist2_Val_Spec.npy')\n",
    "concVal = np.load('Dataset44_Dist2_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "ValSpectra = np.load(\"Dataset44_Dist2_RepresentativeExamples_Spectra.npy\")\n",
    "ValConc = np.load(\"Dataset44_Dist2_RepresentativeExamples_Concentrations.npy\")\n",
    "ValSpecNames = np.load(\"Dataset44_Dist2_RepresentativeExamples_VariableNames.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ValConc = torch.tensor(ValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=23552, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ValConc[i]\n",
    "    Prediction = model_aq(ValSpectra[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.38  -  AllAq1\n",
      "1.5  -  AllAq5\n",
      "0.78  -  AllAq25\n",
      "6.13  -  AllAq50\n",
      "1.18  -  ThreeAddedSinglets\n",
      "4.64  -  ThirtyAddedSinglets\n",
      "79.5  -  ShiftedSpec\n",
      "36.42  -  SineBase\n",
      "136.62  -  HighDynamicRange\n",
      "inf  -  HalfZeros\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - \",ValSpecNames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist2 - HD-Range w/ 1's\n",
      "tensor([4.8166e-02, 4.4696e+01, 0.0000e+00, 4.5432e+01, 0.0000e+00, 4.7337e+01,\n",
      "        0.0000e+00, 4.8111e+01, 0.0000e+00, 4.9373e+01, 0.0000e+00, 4.4919e+01,\n",
      "        0.0000e+00, 4.4947e+01, 1.6794e+00, 4.7542e+01, 0.0000e+00, 4.7697e+01,\n",
      "        0.0000e+00, 4.6548e+01, 1.0394e+00, 4.3974e+01, 0.0000e+00, 4.5709e+01,\n",
      "        0.0000e+00, 4.9020e+01, 0.0000e+00, 4.7088e+01, 3.4641e-01, 4.5337e+01,\n",
      "        0.0000e+00, 4.7617e+01, 8.9492e-01, 4.6393e+01, 4.0481e-02, 4.7396e+01,\n",
      "        0.0000e+00, 4.6216e+01, 0.0000e+00, 4.5548e+01, 0.0000e+00, 4.5240e+01,\n",
      "        0.0000e+00, 4.5834e+01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Dist2 - HD-Range w/ 0's\n",
      "tensor([ 0.0000, 44.5389,  0.0000, 45.4686,  0.0000, 47.3279,  0.0000, 48.0540,\n",
      "         0.0000, 49.4115,  0.0000, 44.8882,  0.0000, 44.8596,  0.7337, 47.4862,\n",
      "         0.0000, 47.6186,  0.0000, 46.5791,  0.0000, 43.9725,  0.0000, 45.7794,\n",
      "         0.0000, 49.1024,  0.0000, 46.9625,  0.0000, 45.2782,  0.0000, 47.5793,\n",
      "         0.0000, 46.3613,  0.0000, 47.2511,  0.0000, 46.1702,  0.0000, 45.5009,\n",
      "         0.0000, 45.2134,  0.0000, 46.0006], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Dist2 - Blank\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.2985, 0.0000, 0.1576, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0454, 0.0041, 0.0000, 0.0000, 0.0170, 0.0368, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0331, 0.1018, 0.0000, 0.0000, 0.0225, 0.0000, 0.0000, 0.0977,\n",
      "        0.0000, 0.2217, 0.1880, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(ValSpectra[8])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist2 - HD-Range w/ 1's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[9])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist2 - HD-Range w/ 0's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[10])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist2 - Blank\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
