{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer Encoder on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"Transformer_44met_TrainingAndValidation_1000bin_NoDropout_Dist5_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('Dataset44_Dist5_Spec.npy')\n",
    "conc1 = np.load('Dataset44_Dist5_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "#spectraVal = np.load('Dataset44_Dist5_Val_Spec.npy')\n",
    "#concVal = np.load('Dataset44_Dist5_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "#ValSpectra = np.load(\"Dataset44_Dist5_RepresentativeExamples_Spectra.npy\")\n",
    "#ValConc = np.load(\"Dataset44_Dist5_RepresentativeExamples_Concentrations.npy\")\n",
    "#ValSpecNames = np.load(\"Dataset44_Dist5_RepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 32, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.Linear(23552, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Binning\n",
    "        batch_size, seq_length = x.size()\n",
    "        num_bins = seq_length // self.input_dim\n",
    "        x = x.view(batch_size, num_bins, self.input_dim)  # (batch_size, num_bins, input_dim)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = x.permute(1, 0, 2)  # (num_bins, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)  # (num_bins, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Reconstruct original sequence\n",
    "        x = x.reshape(batch_size, num_bins * d_model)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder(x)  # (batch_size, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "input_dim = 1000  # Size of each bin\n",
    "d_model = 512     # Embedding dimension\n",
    "nhead = 1         # Number of attention heads\n",
    "num_encoder_layers = 1  # Number of transformer encoder layers\n",
    "dim_feedforward = 2048  # Feedforward dimension\n",
    "dropout = 0.0     # Dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "       \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/1000], Train Loss: 4369360.0166, Test Loss: 1006840.3535\n",
      "Epoch [2/1000], Train Loss: 2813543.1860, Test Loss: 555389.5039\n",
      "Epoch [3/1000], Train Loss: 1528278.0654, Test Loss: 221569.2198\n",
      "Epoch [4/1000], Train Loss: 571413.1769, Test Loss: 83165.9757\n",
      "Epoch [5/1000], Train Loss: 245189.0149, Test Loss: 47893.3774\n",
      "Epoch [6/1000], Train Loss: 157966.8657, Test Loss: 32852.3861\n",
      "Epoch [7/1000], Train Loss: 114242.8048, Test Loss: 24819.5542\n",
      "Epoch [8/1000], Train Loss: 90514.0927, Test Loss: 22484.4232\n",
      "Epoch [9/1000], Train Loss: 76298.1618, Test Loss: 19374.6333\n",
      "Epoch [10/1000], Train Loss: 64886.5840, Test Loss: 16240.5978\n",
      "Epoch [11/1000], Train Loss: 56127.2765, Test Loss: 13726.7287\n",
      "Epoch [12/1000], Train Loss: 48910.5915, Test Loss: 12304.2846\n",
      "Epoch [13/1000], Train Loss: 43190.7371, Test Loss: 11177.8027\n",
      "Epoch [14/1000], Train Loss: 39805.4860, Test Loss: 10367.3500\n",
      "Epoch [15/1000], Train Loss: 35937.8120, Test Loss: 9592.3829\n",
      "Epoch [16/1000], Train Loss: 32223.8781, Test Loss: 7952.7220\n",
      "Epoch [17/1000], Train Loss: 30031.1231, Test Loss: 8255.7936\n",
      "Epoch [18/1000], Train Loss: 27323.9379, Test Loss: 8422.2666\n",
      "Epoch [19/1000], Train Loss: 28203.1893, Test Loss: 11427.5163\n",
      "Epoch [20/1000], Train Loss: 25877.6951, Test Loss: 6920.5093\n",
      "Epoch [21/1000], Train Loss: 23853.8052, Test Loss: 6520.5959\n",
      "Epoch [22/1000], Train Loss: 21711.5221, Test Loss: 7315.8226\n",
      "Epoch [23/1000], Train Loss: 20595.6941, Test Loss: 6905.4508\n",
      "Epoch [24/1000], Train Loss: 273395.8669, Test Loss: 65086.7147\n",
      "Epoch [25/1000], Train Loss: 62585.1359, Test Loss: 7859.9584\n",
      "Epoch [26/1000], Train Loss: 26100.2515, Test Loss: 6278.3427\n",
      "Epoch [27/1000], Train Loss: 19736.7289, Test Loss: 5705.1424\n",
      "Epoch [28/1000], Train Loss: 18078.3826, Test Loss: 5184.9675\n",
      "Epoch [29/1000], Train Loss: 16810.4692, Test Loss: 5245.7167\n",
      "Epoch [30/1000], Train Loss: 16805.0438, Test Loss: 5016.3070\n",
      "Epoch [31/1000], Train Loss: 15443.7330, Test Loss: 4615.4556\n",
      "Epoch [32/1000], Train Loss: 15387.5528, Test Loss: 4783.4538\n",
      "Epoch [33/1000], Train Loss: 15430.5989, Test Loss: 4673.6485\n",
      "Epoch [34/1000], Train Loss: 61290.0995, Test Loss: 7674.7913\n",
      "Epoch [35/1000], Train Loss: 16685.1553, Test Loss: 4416.4626\n",
      "Epoch [36/1000], Train Loss: 13451.2486, Test Loss: 4255.5050\n",
      "Epoch [37/1000], Train Loss: 13048.6449, Test Loss: 4252.7784\n",
      "Epoch [38/1000], Train Loss: 12882.4143, Test Loss: 4319.1792\n",
      "Epoch [39/1000], Train Loss: 12796.1118, Test Loss: 4593.3015\n",
      "Epoch [40/1000], Train Loss: 13153.1759, Test Loss: 4260.2768\n",
      "Epoch [41/1000], Train Loss: 12925.9342, Test Loss: 4006.5904\n",
      "Epoch [42/1000], Train Loss: 13374.1187, Test Loss: 4363.9696\n",
      "Epoch [43/1000], Train Loss: 13269.6981, Test Loss: 4703.4954\n",
      "Epoch [44/1000], Train Loss: 13022.9125, Test Loss: 4483.4479\n",
      "Epoch [45/1000], Train Loss: 13152.8382, Test Loss: 4735.3668\n",
      "Epoch [46/1000], Train Loss: 44435.3156, Test Loss: 4105.9203\n",
      "Epoch [47/1000], Train Loss: 11671.8664, Test Loss: 3608.4235\n",
      "Epoch [48/1000], Train Loss: 10085.8421, Test Loss: 3362.2677\n",
      "Epoch [49/1000], Train Loss: 9818.7631, Test Loss: 3328.6247\n",
      "Epoch [50/1000], Train Loss: 9700.5620, Test Loss: 3400.4173\n",
      "Epoch [51/1000], Train Loss: 10440.2378, Test Loss: 3523.4657\n",
      "Epoch [52/1000], Train Loss: 10170.1881, Test Loss: 3623.5679\n",
      "Epoch [53/1000], Train Loss: 10708.0927, Test Loss: 4034.3627\n",
      "Epoch [54/1000], Train Loss: 11028.0941, Test Loss: 4704.9559\n",
      "Epoch [55/1000], Train Loss: 19749.0880, Test Loss: 6412.1626\n",
      "Epoch [56/1000], Train Loss: 19610.8145, Test Loss: 3236.6909\n",
      "Epoch [57/1000], Train Loss: 9194.7753, Test Loss: 4688.8982\n",
      "Epoch [58/1000], Train Loss: 9893.7322, Test Loss: 4422.0226\n",
      "Epoch [59/1000], Train Loss: 9675.1569, Test Loss: 3421.9529\n",
      "Epoch [60/1000], Train Loss: 9111.8136, Test Loss: 3282.5710\n",
      "Epoch [61/1000], Train Loss: 10065.9736, Test Loss: 5751.8597\n",
      "Epoch [62/1000], Train Loss: 11121.9036, Test Loss: 7865.9833\n",
      "Epoch [63/1000], Train Loss: 25393.4573, Test Loss: 3721.9175\n",
      "Epoch [64/1000], Train Loss: 12015.8957, Test Loss: 4770.6313\n",
      "Epoch [65/1000], Train Loss: 20567.7628, Test Loss: 3249.5305\n",
      "Epoch [66/1000], Train Loss: 11056.4768, Test Loss: 3990.1374\n",
      "Epoch [67/1000], Train Loss: 9461.9186, Test Loss: 3643.0614\n",
      "Epoch [68/1000], Train Loss: 7928.8387, Test Loss: 2903.6450\n",
      "Epoch [69/1000], Train Loss: 7598.2230, Test Loss: 2982.2091\n",
      "Epoch [70/1000], Train Loss: 8594.8922, Test Loss: 3065.1563\n",
      "Epoch [71/1000], Train Loss: 8039.4822, Test Loss: 3134.7175\n",
      "Epoch [72/1000], Train Loss: 8902.5668, Test Loss: 3453.9662\n",
      "Epoch [73/1000], Train Loss: 9599.8570, Test Loss: 3190.1790\n",
      "Epoch [74/1000], Train Loss: 30863.8973, Test Loss: 61687.1045\n",
      "Epoch [75/1000], Train Loss: 16267.0225, Test Loss: 2967.7272\n",
      "Epoch [76/1000], Train Loss: 6903.5675, Test Loss: 2605.7074\n",
      "Epoch [77/1000], Train Loss: 6502.7938, Test Loss: 2786.5544\n",
      "Epoch [78/1000], Train Loss: 6541.8554, Test Loss: 2791.4864\n",
      "Epoch [79/1000], Train Loss: 6813.4793, Test Loss: 3069.2811\n",
      "Epoch [80/1000], Train Loss: 6899.0877, Test Loss: 2762.9950\n",
      "Epoch [81/1000], Train Loss: 7353.7076, Test Loss: 3346.3661\n",
      "Epoch [82/1000], Train Loss: 639809.7170, Test Loss: 16867.8111\n",
      "Epoch [83/1000], Train Loss: 46390.0063, Test Loss: 7720.6983\n",
      "Epoch [84/1000], Train Loss: 24633.2056, Test Loss: 6482.0154\n",
      "Epoch [85/1000], Train Loss: 24778.1294, Test Loss: 5705.8096\n",
      "Epoch [86/1000], Train Loss: 15989.5839, Test Loss: 4756.1253\n",
      "Epoch [87/1000], Train Loss: 14437.8194, Test Loss: 4290.1702\n",
      "Epoch [88/1000], Train Loss: 13788.6106, Test Loss: 3494.2624\n",
      "Epoch [89/1000], Train Loss: 22185.1522, Test Loss: 3489.2486\n",
      "Epoch [90/1000], Train Loss: 10328.8518, Test Loss: 3874.2945\n",
      "Epoch [91/1000], Train Loss: 22173.3688, Test Loss: 3670.9818\n",
      "Epoch [92/1000], Train Loss: 14895.4104, Test Loss: 3929.2858\n",
      "Epoch [93/1000], Train Loss: 8762.2412, Test Loss: 3061.4447\n",
      "Epoch [94/1000], Train Loss: 8444.0434, Test Loss: 3113.8252\n",
      "Epoch [95/1000], Train Loss: 8350.3946, Test Loss: 2978.6709\n",
      "Epoch [96/1000], Train Loss: 8225.1951, Test Loss: 3233.3729\n",
      "Epoch [97/1000], Train Loss: 8059.0085, Test Loss: 2823.8915\n",
      "Epoch [98/1000], Train Loss: 8099.3433, Test Loss: 3046.0671\n",
      "Epoch [99/1000], Train Loss: 8097.3432, Test Loss: 2889.4878\n",
      "Epoch [100/1000], Train Loss: 21324.3482, Test Loss: 2770.8755\n",
      "Epoch [101/1000], Train Loss: 7079.4169, Test Loss: 2768.0433\n",
      "Epoch [102/1000], Train Loss: 6779.9669, Test Loss: 2589.7194\n",
      "Epoch [103/1000], Train Loss: 6796.3177, Test Loss: 2722.7609\n",
      "Epoch [104/1000], Train Loss: 7136.2963, Test Loss: 2881.7429\n",
      "Epoch [105/1000], Train Loss: 7379.6136, Test Loss: 2963.5376\n",
      "Epoch [106/1000], Train Loss: 7446.0996, Test Loss: 2937.5166\n",
      "Epoch [107/1000], Train Loss: 116114.7306, Test Loss: 14045.8148\n",
      "Epoch [108/1000], Train Loss: 17229.8242, Test Loss: 3574.5888\n",
      "Epoch [109/1000], Train Loss: 9080.2400, Test Loss: 3148.8921\n",
      "Epoch [110/1000], Train Loss: 7621.6770, Test Loss: 2779.5943\n",
      "Epoch [111/1000], Train Loss: 7216.2143, Test Loss: 3064.0881\n",
      "Epoch [112/1000], Train Loss: 7017.1420, Test Loss: 2763.7786\n",
      "Epoch [113/1000], Train Loss: 6804.7660, Test Loss: 2803.5584\n",
      "Epoch [114/1000], Train Loss: 6836.9823, Test Loss: 2728.0634\n",
      "Epoch [115/1000], Train Loss: 6826.5123, Test Loss: 2743.0912\n",
      "Epoch [116/1000], Train Loss: 6796.7286, Test Loss: 2886.2187\n",
      "Epoch [117/1000], Train Loss: 6980.9871, Test Loss: 2774.0114\n",
      "Epoch [118/1000], Train Loss: 7177.1908, Test Loss: 2680.3095\n",
      "Epoch [119/1000], Train Loss: 9739.1274, Test Loss: 3360.1912\n",
      "Epoch [120/1000], Train Loss: 7056.0879, Test Loss: 2635.1015\n",
      "Epoch [121/1000], Train Loss: 6542.0578, Test Loss: 2657.7731\n",
      "Epoch [122/1000], Train Loss: 6613.3384, Test Loss: 2573.6808\n",
      "Epoch [123/1000], Train Loss: 6664.6995, Test Loss: 2791.8965\n",
      "Epoch [124/1000], Train Loss: 7330.4220, Test Loss: 3173.4117\n",
      "Epoch [125/1000], Train Loss: 6906.1005, Test Loss: 2775.8824\n",
      "Epoch [126/1000], Train Loss: 6928.7083, Test Loss: 2756.4131\n",
      "Epoch [127/1000], Train Loss: 6934.7744, Test Loss: 2868.2520\n",
      "Epoch [128/1000], Train Loss: 6889.2214, Test Loss: 2823.9135\n",
      "Epoch [129/1000], Train Loss: 8911.2534, Test Loss: 3342.4676\n",
      "Epoch [130/1000], Train Loss: 6359.0151, Test Loss: 2392.1615\n",
      "Epoch [131/1000], Train Loss: 5860.3800, Test Loss: 2540.4815\n",
      "Epoch [132/1000], Train Loss: 6100.7065, Test Loss: 2536.6633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [133/1000], Train Loss: 6131.4861, Test Loss: 2745.5052\n",
      "Epoch [134/1000], Train Loss: 6583.1772, Test Loss: 2807.2458\n",
      "Epoch [135/1000], Train Loss: 6283.2372, Test Loss: 2562.5606\n",
      "Epoch [136/1000], Train Loss: 7095.4123, Test Loss: 2745.8648\n",
      "Epoch [137/1000], Train Loss: 6352.4601, Test Loss: 3578.8279\n",
      "Epoch [138/1000], Train Loss: 6412.4513, Test Loss: 2361.5124\n",
      "Epoch [139/1000], Train Loss: 6161.8786, Test Loss: 2769.3006\n",
      "Epoch [140/1000], Train Loss: 6019.8007, Test Loss: 2400.0540\n",
      "Epoch [141/1000], Train Loss: 11451.7848, Test Loss: 2425.8925\n",
      "Epoch [142/1000], Train Loss: 5446.7123, Test Loss: 2416.5074\n",
      "Epoch [143/1000], Train Loss: 5044.4783, Test Loss: 2485.8923\n",
      "Epoch [144/1000], Train Loss: 5260.8592, Test Loss: 2637.1394\n",
      "Epoch [145/1000], Train Loss: 5266.4874, Test Loss: 2609.2795\n",
      "Epoch [146/1000], Train Loss: 7904.9995, Test Loss: 2556.0675\n",
      "Epoch [147/1000], Train Loss: 5073.6626, Test Loss: 2451.2199\n",
      "Epoch [148/1000], Train Loss: 5142.1595, Test Loss: 2479.2234\n",
      "Epoch [149/1000], Train Loss: 5314.5373, Test Loss: 2575.7549\n",
      "Epoch [150/1000], Train Loss: 5397.6833, Test Loss: 2621.8579\n",
      "Epoch [151/1000], Train Loss: 5721.3526, Test Loss: 2607.4278\n",
      "Epoch [152/1000], Train Loss: 6391.8107, Test Loss: 2475.8843\n",
      "Epoch [153/1000], Train Loss: 5408.5063, Test Loss: 2364.5697\n",
      "Epoch [154/1000], Train Loss: 6283.0433, Test Loss: 2523.4744\n",
      "Epoch [155/1000], Train Loss: 5184.5679, Test Loss: 2574.0932\n",
      "Epoch [156/1000], Train Loss: 6421.3014, Test Loss: 2668.3422\n",
      "Epoch [157/1000], Train Loss: 5402.5135, Test Loss: 2416.1006\n",
      "Epoch [158/1000], Train Loss: 5286.5674, Test Loss: 2383.0492\n",
      "Epoch [159/1000], Train Loss: 5268.4886, Test Loss: 2407.8690\n",
      "Epoch [160/1000], Train Loss: 5690.8068, Test Loss: 2565.2787\n",
      "Epoch [161/1000], Train Loss: 5315.1980, Test Loss: 2565.9405\n",
      "Epoch [162/1000], Train Loss: 5254.2261, Test Loss: 2314.5164\n",
      "Epoch [163/1000], Train Loss: 5381.1685, Test Loss: 2399.3851\n",
      "Epoch [164/1000], Train Loss: 4873.1446, Test Loss: 2453.5295\n",
      "Epoch [165/1000], Train Loss: 5146.6025, Test Loss: 2490.2595\n",
      "Epoch [166/1000], Train Loss: 5066.8975, Test Loss: 2383.3975\n",
      "Epoch [167/1000], Train Loss: 5191.5335, Test Loss: 2391.6290\n",
      "Epoch [168/1000], Train Loss: 5274.7126, Test Loss: 2471.3577\n",
      "Epoch [169/1000], Train Loss: 4940.4296, Test Loss: 2932.7242\n",
      "Epoch [170/1000], Train Loss: 4922.0658, Test Loss: 2287.6806\n",
      "Epoch [171/1000], Train Loss: 5151.2159, Test Loss: 2306.7534\n",
      "Epoch [172/1000], Train Loss: 4890.1788, Test Loss: 2296.0741\n",
      "Epoch [173/1000], Train Loss: 5008.2412, Test Loss: 2397.5356\n",
      "Epoch [174/1000], Train Loss: 5677.0227, Test Loss: 2418.0881\n",
      "Epoch [175/1000], Train Loss: 4518.6515, Test Loss: 2396.1349\n",
      "Epoch [176/1000], Train Loss: 4773.4121, Test Loss: 2468.3781\n",
      "Epoch [177/1000], Train Loss: 4612.4341, Test Loss: 2294.0104\n",
      "Epoch [178/1000], Train Loss: 5900.9487, Test Loss: 2659.4776\n",
      "Epoch [179/1000], Train Loss: 4642.2187, Test Loss: 2248.8741\n",
      "Epoch [180/1000], Train Loss: 4491.5613, Test Loss: 2282.6513\n",
      "Epoch [181/1000], Train Loss: 4519.3725, Test Loss: 2275.3698\n",
      "Epoch [182/1000], Train Loss: 4545.4177, Test Loss: 2351.4531\n",
      "Epoch [183/1000], Train Loss: 4730.8599, Test Loss: 2410.3160\n",
      "Epoch [184/1000], Train Loss: 4513.4276, Test Loss: 2232.2545\n",
      "Epoch [185/1000], Train Loss: 4384.3698, Test Loss: 2137.0233\n",
      "Epoch [186/1000], Train Loss: 7402.7386, Test Loss: 2950.7254\n",
      "Epoch [187/1000], Train Loss: 4357.3382, Test Loss: 2314.6719\n",
      "Epoch [188/1000], Train Loss: 4063.7856, Test Loss: 2330.5526\n",
      "Epoch [189/1000], Train Loss: 3971.4961, Test Loss: 2201.6641\n",
      "Epoch [190/1000], Train Loss: 4006.3187, Test Loss: 2624.4324\n",
      "Epoch [191/1000], Train Loss: 4276.2930, Test Loss: 2916.1486\n",
      "Epoch [192/1000], Train Loss: 4503.0906, Test Loss: 2296.6566\n",
      "Epoch [193/1000], Train Loss: 4247.4588, Test Loss: 2283.4293\n",
      "Epoch [194/1000], Train Loss: 5845.0219, Test Loss: 2343.4883\n",
      "Epoch [195/1000], Train Loss: 4400.2139, Test Loss: 2463.6833\n",
      "Epoch [196/1000], Train Loss: 4079.2433, Test Loss: 2212.0684\n",
      "Epoch [197/1000], Train Loss: 3953.0287, Test Loss: 2360.1511\n",
      "Epoch [198/1000], Train Loss: 4374.8559, Test Loss: 2240.2110\n",
      "Epoch [199/1000], Train Loss: 4747.8236, Test Loss: 2719.8384\n",
      "Epoch [200/1000], Train Loss: 4517.9866, Test Loss: 2488.9595\n",
      "Epoch [201/1000], Train Loss: 4038.4661, Test Loss: 2182.8568\n",
      "Epoch [202/1000], Train Loss: 4002.7450, Test Loss: 2240.7611\n",
      "Epoch [203/1000], Train Loss: 4154.1785, Test Loss: 2151.1795\n",
      "Epoch [204/1000], Train Loss: 4005.6499, Test Loss: 2319.1144\n",
      "Epoch [205/1000], Train Loss: 4220.2784, Test Loss: 2292.5232\n",
      "Epoch [206/1000], Train Loss: 4394.1649, Test Loss: 2179.9990\n",
      "Epoch [207/1000], Train Loss: 4196.3121, Test Loss: 2357.5736\n",
      "Epoch [208/1000], Train Loss: 4384.7324, Test Loss: 2223.2681\n",
      "Epoch [209/1000], Train Loss: 4020.5143, Test Loss: 2188.6124\n",
      "Epoch [210/1000], Train Loss: 4060.2058, Test Loss: 2702.9599\n",
      "Epoch [211/1000], Train Loss: 4301.4513, Test Loss: 2186.7220\n",
      "Epoch [212/1000], Train Loss: 3907.5374, Test Loss: 2170.5694\n",
      "Epoch [213/1000], Train Loss: 4394.6508, Test Loss: 2285.5416\n",
      "Epoch [214/1000], Train Loss: 3894.4780, Test Loss: 2035.4799\n",
      "Epoch [215/1000], Train Loss: 4130.7428, Test Loss: 2425.2874\n",
      "Epoch [216/1000], Train Loss: 4690.8047, Test Loss: 2129.9549\n",
      "Epoch [217/1000], Train Loss: 3571.4809, Test Loss: 2362.0700\n",
      "Epoch [218/1000], Train Loss: 3696.7600, Test Loss: 2238.6083\n",
      "Epoch [219/1000], Train Loss: 3958.5873, Test Loss: 2191.2811\n",
      "Epoch [220/1000], Train Loss: 3959.3184, Test Loss: 2225.5987\n",
      "Epoch [221/1000], Train Loss: 7004.5041, Test Loss: 2504.9572\n",
      "Epoch [222/1000], Train Loss: 3739.5887, Test Loss: 2203.8413\n",
      "Epoch [223/1000], Train Loss: 3245.7610, Test Loss: 2062.5322\n",
      "Epoch [224/1000], Train Loss: 3270.7432, Test Loss: 2193.1416\n",
      "Epoch [225/1000], Train Loss: 3590.8733, Test Loss: 2109.2820\n",
      "Epoch [226/1000], Train Loss: 3456.2273, Test Loss: 2177.8120\n",
      "Epoch [227/1000], Train Loss: 4042.3533, Test Loss: 2626.8001\n",
      "Epoch [228/1000], Train Loss: 3644.5292, Test Loss: 2196.8296\n",
      "Epoch [229/1000], Train Loss: 3672.9667, Test Loss: 2093.0068\n",
      "Epoch [230/1000], Train Loss: 3800.7633, Test Loss: 2284.7967\n",
      "Epoch [231/1000], Train Loss: 3936.3528, Test Loss: 2155.7153\n",
      "Epoch [232/1000], Train Loss: 3704.4074, Test Loss: 2176.9144\n",
      "Epoch [233/1000], Train Loss: 4838.3056, Test Loss: 2100.3205\n",
      "Epoch [234/1000], Train Loss: 3403.8274, Test Loss: 2347.1402\n",
      "Epoch [235/1000], Train Loss: 3343.2313, Test Loss: 2133.1767\n",
      "Epoch [236/1000], Train Loss: 3389.1918, Test Loss: 2123.6642\n",
      "Epoch [237/1000], Train Loss: 4137.5020, Test Loss: 2532.9028\n",
      "Epoch [238/1000], Train Loss: 3829.2567, Test Loss: 2260.0214\n",
      "Epoch [239/1000], Train Loss: 3369.7235, Test Loss: 2057.3312\n",
      "Epoch [240/1000], Train Loss: 3484.5584, Test Loss: 2161.3471\n",
      "Epoch [241/1000], Train Loss: 3535.1856, Test Loss: 2168.4572\n",
      "Epoch [242/1000], Train Loss: 3743.9301, Test Loss: 2326.3590\n",
      "Epoch [243/1000], Train Loss: 3972.9552, Test Loss: 2138.2013\n",
      "Epoch [244/1000], Train Loss: 3488.5000, Test Loss: 2144.0073\n",
      "Epoch [245/1000], Train Loss: 3338.2042, Test Loss: 2217.4632\n",
      "Epoch [246/1000], Train Loss: 3506.9794, Test Loss: 2358.0292\n",
      "Epoch [247/1000], Train Loss: 3624.4211, Test Loss: 2130.4800\n",
      "Epoch [248/1000], Train Loss: 3569.6684, Test Loss: 2301.5421\n",
      "Epoch [249/1000], Train Loss: 3248.7135, Test Loss: 2060.9485\n",
      "Epoch [250/1000], Train Loss: 7282.6478, Test Loss: 2064.8194\n",
      "Epoch [251/1000], Train Loss: 2884.6129, Test Loss: 2116.7096\n",
      "Epoch [252/1000], Train Loss: 2770.1762, Test Loss: 2035.7315\n",
      "Epoch [253/1000], Train Loss: 2981.9245, Test Loss: 2196.1769\n",
      "Epoch [254/1000], Train Loss: 3149.8687, Test Loss: 2127.9424\n",
      "Epoch [255/1000], Train Loss: 3194.0231, Test Loss: 2085.3938\n",
      "Epoch [256/1000], Train Loss: 3065.7116, Test Loss: 2573.9137\n",
      "Epoch [257/1000], Train Loss: 3806.7356, Test Loss: 2237.4607\n",
      "Epoch [258/1000], Train Loss: 3122.0797, Test Loss: 2135.9734\n",
      "Epoch [259/1000], Train Loss: 3898.1020, Test Loss: 2055.7179\n",
      "Epoch [260/1000], Train Loss: 2949.6350, Test Loss: 2289.0858\n",
      "Epoch [261/1000], Train Loss: 3056.5627, Test Loss: 2120.5481\n",
      "Epoch [262/1000], Train Loss: 3520.3366, Test Loss: 2555.5895\n",
      "Epoch [263/1000], Train Loss: 4102.2674, Test Loss: 2040.0428\n",
      "Epoch [264/1000], Train Loss: 3159.3342, Test Loss: 2213.6376\n",
      "Epoch [265/1000], Train Loss: 3065.1236, Test Loss: 2211.3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [266/1000], Train Loss: 3523.1410, Test Loss: 2339.7829\n",
      "Epoch [267/1000], Train Loss: 3162.4724, Test Loss: 2065.6947\n",
      "Epoch [268/1000], Train Loss: 3065.9638, Test Loss: 2215.0701\n",
      "Epoch [269/1000], Train Loss: 3350.9558, Test Loss: 2483.2329\n",
      "Epoch [270/1000], Train Loss: 3425.3594, Test Loss: 2227.0854\n",
      "Epoch [271/1000], Train Loss: 3244.6595, Test Loss: 2060.2157\n",
      "Epoch [272/1000], Train Loss: 3013.0728, Test Loss: 2049.3055\n",
      "Epoch [273/1000], Train Loss: 3535.8040, Test Loss: 2059.1684\n",
      "Epoch [274/1000], Train Loss: 2977.9206, Test Loss: 2238.0903\n",
      "Epoch [275/1000], Train Loss: 4003.7485, Test Loss: 2237.7747\n",
      "Epoch [276/1000], Train Loss: 3548.8031, Test Loss: 2151.9642\n",
      "Epoch [277/1000], Train Loss: 2856.6641, Test Loss: 2091.3683\n",
      "Epoch [278/1000], Train Loss: 2827.6012, Test Loss: 2202.6137\n",
      "Epoch [279/1000], Train Loss: 2839.6183, Test Loss: 2506.4432\n",
      "Epoch [280/1000], Train Loss: 3362.9096, Test Loss: 2205.9386\n",
      "Epoch [281/1000], Train Loss: 2989.4387, Test Loss: 2057.6932\n",
      "Epoch [282/1000], Train Loss: 4134.0521, Test Loss: 2114.8271\n",
      "Epoch [283/1000], Train Loss: 3278.6209, Test Loss: 2302.6624\n",
      "Epoch [284/1000], Train Loss: 3051.9977, Test Loss: 2167.7274\n",
      "Epoch [285/1000], Train Loss: 2991.1739, Test Loss: 2170.7786\n",
      "Epoch [286/1000], Train Loss: 3142.1799, Test Loss: 2140.6867\n",
      "Epoch [287/1000], Train Loss: 3171.1808, Test Loss: 2263.2856\n",
      "Epoch [288/1000], Train Loss: 2995.4392, Test Loss: 2068.9709\n",
      "Epoch [289/1000], Train Loss: 2814.4067, Test Loss: 2238.2220\n",
      "Epoch [290/1000], Train Loss: 4989.6500, Test Loss: 2009.6847\n",
      "Epoch [291/1000], Train Loss: 2831.4502, Test Loss: 2055.5049\n",
      "Epoch [292/1000], Train Loss: 2606.8412, Test Loss: 2103.5369\n",
      "Epoch [293/1000], Train Loss: 2749.1259, Test Loss: 2038.7939\n",
      "Epoch [294/1000], Train Loss: 2796.0363, Test Loss: 2176.0893\n",
      "Epoch [295/1000], Train Loss: 2818.7486, Test Loss: 2093.3229\n",
      "Epoch [296/1000], Train Loss: 3040.6550, Test Loss: 4113.6629\n",
      "Epoch [297/1000], Train Loss: 3897.8478, Test Loss: 2096.2760\n",
      "Epoch [298/1000], Train Loss: 2619.0935, Test Loss: 2056.0763\n",
      "Epoch [299/1000], Train Loss: 2614.0410, Test Loss: 2071.1089\n",
      "Epoch [300/1000], Train Loss: 2701.7031, Test Loss: 2154.6584\n",
      "Epoch [301/1000], Train Loss: 2966.7784, Test Loss: 2039.3508\n",
      "Epoch [302/1000], Train Loss: 3066.0195, Test Loss: 2201.3845\n",
      "Epoch [303/1000], Train Loss: 3275.4658, Test Loss: 2085.8084\n",
      "Epoch [304/1000], Train Loss: 2791.0531, Test Loss: 2099.8980\n",
      "Epoch [305/1000], Train Loss: 2991.4860, Test Loss: 2635.5956\n",
      "Epoch [306/1000], Train Loss: 3030.3501, Test Loss: 2089.4876\n",
      "Epoch [307/1000], Train Loss: 2924.0501, Test Loss: 2226.5157\n",
      "Epoch [308/1000], Train Loss: 3558.5822, Test Loss: 2311.2751\n",
      "Epoch [309/1000], Train Loss: 2897.7950, Test Loss: 2034.5366\n",
      "Epoch [310/1000], Train Loss: 2729.9098, Test Loss: 2089.2264\n",
      "Epoch [311/1000], Train Loss: 2808.8286, Test Loss: 2189.9216\n",
      "Epoch [312/1000], Train Loss: 3385.5411, Test Loss: 2099.0332\n",
      "Epoch [313/1000], Train Loss: 3113.0302, Test Loss: 2362.3758\n",
      "Epoch [314/1000], Train Loss: 4338.4751, Test Loss: 2042.6006\n",
      "Epoch [315/1000], Train Loss: 2562.8892, Test Loss: 2043.9029\n",
      "Epoch [316/1000], Train Loss: 2341.3460, Test Loss: 2033.1981\n",
      "Epoch [317/1000], Train Loss: 2441.8070, Test Loss: 2033.8591\n",
      "Epoch [318/1000], Train Loss: 2450.0991, Test Loss: 2089.8739\n",
      "Epoch [319/1000], Train Loss: 2543.6480, Test Loss: 2016.7886\n",
      "Epoch [320/1000], Train Loss: 2784.3368, Test Loss: 2321.8920\n",
      "Epoch [321/1000], Train Loss: 2735.9587, Test Loss: 2055.8549\n",
      "Epoch [322/1000], Train Loss: 2762.8454, Test Loss: 2151.6171\n",
      "Epoch [323/1000], Train Loss: 24535.0012, Test Loss: 2500.1079\n",
      "Epoch [324/1000], Train Loss: 3284.8024, Test Loss: 2000.1966\n",
      "Epoch [325/1000], Train Loss: 2329.5391, Test Loss: 1985.9508\n",
      "Epoch [326/1000], Train Loss: 2128.4136, Test Loss: 2264.8976\n",
      "Epoch [327/1000], Train Loss: 2207.7963, Test Loss: 2071.2609\n",
      "Epoch [328/1000], Train Loss: 2286.3037, Test Loss: 2073.1955\n",
      "Epoch [329/1000], Train Loss: 2434.2734, Test Loss: 2068.1066\n",
      "Epoch [330/1000], Train Loss: 2363.4609, Test Loss: 2041.0489\n",
      "Epoch [331/1000], Train Loss: 2427.8499, Test Loss: 2225.0405\n",
      "Epoch [332/1000], Train Loss: 2604.6904, Test Loss: 2296.4841\n",
      "Epoch [333/1000], Train Loss: 2563.9907, Test Loss: 2391.4966\n",
      "Epoch [334/1000], Train Loss: 2459.3573, Test Loss: 2089.5117\n",
      "Epoch [335/1000], Train Loss: 2566.9838, Test Loss: 2246.8193\n",
      "Epoch [336/1000], Train Loss: 2768.5008, Test Loss: 2074.3243\n",
      "Epoch [337/1000], Train Loss: 2507.8938, Test Loss: 2056.6171\n",
      "Epoch [338/1000], Train Loss: 2528.0188, Test Loss: 2276.7057\n",
      "Epoch [339/1000], Train Loss: 3149.9037, Test Loss: 2135.5622\n",
      "Epoch [340/1000], Train Loss: 2974.1946, Test Loss: 2016.6587\n",
      "Epoch [341/1000], Train Loss: 2791.3002, Test Loss: 2078.6796\n",
      "Epoch [342/1000], Train Loss: 2397.0916, Test Loss: 2143.6469\n",
      "Epoch [343/1000], Train Loss: 2400.4780, Test Loss: 2155.9727\n",
      "Epoch [344/1000], Train Loss: 2862.8853, Test Loss: 2309.5146\n",
      "Epoch [345/1000], Train Loss: 2932.2925, Test Loss: 2164.2593\n",
      "Epoch [346/1000], Train Loss: 2641.4146, Test Loss: 2102.2129\n",
      "Epoch [347/1000], Train Loss: 2814.0141, Test Loss: 2394.6218\n",
      "Epoch [348/1000], Train Loss: 2871.4443, Test Loss: 2069.0415\n",
      "Epoch [349/1000], Train Loss: 2629.6164, Test Loss: 2316.1548\n",
      "Epoch [350/1000], Train Loss: 2544.4955, Test Loss: 2220.0106\n",
      "Epoch [351/1000], Train Loss: 2528.2291, Test Loss: 2195.6135\n",
      "Epoch [352/1000], Train Loss: 2656.3984, Test Loss: 2177.0446\n",
      "Epoch [353/1000], Train Loss: 2668.3011, Test Loss: 2111.4360\n",
      "Epoch [354/1000], Train Loss: 2480.4501, Test Loss: 2278.8349\n",
      "Epoch [355/1000], Train Loss: 2408.5992, Test Loss: 2125.1224\n",
      "Epoch [356/1000], Train Loss: 2354.2651, Test Loss: 2131.3868\n",
      "Epoch [357/1000], Train Loss: 3433.2769, Test Loss: 2638.6068\n",
      "Epoch [358/1000], Train Loss: 2643.8109, Test Loss: 2069.3319\n",
      "Epoch [359/1000], Train Loss: 2454.0191, Test Loss: 2098.2020\n",
      "Epoch [360/1000], Train Loss: 2298.0997, Test Loss: 2179.4266\n",
      "Epoch [361/1000], Train Loss: 2435.5171, Test Loss: 2141.2976\n",
      "Epoch [362/1000], Train Loss: 3410.9927, Test Loss: 2275.5796\n",
      "Epoch [363/1000], Train Loss: 2365.1991, Test Loss: 2289.5928\n",
      "Epoch [364/1000], Train Loss: 2468.8338, Test Loss: 2262.6662\n",
      "Epoch [365/1000], Train Loss: 2744.6592, Test Loss: 1993.9570\n",
      "Epoch [366/1000], Train Loss: 2149.3392, Test Loss: 2094.0009\n",
      "Epoch [367/1000], Train Loss: 2465.7417, Test Loss: 2169.6793\n",
      "Epoch [368/1000], Train Loss: 2320.6263, Test Loss: 2132.6074\n",
      "Epoch [369/1000], Train Loss: 2461.1825, Test Loss: 2183.6979\n",
      "Epoch [370/1000], Train Loss: 2986.3534, Test Loss: 2171.3291\n",
      "Epoch [371/1000], Train Loss: 2616.5208, Test Loss: 2109.8462\n",
      "Epoch [372/1000], Train Loss: 3247.0090, Test Loss: 2309.1400\n",
      "Epoch [373/1000], Train Loss: 2229.6751, Test Loss: 2031.4911\n",
      "Epoch [374/1000], Train Loss: 3103.8101, Test Loss: 3144.2273\n",
      "Epoch [375/1000], Train Loss: 3114.5623, Test Loss: 2150.7397\n",
      "Epoch [376/1000], Train Loss: 2564.7136, Test Loss: 2055.7375\n",
      "Epoch [377/1000], Train Loss: 2150.6914, Test Loss: 2069.9164\n",
      "Epoch [378/1000], Train Loss: 2345.1483, Test Loss: 2055.9275\n",
      "Epoch [379/1000], Train Loss: 2233.2222, Test Loss: 2169.7374\n",
      "Epoch [380/1000], Train Loss: 2674.3412, Test Loss: 2095.8490\n",
      "Epoch [381/1000], Train Loss: 2515.3732, Test Loss: 2172.4516\n",
      "Epoch [382/1000], Train Loss: 2283.1961, Test Loss: 2083.5395\n",
      "Epoch [383/1000], Train Loss: 2258.2939, Test Loss: 2109.5524\n",
      "Epoch [384/1000], Train Loss: 2289.2857, Test Loss: 2217.8944\n",
      "Epoch [385/1000], Train Loss: 2570.2818, Test Loss: 2104.7002\n",
      "Epoch [386/1000], Train Loss: 2608.5689, Test Loss: 2034.4660\n",
      "Epoch [387/1000], Train Loss: 2467.1105, Test Loss: 2026.5158\n",
      "Epoch [388/1000], Train Loss: 2220.3691, Test Loss: 2194.8383\n",
      "Epoch [389/1000], Train Loss: 2269.4161, Test Loss: 2177.7319\n",
      "Epoch [390/1000], Train Loss: 4120.1080, Test Loss: 2084.0646\n",
      "Epoch [391/1000], Train Loss: 2191.9083, Test Loss: 2198.1087\n",
      "Epoch [392/1000], Train Loss: 2222.5230, Test Loss: 2095.0288\n",
      "Epoch [393/1000], Train Loss: 2204.6380, Test Loss: 2122.5997\n",
      "Epoch [394/1000], Train Loss: 7981.5390, Test Loss: 2257.0435\n",
      "Epoch [395/1000], Train Loss: 2376.9128, Test Loss: 2004.3364\n",
      "Epoch [396/1000], Train Loss: 1858.6577, Test Loss: 2059.6691\n",
      "Epoch [397/1000], Train Loss: 1786.4467, Test Loss: 2004.6723\n",
      "Epoch [398/1000], Train Loss: 1991.4453, Test Loss: 2029.8923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [399/1000], Train Loss: 2036.8662, Test Loss: 2061.7445\n",
      "Epoch [400/1000], Train Loss: 1994.5520, Test Loss: 2077.5549\n",
      "Epoch [401/1000], Train Loss: 2031.4671, Test Loss: 2114.1121\n",
      "Epoch [402/1000], Train Loss: 2261.7014, Test Loss: 2150.8070\n",
      "Epoch [403/1000], Train Loss: 2182.6399, Test Loss: 2085.1664\n",
      "Epoch [404/1000], Train Loss: 2262.9374, Test Loss: 2162.1850\n",
      "Epoch [405/1000], Train Loss: 2102.2711, Test Loss: 2195.3092\n",
      "Epoch [406/1000], Train Loss: 2743.1111, Test Loss: 2222.4351\n",
      "Epoch [407/1000], Train Loss: 2128.0066, Test Loss: 2101.5755\n",
      "Epoch [408/1000], Train Loss: 2063.9809, Test Loss: 2151.1825\n",
      "Epoch [409/1000], Train Loss: 2496.0659, Test Loss: 2410.4303\n",
      "Epoch [410/1000], Train Loss: 2315.9901, Test Loss: 2060.3606\n",
      "Epoch [411/1000], Train Loss: 2299.1827, Test Loss: 2084.2139\n",
      "Epoch [412/1000], Train Loss: 2118.8016, Test Loss: 2100.8966\n",
      "Epoch [413/1000], Train Loss: 2440.5853, Test Loss: 2138.1127\n",
      "Epoch [414/1000], Train Loss: 2662.5023, Test Loss: 2277.8667\n",
      "Epoch [415/1000], Train Loss: 2265.1432, Test Loss: 2120.6980\n",
      "Epoch [416/1000], Train Loss: 1979.9642, Test Loss: 2084.3959\n",
      "Epoch [417/1000], Train Loss: 2488.2224, Test Loss: 2108.3461\n",
      "Epoch [418/1000], Train Loss: 2024.7932, Test Loss: 2036.1626\n",
      "Epoch [419/1000], Train Loss: 1965.5369, Test Loss: 2219.8892\n",
      "Epoch [420/1000], Train Loss: 4806.6988, Test Loss: 2323.1888\n",
      "Epoch [421/1000], Train Loss: 2160.4644, Test Loss: 2032.5791\n",
      "Epoch [422/1000], Train Loss: 1987.1833, Test Loss: 2056.6091\n",
      "Epoch [423/1000], Train Loss: 1769.1650, Test Loss: 2026.7225\n",
      "Epoch [424/1000], Train Loss: 1918.4650, Test Loss: 2111.5642\n",
      "Epoch [425/1000], Train Loss: 1899.0393, Test Loss: 2073.7974\n",
      "Epoch [426/1000], Train Loss: 1999.5615, Test Loss: 2093.5082\n",
      "Epoch [427/1000], Train Loss: 2070.1184, Test Loss: 2146.4941\n",
      "Epoch [428/1000], Train Loss: 2345.8056, Test Loss: 2267.5759\n",
      "Epoch [429/1000], Train Loss: 2326.0774, Test Loss: 2138.0654\n",
      "Epoch [430/1000], Train Loss: 2134.6536, Test Loss: 2357.4603\n",
      "Epoch [431/1000], Train Loss: 2569.0891, Test Loss: 2080.1787\n",
      "Epoch [432/1000], Train Loss: 2361.9548, Test Loss: 2096.0915\n",
      "Epoch [433/1000], Train Loss: 2148.8070, Test Loss: 2130.4325\n",
      "Epoch [434/1000], Train Loss: 2635.8250, Test Loss: 2124.7357\n",
      "Epoch [435/1000], Train Loss: 2000.2609, Test Loss: 2049.9384\n",
      "Epoch [436/1000], Train Loss: 1915.8483, Test Loss: 2144.2829\n",
      "Epoch [437/1000], Train Loss: 2133.6036, Test Loss: 2095.5640\n",
      "Epoch [438/1000], Train Loss: 1986.8020, Test Loss: 2146.3092\n",
      "Epoch [439/1000], Train Loss: 2134.9109, Test Loss: 2116.3754\n",
      "Epoch [440/1000], Train Loss: 2071.4647, Test Loss: 2113.3430\n",
      "Epoch [441/1000], Train Loss: 2122.4979, Test Loss: 2163.2352\n",
      "Epoch [442/1000], Train Loss: 2056.3959, Test Loss: 2080.6158\n",
      "Epoch [443/1000], Train Loss: 2141.6487, Test Loss: 2160.5590\n",
      "Epoch [444/1000], Train Loss: 2021.6132, Test Loss: 2167.3327\n",
      "Epoch [445/1000], Train Loss: 2681.6111, Test Loss: 2156.1265\n",
      "Epoch [446/1000], Train Loss: 2012.9190, Test Loss: 2090.7281\n",
      "Epoch [447/1000], Train Loss: 2188.8825, Test Loss: 2225.9202\n",
      "Epoch [448/1000], Train Loss: 2019.4190, Test Loss: 2377.4989\n",
      "Epoch [449/1000], Train Loss: 2628.2286, Test Loss: 2452.8843\n",
      "Epoch [450/1000], Train Loss: 2220.5437, Test Loss: 2217.7627\n",
      "Epoch [451/1000], Train Loss: 3487.0593, Test Loss: 2007.0099\n",
      "Epoch [452/1000], Train Loss: 1637.2414, Test Loss: 2022.4536\n",
      "Epoch [453/1000], Train Loss: 1699.9920, Test Loss: 2268.9772\n",
      "Epoch [454/1000], Train Loss: 1859.7010, Test Loss: 2234.5166\n",
      "Epoch [455/1000], Train Loss: 1873.8985, Test Loss: 2185.8017\n",
      "Epoch [456/1000], Train Loss: 1938.7558, Test Loss: 2319.3036\n",
      "Epoch [457/1000], Train Loss: 2024.7925, Test Loss: 2474.8793\n",
      "Epoch [458/1000], Train Loss: 2329.4900, Test Loss: 2375.3302\n",
      "Epoch [459/1000], Train Loss: 2250.3446, Test Loss: 2370.8022\n",
      "Epoch [460/1000], Train Loss: 2769.6268, Test Loss: 2059.4867\n",
      "Epoch [461/1000], Train Loss: 2006.3870, Test Loss: 2191.0591\n",
      "Epoch [462/1000], Train Loss: 2592.9540, Test Loss: 2257.6840\n",
      "Epoch [463/1000], Train Loss: 1837.7051, Test Loss: 2111.3569\n",
      "Epoch [464/1000], Train Loss: 1727.1955, Test Loss: 2095.2540\n",
      "Epoch [465/1000], Train Loss: 2183.5473, Test Loss: 2266.3288\n",
      "Epoch [466/1000], Train Loss: 3040.4997, Test Loss: 2328.8460\n",
      "Epoch [467/1000], Train Loss: 2235.3800, Test Loss: 2138.7686\n",
      "Epoch [468/1000], Train Loss: 1804.2255, Test Loss: 2184.0765\n",
      "Epoch [469/1000], Train Loss: 1865.2367, Test Loss: 2159.9054\n",
      "Epoch [470/1000], Train Loss: 1849.4396, Test Loss: 2092.6646\n",
      "Epoch [471/1000], Train Loss: 1797.8673, Test Loss: 2135.7268\n",
      "Epoch [472/1000], Train Loss: 1957.4281, Test Loss: 2385.7640\n",
      "Epoch [473/1000], Train Loss: 1914.0958, Test Loss: 2157.4839\n",
      "Epoch [474/1000], Train Loss: 1859.0529, Test Loss: 2249.7098\n",
      "Epoch [475/1000], Train Loss: 2456.6578, Test Loss: 2105.8538\n",
      "Epoch [476/1000], Train Loss: 1776.8610, Test Loss: 2181.6652\n",
      "Epoch [477/1000], Train Loss: 2312.8399, Test Loss: 2243.7972\n",
      "Epoch [478/1000], Train Loss: 1984.8140, Test Loss: 2123.3324\n",
      "Epoch [479/1000], Train Loss: 12583.1231, Test Loss: 3075.6342\n",
      "Epoch [480/1000], Train Loss: 2988.8890, Test Loss: 2354.9746\n",
      "Epoch [481/1000], Train Loss: 7722.2483, Test Loss: 2348.0287\n",
      "Epoch [482/1000], Train Loss: 73144.5143, Test Loss: 12929.9350\n",
      "Epoch [483/1000], Train Loss: 29578.1375, Test Loss: 3889.1232\n",
      "Epoch [484/1000], Train Loss: 42327.8580, Test Loss: 2509.9614\n",
      "Epoch [485/1000], Train Loss: 3355.1681, Test Loss: 2036.2883\n",
      "Epoch [486/1000], Train Loss: 31836.4198, Test Loss: 3611.3682\n",
      "Epoch [487/1000], Train Loss: 9406.7690, Test Loss: 2978.7284\n",
      "Epoch [488/1000], Train Loss: 3398.8363, Test Loss: 2221.9732\n",
      "Epoch [489/1000], Train Loss: 2948.5189, Test Loss: 2372.2242\n",
      "Epoch [490/1000], Train Loss: 2914.5275, Test Loss: 2398.2634\n",
      "Epoch [491/1000], Train Loss: 17172.9512, Test Loss: 2163.1625\n",
      "Epoch [492/1000], Train Loss: 2300.4303, Test Loss: 2011.5739\n",
      "Epoch [493/1000], Train Loss: 2543.8093, Test Loss: 2036.9517\n",
      "Epoch [494/1000], Train Loss: 2033.4034, Test Loss: 2123.8445\n",
      "Epoch [495/1000], Train Loss: 12021.4278, Test Loss: 3944.9942\n",
      "Epoch [496/1000], Train Loss: 5108.2831, Test Loss: 2056.1035\n",
      "Epoch [497/1000], Train Loss: 1821.0962, Test Loss: 1970.7295\n",
      "Epoch [498/1000], Train Loss: 1820.5357, Test Loss: 2009.0365\n",
      "Epoch [499/1000], Train Loss: 1837.4252, Test Loss: 1999.2850\n",
      "Epoch [500/1000], Train Loss: 1800.6573, Test Loss: 2064.3025\n",
      "Epoch [501/1000], Train Loss: 1851.9772, Test Loss: 2018.2078\n",
      "Epoch [502/1000], Train Loss: 1863.4713, Test Loss: 2027.8277\n",
      "Epoch [503/1000], Train Loss: 1820.4923, Test Loss: 2161.0536\n",
      "Epoch [504/1000], Train Loss: 2020.1452, Test Loss: 2177.0270\n",
      "Epoch [505/1000], Train Loss: 1860.8317, Test Loss: 2117.7735\n",
      "Epoch [506/1000], Train Loss: 2169.0369, Test Loss: 2115.2441\n",
      "Epoch [507/1000], Train Loss: 1931.0970, Test Loss: 2090.4693\n",
      "Epoch [508/1000], Train Loss: 1947.3148, Test Loss: 2177.7711\n",
      "Epoch [509/1000], Train Loss: 2353.8490, Test Loss: 2089.2911\n",
      "Epoch [510/1000], Train Loss: 2498.0690, Test Loss: 3516.6804\n",
      "Epoch [511/1000], Train Loss: 2339.1617, Test Loss: 2110.3861\n",
      "Epoch [512/1000], Train Loss: 2190.4156, Test Loss: 2078.6630\n",
      "Epoch [513/1000], Train Loss: 1739.8857, Test Loss: 2065.2634\n",
      "Epoch [514/1000], Train Loss: 1803.0096, Test Loss: 2051.1415\n",
      "Epoch [515/1000], Train Loss: 1909.8668, Test Loss: 2260.3057\n",
      "Epoch [516/1000], Train Loss: 2174.7662, Test Loss: 2085.9563\n",
      "Epoch [517/1000], Train Loss: 1913.1317, Test Loss: 2108.2688\n",
      "Epoch [518/1000], Train Loss: 2891.2286, Test Loss: 2037.9700\n",
      "Epoch [519/1000], Train Loss: 1731.3801, Test Loss: 2359.8052\n",
      "Epoch [520/1000], Train Loss: 1747.9588, Test Loss: 2030.5393\n",
      "Epoch [521/1000], Train Loss: 1736.5752, Test Loss: 2130.2754\n",
      "Epoch [522/1000], Train Loss: 2293.5194, Test Loss: 2076.2943\n",
      "Epoch [523/1000], Train Loss: 1793.8048, Test Loss: 2177.1801\n",
      "Epoch [524/1000], Train Loss: 1767.3907, Test Loss: 2113.1134\n",
      "Epoch [525/1000], Train Loss: 1848.7387, Test Loss: 2061.3538\n",
      "Epoch [526/1000], Train Loss: 2185.9251, Test Loss: 2175.3376\n",
      "Epoch [527/1000], Train Loss: 2192.1022, Test Loss: 2145.6724\n",
      "Epoch [528/1000], Train Loss: 2204.3018, Test Loss: 2106.8592\n",
      "Epoch [529/1000], Train Loss: 1792.1689, Test Loss: 2131.3307\n",
      "Epoch [530/1000], Train Loss: 1874.5645, Test Loss: 2427.4088\n",
      "Epoch [531/1000], Train Loss: 2145.6601, Test Loss: 2081.0887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [532/1000], Train Loss: 1794.7035, Test Loss: 2057.8900\n",
      "Epoch [533/1000], Train Loss: 1629.5139, Test Loss: 2244.6138\n",
      "Epoch [534/1000], Train Loss: 2088.9940, Test Loss: 2059.3092\n",
      "Epoch [535/1000], Train Loss: 2079.3039, Test Loss: 2084.6750\n",
      "Epoch [536/1000], Train Loss: 2065.9437, Test Loss: 2170.8533\n",
      "Epoch [537/1000], Train Loss: 1925.0531, Test Loss: 2035.4135\n",
      "Epoch [538/1000], Train Loss: 1636.1043, Test Loss: 2207.9952\n",
      "Epoch [539/1000], Train Loss: 1763.6659, Test Loss: 2139.4610\n",
      "Epoch [540/1000], Train Loss: 1935.5193, Test Loss: 2217.9338\n",
      "Epoch [541/1000], Train Loss: 1798.2810, Test Loss: 2101.7441\n",
      "Epoch [542/1000], Train Loss: 1840.7129, Test Loss: 2247.1317\n",
      "Epoch [543/1000], Train Loss: 2421.0248, Test Loss: 2379.1626\n",
      "Epoch [544/1000], Train Loss: 2230.1643, Test Loss: 2110.8596\n",
      "Epoch [545/1000], Train Loss: 1856.2455, Test Loss: 2065.3306\n",
      "Epoch [546/1000], Train Loss: 1756.8886, Test Loss: 2101.5350\n",
      "Epoch [547/1000], Train Loss: 1770.2479, Test Loss: 2096.7286\n",
      "Epoch [548/1000], Train Loss: 1610.8383, Test Loss: 2084.5833\n",
      "Epoch [549/1000], Train Loss: 1950.9620, Test Loss: 2099.9414\n",
      "Epoch [550/1000], Train Loss: 1826.1980, Test Loss: 2213.5264\n",
      "Epoch [551/1000], Train Loss: 2044.0448, Test Loss: 2108.9541\n",
      "Epoch [552/1000], Train Loss: 1540.1768, Test Loss: 2090.8173\n",
      "Epoch [553/1000], Train Loss: 1880.5948, Test Loss: 2569.7906\n",
      "Epoch [554/1000], Train Loss: 2153.1215, Test Loss: 2083.5245\n",
      "Epoch [555/1000], Train Loss: 1579.1499, Test Loss: 2106.3633\n",
      "Epoch [556/1000], Train Loss: 1767.5766, Test Loss: 2206.7456\n",
      "Epoch [557/1000], Train Loss: 1776.9341, Test Loss: 2131.9946\n",
      "Epoch [558/1000], Train Loss: 1704.3710, Test Loss: 2243.9127\n",
      "Epoch [559/1000], Train Loss: 2005.3738, Test Loss: 2042.0297\n",
      "Epoch [560/1000], Train Loss: 1857.9404, Test Loss: 2218.7742\n",
      "Epoch [561/1000], Train Loss: 2746.6991, Test Loss: 2118.5127\n",
      "Epoch [562/1000], Train Loss: 2508.6653, Test Loss: 2051.0490\n",
      "Epoch [563/1000], Train Loss: 1447.1481, Test Loss: 2140.4142\n",
      "Epoch [564/1000], Train Loss: 1488.6195, Test Loss: 2089.3054\n",
      "Epoch [565/1000], Train Loss: 1763.4241, Test Loss: 2196.0316\n",
      "Epoch [566/1000], Train Loss: 1943.2886, Test Loss: 2157.1766\n",
      "Epoch [567/1000], Train Loss: 1889.2445, Test Loss: 2206.5765\n",
      "Epoch [568/1000], Train Loss: 1628.1333, Test Loss: 2193.1448\n",
      "Epoch [569/1000], Train Loss: 2431.9133, Test Loss: 2144.6604\n",
      "Epoch [570/1000], Train Loss: 1571.8329, Test Loss: 2105.1929\n",
      "Epoch [571/1000], Train Loss: 1960.5817, Test Loss: 2262.0806\n",
      "Epoch [572/1000], Train Loss: 1657.5606, Test Loss: 2051.3946\n",
      "Epoch [573/1000], Train Loss: 1595.3746, Test Loss: 2097.8064\n",
      "Epoch [574/1000], Train Loss: 1676.7323, Test Loss: 2066.5653\n",
      "Epoch [575/1000], Train Loss: 1880.7675, Test Loss: 2115.6543\n",
      "Epoch [576/1000], Train Loss: 2284.1134, Test Loss: 2145.4931\n",
      "Epoch [577/1000], Train Loss: 1629.7226, Test Loss: 2107.1047\n",
      "Epoch [578/1000], Train Loss: 1546.0522, Test Loss: 2204.9413\n",
      "Epoch [579/1000], Train Loss: 2336.7643, Test Loss: 3923.4726\n",
      "Epoch [580/1000], Train Loss: 2417.1119, Test Loss: 2119.8387\n",
      "Epoch [581/1000], Train Loss: 1704.7767, Test Loss: 2105.0311\n",
      "Epoch [582/1000], Train Loss: 1482.8062, Test Loss: 2068.1433\n",
      "Epoch [583/1000], Train Loss: 1483.1654, Test Loss: 2151.4510\n",
      "Epoch [584/1000], Train Loss: 1622.5520, Test Loss: 2145.3032\n",
      "Epoch [585/1000], Train Loss: 1561.5600, Test Loss: 2085.6349\n",
      "Epoch [586/1000], Train Loss: 1727.3257, Test Loss: 2074.5354\n",
      "Epoch [587/1000], Train Loss: 1788.6816, Test Loss: 2205.5275\n",
      "Epoch [588/1000], Train Loss: 1511.5557, Test Loss: 2264.0294\n",
      "Epoch [589/1000], Train Loss: 2232.6653, Test Loss: 2150.8206\n",
      "Epoch [590/1000], Train Loss: 1758.2546, Test Loss: 2281.0127\n",
      "Epoch [591/1000], Train Loss: 2049.2226, Test Loss: 2050.0020\n",
      "Epoch [592/1000], Train Loss: 1361.0793, Test Loss: 2148.4512\n",
      "Epoch [593/1000], Train Loss: 1403.7774, Test Loss: 2127.3516\n",
      "Epoch [594/1000], Train Loss: 1530.2496, Test Loss: 2110.6427\n",
      "Epoch [595/1000], Train Loss: 2280.0532, Test Loss: 2067.5840\n",
      "Epoch [596/1000], Train Loss: 1482.4198, Test Loss: 2146.5139\n",
      "Epoch [597/1000], Train Loss: 1396.6536, Test Loss: 2146.1484\n",
      "Epoch [598/1000], Train Loss: 2033.8329, Test Loss: 2114.0675\n",
      "Epoch [599/1000], Train Loss: 1490.4400, Test Loss: 2128.9614\n",
      "Epoch [600/1000], Train Loss: 1933.8566, Test Loss: 2059.4128\n",
      "Epoch [601/1000], Train Loss: 1556.7047, Test Loss: 2114.4448\n",
      "Epoch [602/1000], Train Loss: 1661.8495, Test Loss: 2152.9952\n",
      "Epoch [603/1000], Train Loss: 2109.3976, Test Loss: 2203.7051\n",
      "Epoch [604/1000], Train Loss: 3905.1263, Test Loss: 2168.4911\n",
      "Epoch [605/1000], Train Loss: 1817.3672, Test Loss: 2073.1635\n",
      "Epoch [606/1000], Train Loss: 1321.4484, Test Loss: 2085.8511\n",
      "Epoch [607/1000], Train Loss: 1358.5901, Test Loss: 2113.0153\n",
      "Epoch [608/1000], Train Loss: 1271.4122, Test Loss: 2076.2671\n",
      "Epoch [609/1000], Train Loss: 1405.2027, Test Loss: 2179.7725\n",
      "Epoch [610/1000], Train Loss: 1702.5044, Test Loss: 2108.4980\n",
      "Epoch [611/1000], Train Loss: 1531.0682, Test Loss: 2099.0787\n",
      "Epoch [612/1000], Train Loss: 1716.5802, Test Loss: 2119.6838\n",
      "Epoch [613/1000], Train Loss: 3009.0418, Test Loss: 2523.7555\n",
      "Epoch [614/1000], Train Loss: 1822.6891, Test Loss: 2118.1609\n",
      "Epoch [615/1000], Train Loss: 1252.5458, Test Loss: 2042.1700\n",
      "Epoch [616/1000], Train Loss: 1248.0054, Test Loss: 2096.6805\n",
      "Epoch [617/1000], Train Loss: 1428.1249, Test Loss: 2172.6806\n",
      "Epoch [618/1000], Train Loss: 1480.0596, Test Loss: 2136.3570\n",
      "Epoch [619/1000], Train Loss: 1408.0451, Test Loss: 2143.2190\n",
      "Epoch [620/1000], Train Loss: 2398.1125, Test Loss: 3110.8445\n",
      "Epoch [621/1000], Train Loss: 2311.1258, Test Loss: 2093.9238\n",
      "Epoch [622/1000], Train Loss: 1417.7167, Test Loss: 2127.6055\n",
      "Epoch [623/1000], Train Loss: 1320.2660, Test Loss: 2105.2462\n",
      "Epoch [624/1000], Train Loss: 1635.2667, Test Loss: 2097.3583\n",
      "Epoch [625/1000], Train Loss: 1536.8469, Test Loss: 2168.0897\n",
      "Epoch [626/1000], Train Loss: 1470.0888, Test Loss: 2220.6227\n",
      "Epoch [627/1000], Train Loss: 1805.8377, Test Loss: 2244.1857\n",
      "Epoch [628/1000], Train Loss: 3942.0166, Test Loss: 2180.4971\n",
      "Epoch [629/1000], Train Loss: 1555.2572, Test Loss: 2035.5626\n",
      "Epoch [630/1000], Train Loss: 1181.9122, Test Loss: 2069.2328\n",
      "Epoch [631/1000], Train Loss: 1147.6757, Test Loss: 2125.8673\n",
      "Epoch [632/1000], Train Loss: 1341.1994, Test Loss: 2117.9550\n",
      "Epoch [633/1000], Train Loss: 1427.7364, Test Loss: 2094.5419\n",
      "Epoch [634/1000], Train Loss: 1342.8402, Test Loss: 2112.3946\n",
      "Epoch [635/1000], Train Loss: 1600.7865, Test Loss: 2285.9944\n",
      "Epoch [636/1000], Train Loss: 1465.4758, Test Loss: 2139.4139\n",
      "Epoch [637/1000], Train Loss: 2259.3934, Test Loss: 3243.9435\n",
      "Epoch [638/1000], Train Loss: 1761.8275, Test Loss: 2175.1301\n",
      "Epoch [639/1000], Train Loss: 1272.0455, Test Loss: 2147.4013\n",
      "Epoch [640/1000], Train Loss: 1547.4006, Test Loss: 2104.7456\n",
      "Epoch [641/1000], Train Loss: 1380.3036, Test Loss: 2143.7775\n",
      "Epoch [642/1000], Train Loss: 4680.0597, Test Loss: 2399.0633\n",
      "Epoch [643/1000], Train Loss: 2041.6265, Test Loss: 2106.9140\n",
      "Epoch [644/1000], Train Loss: 1285.0399, Test Loss: 2110.8514\n",
      "Epoch [645/1000], Train Loss: 1334.8825, Test Loss: 2099.8820\n",
      "Epoch [646/1000], Train Loss: 1291.9158, Test Loss: 2165.8172\n",
      "Epoch [647/1000], Train Loss: 1250.2988, Test Loss: 2166.6948\n",
      "Epoch [648/1000], Train Loss: 1458.6840, Test Loss: 2194.7848\n",
      "Epoch [649/1000], Train Loss: 1918.3772, Test Loss: 2820.6871\n",
      "Epoch [650/1000], Train Loss: 1537.4953, Test Loss: 2088.9853\n",
      "Epoch [651/1000], Train Loss: 1779.3929, Test Loss: 2082.8907\n",
      "Epoch [652/1000], Train Loss: 1388.8658, Test Loss: 2128.1451\n",
      "Epoch [653/1000], Train Loss: 1542.2191, Test Loss: 2379.2854\n",
      "Epoch [654/1000], Train Loss: 1515.9214, Test Loss: 2204.5488\n",
      "Epoch [655/1000], Train Loss: 1568.4218, Test Loss: 2109.0747\n",
      "Epoch [656/1000], Train Loss: 1321.3370, Test Loss: 2134.5258\n",
      "Epoch [657/1000], Train Loss: 1589.8462, Test Loss: 2192.5991\n",
      "Epoch [658/1000], Train Loss: 1355.1185, Test Loss: 2186.1654\n",
      "Epoch [659/1000], Train Loss: 2284.5602, Test Loss: 2194.7113\n",
      "Epoch [660/1000], Train Loss: 1244.4566, Test Loss: 2141.7536\n",
      "Epoch [661/1000], Train Loss: 1259.1285, Test Loss: 2075.3095\n",
      "Epoch [662/1000], Train Loss: 1344.7052, Test Loss: 2212.3946\n",
      "Epoch [663/1000], Train Loss: 1538.6587, Test Loss: 2165.2561\n",
      "Epoch [664/1000], Train Loss: 1871.5635, Test Loss: 2161.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [665/1000], Train Loss: 1538.9975, Test Loss: 2101.6528\n",
      "Epoch [666/1000], Train Loss: 1360.6629, Test Loss: 2223.2633\n",
      "Epoch [667/1000], Train Loss: 1517.2972, Test Loss: 2099.0259\n",
      "Epoch [668/1000], Train Loss: 1344.0975, Test Loss: 2101.1352\n",
      "Epoch [669/1000], Train Loss: 1478.3625, Test Loss: 2349.6320\n",
      "Epoch [670/1000], Train Loss: 2191.0023, Test Loss: 2116.5636\n",
      "Epoch [671/1000], Train Loss: 1359.1800, Test Loss: 2155.8519\n",
      "Epoch [672/1000], Train Loss: 1608.1624, Test Loss: 2116.9556\n",
      "Epoch [673/1000], Train Loss: 1641.1865, Test Loss: 2195.5447\n",
      "Epoch [674/1000], Train Loss: 1788.4222, Test Loss: 2105.7347\n",
      "Epoch [675/1000], Train Loss: 1332.1844, Test Loss: 2255.5807\n",
      "Epoch [676/1000], Train Loss: 1500.6078, Test Loss: 2098.8469\n",
      "Epoch [677/1000], Train Loss: 1351.4767, Test Loss: 2137.7338\n",
      "Epoch [678/1000], Train Loss: 1258.0953, Test Loss: 2152.3744\n",
      "Epoch [679/1000], Train Loss: 1308.1521, Test Loss: 2178.2884\n",
      "Epoch [680/1000], Train Loss: 1282.4579, Test Loss: 2226.4688\n",
      "Epoch [681/1000], Train Loss: 1375.6948, Test Loss: 2264.1836\n",
      "Epoch [682/1000], Train Loss: 1700.9396, Test Loss: 2285.0022\n",
      "Epoch [683/1000], Train Loss: 2614.3272, Test Loss: 2168.6723\n",
      "Epoch [684/1000], Train Loss: 1409.2608, Test Loss: 2084.1105\n",
      "Epoch [685/1000], Train Loss: 1476.0614, Test Loss: 2090.2152\n",
      "Epoch [686/1000], Train Loss: 1769.5508, Test Loss: 2150.4968\n",
      "Epoch [687/1000], Train Loss: 1266.7130, Test Loss: 2157.9611\n",
      "Epoch [688/1000], Train Loss: 1359.2341, Test Loss: 2136.3277\n",
      "Epoch [689/1000], Train Loss: 1246.3866, Test Loss: 2101.8641\n",
      "Epoch [690/1000], Train Loss: 1289.8590, Test Loss: 2091.9324\n",
      "Epoch [691/1000], Train Loss: 1454.5215, Test Loss: 2289.7402\n",
      "Epoch [692/1000], Train Loss: 2269.4521, Test Loss: 2083.3588\n",
      "Epoch [693/1000], Train Loss: 1533.7380, Test Loss: 2310.5568\n",
      "Epoch [694/1000], Train Loss: 1298.8133, Test Loss: 2181.0831\n",
      "Epoch [695/1000], Train Loss: 1552.1463, Test Loss: 2155.5793\n",
      "Epoch [696/1000], Train Loss: 1739.1771, Test Loss: 2146.3108\n",
      "Epoch [697/1000], Train Loss: 1682.1256, Test Loss: 2153.0638\n",
      "Epoch [698/1000], Train Loss: 1393.0349, Test Loss: 2107.4169\n",
      "Epoch [699/1000], Train Loss: 2474.1035, Test Loss: 2331.0141\n",
      "Epoch [700/1000], Train Loss: 1896.2518, Test Loss: 2159.6530\n",
      "Epoch [701/1000], Train Loss: 1331.7808, Test Loss: 2097.4462\n",
      "Epoch [702/1000], Train Loss: 1431.7290, Test Loss: 2195.5183\n",
      "Epoch [703/1000], Train Loss: 1656.2516, Test Loss: 2121.2901\n",
      "Epoch [704/1000], Train Loss: 1841.3379, Test Loss: 2079.5561\n",
      "Epoch [705/1000], Train Loss: 2195.2036, Test Loss: 2123.4721\n",
      "Epoch [706/1000], Train Loss: 2417.2527, Test Loss: 2078.9060\n",
      "Epoch [707/1000], Train Loss: 1463.7634, Test Loss: 2101.9160\n",
      "Epoch [708/1000], Train Loss: 1196.2176, Test Loss: 2089.3222\n",
      "Epoch [709/1000], Train Loss: 1141.7406, Test Loss: 2104.1485\n",
      "Epoch [710/1000], Train Loss: 1497.5539, Test Loss: 2956.9408\n",
      "Epoch [711/1000], Train Loss: 1697.8677, Test Loss: 2152.7269\n",
      "Epoch [712/1000], Train Loss: 1227.7874, Test Loss: 2228.3244\n",
      "Epoch [713/1000], Train Loss: 1422.9087, Test Loss: 2181.1566\n",
      "Epoch [714/1000], Train Loss: 1623.3925, Test Loss: 2180.1173\n",
      "Epoch [715/1000], Train Loss: 1342.2944, Test Loss: 2272.2707\n",
      "Epoch [716/1000], Train Loss: 1342.3176, Test Loss: 2119.5245\n",
      "Epoch [717/1000], Train Loss: 1990.0492, Test Loss: 2131.7098\n",
      "Epoch [718/1000], Train Loss: 1496.6549, Test Loss: 2079.8668\n",
      "Epoch [719/1000], Train Loss: 1374.8862, Test Loss: 2110.5308\n",
      "Epoch [720/1000], Train Loss: 1482.2338, Test Loss: 2086.0834\n",
      "Epoch [721/1000], Train Loss: 1618.4025, Test Loss: 2146.5750\n",
      "Epoch [722/1000], Train Loss: 1636.2441, Test Loss: 2213.5262\n",
      "Epoch [723/1000], Train Loss: 1203.6154, Test Loss: 2149.0619\n",
      "Epoch [724/1000], Train Loss: 1270.8618, Test Loss: 2172.6890\n",
      "Epoch [725/1000], Train Loss: 1309.9376, Test Loss: 2133.2110\n",
      "Epoch [726/1000], Train Loss: 1230.5945, Test Loss: 2147.6635\n",
      "Epoch [727/1000], Train Loss: 1290.4983, Test Loss: 2165.3162\n",
      "Epoch [728/1000], Train Loss: 1659.7650, Test Loss: 2246.7823\n",
      "Epoch [729/1000], Train Loss: 1417.4333, Test Loss: 2223.7786\n",
      "Epoch [730/1000], Train Loss: 1401.5850, Test Loss: 2176.5583\n",
      "Epoch [731/1000], Train Loss: 1427.9577, Test Loss: 2114.7424\n",
      "Epoch [732/1000], Train Loss: 1419.8415, Test Loss: 2143.8243\n",
      "Epoch [733/1000], Train Loss: 1310.7065, Test Loss: 2214.2521\n",
      "Epoch [734/1000], Train Loss: 1788.5558, Test Loss: 2215.6336\n",
      "Epoch [735/1000], Train Loss: 1941.7000, Test Loss: 2157.7357\n",
      "Epoch [736/1000], Train Loss: 1834.1220, Test Loss: 2543.5328\n",
      "Epoch [737/1000], Train Loss: 1997.9671, Test Loss: 2136.3448\n",
      "Epoch [738/1000], Train Loss: 1524.4236, Test Loss: 2100.5119\n",
      "Epoch [739/1000], Train Loss: 1925.0340, Test Loss: 2227.0454\n",
      "Epoch [740/1000], Train Loss: 1862.6728, Test Loss: 2540.4905\n",
      "Epoch [741/1000], Train Loss: 1197.9581, Test Loss: 2060.4118\n",
      "Epoch [742/1000], Train Loss: 1062.4433, Test Loss: 2065.1197\n",
      "Epoch [743/1000], Train Loss: 1044.3551, Test Loss: 2122.2470\n",
      "Epoch [744/1000], Train Loss: 1155.5805, Test Loss: 2121.9370\n",
      "Epoch [745/1000], Train Loss: 1582.5064, Test Loss: 2129.1282\n",
      "Epoch [746/1000], Train Loss: 1156.8830, Test Loss: 2181.2247\n",
      "Epoch [747/1000], Train Loss: 1245.6471, Test Loss: 2747.8897\n",
      "Epoch [748/1000], Train Loss: 2736.7905, Test Loss: 2186.1190\n",
      "Epoch [749/1000], Train Loss: 1126.2539, Test Loss: 2102.7727\n",
      "Epoch [750/1000], Train Loss: 6267.6726, Test Loss: 2412.9630\n",
      "Epoch [751/1000], Train Loss: 1304.3435, Test Loss: 2059.8847\n",
      "Epoch [752/1000], Train Loss: 981.2366, Test Loss: 2099.5257\n",
      "Epoch [753/1000], Train Loss: 1133.3663, Test Loss: 2148.3249\n",
      "Epoch [754/1000], Train Loss: 1210.9289, Test Loss: 2116.2192\n",
      "Epoch [755/1000], Train Loss: 1043.4174, Test Loss: 2115.1245\n",
      "Epoch [756/1000], Train Loss: 1109.2657, Test Loss: 2167.7638\n",
      "Epoch [757/1000], Train Loss: 1150.0818, Test Loss: 2390.4055\n",
      "Epoch [758/1000], Train Loss: 1693.8469, Test Loss: 2406.2834\n",
      "Epoch [759/1000], Train Loss: 1640.9126, Test Loss: 2151.3384\n",
      "Epoch [760/1000], Train Loss: 1253.4380, Test Loss: 2173.6725\n",
      "Epoch [761/1000], Train Loss: 1245.0310, Test Loss: 2099.0868\n",
      "Epoch [762/1000], Train Loss: 1162.5010, Test Loss: 2162.8217\n",
      "Epoch [763/1000], Train Loss: 1418.9005, Test Loss: 2157.5770\n",
      "Epoch [764/1000], Train Loss: 1977.8140, Test Loss: 2079.2662\n",
      "Epoch [765/1000], Train Loss: 1674.5360, Test Loss: 2430.1353\n",
      "Epoch [766/1000], Train Loss: 1351.3611, Test Loss: 2081.3736\n",
      "Epoch [767/1000], Train Loss: 1012.6981, Test Loss: 2128.4085\n",
      "Epoch [768/1000], Train Loss: 1159.4063, Test Loss: 2115.6447\n",
      "Epoch [769/1000], Train Loss: 1569.5121, Test Loss: 2295.8664\n",
      "Epoch [770/1000], Train Loss: 1555.9746, Test Loss: 2861.7349\n",
      "Epoch [771/1000], Train Loss: 1814.2581, Test Loss: 2104.5062\n",
      "Epoch [772/1000], Train Loss: 1163.6888, Test Loss: 2110.9872\n",
      "Epoch [773/1000], Train Loss: 1062.5465, Test Loss: 2175.2581\n",
      "Epoch [774/1000], Train Loss: 1403.5721, Test Loss: 2880.3220\n",
      "Epoch [775/1000], Train Loss: 2264.7062, Test Loss: 2081.7559\n",
      "Epoch [776/1000], Train Loss: 1164.0786, Test Loss: 2120.1660\n",
      "Epoch [777/1000], Train Loss: 1776.9549, Test Loss: 3180.2122\n",
      "Epoch [778/1000], Train Loss: 1607.5953, Test Loss: 2160.5115\n",
      "Epoch [779/1000], Train Loss: 1886.9160, Test Loss: 5723.6942\n",
      "Epoch [780/1000], Train Loss: 1982.7633, Test Loss: 2067.1496\n",
      "Epoch [781/1000], Train Loss: 2174.0352, Test Loss: 2046.9349\n",
      "Epoch [782/1000], Train Loss: 1049.7133, Test Loss: 2034.5246\n",
      "Epoch [783/1000], Train Loss: 983.5501, Test Loss: 2097.1677\n",
      "Epoch [784/1000], Train Loss: 1044.6867, Test Loss: 2100.3190\n",
      "Epoch [785/1000], Train Loss: 1159.7217, Test Loss: 2181.0714\n",
      "Epoch [786/1000], Train Loss: 1323.9180, Test Loss: 2128.5107\n",
      "Epoch [787/1000], Train Loss: 1567.7321, Test Loss: 2152.7447\n",
      "Epoch [788/1000], Train Loss: 2036.2609, Test Loss: 2175.1889\n",
      "Epoch [789/1000], Train Loss: 1467.4597, Test Loss: 2120.3378\n",
      "Epoch [790/1000], Train Loss: 1195.3945, Test Loss: 2113.4549\n",
      "Epoch [791/1000], Train Loss: 1252.0245, Test Loss: 2140.8333\n",
      "Epoch [792/1000], Train Loss: 1284.9238, Test Loss: 2243.9016\n",
      "Epoch [793/1000], Train Loss: 1733.5236, Test Loss: 2396.8040\n",
      "Epoch [794/1000], Train Loss: 2063.4186, Test Loss: 2180.4434\n",
      "Epoch [795/1000], Train Loss: 1386.7461, Test Loss: 2086.0864\n",
      "Epoch [796/1000], Train Loss: 1291.7109, Test Loss: 2168.6260\n",
      "Epoch [797/1000], Train Loss: 1210.1190, Test Loss: 2218.5852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [798/1000], Train Loss: 1298.5076, Test Loss: 2109.3588\n",
      "Epoch [799/1000], Train Loss: 1240.1492, Test Loss: 2245.3101\n",
      "Epoch [800/1000], Train Loss: 1523.8059, Test Loss: 2217.2256\n",
      "Epoch [801/1000], Train Loss: 1824.0553, Test Loss: 2192.3569\n",
      "Epoch [802/1000], Train Loss: 2531.0916, Test Loss: 2164.8786\n",
      "Epoch [803/1000], Train Loss: 1489.2599, Test Loss: 2098.6297\n",
      "Epoch [804/1000], Train Loss: 1043.7973, Test Loss: 2069.3486\n",
      "Epoch [805/1000], Train Loss: 1013.0715, Test Loss: 2089.3294\n",
      "Epoch [806/1000], Train Loss: 1046.5415, Test Loss: 2090.4552\n",
      "Epoch [807/1000], Train Loss: 1578.6287, Test Loss: 2535.5927\n",
      "Epoch [808/1000], Train Loss: 1767.2931, Test Loss: 2312.4159\n",
      "Epoch [809/1000], Train Loss: 1193.4173, Test Loss: 2199.3202\n",
      "Epoch [810/1000], Train Loss: 1076.1533, Test Loss: 2139.2220\n",
      "Epoch [811/1000], Train Loss: 1332.1783, Test Loss: 2148.7286\n",
      "Epoch [812/1000], Train Loss: 1182.3725, Test Loss: 2218.4828\n",
      "Epoch [813/1000], Train Loss: 2686.1669, Test Loss: 2054.4350\n",
      "Epoch [814/1000], Train Loss: 1228.9696, Test Loss: 2163.9490\n",
      "Epoch [815/1000], Train Loss: 1182.5858, Test Loss: 2150.7154\n",
      "Epoch [816/1000], Train Loss: 1101.6424, Test Loss: 2110.0082\n",
      "Epoch [817/1000], Train Loss: 1081.0661, Test Loss: 2199.5446\n",
      "Epoch [818/1000], Train Loss: 1209.5593, Test Loss: 2169.0054\n",
      "Epoch [819/1000], Train Loss: 1141.3529, Test Loss: 2126.2774\n",
      "Epoch [820/1000], Train Loss: 1595.1572, Test Loss: 2593.2496\n",
      "Epoch [821/1000], Train Loss: 1499.6788, Test Loss: 2456.5743\n",
      "Epoch [822/1000], Train Loss: 1311.7367, Test Loss: 2133.6830\n",
      "Epoch [823/1000], Train Loss: 1004.6023, Test Loss: 2156.8490\n",
      "Epoch [824/1000], Train Loss: 1254.8783, Test Loss: 2184.7273\n",
      "Epoch [825/1000], Train Loss: 1147.9299, Test Loss: 2139.6806\n",
      "Epoch [826/1000], Train Loss: 1153.8116, Test Loss: 2160.4760\n",
      "Epoch [827/1000], Train Loss: 1251.4296, Test Loss: 2123.9715\n",
      "Epoch [828/1000], Train Loss: 1174.5506, Test Loss: 2145.8100\n",
      "Epoch [829/1000], Train Loss: 1158.1075, Test Loss: 2188.8416\n",
      "Epoch [830/1000], Train Loss: 1764.2013, Test Loss: 2195.0367\n",
      "Epoch [831/1000], Train Loss: 1144.8436, Test Loss: 2094.3837\n",
      "Epoch [832/1000], Train Loss: 1126.4840, Test Loss: 2299.8833\n",
      "Epoch [833/1000], Train Loss: 2943.7509, Test Loss: 2083.7557\n",
      "Epoch [834/1000], Train Loss: 1072.0667, Test Loss: 2094.8871\n",
      "Epoch [835/1000], Train Loss: 998.8498, Test Loss: 2091.7713\n",
      "Epoch [836/1000], Train Loss: 938.3934, Test Loss: 2112.1819\n",
      "Epoch [837/1000], Train Loss: 962.2878, Test Loss: 2083.2784\n",
      "Epoch [838/1000], Train Loss: 1121.7036, Test Loss: 2164.6602\n",
      "Epoch [839/1000], Train Loss: 1108.2630, Test Loss: 2151.3969\n",
      "Epoch [840/1000], Train Loss: 1241.5370, Test Loss: 2118.0956\n",
      "Epoch [841/1000], Train Loss: 1218.2686, Test Loss: 2179.0370\n",
      "Epoch [842/1000], Train Loss: 1143.9468, Test Loss: 2064.3460\n",
      "Epoch [843/1000], Train Loss: 1054.9410, Test Loss: 2148.0848\n",
      "Epoch [844/1000], Train Loss: 1054.8369, Test Loss: 2247.7341\n",
      "Epoch [845/1000], Train Loss: 1670.8366, Test Loss: 2388.9761\n",
      "Epoch [846/1000], Train Loss: 1574.4799, Test Loss: 2177.8570\n",
      "Epoch [847/1000], Train Loss: 1285.5060, Test Loss: 2125.6285\n",
      "Epoch [848/1000], Train Loss: 1025.4758, Test Loss: 2081.1379\n",
      "Epoch [849/1000], Train Loss: 1055.2845, Test Loss: 2202.5408\n",
      "Epoch [850/1000], Train Loss: 1374.3990, Test Loss: 2115.7976\n",
      "Epoch [851/1000], Train Loss: 1216.1429, Test Loss: 2091.8209\n",
      "Epoch [852/1000], Train Loss: 1144.1275, Test Loss: 2138.4710\n",
      "Epoch [853/1000], Train Loss: 1064.7948, Test Loss: 2155.8014\n",
      "Epoch [854/1000], Train Loss: 1070.8097, Test Loss: 2202.0988\n",
      "Epoch [855/1000], Train Loss: 1174.8581, Test Loss: 2227.6322\n",
      "Epoch [856/1000], Train Loss: 1344.1609, Test Loss: 2246.9931\n",
      "Epoch [857/1000], Train Loss: 2156.3167, Test Loss: 2131.4749\n",
      "Epoch [858/1000], Train Loss: 1322.3590, Test Loss: 2083.2387\n",
      "Epoch [859/1000], Train Loss: 1450.4057, Test Loss: 2114.5484\n",
      "Epoch [860/1000], Train Loss: 1225.6643, Test Loss: 2125.3961\n",
      "Epoch [861/1000], Train Loss: 1116.6372, Test Loss: 2094.9294\n",
      "Epoch [862/1000], Train Loss: 971.9281, Test Loss: 2171.8286\n",
      "Epoch [863/1000], Train Loss: 1041.0126, Test Loss: 2103.9023\n",
      "Epoch [864/1000], Train Loss: 2378.4165, Test Loss: 2926.3560\n",
      "Epoch [865/1000], Train Loss: 2004.8462, Test Loss: 2108.5653\n",
      "Epoch [866/1000], Train Loss: 991.7009, Test Loss: 2069.2845\n",
      "Epoch [867/1000], Train Loss: 1130.3407, Test Loss: 2126.7187\n",
      "Epoch [868/1000], Train Loss: 1008.1014, Test Loss: 2154.2348\n",
      "Epoch [869/1000], Train Loss: 1022.8907, Test Loss: 2094.2589\n",
      "Epoch [870/1000], Train Loss: 1458.0934, Test Loss: 2116.5437\n",
      "Epoch [871/1000], Train Loss: 1189.9435, Test Loss: 2106.8744\n",
      "Epoch [872/1000], Train Loss: 985.4099, Test Loss: 2125.4080\n",
      "Epoch [873/1000], Train Loss: 1268.9010, Test Loss: 2875.7142\n",
      "Epoch [874/1000], Train Loss: 1409.4613, Test Loss: 2222.2981\n",
      "Epoch [875/1000], Train Loss: 1061.2169, Test Loss: 2127.7083\n",
      "Epoch [876/1000], Train Loss: 961.5355, Test Loss: 2270.1906\n",
      "Epoch [877/1000], Train Loss: 1460.4006, Test Loss: 2208.6162\n",
      "Epoch [878/1000], Train Loss: 1139.4143, Test Loss: 2253.3370\n",
      "Epoch [879/1000], Train Loss: 1766.6645, Test Loss: 2107.1643\n",
      "Epoch [880/1000], Train Loss: 1027.7500, Test Loss: 2096.5794\n",
      "Epoch [881/1000], Train Loss: 974.1363, Test Loss: 2138.2325\n",
      "Epoch [882/1000], Train Loss: 1166.2974, Test Loss: 2122.8139\n",
      "Epoch [883/1000], Train Loss: 1353.7679, Test Loss: 2445.6983\n",
      "Epoch [884/1000], Train Loss: 1260.9593, Test Loss: 2214.9833\n",
      "Epoch [885/1000], Train Loss: 1033.9118, Test Loss: 2166.4162\n",
      "Epoch [886/1000], Train Loss: 966.6075, Test Loss: 2173.6120\n",
      "Epoch [887/1000], Train Loss: 1037.0305, Test Loss: 2235.5725\n",
      "Epoch [888/1000], Train Loss: 1616.2030, Test Loss: 2173.1904\n",
      "Epoch [889/1000], Train Loss: 1133.5202, Test Loss: 2118.7023\n",
      "Epoch [890/1000], Train Loss: 1738.4614, Test Loss: 2229.4781\n",
      "Epoch [891/1000], Train Loss: 1092.3729, Test Loss: 2160.2397\n",
      "Epoch [892/1000], Train Loss: 913.1258, Test Loss: 2088.8180\n",
      "Epoch [893/1000], Train Loss: 958.3431, Test Loss: 2174.6987\n",
      "Epoch [894/1000], Train Loss: 1417.9770, Test Loss: 2149.1264\n",
      "Epoch [895/1000], Train Loss: 1674.1922, Test Loss: 2961.8731\n",
      "Epoch [896/1000], Train Loss: 2173.4876, Test Loss: 2101.2818\n",
      "Epoch [897/1000], Train Loss: 936.9153, Test Loss: 2141.6214\n",
      "Epoch [898/1000], Train Loss: 1135.6586, Test Loss: 2179.2150\n",
      "Epoch [899/1000], Train Loss: 1219.7487, Test Loss: 2117.8858\n",
      "Epoch [900/1000], Train Loss: 1120.6835, Test Loss: 2089.8010\n",
      "Epoch [901/1000], Train Loss: 921.0790, Test Loss: 2151.8639\n",
      "Epoch [902/1000], Train Loss: 990.8367, Test Loss: 2215.4826\n",
      "Epoch [903/1000], Train Loss: 1074.1214, Test Loss: 2125.1309\n",
      "Epoch [904/1000], Train Loss: 1002.5789, Test Loss: 2142.1675\n",
      "Epoch [905/1000], Train Loss: 1006.0980, Test Loss: 2184.4082\n",
      "Epoch [906/1000], Train Loss: 1064.6368, Test Loss: 2131.2642\n",
      "Epoch [907/1000], Train Loss: 1323.2843, Test Loss: 2389.6300\n",
      "Epoch [908/1000], Train Loss: 1271.8251, Test Loss: 2133.8932\n",
      "Epoch [909/1000], Train Loss: 1012.6030, Test Loss: 2253.8833\n",
      "Epoch [910/1000], Train Loss: 1028.0022, Test Loss: 2111.7082\n",
      "Epoch [911/1000], Train Loss: 1204.9350, Test Loss: 2144.9034\n",
      "Epoch [912/1000], Train Loss: 1248.6290, Test Loss: 2117.0173\n",
      "Epoch [913/1000], Train Loss: 1629.5881, Test Loss: 2129.6622\n",
      "Epoch [914/1000], Train Loss: 1797.8286, Test Loss: 2272.4144\n",
      "Epoch [915/1000], Train Loss: 1229.1727, Test Loss: 2108.0230\n",
      "Epoch [916/1000], Train Loss: 857.1636, Test Loss: 2109.4534\n",
      "Epoch [917/1000], Train Loss: 819.3945, Test Loss: 2116.8230\n",
      "Epoch [918/1000], Train Loss: 913.9767, Test Loss: 2090.6015\n",
      "Epoch [919/1000], Train Loss: 1467.0822, Test Loss: 2096.8404\n",
      "Epoch [920/1000], Train Loss: 947.0583, Test Loss: 2081.4764\n",
      "Epoch [921/1000], Train Loss: 1023.5578, Test Loss: 2132.9888\n",
      "Epoch [922/1000], Train Loss: 1144.7238, Test Loss: 2095.7171\n",
      "Epoch [923/1000], Train Loss: 974.9574, Test Loss: 2215.4256\n",
      "Epoch [924/1000], Train Loss: 2521.2237, Test Loss: 2139.7219\n",
      "Epoch [925/1000], Train Loss: 1016.2468, Test Loss: 2085.2997\n",
      "Epoch [926/1000], Train Loss: 878.3715, Test Loss: 2110.9598\n",
      "Epoch [927/1000], Train Loss: 867.4547, Test Loss: 2171.7700\n",
      "Epoch [928/1000], Train Loss: 914.6997, Test Loss: 2166.7905\n",
      "Epoch [929/1000], Train Loss: 1084.4988, Test Loss: 2145.9066\n",
      "Epoch [930/1000], Train Loss: 1035.0256, Test Loss: 2147.6684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [931/1000], Train Loss: 1679.4984, Test Loss: 2149.2289\n",
      "Epoch [932/1000], Train Loss: 1077.1855, Test Loss: 2173.4552\n",
      "Epoch [933/1000], Train Loss: 910.6943, Test Loss: 2096.5249\n",
      "Epoch [934/1000], Train Loss: 1001.2980, Test Loss: 2098.9282\n",
      "Epoch [935/1000], Train Loss: 890.4046, Test Loss: 2127.8698\n",
      "Epoch [936/1000], Train Loss: 1224.5296, Test Loss: 2303.4605\n",
      "Epoch [937/1000], Train Loss: 1298.7971, Test Loss: 2097.8600\n",
      "Epoch [938/1000], Train Loss: 906.5620, Test Loss: 2129.8804\n",
      "Epoch [939/1000], Train Loss: 1475.7449, Test Loss: 2080.6953\n",
      "Epoch [940/1000], Train Loss: 950.1741, Test Loss: 2133.3351\n",
      "Epoch [941/1000], Train Loss: 878.5257, Test Loss: 2122.8453\n",
      "Epoch [942/1000], Train Loss: 960.2165, Test Loss: 2142.7351\n",
      "Epoch [943/1000], Train Loss: 1145.6512, Test Loss: 2196.4040\n",
      "Epoch [944/1000], Train Loss: 1135.6938, Test Loss: 2209.9825\n",
      "Epoch [945/1000], Train Loss: 1257.0421, Test Loss: 2207.7236\n",
      "Epoch [946/1000], Train Loss: 1014.6460, Test Loss: 2108.5459\n",
      "Epoch [947/1000], Train Loss: 966.8696, Test Loss: 2379.7226\n",
      "Epoch [948/1000], Train Loss: 1526.6344, Test Loss: 2242.7898\n",
      "Epoch [949/1000], Train Loss: 1085.7697, Test Loss: 2152.4892\n",
      "Epoch [950/1000], Train Loss: 910.0874, Test Loss: 2102.2971\n",
      "Epoch [951/1000], Train Loss: 851.9694, Test Loss: 2100.0586\n",
      "Epoch [952/1000], Train Loss: 941.7226, Test Loss: 2122.6196\n",
      "Epoch [953/1000], Train Loss: 1045.2039, Test Loss: 2133.9185\n",
      "Epoch [954/1000], Train Loss: 1276.8851, Test Loss: 2821.4055\n",
      "Epoch [955/1000], Train Loss: 1491.8363, Test Loss: 2103.4718\n",
      "Epoch [956/1000], Train Loss: 1098.1677, Test Loss: 2373.4009\n",
      "Epoch [957/1000], Train Loss: 1028.8762, Test Loss: 2386.5488\n",
      "Epoch [958/1000], Train Loss: 1564.8739, Test Loss: 2321.0558\n",
      "Epoch [959/1000], Train Loss: 862.9419, Test Loss: 2119.5587\n",
      "Epoch [960/1000], Train Loss: 818.0667, Test Loss: 2113.7078\n",
      "Epoch [961/1000], Train Loss: 835.3109, Test Loss: 2165.8505\n",
      "Epoch [962/1000], Train Loss: 1747.1301, Test Loss: 2197.4754\n",
      "Epoch [963/1000], Train Loss: 956.8012, Test Loss: 2168.9424\n",
      "Epoch [964/1000], Train Loss: 864.7184, Test Loss: 2129.2538\n",
      "Epoch [965/1000], Train Loss: 1520.2822, Test Loss: 2378.3373\n",
      "Epoch [966/1000], Train Loss: 1691.6638, Test Loss: 2160.5112\n",
      "Epoch [967/1000], Train Loss: 898.4161, Test Loss: 2191.1389\n",
      "Epoch [968/1000], Train Loss: 833.8412, Test Loss: 2107.0174\n",
      "Epoch [969/1000], Train Loss: 902.8304, Test Loss: 2104.7071\n",
      "Epoch [970/1000], Train Loss: 842.2925, Test Loss: 2129.7680\n",
      "Epoch [971/1000], Train Loss: 946.5343, Test Loss: 2139.7261\n",
      "Epoch [972/1000], Train Loss: 920.5193, Test Loss: 2163.8185\n",
      "Epoch [973/1000], Train Loss: 1054.9676, Test Loss: 2133.8951\n",
      "Epoch [974/1000], Train Loss: 1233.4479, Test Loss: 2152.4591\n",
      "Epoch [975/1000], Train Loss: 952.1710, Test Loss: 2141.6784\n",
      "Epoch [976/1000], Train Loss: 1131.1502, Test Loss: 2219.1311\n",
      "Epoch [977/1000], Train Loss: 1039.0702, Test Loss: 2154.8134\n",
      "Epoch [978/1000], Train Loss: 878.9836, Test Loss: 2207.7701\n",
      "Epoch [979/1000], Train Loss: 1108.8807, Test Loss: 2172.6576\n",
      "Epoch [980/1000], Train Loss: 957.7219, Test Loss: 2191.0756\n",
      "Epoch [981/1000], Train Loss: 1050.3884, Test Loss: 2166.6417\n",
      "Epoch [982/1000], Train Loss: 1025.7015, Test Loss: 2181.0353\n",
      "Epoch [983/1000], Train Loss: 1449.1315, Test Loss: 2599.9895\n",
      "Epoch [984/1000], Train Loss: 1229.5881, Test Loss: 2151.6838\n",
      "Epoch [985/1000], Train Loss: 1242.8912, Test Loss: 2161.3887\n",
      "Epoch [986/1000], Train Loss: 853.9269, Test Loss: 2108.0198\n",
      "Epoch [987/1000], Train Loss: 959.1786, Test Loss: 2141.6550\n",
      "Epoch [988/1000], Train Loss: 918.7123, Test Loss: 2124.0694\n",
      "Epoch [989/1000], Train Loss: 922.2756, Test Loss: 2146.9237\n",
      "Epoch [990/1000], Train Loss: 1190.7222, Test Loss: 5531.3938\n",
      "Epoch [991/1000], Train Loss: 1505.7494, Test Loss: 2168.0608\n",
      "Epoch [992/1000], Train Loss: 972.9071, Test Loss: 2129.9170\n",
      "Epoch [993/1000], Train Loss: 977.6389, Test Loss: 2193.0240\n",
      "Epoch [994/1000], Train Loss: 1068.1114, Test Loss: 2159.7725\n",
      "Epoch [995/1000], Train Loss: 911.1267, Test Loss: 2185.2406\n",
      "Epoch [996/1000], Train Loss: 1561.9193, Test Loss: 2175.8479\n",
      "Epoch [997/1000], Train Loss: 921.3191, Test Loss: 2097.2947\n",
      "Epoch [998/1000], Train Loss: 820.6986, Test Loss: 2133.1256\n",
      "Epoch [999/1000], Train Loss: 1174.7142, Test Loss: 2136.7658\n",
      "Epoch [1000/1000], Train Loss: 1523.7996, Test Loss: 2104.8571\n",
      "Execution time: 2638.933397769928 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1970.7294702529907"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load('Dataset44_Dist5_Val_Spec.npy')\n",
    "concVal = np.load('Dataset44_Dist5_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "ValSpectra = np.load(\"Dataset44_Dist5_RepresentativeExamples_Spectra.npy\")\n",
    "ValConc = np.load(\"Dataset44_Dist5_RepresentativeExamples_Concentrations.npy\")\n",
    "ValSpecNames = np.load(\"Dataset44_Dist5_RepresentativeExamples_VariableNames.npy\")\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ValConc = torch.tensor(ValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=23552, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ValConc[i]\n",
    "    Prediction = model_aq(ValSpectra[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.63  -  AllAq1\n",
      "1.67  -  AllAq5\n",
      "0.41  -  AllAq25\n",
      "1.81  -  AllAq50\n",
      "0.87  -  ThreeAddedSinglets\n",
      "4.88  -  ThirtyAddedSinglets\n",
      "74.37  -  ShiftedSpec\n",
      "25.33  -  SineBase\n",
      "86.19  -  HighDynamicRange\n",
      "inf  -  HalfZeros\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - \",ValSpecNames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist5 - HD-Range w/ 1's\n",
      "tensor([ 2.0292, 47.8494,  0.4565, 50.0330,  0.9647, 50.4537,  8.3679, 49.0054,\n",
      "         0.3870, 49.4714,  0.0000, 50.1144,  1.4919, 50.4791,  0.0000, 49.8393,\n",
      "         0.5880, 48.8594,  0.0000, 48.6608,  0.2728, 49.5954,  0.2565, 49.8158,\n",
      "         2.3976, 50.9123,  7.3353, 48.2488,  2.2607, 48.0161,  3.6874, 49.0018,\n",
      "         1.4501, 48.4171,  2.7420, 47.6225,  2.4129, 47.4585,  1.8402, 49.3917,\n",
      "         1.1108, 49.2213,  0.0000, 49.4570], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Dist5 - HD-Range w/ 0's\n",
      "tensor([ 0.9205, 47.8344,  0.0000, 50.0625,  0.0000, 50.4457,  7.3708, 49.0103,\n",
      "         0.0000, 49.5218,  0.0000, 50.0597,  0.4434, 50.4880,  0.0000, 49.8560,\n",
      "         0.0000, 48.7390,  0.0000, 48.6833,  0.0000, 49.5822,  0.0000, 49.8050,\n",
      "         1.3795, 50.9021,  6.3249, 48.2851,  1.2410, 47.9962,  2.7258, 48.7814,\n",
      "         0.4654, 48.3978,  1.8146, 47.5622,  1.4341, 47.4695,  0.8482, 49.3876,\n",
      "         0.1329, 49.2187,  0.0000, 49.4959], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Dist5 - Blank\n",
      "tensor([0.0863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0068, 0.0000, 0.0128, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0532, 0.0000, 0.0348, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0384, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0044, 0.0000, 0.0140, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(ValSpectra[8])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist5 - HD-Range w/ 1's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[9])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist5 - HD-Range w/ 0's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[10])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist5 - Blank\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
