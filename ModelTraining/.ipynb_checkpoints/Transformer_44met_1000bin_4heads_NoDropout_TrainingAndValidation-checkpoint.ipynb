{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer Encoder on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"Transformer_44met_TrainingAndValidation_1000bin_4heads_NoDropout_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('Dataset44_Spec.npy')\n",
    "conc1 = np.load('Dataset44_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "#spectraVal = np.load('Dataset44_Val_Spec.npy')\n",
    "#concVal = np.load('Dataset44_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "#ValSpectra = np.load(\"Dataset44_RepresentativeExamples_Spectra.npy\")\n",
    "#ValConc = np.load(\"Dataset44_RepresentativeExamples_Concentrations.npy\")\n",
    "#ValSpecNames = np.load(\"Dataset44_RepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 32, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.Linear(23552, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Binning\n",
    "        batch_size, seq_length = x.size()\n",
    "        num_bins = seq_length // self.input_dim\n",
    "        x = x.view(batch_size, num_bins, self.input_dim)  # (batch_size, num_bins, input_dim)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = x.permute(1, 0, 2)  # (num_bins, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)  # (num_bins, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Reconstruct original sequence\n",
    "        x = x.reshape(batch_size, num_bins * d_model)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder(x)  # (batch_size, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "input_dim = 1000  # Size of each bin\n",
    "d_model = 512     # Embedding dimension\n",
    "nhead = 4         # Number of attention heads\n",
    "num_encoder_layers = 1  # Number of transformer encoder layers\n",
    "dim_feedforward = 2048  # Feedforward dimension\n",
    "dropout = 0.0     # Dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "       \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/1000], Train Loss: 3223289.6785, Test Loss: 494386.7336\n",
      "Epoch [2/1000], Train Loss: 1588875.2906, Test Loss: 232370.5642\n",
      "Epoch [3/1000], Train Loss: 582309.5753, Test Loss: 100001.7443\n",
      "Epoch [4/1000], Train Loss: 291028.2542, Test Loss: 53282.2319\n",
      "Epoch [5/1000], Train Loss: 172291.6055, Test Loss: 35232.1587\n",
      "Epoch [6/1000], Train Loss: 116409.9022, Test Loss: 25725.2845\n",
      "Epoch [7/1000], Train Loss: 84757.1159, Test Loss: 18569.2329\n",
      "Epoch [8/1000], Train Loss: 66919.7829, Test Loss: 15893.8409\n",
      "Epoch [9/1000], Train Loss: 56293.7736, Test Loss: 12983.9451\n",
      "Epoch [10/1000], Train Loss: 46404.3309, Test Loss: 11348.7503\n",
      "Epoch [11/1000], Train Loss: 40249.3888, Test Loss: 11119.2258\n",
      "Epoch [12/1000], Train Loss: 36795.1137, Test Loss: 9710.9609\n",
      "Epoch [13/1000], Train Loss: 31414.0224, Test Loss: 9323.8285\n",
      "Epoch [14/1000], Train Loss: 29773.6446, Test Loss: 7898.6730\n",
      "Epoch [15/1000], Train Loss: 2422593.3389, Test Loss: 927195.4170\n",
      "Epoch [16/1000], Train Loss: 2935028.2322, Test Loss: 482835.5959\n",
      "Epoch [17/1000], Train Loss: 1492728.8960, Test Loss: 259004.7878\n",
      "Epoch [18/1000], Train Loss: 358079.5599, Test Loss: 26779.3512\n",
      "Epoch [19/1000], Train Loss: 66633.2430, Test Loss: 14262.6141\n",
      "Epoch [20/1000], Train Loss: 43567.2500, Test Loss: 11494.4811\n",
      "Epoch [21/1000], Train Loss: 34881.8260, Test Loss: 8906.4789\n",
      "Epoch [22/1000], Train Loss: 30695.6599, Test Loss: 8186.2933\n",
      "Epoch [23/1000], Train Loss: 27964.9529, Test Loss: 7923.9053\n",
      "Epoch [24/1000], Train Loss: 25794.2987, Test Loss: 7840.0423\n",
      "Epoch [25/1000], Train Loss: 1555744.7489, Test Loss: 321489.1177\n",
      "Epoch [26/1000], Train Loss: 1004502.1393, Test Loss: 188139.0033\n",
      "Epoch [27/1000], Train Loss: 910077.9147, Test Loss: 174227.2426\n",
      "Epoch [28/1000], Train Loss: 354906.0393, Test Loss: 34743.9248\n",
      "Epoch [29/1000], Train Loss: 95612.0240, Test Loss: 16631.8517\n",
      "Epoch [30/1000], Train Loss: 52571.9506, Test Loss: 11878.1104\n",
      "Epoch [31/1000], Train Loss: 39073.0014, Test Loss: 9587.5787\n",
      "Epoch [32/1000], Train Loss: 32512.5818, Test Loss: 8218.8130\n",
      "Epoch [33/1000], Train Loss: 28997.9189, Test Loss: 7752.1499\n",
      "Epoch [34/1000], Train Loss: 26759.1011, Test Loss: 7095.2493\n",
      "Epoch [35/1000], Train Loss: 24813.7741, Test Loss: 6673.9480\n",
      "Epoch [36/1000], Train Loss: 23207.2422, Test Loss: 7111.7306\n",
      "Epoch [37/1000], Train Loss: 22674.1252, Test Loss: 5993.2636\n",
      "Epoch [38/1000], Train Loss: 20939.4385, Test Loss: 6135.6629\n",
      "Epoch [39/1000], Train Loss: 20540.6513, Test Loss: 5989.7510\n",
      "Epoch [40/1000], Train Loss: 19452.8308, Test Loss: 5858.2946\n",
      "Epoch [41/1000], Train Loss: 19424.2122, Test Loss: 5974.3225\n",
      "Epoch [42/1000], Train Loss: 18576.2674, Test Loss: 5520.8188\n",
      "Epoch [43/1000], Train Loss: 17868.5995, Test Loss: 5752.0702\n",
      "Epoch [44/1000], Train Loss: 17327.6512, Test Loss: 5394.0838\n",
      "Epoch [45/1000], Train Loss: 16225.8406, Test Loss: 5800.6832\n",
      "Epoch [46/1000], Train Loss: 16809.0631, Test Loss: 5215.8562\n",
      "Epoch [47/1000], Train Loss: 14973.9311, Test Loss: 5002.2413\n",
      "Epoch [48/1000], Train Loss: 15656.0849, Test Loss: 4654.7039\n",
      "Epoch [49/1000], Train Loss: 14646.5577, Test Loss: 5350.1944\n",
      "Epoch [50/1000], Train Loss: 15947.9209, Test Loss: 4195.5308\n",
      "Epoch [51/1000], Train Loss: 12827.9351, Test Loss: 4150.1462\n",
      "Epoch [52/1000], Train Loss: 13103.3886, Test Loss: 4288.5920\n",
      "Epoch [53/1000], Train Loss: 12858.4738, Test Loss: 4577.6120\n",
      "Epoch [54/1000], Train Loss: 12650.2832, Test Loss: 4084.0998\n",
      "Epoch [55/1000], Train Loss: 12839.6529, Test Loss: 4275.1131\n",
      "Epoch [56/1000], Train Loss: 2852941.1918, Test Loss: 454617.9956\n",
      "Epoch [57/1000], Train Loss: 1411716.0938, Test Loss: 302660.7319\n",
      "Epoch [58/1000], Train Loss: 1034693.4395, Test Loss: 231505.7791\n",
      "Epoch [59/1000], Train Loss: 612047.4591, Test Loss: 57021.8063\n",
      "Epoch [60/1000], Train Loss: 125414.0173, Test Loss: 19106.5473\n",
      "Epoch [61/1000], Train Loss: 60991.3744, Test Loss: 11933.0667\n",
      "Epoch [62/1000], Train Loss: 39381.2925, Test Loss: 8645.3350\n",
      "Epoch [63/1000], Train Loss: 30063.6834, Test Loss: 7473.3031\n",
      "Epoch [64/1000], Train Loss: 25260.5082, Test Loss: 5972.7176\n",
      "Epoch [65/1000], Train Loss: 21563.8840, Test Loss: 6151.1263\n",
      "Epoch [66/1000], Train Loss: 19415.9738, Test Loss: 5493.8666\n",
      "Epoch [67/1000], Train Loss: 17480.5283, Test Loss: 5162.8865\n",
      "Epoch [68/1000], Train Loss: 16359.5929, Test Loss: 4897.1420\n",
      "Epoch [69/1000], Train Loss: 341086.4458, Test Loss: 500097.2979\n",
      "Epoch [70/1000], Train Loss: 1187808.0336, Test Loss: 225664.5535\n",
      "Epoch [71/1000], Train Loss: 329447.9608, Test Loss: 10758.4959\n",
      "Epoch [72/1000], Train Loss: 29105.6962, Test Loss: 6933.5368\n",
      "Epoch [73/1000], Train Loss: 20342.8404, Test Loss: 5540.0016\n",
      "Epoch [74/1000], Train Loss: 17716.0623, Test Loss: 4763.7692\n",
      "Epoch [75/1000], Train Loss: 16807.5924, Test Loss: 4599.3427\n",
      "Epoch [76/1000], Train Loss: 15159.0121, Test Loss: 5537.4071\n",
      "Epoch [77/1000], Train Loss: 14724.6801, Test Loss: 4492.9931\n",
      "Epoch [78/1000], Train Loss: 14278.4529, Test Loss: 4368.6788\n",
      "Epoch [79/1000], Train Loss: 14133.0170, Test Loss: 4487.5676\n",
      "Epoch [80/1000], Train Loss: 14085.4676, Test Loss: 4775.1503\n",
      "Epoch [81/1000], Train Loss: 13298.8433, Test Loss: 4536.4670\n",
      "Epoch [82/1000], Train Loss: 13112.1298, Test Loss: 5294.2223\n",
      "Epoch [83/1000], Train Loss: 13140.2828, Test Loss: 4052.0782\n",
      "Epoch [84/1000], Train Loss: 12519.0367, Test Loss: 3804.6994\n",
      "Epoch [85/1000], Train Loss: 77203.6375, Test Loss: 5072.9138\n",
      "Epoch [86/1000], Train Loss: 12429.3490, Test Loss: 3915.8405\n",
      "Epoch [87/1000], Train Loss: 10297.9087, Test Loss: 3589.7532\n",
      "Epoch [88/1000], Train Loss: 9615.3514, Test Loss: 3379.6068\n",
      "Epoch [89/1000], Train Loss: 9568.8300, Test Loss: 3453.4989\n",
      "Epoch [90/1000], Train Loss: 9431.4785, Test Loss: 3446.0841\n",
      "Epoch [91/1000], Train Loss: 9495.3926, Test Loss: 3407.6038\n",
      "Epoch [92/1000], Train Loss: 9794.3943, Test Loss: 3481.6024\n",
      "Epoch [93/1000], Train Loss: 10559.6253, Test Loss: 3473.7620\n",
      "Epoch [94/1000], Train Loss: 9752.1075, Test Loss: 3571.8218\n",
      "Epoch [95/1000], Train Loss: 10718.6730, Test Loss: 3573.2644\n",
      "Epoch [96/1000], Train Loss: 9732.9735, Test Loss: 3366.6221\n",
      "Epoch [97/1000], Train Loss: 9747.1245, Test Loss: 3727.1368\n",
      "Epoch [98/1000], Train Loss: 10159.1085, Test Loss: 3550.1831\n",
      "Epoch [99/1000], Train Loss: 10019.0844, Test Loss: 3538.4840\n",
      "Epoch [100/1000], Train Loss: 9740.8242, Test Loss: 3402.5878\n",
      "Epoch [101/1000], Train Loss: 9268.1129, Test Loss: 3301.2551\n",
      "Epoch [102/1000], Train Loss: 9120.5805, Test Loss: 3220.5912\n",
      "Epoch [103/1000], Train Loss: 9342.5125, Test Loss: 3698.4905\n",
      "Epoch [104/1000], Train Loss: 9579.5280, Test Loss: 3279.1241\n",
      "Epoch [105/1000], Train Loss: 291942.6626, Test Loss: 21164.6255\n",
      "Epoch [106/1000], Train Loss: 24030.3258, Test Loss: 4575.4209\n",
      "Epoch [107/1000], Train Loss: 11376.1422, Test Loss: 3368.1185\n",
      "Epoch [108/1000], Train Loss: 9559.1277, Test Loss: 3129.3790\n",
      "Epoch [109/1000], Train Loss: 8574.6833, Test Loss: 3539.5809\n",
      "Epoch [110/1000], Train Loss: 8198.4358, Test Loss: 3155.9442\n",
      "Epoch [111/1000], Train Loss: 7867.9464, Test Loss: 2934.0806\n",
      "Epoch [112/1000], Train Loss: 7512.4472, Test Loss: 2976.2017\n",
      "Epoch [113/1000], Train Loss: 7605.9089, Test Loss: 2990.6966\n",
      "Epoch [114/1000], Train Loss: 7172.6539, Test Loss: 2891.6275\n",
      "Epoch [115/1000], Train Loss: 7207.2610, Test Loss: 2872.2860\n",
      "Epoch [116/1000], Train Loss: 7126.7257, Test Loss: 2826.5102\n",
      "Epoch [117/1000], Train Loss: 7219.8664, Test Loss: 3053.0269\n",
      "Epoch [118/1000], Train Loss: 7758.4699, Test Loss: 2932.7426\n",
      "Epoch [119/1000], Train Loss: 7316.5113, Test Loss: 3135.4589\n",
      "Epoch [120/1000], Train Loss: 7951.1222, Test Loss: 3095.3130\n",
      "Epoch [121/1000], Train Loss: 7638.9117, Test Loss: 3079.1193\n",
      "Epoch [122/1000], Train Loss: 10951.0834, Test Loss: 3199.0509\n",
      "Epoch [123/1000], Train Loss: 8001.7799, Test Loss: 2972.5529\n",
      "Epoch [124/1000], Train Loss: 7724.8402, Test Loss: 3037.5738\n",
      "Epoch [125/1000], Train Loss: 7904.1724, Test Loss: 2997.8167\n",
      "Epoch [126/1000], Train Loss: 8662.4734, Test Loss: 3311.9675\n",
      "Epoch [127/1000], Train Loss: 7436.4853, Test Loss: 2941.1750\n",
      "Epoch [128/1000], Train Loss: 7290.4999, Test Loss: 3205.8514\n",
      "Epoch [129/1000], Train Loss: 7258.3578, Test Loss: 2960.6579\n",
      "Epoch [130/1000], Train Loss: 7221.8227, Test Loss: 3172.6166\n",
      "Epoch [131/1000], Train Loss: 7617.4223, Test Loss: 3209.8822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [132/1000], Train Loss: 7523.0611, Test Loss: 2903.0900\n",
      "Epoch [133/1000], Train Loss: 8002.3047, Test Loss: 3183.5911\n",
      "Epoch [134/1000], Train Loss: 9481.4633, Test Loss: 3125.2069\n",
      "Epoch [135/1000], Train Loss: 7552.4940, Test Loss: 5134.7509\n",
      "Epoch [136/1000], Train Loss: 7730.8234, Test Loss: 3219.8190\n",
      "Epoch [137/1000], Train Loss: 6681.6763, Test Loss: 3304.0563\n",
      "Epoch [138/1000], Train Loss: 115763.8761, Test Loss: 4009.6067\n",
      "Epoch [139/1000], Train Loss: 9087.0517, Test Loss: 2976.0587\n",
      "Epoch [140/1000], Train Loss: 7536.6665, Test Loss: 2781.1944\n",
      "Epoch [141/1000], Train Loss: 6451.0220, Test Loss: 2867.7752\n",
      "Epoch [142/1000], Train Loss: 6085.5807, Test Loss: 3006.0274\n",
      "Epoch [143/1000], Train Loss: 5822.7534, Test Loss: 2535.6395\n",
      "Epoch [144/1000], Train Loss: 5544.9011, Test Loss: 2738.1322\n",
      "Epoch [145/1000], Train Loss: 5615.3895, Test Loss: 2701.0087\n",
      "Epoch [146/1000], Train Loss: 5750.5212, Test Loss: 2688.8213\n",
      "Epoch [147/1000], Train Loss: 5687.6384, Test Loss: 2719.6202\n",
      "Epoch [148/1000], Train Loss: 5669.0983, Test Loss: 2775.4835\n",
      "Epoch [149/1000], Train Loss: 5859.4459, Test Loss: 2720.2697\n",
      "Epoch [150/1000], Train Loss: 6603.3863, Test Loss: 5102.7697\n",
      "Epoch [151/1000], Train Loss: 7868.4960, Test Loss: 2548.8475\n",
      "Epoch [152/1000], Train Loss: 6004.0625, Test Loss: 2645.3693\n",
      "Epoch [153/1000], Train Loss: 7347.6211, Test Loss: 2669.4283\n",
      "Epoch [154/1000], Train Loss: 6225.3352, Test Loss: 2740.9035\n",
      "Epoch [155/1000], Train Loss: 6217.3161, Test Loss: 2689.4076\n",
      "Epoch [156/1000], Train Loss: 76164.2603, Test Loss: 5309.8785\n",
      "Epoch [157/1000], Train Loss: 9667.5109, Test Loss: 2852.2401\n",
      "Epoch [158/1000], Train Loss: 6317.5260, Test Loss: 2667.7424\n",
      "Epoch [159/1000], Train Loss: 5563.5291, Test Loss: 2452.3742\n",
      "Epoch [160/1000], Train Loss: 5181.1513, Test Loss: 2666.5734\n",
      "Epoch [161/1000], Train Loss: 4998.8769, Test Loss: 2450.0028\n",
      "Epoch [162/1000], Train Loss: 5042.7457, Test Loss: 2508.9938\n",
      "Epoch [163/1000], Train Loss: 5238.8621, Test Loss: 2641.6052\n",
      "Epoch [164/1000], Train Loss: 5238.3043, Test Loss: 2660.3642\n",
      "Epoch [165/1000], Train Loss: 5305.1723, Test Loss: 2438.8265\n",
      "Epoch [166/1000], Train Loss: 5434.3971, Test Loss: 2703.3070\n",
      "Epoch [167/1000], Train Loss: 5606.3056, Test Loss: 2734.2475\n",
      "Epoch [168/1000], Train Loss: 6068.2352, Test Loss: 2785.1883\n",
      "Epoch [169/1000], Train Loss: 6608.7389, Test Loss: 3471.7358\n",
      "Epoch [170/1000], Train Loss: 7528.3174, Test Loss: 2818.6930\n",
      "Epoch [171/1000], Train Loss: 6735.9560, Test Loss: 2567.3060\n",
      "Epoch [172/1000], Train Loss: 5854.8695, Test Loss: 2820.0940\n",
      "Epoch [173/1000], Train Loss: 147723.6470, Test Loss: 10966.4136\n",
      "Epoch [174/1000], Train Loss: 18841.7134, Test Loss: 3827.7198\n",
      "Epoch [175/1000], Train Loss: 11677.8793, Test Loss: 3308.5010\n",
      "Epoch [176/1000], Train Loss: 7893.5972, Test Loss: 2955.9230\n",
      "Epoch [177/1000], Train Loss: 7260.4753, Test Loss: 2828.8986\n",
      "Epoch [178/1000], Train Loss: 6738.8655, Test Loss: 2758.2954\n",
      "Epoch [179/1000], Train Loss: 6536.6631, Test Loss: 2677.2595\n",
      "Epoch [180/1000], Train Loss: 5950.6121, Test Loss: 2618.3553\n",
      "Epoch [181/1000], Train Loss: 5391.0437, Test Loss: 2646.1547\n",
      "Epoch [182/1000], Train Loss: 5130.5009, Test Loss: 2606.8021\n",
      "Epoch [183/1000], Train Loss: 5182.5726, Test Loss: 2523.2667\n",
      "Epoch [184/1000], Train Loss: 5037.9215, Test Loss: 2549.2200\n",
      "Epoch [185/1000], Train Loss: 5353.5213, Test Loss: 5886.2332\n",
      "Epoch [186/1000], Train Loss: 9669.0717, Test Loss: 2568.8359\n",
      "Epoch [187/1000], Train Loss: 5074.0040, Test Loss: 2553.5833\n",
      "Epoch [188/1000], Train Loss: 5128.7730, Test Loss: 2447.2952\n",
      "Epoch [189/1000], Train Loss: 5896.3739, Test Loss: 2810.8723\n",
      "Epoch [190/1000], Train Loss: 5948.6185, Test Loss: 2448.4226\n",
      "Epoch [191/1000], Train Loss: 5268.0183, Test Loss: 2593.8206\n",
      "Epoch [192/1000], Train Loss: 5559.1110, Test Loss: 2647.8274\n",
      "Epoch [193/1000], Train Loss: 8767.8608, Test Loss: 4026.6360\n",
      "Epoch [194/1000], Train Loss: 6375.8254, Test Loss: 4260.3336\n",
      "Epoch [195/1000], Train Loss: 5571.1449, Test Loss: 2779.3934\n",
      "Epoch [196/1000], Train Loss: 5264.3991, Test Loss: 2479.8360\n",
      "Epoch [197/1000], Train Loss: 5166.7468, Test Loss: 2827.2882\n",
      "Epoch [198/1000], Train Loss: 8715.2526, Test Loss: 2487.2343\n",
      "Epoch [199/1000], Train Loss: 5103.6868, Test Loss: 2985.5492\n",
      "Epoch [200/1000], Train Loss: 7483.6944, Test Loss: 3201.2488\n",
      "Epoch [201/1000], Train Loss: 6000.6616, Test Loss: 2382.1296\n",
      "Epoch [202/1000], Train Loss: 5318.3581, Test Loss: 2570.5827\n",
      "Epoch [203/1000], Train Loss: 5226.4419, Test Loss: 2795.4732\n",
      "Epoch [204/1000], Train Loss: 5638.7936, Test Loss: 2681.5387\n",
      "Epoch [205/1000], Train Loss: 8128.4348, Test Loss: 2535.1972\n",
      "Epoch [206/1000], Train Loss: 5214.1181, Test Loss: 2501.5276\n",
      "Epoch [207/1000], Train Loss: 4992.4094, Test Loss: 2580.9226\n",
      "Epoch [208/1000], Train Loss: 5340.8142, Test Loss: 2431.9128\n",
      "Epoch [209/1000], Train Loss: 5316.1596, Test Loss: 2769.4171\n",
      "Epoch [210/1000], Train Loss: 7573.6374, Test Loss: 3121.1997\n",
      "Epoch [211/1000], Train Loss: 6020.8641, Test Loss: 2443.5867\n",
      "Epoch [212/1000], Train Loss: 8423.5722, Test Loss: 3258.2071\n",
      "Epoch [213/1000], Train Loss: 16053.1913, Test Loss: 2511.8241\n",
      "Epoch [214/1000], Train Loss: 4727.6470, Test Loss: 2356.2808\n",
      "Epoch [215/1000], Train Loss: 4438.9482, Test Loss: 2323.1541\n",
      "Epoch [216/1000], Train Loss: 4475.6327, Test Loss: 2394.9820\n",
      "Epoch [217/1000], Train Loss: 4482.8282, Test Loss: 2356.1665\n",
      "Epoch [218/1000], Train Loss: 4719.2864, Test Loss: 2534.5201\n",
      "Epoch [219/1000], Train Loss: 4965.2851, Test Loss: 2457.4949\n",
      "Epoch [220/1000], Train Loss: 5422.5772, Test Loss: 3158.2594\n",
      "Epoch [221/1000], Train Loss: 6606.9172, Test Loss: 2541.8090\n",
      "Epoch [222/1000], Train Loss: 5712.3468, Test Loss: 2306.1162\n",
      "Epoch [223/1000], Train Loss: 4537.4381, Test Loss: 2210.2911\n",
      "Epoch [224/1000], Train Loss: 4798.3276, Test Loss: 2609.4434\n",
      "Epoch [225/1000], Train Loss: 5581.5446, Test Loss: 2316.6453\n",
      "Epoch [226/1000], Train Loss: 4713.7387, Test Loss: 2381.2062\n",
      "Epoch [227/1000], Train Loss: 4721.5355, Test Loss: 2474.1810\n",
      "Epoch [228/1000], Train Loss: 4651.9389, Test Loss: 2334.6591\n",
      "Epoch [229/1000], Train Loss: 6546.9618, Test Loss: 2590.6983\n",
      "Epoch [230/1000], Train Loss: 4693.9412, Test Loss: 2304.6614\n",
      "Epoch [231/1000], Train Loss: 4230.8875, Test Loss: 2329.0092\n",
      "Epoch [232/1000], Train Loss: 4867.9539, Test Loss: 2415.9096\n",
      "Epoch [233/1000], Train Loss: 4706.5943, Test Loss: 2350.2343\n",
      "Epoch [234/1000], Train Loss: 4243.9869, Test Loss: 2534.8487\n",
      "Epoch [235/1000], Train Loss: 4512.2507, Test Loss: 2434.2224\n",
      "Epoch [236/1000], Train Loss: 5111.9988, Test Loss: 2241.7628\n",
      "Epoch [237/1000], Train Loss: 5908.6986, Test Loss: 3387.5923\n",
      "Epoch [238/1000], Train Loss: 4783.3128, Test Loss: 2290.8206\n",
      "Epoch [239/1000], Train Loss: 4059.6741, Test Loss: 2379.5216\n",
      "Epoch [240/1000], Train Loss: 4933.8852, Test Loss: 3479.2104\n",
      "Epoch [241/1000], Train Loss: 4445.2257, Test Loss: 2789.7368\n",
      "Epoch [242/1000], Train Loss: 4359.4282, Test Loss: 2402.6558\n",
      "Epoch [243/1000], Train Loss: 4411.3551, Test Loss: 2343.5685\n",
      "Epoch [244/1000], Train Loss: 4749.6490, Test Loss: 2419.1961\n",
      "Epoch [245/1000], Train Loss: 4252.0420, Test Loss: 2394.4854\n",
      "Epoch [246/1000], Train Loss: 4827.7180, Test Loss: 2332.1660\n",
      "Epoch [247/1000], Train Loss: 4960.0940, Test Loss: 2315.0837\n",
      "Epoch [248/1000], Train Loss: 4447.9692, Test Loss: 2434.8047\n",
      "Epoch [249/1000], Train Loss: 4707.4107, Test Loss: 2533.3516\n",
      "Epoch [250/1000], Train Loss: 4220.9208, Test Loss: 2336.0193\n",
      "Epoch [251/1000], Train Loss: 4013.2728, Test Loss: 2211.2364\n",
      "Epoch [252/1000], Train Loss: 3953.8550, Test Loss: 2235.9914\n",
      "Epoch [253/1000], Train Loss: 4640.3488, Test Loss: 2404.5524\n",
      "Epoch [254/1000], Train Loss: 12581.4485, Test Loss: 2542.8190\n",
      "Epoch [255/1000], Train Loss: 4025.9995, Test Loss: 2020.7051\n",
      "Epoch [256/1000], Train Loss: 6466.0130, Test Loss: 56810.5409\n",
      "Epoch [257/1000], Train Loss: 11607.1335, Test Loss: 2103.3991\n",
      "Epoch [258/1000], Train Loss: 3224.4312, Test Loss: 2162.5864\n",
      "Epoch [259/1000], Train Loss: 3266.3070, Test Loss: 2171.8318\n",
      "Epoch [260/1000], Train Loss: 4127.5362, Test Loss: 2307.6444\n",
      "Epoch [261/1000], Train Loss: 3671.1732, Test Loss: 2211.9086\n",
      "Epoch [262/1000], Train Loss: 3657.6478, Test Loss: 2216.0834\n",
      "Epoch [263/1000], Train Loss: 3984.7876, Test Loss: 2440.1962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [264/1000], Train Loss: 4440.9695, Test Loss: 2572.6695\n",
      "Epoch [265/1000], Train Loss: 73686.6255, Test Loss: 2666.4088\n",
      "Epoch [266/1000], Train Loss: 4389.3153, Test Loss: 2196.3857\n",
      "Epoch [267/1000], Train Loss: 3522.7048, Test Loss: 2111.3753\n",
      "Epoch [268/1000], Train Loss: 3233.5977, Test Loss: 2035.8586\n",
      "Epoch [269/1000], Train Loss: 3194.0671, Test Loss: 2163.3607\n",
      "Epoch [270/1000], Train Loss: 3525.8037, Test Loss: 2079.1761\n",
      "Epoch [271/1000], Train Loss: 3303.3658, Test Loss: 2152.7249\n",
      "Epoch [272/1000], Train Loss: 3382.9486, Test Loss: 2190.3445\n",
      "Epoch [273/1000], Train Loss: 3416.5783, Test Loss: 2147.6657\n",
      "Epoch [274/1000], Train Loss: 3393.2074, Test Loss: 2140.4642\n",
      "Epoch [275/1000], Train Loss: 3276.1716, Test Loss: 2062.0637\n",
      "Epoch [276/1000], Train Loss: 3307.9447, Test Loss: 2112.7535\n",
      "Epoch [277/1000], Train Loss: 3835.1412, Test Loss: 2275.7461\n",
      "Epoch [278/1000], Train Loss: 4267.3731, Test Loss: 2172.7121\n",
      "Epoch [279/1000], Train Loss: 4057.3925, Test Loss: 2321.7193\n",
      "Epoch [280/1000], Train Loss: 5007.4801, Test Loss: 2238.4994\n",
      "Epoch [281/1000], Train Loss: 3716.4437, Test Loss: 2206.3046\n",
      "Epoch [282/1000], Train Loss: 5776.9265, Test Loss: 2827.7032\n",
      "Epoch [283/1000], Train Loss: 4260.7722, Test Loss: 2319.9451\n",
      "Epoch [284/1000], Train Loss: 3711.7416, Test Loss: 2206.6805\n",
      "Epoch [285/1000], Train Loss: 3765.3689, Test Loss: 2359.4206\n",
      "Epoch [286/1000], Train Loss: 3900.2957, Test Loss: 2606.6493\n",
      "Epoch [287/1000], Train Loss: 53419.5864, Test Loss: 2609.7982\n",
      "Epoch [288/1000], Train Loss: 4177.6386, Test Loss: 2156.0151\n",
      "Epoch [289/1000], Train Loss: 3356.7347, Test Loss: 2171.2579\n",
      "Epoch [290/1000], Train Loss: 3257.5019, Test Loss: 2181.0557\n",
      "Epoch [291/1000], Train Loss: 2971.9893, Test Loss: 2094.0214\n",
      "Epoch [292/1000], Train Loss: 3027.2083, Test Loss: 2136.1060\n",
      "Epoch [293/1000], Train Loss: 3091.1756, Test Loss: 2122.4500\n",
      "Epoch [294/1000], Train Loss: 3151.5150, Test Loss: 1990.4556\n",
      "Epoch [295/1000], Train Loss: 3036.7376, Test Loss: 2304.1036\n",
      "Epoch [296/1000], Train Loss: 3187.2666, Test Loss: 2140.8480\n",
      "Epoch [297/1000], Train Loss: 3366.8906, Test Loss: 2177.0192\n",
      "Epoch [298/1000], Train Loss: 3265.1097, Test Loss: 2086.3737\n",
      "Epoch [299/1000], Train Loss: 3680.9503, Test Loss: 2148.9745\n",
      "Epoch [300/1000], Train Loss: 3819.9578, Test Loss: 2280.5948\n",
      "Epoch [301/1000], Train Loss: 3420.3586, Test Loss: 2349.8704\n",
      "Epoch [302/1000], Train Loss: 16543.1180, Test Loss: 2387.1905\n",
      "Epoch [303/1000], Train Loss: 3469.9451, Test Loss: 2079.4382\n",
      "Epoch [304/1000], Train Loss: 3074.3411, Test Loss: 2041.0560\n",
      "Epoch [305/1000], Train Loss: 2927.1456, Test Loss: 2119.3165\n",
      "Epoch [306/1000], Train Loss: 3084.8595, Test Loss: 2062.0654\n",
      "Epoch [307/1000], Train Loss: 5770.7204, Test Loss: 2917.9869\n",
      "Epoch [308/1000], Train Loss: 3839.5657, Test Loss: 2124.6626\n",
      "Epoch [309/1000], Train Loss: 3094.3568, Test Loss: 2067.1474\n",
      "Epoch [310/1000], Train Loss: 3067.4514, Test Loss: 2106.4049\n",
      "Epoch [311/1000], Train Loss: 3172.0201, Test Loss: 2144.2861\n",
      "Epoch [312/1000], Train Loss: 3725.8194, Test Loss: 2284.3966\n",
      "Epoch [313/1000], Train Loss: 6838.6058, Test Loss: 2260.8090\n",
      "Epoch [314/1000], Train Loss: 3338.8621, Test Loss: 2125.7808\n",
      "Epoch [315/1000], Train Loss: 3244.3894, Test Loss: 2142.2403\n",
      "Epoch [316/1000], Train Loss: 3100.7195, Test Loss: 2076.6536\n",
      "Epoch [317/1000], Train Loss: 3124.8309, Test Loss: 2062.3763\n",
      "Epoch [318/1000], Train Loss: 3161.2428, Test Loss: 2310.0405\n",
      "Epoch [319/1000], Train Loss: 3204.2733, Test Loss: 2141.4368\n",
      "Epoch [320/1000], Train Loss: 3416.6417, Test Loss: 2098.3556\n",
      "Epoch [321/1000], Train Loss: 5065.7297, Test Loss: 2500.7495\n",
      "Epoch [322/1000], Train Loss: 3581.1793, Test Loss: 2088.0866\n",
      "Epoch [323/1000], Train Loss: 3135.5750, Test Loss: 2180.5698\n",
      "Epoch [324/1000], Train Loss: 3225.1834, Test Loss: 2167.8397\n",
      "Epoch [325/1000], Train Loss: 2893.8634, Test Loss: 2044.5131\n",
      "Epoch [326/1000], Train Loss: 8784.8873, Test Loss: 2192.8803\n",
      "Epoch [327/1000], Train Loss: 3430.2701, Test Loss: 2056.8872\n",
      "Epoch [328/1000], Train Loss: 2872.1525, Test Loss: 2002.8075\n",
      "Epoch [329/1000], Train Loss: 2656.0292, Test Loss: 1994.9748\n",
      "Epoch [330/1000], Train Loss: 2781.1223, Test Loss: 2144.6879\n",
      "Epoch [331/1000], Train Loss: 3239.7408, Test Loss: 2177.9669\n",
      "Epoch [332/1000], Train Loss: 3147.2831, Test Loss: 2012.2733\n",
      "Epoch [333/1000], Train Loss: 8155.0156, Test Loss: 3244.1507\n",
      "Epoch [334/1000], Train Loss: 3665.9034, Test Loss: 2616.6090\n",
      "Epoch [335/1000], Train Loss: 9363.3790, Test Loss: 1998.3782\n",
      "Epoch [336/1000], Train Loss: 2556.1806, Test Loss: 1967.1264\n",
      "Epoch [337/1000], Train Loss: 2369.8567, Test Loss: 1964.7099\n",
      "Epoch [338/1000], Train Loss: 2637.6517, Test Loss: 1974.9214\n",
      "Epoch [339/1000], Train Loss: 2672.2322, Test Loss: 2032.2699\n",
      "Epoch [340/1000], Train Loss: 2875.0662, Test Loss: 2106.3707\n",
      "Epoch [341/1000], Train Loss: 2989.1577, Test Loss: 2310.4322\n",
      "Epoch [342/1000], Train Loss: 3089.4440, Test Loss: 2108.3365\n",
      "Epoch [343/1000], Train Loss: 3018.7031, Test Loss: 2010.2018\n",
      "Epoch [344/1000], Train Loss: 5965.0458, Test Loss: 2513.0478\n",
      "Epoch [345/1000], Train Loss: 4881.1426, Test Loss: 4073.3220\n",
      "Epoch [346/1000], Train Loss: 3719.0699, Test Loss: 2054.3458\n",
      "Epoch [347/1000], Train Loss: 2893.4836, Test Loss: 2833.0053\n",
      "Epoch [348/1000], Train Loss: 7030.7343, Test Loss: 7712.3493\n",
      "Epoch [349/1000], Train Loss: 6647.1238, Test Loss: 2063.0341\n",
      "Epoch [350/1000], Train Loss: 2560.6127, Test Loss: 2034.5532\n",
      "Epoch [351/1000], Train Loss: 2400.7046, Test Loss: 1936.5072\n",
      "Epoch [352/1000], Train Loss: 2576.2179, Test Loss: 2003.5867\n",
      "Epoch [353/1000], Train Loss: 2896.4085, Test Loss: 2155.8295\n",
      "Epoch [354/1000], Train Loss: 4164.0162, Test Loss: 2163.6318\n",
      "Epoch [355/1000], Train Loss: 17747.1749, Test Loss: 4295.2580\n",
      "Epoch [356/1000], Train Loss: 4562.1047, Test Loss: 2118.7315\n",
      "Epoch [357/1000], Train Loss: 2593.6590, Test Loss: 2000.7012\n",
      "Epoch [358/1000], Train Loss: 2490.6943, Test Loss: 1977.5096\n",
      "Epoch [359/1000], Train Loss: 2554.7444, Test Loss: 2206.4985\n",
      "Epoch [360/1000], Train Loss: 9310.3432, Test Loss: 2616.2872\n",
      "Epoch [361/1000], Train Loss: 3162.8176, Test Loss: 2003.8397\n",
      "Epoch [362/1000], Train Loss: 4854.4034, Test Loss: 3048.2906\n",
      "Epoch [363/1000], Train Loss: 4011.7526, Test Loss: 2283.9395\n",
      "Epoch [364/1000], Train Loss: 2995.1622, Test Loss: 2135.5231\n",
      "Epoch [365/1000], Train Loss: 3324.7563, Test Loss: 2407.2291\n",
      "Epoch [366/1000], Train Loss: 4480.2974, Test Loss: 2136.5797\n",
      "Epoch [367/1000], Train Loss: 18752.2993, Test Loss: 2444.1803\n",
      "Epoch [368/1000], Train Loss: 3305.9748, Test Loss: 2034.0596\n",
      "Epoch [369/1000], Train Loss: 2659.8332, Test Loss: 2074.5569\n",
      "Epoch [370/1000], Train Loss: 2732.2566, Test Loss: 1951.7573\n",
      "Epoch [371/1000], Train Loss: 2827.5657, Test Loss: 2087.0199\n",
      "Epoch [372/1000], Train Loss: 3243.4722, Test Loss: 2011.4133\n",
      "Epoch [373/1000], Train Loss: 81614.8109, Test Loss: 7904.4846\n",
      "Epoch [374/1000], Train Loss: 10088.7987, Test Loss: 2703.7791\n",
      "Epoch [375/1000], Train Loss: 4505.9651, Test Loss: 2285.5667\n",
      "Epoch [376/1000], Train Loss: 3577.1478, Test Loss: 2193.4887\n",
      "Epoch [377/1000], Train Loss: 3138.1657, Test Loss: 2266.5943\n",
      "Epoch [378/1000], Train Loss: 3333.9990, Test Loss: 2141.0516\n",
      "Epoch [379/1000], Train Loss: 3261.6178, Test Loss: 2743.1007\n",
      "Epoch [380/1000], Train Loss: 3225.1891, Test Loss: 2425.6685\n",
      "Epoch [381/1000], Train Loss: 3179.4855, Test Loss: 2197.7366\n",
      "Epoch [382/1000], Train Loss: 3253.7013, Test Loss: 2254.7617\n",
      "Epoch [383/1000], Train Loss: 3304.1991, Test Loss: 2370.2379\n",
      "Epoch [384/1000], Train Loss: 5320.2870, Test Loss: 3636.8236\n",
      "Epoch [385/1000], Train Loss: 4413.0939, Test Loss: 2252.1987\n",
      "Epoch [386/1000], Train Loss: 3593.8431, Test Loss: 2511.3686\n",
      "Epoch [387/1000], Train Loss: 3287.5196, Test Loss: 2529.1623\n",
      "Epoch [388/1000], Train Loss: 4152.4386, Test Loss: 2195.5478\n",
      "Epoch [389/1000], Train Loss: 3557.5414, Test Loss: 2393.5189\n",
      "Epoch [390/1000], Train Loss: 5078.3669, Test Loss: 2354.7326\n",
      "Epoch [391/1000], Train Loss: 4447.4428, Test Loss: 2267.8869\n",
      "Epoch [392/1000], Train Loss: 5315.8618, Test Loss: 2293.7190\n",
      "Epoch [393/1000], Train Loss: 3643.8546, Test Loss: 2327.1587\n",
      "Epoch [394/1000], Train Loss: 3557.1845, Test Loss: 2332.2267\n",
      "Epoch [395/1000], Train Loss: 4202.0357, Test Loss: 2158.9995\n",
      "Epoch [396/1000], Train Loss: 7851.9599, Test Loss: 2360.0549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [397/1000], Train Loss: 3271.0709, Test Loss: 2163.3594\n",
      "Epoch [398/1000], Train Loss: 2953.1600, Test Loss: 2095.2509\n",
      "Epoch [399/1000], Train Loss: 6334.9439, Test Loss: 2217.5390\n",
      "Epoch [400/1000], Train Loss: 3249.3185, Test Loss: 2115.1883\n",
      "Epoch [401/1000], Train Loss: 3686.7224, Test Loss: 2010.2249\n",
      "Epoch [402/1000], Train Loss: 3279.5138, Test Loss: 2049.7764\n",
      "Epoch [403/1000], Train Loss: 7978.0467, Test Loss: 2050.4153\n",
      "Epoch [404/1000], Train Loss: 2856.6859, Test Loss: 2042.1921\n",
      "Epoch [405/1000], Train Loss: 2670.6811, Test Loss: 2050.9749\n",
      "Epoch [406/1000], Train Loss: 4132.8755, Test Loss: 2754.4803\n",
      "Epoch [407/1000], Train Loss: 6396.4418, Test Loss: 2124.1890\n",
      "Epoch [408/1000], Train Loss: 3111.5807, Test Loss: 2195.2762\n",
      "Epoch [409/1000], Train Loss: 3312.9394, Test Loss: 2183.0925\n",
      "Epoch [410/1000], Train Loss: 4442.2257, Test Loss: 2196.2617\n",
      "Epoch [411/1000], Train Loss: 3382.4740, Test Loss: 2501.4612\n",
      "Epoch [412/1000], Train Loss: 9274.2880, Test Loss: 5101.2305\n",
      "Epoch [413/1000], Train Loss: 5370.1034, Test Loss: 2003.3872\n",
      "Epoch [414/1000], Train Loss: 2764.1572, Test Loss: 2050.2325\n",
      "Epoch [415/1000], Train Loss: 2665.6974, Test Loss: 2159.2827\n",
      "Epoch [416/1000], Train Loss: 3650.1306, Test Loss: 2853.4339\n",
      "Epoch [417/1000], Train Loss: 16751.5077, Test Loss: 2103.4405\n",
      "Epoch [418/1000], Train Loss: 2698.6478, Test Loss: 1890.8315\n",
      "Epoch [419/1000], Train Loss: 2153.2400, Test Loss: 1871.8402\n",
      "Epoch [420/1000], Train Loss: 2130.2581, Test Loss: 1884.3449\n",
      "Epoch [421/1000], Train Loss: 2217.7162, Test Loss: 1898.2102\n",
      "Epoch [422/1000], Train Loss: 2329.2272, Test Loss: 1953.5851\n",
      "Epoch [423/1000], Train Loss: 2622.4103, Test Loss: 1997.1244\n",
      "Epoch [424/1000], Train Loss: 5765.9502, Test Loss: 2909.6708\n",
      "Epoch [425/1000], Train Loss: 4823.5727, Test Loss: 2103.6406\n",
      "Epoch [426/1000], Train Loss: 3134.4047, Test Loss: 2387.9633\n",
      "Epoch [427/1000], Train Loss: 2942.0733, Test Loss: 2259.7743\n",
      "Epoch [428/1000], Train Loss: 3355.6184, Test Loss: 2157.1062\n",
      "Epoch [429/1000], Train Loss: 4999.3521, Test Loss: 2760.6162\n",
      "Epoch [430/1000], Train Loss: 12232.3204, Test Loss: 2153.5103\n",
      "Epoch [431/1000], Train Loss: 2917.4916, Test Loss: 2009.7730\n",
      "Epoch [432/1000], Train Loss: 2410.6724, Test Loss: 2032.8815\n",
      "Epoch [433/1000], Train Loss: 2146.4995, Test Loss: 1847.1326\n",
      "Epoch [434/1000], Train Loss: 2118.7889, Test Loss: 1958.2296\n",
      "Epoch [435/1000], Train Loss: 2250.3042, Test Loss: 1988.0992\n",
      "Epoch [436/1000], Train Loss: 5008.9583, Test Loss: 2441.8742\n",
      "Epoch [437/1000], Train Loss: 4741.9081, Test Loss: 2085.0204\n",
      "Epoch [438/1000], Train Loss: 3345.3909, Test Loss: 2128.3343\n",
      "Epoch [439/1000], Train Loss: 2415.7327, Test Loss: 2028.8640\n",
      "Epoch [440/1000], Train Loss: 2880.2002, Test Loss: 2302.5567\n",
      "Epoch [441/1000], Train Loss: 2908.9488, Test Loss: 2078.0736\n",
      "Epoch [442/1000], Train Loss: 4489.0166, Test Loss: 2254.1720\n",
      "Epoch [443/1000], Train Loss: 9875.7014, Test Loss: 2816.1090\n",
      "Epoch [444/1000], Train Loss: 5025.8461, Test Loss: 2540.4476\n",
      "Epoch [445/1000], Train Loss: 2920.0711, Test Loss: 2458.3931\n",
      "Epoch [446/1000], Train Loss: 2859.5215, Test Loss: 2177.0591\n",
      "Epoch [447/1000], Train Loss: 2655.0956, Test Loss: 2248.7081\n",
      "Epoch [448/1000], Train Loss: 3588.7943, Test Loss: 2119.4693\n",
      "Epoch [449/1000], Train Loss: 2833.7830, Test Loss: 1923.3287\n",
      "Epoch [450/1000], Train Loss: 49725.0019, Test Loss: 3101.4070\n",
      "Epoch [451/1000], Train Loss: 4307.6670, Test Loss: 1950.9247\n",
      "Epoch [452/1000], Train Loss: 2556.3464, Test Loss: 1896.0358\n",
      "Epoch [453/1000], Train Loss: 2211.8873, Test Loss: 1895.0432\n",
      "Epoch [454/1000], Train Loss: 2371.6956, Test Loss: 1825.5270\n",
      "Epoch [455/1000], Train Loss: 2402.2757, Test Loss: 1968.4519\n",
      "Epoch [456/1000], Train Loss: 2379.5117, Test Loss: 1934.1004\n",
      "Epoch [457/1000], Train Loss: 2304.7631, Test Loss: 2039.9012\n",
      "Epoch [458/1000], Train Loss: 2569.6812, Test Loss: 2033.9226\n",
      "Epoch [459/1000], Train Loss: 2750.0453, Test Loss: 2078.2822\n",
      "Epoch [460/1000], Train Loss: 2642.6105, Test Loss: 2095.1148\n",
      "Epoch [461/1000], Train Loss: 2612.9607, Test Loss: 2010.9218\n",
      "Epoch [462/1000], Train Loss: 2543.2643, Test Loss: 2101.9421\n",
      "Epoch [463/1000], Train Loss: 7053.8209, Test Loss: 1859.5629\n",
      "Epoch [464/1000], Train Loss: 2067.6452, Test Loss: 2053.8826\n",
      "Epoch [465/1000], Train Loss: 2223.7602, Test Loss: 1959.7266\n",
      "Epoch [466/1000], Train Loss: 2442.1211, Test Loss: 2082.9932\n",
      "Epoch [467/1000], Train Loss: 5280.8264, Test Loss: 2271.2280\n",
      "Epoch [468/1000], Train Loss: 33646.3966, Test Loss: 2439.5437\n",
      "Epoch [469/1000], Train Loss: 3281.0759, Test Loss: 1974.0565\n",
      "Epoch [470/1000], Train Loss: 2397.6852, Test Loss: 1853.5332\n",
      "Epoch [471/1000], Train Loss: 2005.0202, Test Loss: 1864.5533\n",
      "Epoch [472/1000], Train Loss: 1894.6888, Test Loss: 1876.7675\n",
      "Epoch [473/1000], Train Loss: 1955.5279, Test Loss: 1854.9475\n",
      "Epoch [474/1000], Train Loss: 1990.8310, Test Loss: 1881.5987\n",
      "Epoch [475/1000], Train Loss: 2192.1178, Test Loss: 1886.1963\n",
      "Epoch [476/1000], Train Loss: 2120.8449, Test Loss: 1897.6404\n",
      "Epoch [477/1000], Train Loss: 2669.4179, Test Loss: 2070.7809\n",
      "Epoch [478/1000], Train Loss: 2340.8934, Test Loss: 2017.4890\n",
      "Epoch [479/1000], Train Loss: 4284.8657, Test Loss: 2121.2142\n",
      "Epoch [480/1000], Train Loss: 18920.0627, Test Loss: 2092.9835\n",
      "Epoch [481/1000], Train Loss: 2689.1657, Test Loss: 1969.9639\n",
      "Epoch [482/1000], Train Loss: 1884.9855, Test Loss: 1926.9219\n",
      "Epoch [483/1000], Train Loss: 1894.1318, Test Loss: 1840.8170\n",
      "Epoch [484/1000], Train Loss: 1952.1455, Test Loss: 1882.4250\n",
      "Epoch [485/1000], Train Loss: 4649.9426, Test Loss: 1928.9206\n",
      "Epoch [486/1000], Train Loss: 2313.3843, Test Loss: 2022.5878\n",
      "Epoch [487/1000], Train Loss: 3352.9356, Test Loss: 2718.6912\n",
      "Epoch [488/1000], Train Loss: 2787.6276, Test Loss: 2040.0207\n",
      "Epoch [489/1000], Train Loss: 3852.2594, Test Loss: 2630.2410\n",
      "Epoch [490/1000], Train Loss: 4109.5432, Test Loss: 2291.9444\n",
      "Epoch [491/1000], Train Loss: 4169.2736, Test Loss: 2060.7065\n",
      "Epoch [492/1000], Train Loss: 2900.8998, Test Loss: 2663.0378\n",
      "Epoch [493/1000], Train Loss: 26583.8148, Test Loss: 2253.5388\n",
      "Epoch [494/1000], Train Loss: 2520.6853, Test Loss: 2046.5154\n",
      "Epoch [495/1000], Train Loss: 2072.2244, Test Loss: 1858.1560\n",
      "Epoch [496/1000], Train Loss: 1833.9312, Test Loss: 1870.5211\n",
      "Epoch [497/1000], Train Loss: 1860.5742, Test Loss: 1972.4388\n",
      "Epoch [498/1000], Train Loss: 2148.9160, Test Loss: 2004.0414\n",
      "Epoch [499/1000], Train Loss: 2355.3443, Test Loss: 2163.7090\n",
      "Epoch [500/1000], Train Loss: 2469.8082, Test Loss: 1878.0498\n",
      "Epoch [501/1000], Train Loss: 3050.3575, Test Loss: 2010.7749\n",
      "Epoch [502/1000], Train Loss: 3078.3941, Test Loss: 2109.5240\n",
      "Epoch [503/1000], Train Loss: 3794.7955, Test Loss: 2115.0614\n",
      "Epoch [504/1000], Train Loss: 2587.5141, Test Loss: 1934.4731\n",
      "Epoch [505/1000], Train Loss: 58242.7681, Test Loss: 325944.6030\n",
      "Epoch [506/1000], Train Loss: 44625.5998, Test Loss: 2571.5294\n",
      "Epoch [507/1000], Train Loss: 3871.7918, Test Loss: 2154.2050\n",
      "Epoch [508/1000], Train Loss: 3504.8193, Test Loss: 2073.4371\n",
      "Epoch [509/1000], Train Loss: 2726.2924, Test Loss: 1982.9099\n",
      "Epoch [510/1000], Train Loss: 2899.2066, Test Loss: 1982.3836\n",
      "Epoch [511/1000], Train Loss: 2473.2055, Test Loss: 1939.8906\n",
      "Epoch [512/1000], Train Loss: 2666.4020, Test Loss: 2111.2110\n",
      "Epoch [513/1000], Train Loss: 2978.5758, Test Loss: 2046.6380\n",
      "Epoch [514/1000], Train Loss: 2485.9021, Test Loss: 1936.6844\n",
      "Epoch [515/1000], Train Loss: 2335.1917, Test Loss: 1907.6682\n",
      "Epoch [516/1000], Train Loss: 2162.4802, Test Loss: 1946.2932\n",
      "Epoch [517/1000], Train Loss: 2681.8160, Test Loss: 1910.9448\n",
      "Epoch [518/1000], Train Loss: 4206.2968, Test Loss: 2813.8950\n",
      "Epoch [519/1000], Train Loss: 6436.1512, Test Loss: 2085.4292\n",
      "Epoch [520/1000], Train Loss: 2463.6703, Test Loss: 2009.0918\n",
      "Epoch [521/1000], Train Loss: 2331.3278, Test Loss: 1974.8797\n",
      "Epoch [522/1000], Train Loss: 4875.6925, Test Loss: 2541.8629\n",
      "Epoch [523/1000], Train Loss: 3688.0000, Test Loss: 2363.9737\n",
      "Epoch [524/1000], Train Loss: 6375.5207, Test Loss: 2668.7664\n",
      "Epoch [525/1000], Train Loss: 2977.0534, Test Loss: 2010.4117\n",
      "Epoch [526/1000], Train Loss: 2621.7193, Test Loss: 2141.5879\n",
      "Epoch [527/1000], Train Loss: 2697.1202, Test Loss: 3601.4666\n",
      "Epoch [528/1000], Train Loss: 20869.5111, Test Loss: 2000.1975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [529/1000], Train Loss: 2309.5244, Test Loss: 1835.1931\n",
      "Epoch [530/1000], Train Loss: 1924.6321, Test Loss: 1874.5923\n",
      "Epoch [531/1000], Train Loss: 1852.5644, Test Loss: 2003.8035\n",
      "Epoch [532/1000], Train Loss: 2003.9669, Test Loss: 1919.0344\n",
      "Epoch [533/1000], Train Loss: 2174.1346, Test Loss: 2062.2091\n",
      "Epoch [534/1000], Train Loss: 2276.4252, Test Loss: 1983.4150\n",
      "Epoch [535/1000], Train Loss: 2616.7249, Test Loss: 2085.3722\n",
      "Epoch [536/1000], Train Loss: 8254.3549, Test Loss: 1969.9127\n",
      "Epoch [537/1000], Train Loss: 2253.8468, Test Loss: 1994.6049\n",
      "Epoch [538/1000], Train Loss: 2091.4624, Test Loss: 1995.5539\n",
      "Epoch [539/1000], Train Loss: 2317.6495, Test Loss: 2125.5254\n",
      "Epoch [540/1000], Train Loss: 3947.9004, Test Loss: 2204.9302\n",
      "Epoch [541/1000], Train Loss: 16915.2976, Test Loss: 2504.7937\n",
      "Epoch [542/1000], Train Loss: 3780.1917, Test Loss: 2057.2919\n",
      "Epoch [543/1000], Train Loss: 2188.1438, Test Loss: 1911.6135\n",
      "Epoch [544/1000], Train Loss: 1992.9554, Test Loss: 1929.8564\n",
      "Epoch [545/1000], Train Loss: 2371.8843, Test Loss: 1909.1381\n",
      "Epoch [546/1000], Train Loss: 4193.1875, Test Loss: 3132.6947\n",
      "Epoch [547/1000], Train Loss: 3949.8435, Test Loss: 2034.6787\n",
      "Epoch [548/1000], Train Loss: 2614.7010, Test Loss: 1995.9677\n",
      "Epoch [549/1000], Train Loss: 2955.9526, Test Loss: 2079.3521\n",
      "Epoch [550/1000], Train Loss: 4224.3033, Test Loss: 2578.1066\n",
      "Epoch [551/1000], Train Loss: 5449.7870, Test Loss: 2021.6508\n",
      "Epoch [552/1000], Train Loss: 2705.7379, Test Loss: 1958.7676\n",
      "Epoch [553/1000], Train Loss: 2355.7672, Test Loss: 1932.9193\n",
      "Epoch [554/1000], Train Loss: 35002.8020, Test Loss: 2197.8530\n",
      "Epoch [555/1000], Train Loss: 2548.9924, Test Loss: 1977.1421\n",
      "Epoch [556/1000], Train Loss: 1919.3296, Test Loss: 1884.1542\n",
      "Epoch [557/1000], Train Loss: 1891.7541, Test Loss: 1882.8474\n",
      "Epoch [558/1000], Train Loss: 1830.4967, Test Loss: 1974.5778\n",
      "Epoch [559/1000], Train Loss: 1958.2289, Test Loss: 1928.9194\n",
      "Epoch [560/1000], Train Loss: 1957.8862, Test Loss: 1905.2522\n",
      "Epoch [561/1000], Train Loss: 2008.1860, Test Loss: 1991.8386\n",
      "Epoch [562/1000], Train Loss: 2144.7808, Test Loss: 1932.4250\n",
      "Epoch [563/1000], Train Loss: 2421.4446, Test Loss: 1910.0859\n",
      "Epoch [564/1000], Train Loss: 10245.7383, Test Loss: 9061.1450\n",
      "Epoch [565/1000], Train Loss: 4348.8013, Test Loss: 2006.9217\n",
      "Epoch [566/1000], Train Loss: 2191.6427, Test Loss: 1900.3793\n",
      "Epoch [567/1000], Train Loss: 1921.6484, Test Loss: 1921.2626\n",
      "Epoch [568/1000], Train Loss: 1965.3700, Test Loss: 1957.6069\n",
      "Epoch [569/1000], Train Loss: 3909.6687, Test Loss: 2088.4879\n",
      "Epoch [570/1000], Train Loss: 3522.5935, Test Loss: 2046.6711\n",
      "Epoch [571/1000], Train Loss: 3479.8433, Test Loss: 2131.6269\n",
      "Epoch [572/1000], Train Loss: 3428.2147, Test Loss: 2232.3628\n",
      "Epoch [573/1000], Train Loss: 2497.3523, Test Loss: 2035.1272\n",
      "Epoch [574/1000], Train Loss: 4313.7698, Test Loss: 4509.9883\n",
      "Epoch [575/1000], Train Loss: 7499.3502, Test Loss: 3257.3865\n",
      "Epoch [576/1000], Train Loss: 5743.2002, Test Loss: 2154.1369\n",
      "Epoch [577/1000], Train Loss: 2945.6387, Test Loss: 1992.9180\n",
      "Epoch [578/1000], Train Loss: 2494.8251, Test Loss: 2192.9048\n",
      "Epoch [579/1000], Train Loss: 3801.8818, Test Loss: 3559.6750\n",
      "Epoch [580/1000], Train Loss: 4210.7491, Test Loss: 2264.3862\n",
      "Epoch [581/1000], Train Loss: 3920.8724, Test Loss: 2150.1750\n",
      "Epoch [582/1000], Train Loss: 2751.0452, Test Loss: 2543.6023\n",
      "Epoch [583/1000], Train Loss: 5205.9870, Test Loss: 2166.4150\n",
      "Epoch [584/1000], Train Loss: 5868.9897, Test Loss: 2254.1605\n",
      "Epoch [585/1000], Train Loss: 3730.9281, Test Loss: 2129.8898\n",
      "Epoch [586/1000], Train Loss: 3184.3944, Test Loss: 3122.7400\n",
      "Epoch [587/1000], Train Loss: 3104.6585, Test Loss: 2240.0737\n",
      "Epoch [588/1000], Train Loss: 2729.6550, Test Loss: 2209.5040\n",
      "Epoch [589/1000], Train Loss: 3161.5938, Test Loss: 2390.8981\n",
      "Epoch [590/1000], Train Loss: 9615.7111, Test Loss: 2698.1704\n",
      "Epoch [591/1000], Train Loss: 3342.5317, Test Loss: 2022.6197\n",
      "Epoch [592/1000], Train Loss: 2634.1625, Test Loss: 2319.6507\n",
      "Epoch [593/1000], Train Loss: 4660.5672, Test Loss: 2340.3275\n",
      "Epoch [594/1000], Train Loss: 3440.3158, Test Loss: 2200.0819\n",
      "Epoch [595/1000], Train Loss: 2991.9835, Test Loss: 2626.9782\n",
      "Epoch [596/1000], Train Loss: 4603.5293, Test Loss: 2653.7929\n",
      "Epoch [597/1000], Train Loss: 8239.6288, Test Loss: 2375.6836\n",
      "Epoch [598/1000], Train Loss: 3434.2053, Test Loss: 2081.8816\n",
      "Epoch [599/1000], Train Loss: 2881.7442, Test Loss: 1974.0469\n",
      "Epoch [600/1000], Train Loss: 3369.9424, Test Loss: 2068.1089\n",
      "Epoch [601/1000], Train Loss: 5971.5513, Test Loss: 2075.2510\n",
      "Epoch [602/1000], Train Loss: 2761.7309, Test Loss: 1995.4035\n",
      "Epoch [603/1000], Train Loss: 2535.9877, Test Loss: 1981.2762\n",
      "Epoch [604/1000], Train Loss: 3260.3215, Test Loss: 2142.4755\n",
      "Epoch [605/1000], Train Loss: 3092.9722, Test Loss: 2210.7262\n",
      "Epoch [606/1000], Train Loss: 3340.0299, Test Loss: 2293.7756\n",
      "Epoch [607/1000], Train Loss: 21324.6778, Test Loss: 2210.6481\n",
      "Epoch [608/1000], Train Loss: 2582.7461, Test Loss: 2051.0012\n",
      "Epoch [609/1000], Train Loss: 1966.8086, Test Loss: 2045.5435\n",
      "Epoch [610/1000], Train Loss: 1966.7585, Test Loss: 1942.1121\n",
      "Epoch [611/1000], Train Loss: 1906.6538, Test Loss: 1989.2953\n",
      "Epoch [612/1000], Train Loss: 2008.2374, Test Loss: 2179.0302\n",
      "Epoch [613/1000], Train Loss: 2873.2297, Test Loss: 2326.3947\n",
      "Epoch [614/1000], Train Loss: 2666.3041, Test Loss: 2660.4712\n",
      "Epoch [615/1000], Train Loss: 27258.7312, Test Loss: 2108.8676\n",
      "Epoch [616/1000], Train Loss: 2108.6185, Test Loss: 1935.2839\n",
      "Epoch [617/1000], Train Loss: 1846.3954, Test Loss: 1910.8702\n",
      "Epoch [618/1000], Train Loss: 1872.9473, Test Loss: 1956.3315\n",
      "Epoch [619/1000], Train Loss: 1827.1568, Test Loss: 1926.3430\n",
      "Epoch [620/1000], Train Loss: 2125.1427, Test Loss: 2124.6279\n",
      "Epoch [621/1000], Train Loss: 2442.4183, Test Loss: 1991.4790\n",
      "Epoch [622/1000], Train Loss: 2089.1304, Test Loss: 2154.1474\n",
      "Epoch [623/1000], Train Loss: 2100.5205, Test Loss: 2124.9057\n",
      "Epoch [624/1000], Train Loss: 2648.4526, Test Loss: 2434.4542\n",
      "Epoch [625/1000], Train Loss: 9658.4055, Test Loss: 2046.4996\n",
      "Epoch [626/1000], Train Loss: 2228.6211, Test Loss: 2063.6477\n",
      "Epoch [627/1000], Train Loss: 1922.4386, Test Loss: 2009.2467\n",
      "Epoch [628/1000], Train Loss: 1928.7158, Test Loss: 1971.1397\n",
      "Epoch [629/1000], Train Loss: 2116.0235, Test Loss: 2185.0628\n",
      "Epoch [630/1000], Train Loss: 3051.9449, Test Loss: 3127.9579\n",
      "Epoch [631/1000], Train Loss: 8564.6172, Test Loss: 2234.3494\n",
      "Epoch [632/1000], Train Loss: 2276.6757, Test Loss: 2005.6322\n",
      "Epoch [633/1000], Train Loss: 2099.8328, Test Loss: 2015.2662\n",
      "Epoch [634/1000], Train Loss: 2157.8399, Test Loss: 1934.5738\n",
      "Epoch [635/1000], Train Loss: 2820.1434, Test Loss: 2330.2607\n",
      "Epoch [636/1000], Train Loss: 5112.8139, Test Loss: 5421.7386\n",
      "Epoch [637/1000], Train Loss: 4901.4333, Test Loss: 2033.6316\n",
      "Epoch [638/1000], Train Loss: 2162.3018, Test Loss: 1951.9133\n",
      "Epoch [639/1000], Train Loss: 1923.3748, Test Loss: 2010.8656\n",
      "Epoch [640/1000], Train Loss: 2188.4700, Test Loss: 1982.1180\n",
      "Epoch [641/1000], Train Loss: 17050.1436, Test Loss: 2092.1692\n",
      "Epoch [642/1000], Train Loss: 2252.0985, Test Loss: 2397.9816\n",
      "Epoch [643/1000], Train Loss: 1879.3666, Test Loss: 1866.7218\n",
      "Epoch [644/1000], Train Loss: 1642.1333, Test Loss: 1948.1495\n",
      "Epoch [645/1000], Train Loss: 1670.1706, Test Loss: 1867.1107\n",
      "Epoch [646/1000], Train Loss: 1756.6983, Test Loss: 1918.3447\n",
      "Epoch [647/1000], Train Loss: 1877.5667, Test Loss: 2021.4440\n",
      "Epoch [648/1000], Train Loss: 2748.9304, Test Loss: 2956.2493\n",
      "Epoch [649/1000], Train Loss: 5573.9264, Test Loss: 2145.5285\n",
      "Epoch [650/1000], Train Loss: 2086.6609, Test Loss: 1985.0749\n",
      "Epoch [651/1000], Train Loss: 3198.4258, Test Loss: 2710.1420\n",
      "Epoch [652/1000], Train Loss: 7168.9835, Test Loss: 2316.5563\n",
      "Epoch [653/1000], Train Loss: 2621.6781, Test Loss: 2004.3851\n",
      "Epoch [654/1000], Train Loss: 2310.9669, Test Loss: 1959.4310\n",
      "Epoch [655/1000], Train Loss: 1973.6479, Test Loss: 1991.8106\n",
      "Epoch [656/1000], Train Loss: 2324.8073, Test Loss: 2310.7612\n",
      "Epoch [657/1000], Train Loss: 3381.1396, Test Loss: 2579.5363\n",
      "Epoch [658/1000], Train Loss: 3208.7340, Test Loss: 3352.0795\n",
      "Epoch [659/1000], Train Loss: 5725.0054, Test Loss: 2049.6023\n",
      "Epoch [660/1000], Train Loss: 2105.7252, Test Loss: 2046.5397\n",
      "Epoch [661/1000], Train Loss: 2634.8680, Test Loss: 2010.8567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [662/1000], Train Loss: 2179.7703, Test Loss: 1972.1633\n",
      "Epoch [663/1000], Train Loss: 9859.1290, Test Loss: 4725.9281\n",
      "Epoch [664/1000], Train Loss: 3917.6999, Test Loss: 2012.9284\n",
      "Epoch [665/1000], Train Loss: 2461.4267, Test Loss: 2026.2811\n",
      "Epoch [666/1000], Train Loss: 1790.4564, Test Loss: 1923.3338\n",
      "Epoch [667/1000], Train Loss: 1818.4174, Test Loss: 1960.4486\n",
      "Epoch [668/1000], Train Loss: 1953.2742, Test Loss: 1957.0958\n",
      "Epoch [669/1000], Train Loss: 4033.2639, Test Loss: 2790.8107\n",
      "Epoch [670/1000], Train Loss: 4748.6408, Test Loss: 2508.7585\n",
      "Epoch [671/1000], Train Loss: 2725.2735, Test Loss: 2329.9524\n",
      "Epoch [672/1000], Train Loss: 2249.4617, Test Loss: 2044.7750\n",
      "Epoch [673/1000], Train Loss: 5717.2515, Test Loss: 3300.9378\n",
      "Epoch [674/1000], Train Loss: 3240.6374, Test Loss: 2213.6021\n",
      "Epoch [675/1000], Train Loss: 2810.5700, Test Loss: 2028.2184\n",
      "Epoch [676/1000], Train Loss: 2283.2664, Test Loss: 2242.7776\n",
      "Epoch [677/1000], Train Loss: 8234.2510, Test Loss: 2197.1655\n",
      "Epoch [678/1000], Train Loss: 2250.4194, Test Loss: 2076.3219\n",
      "Epoch [679/1000], Train Loss: 2107.5194, Test Loss: 1987.5640\n",
      "Epoch [680/1000], Train Loss: 1833.5282, Test Loss: 2025.0870\n",
      "Epoch [681/1000], Train Loss: 3449.1953, Test Loss: 2550.2959\n",
      "Epoch [682/1000], Train Loss: 4572.1006, Test Loss: 2165.4496\n",
      "Epoch [683/1000], Train Loss: 2415.3484, Test Loss: 2180.4306\n",
      "Epoch [684/1000], Train Loss: 2194.6972, Test Loss: 1960.6294\n",
      "Epoch [685/1000], Train Loss: 2010.1656, Test Loss: 2015.6323\n",
      "Epoch [686/1000], Train Loss: 6951.2002, Test Loss: 2584.6653\n",
      "Epoch [687/1000], Train Loss: 2763.9777, Test Loss: 2017.6943\n",
      "Epoch [688/1000], Train Loss: 2324.1153, Test Loss: 2127.0969\n",
      "Epoch [689/1000], Train Loss: 2490.5051, Test Loss: 3354.4472\n",
      "Epoch [690/1000], Train Loss: 7324.8701, Test Loss: 2086.8571\n",
      "Epoch [691/1000], Train Loss: 2526.9978, Test Loss: 2112.8380\n",
      "Epoch [692/1000], Train Loss: 2416.1409, Test Loss: 1946.4996\n",
      "Epoch [693/1000], Train Loss: 2442.2220, Test Loss: 2559.1849\n",
      "Epoch [694/1000], Train Loss: 3925.9834, Test Loss: 2107.1349\n",
      "Epoch [695/1000], Train Loss: 2806.0984, Test Loss: 3065.8517\n",
      "Epoch [696/1000], Train Loss: 2793.0290, Test Loss: 2157.2005\n",
      "Epoch [697/1000], Train Loss: 2567.9418, Test Loss: 2184.7302\n",
      "Epoch [698/1000], Train Loss: 9163.9983, Test Loss: 2945.1151\n",
      "Epoch [699/1000], Train Loss: 3003.3636, Test Loss: 2102.5761\n",
      "Epoch [700/1000], Train Loss: 2104.0124, Test Loss: 2017.3481\n",
      "Epoch [701/1000], Train Loss: 1969.0300, Test Loss: 2161.3277\n",
      "Epoch [702/1000], Train Loss: 2121.7769, Test Loss: 2073.7209\n",
      "Epoch [703/1000], Train Loss: 3158.8966, Test Loss: 3300.1673\n",
      "Epoch [704/1000], Train Loss: 10233.7622, Test Loss: 2170.6342\n",
      "Epoch [705/1000], Train Loss: 2113.9470, Test Loss: 1916.8395\n",
      "Epoch [706/1000], Train Loss: 1876.8898, Test Loss: 1931.7725\n",
      "Epoch [707/1000], Train Loss: 1791.1862, Test Loss: 2042.0565\n",
      "Epoch [708/1000], Train Loss: 2202.0224, Test Loss: 2302.6687\n",
      "Epoch [709/1000], Train Loss: 8723.5776, Test Loss: 25568.8867\n",
      "Epoch [710/1000], Train Loss: 5234.1796, Test Loss: 2060.2820\n",
      "Epoch [711/1000], Train Loss: 1905.4376, Test Loss: 2024.6416\n",
      "Epoch [712/1000], Train Loss: 1741.5585, Test Loss: 1898.1163\n",
      "Epoch [713/1000], Train Loss: 1905.1982, Test Loss: 2137.7497\n",
      "Epoch [714/1000], Train Loss: 2182.1227, Test Loss: 1972.8191\n",
      "Epoch [715/1000], Train Loss: 3848.5036, Test Loss: 5402.7222\n",
      "Epoch [716/1000], Train Loss: 11122.2211, Test Loss: 2042.7396\n",
      "Epoch [717/1000], Train Loss: 1733.7664, Test Loss: 1883.1068\n",
      "Epoch [718/1000], Train Loss: 1603.3180, Test Loss: 1912.7826\n",
      "Epoch [719/1000], Train Loss: 1602.5161, Test Loss: 1961.7207\n",
      "Epoch [720/1000], Train Loss: 2029.7372, Test Loss: 1948.4170\n",
      "Epoch [721/1000], Train Loss: 2100.5406, Test Loss: 1980.6528\n",
      "Epoch [722/1000], Train Loss: 2284.9498, Test Loss: 2268.3710\n",
      "Epoch [723/1000], Train Loss: 3404.4575, Test Loss: 2675.6645\n",
      "Epoch [724/1000], Train Loss: 9290.5192, Test Loss: 2026.0295\n",
      "Epoch [725/1000], Train Loss: 1984.2510, Test Loss: 1928.7315\n",
      "Epoch [726/1000], Train Loss: 1642.3341, Test Loss: 1923.4114\n",
      "Epoch [727/1000], Train Loss: 1735.4403, Test Loss: 2048.9614\n",
      "Epoch [728/1000], Train Loss: 2049.1180, Test Loss: 2577.5755\n",
      "Epoch [729/1000], Train Loss: 6832.0446, Test Loss: 2226.0972\n",
      "Epoch [730/1000], Train Loss: 3445.4436, Test Loss: 2171.0622\n",
      "Epoch [731/1000], Train Loss: 1941.3788, Test Loss: 1976.5991\n",
      "Epoch [732/1000], Train Loss: 2878.2283, Test Loss: 3156.1760\n",
      "Epoch [733/1000], Train Loss: 4447.8288, Test Loss: 3151.6701\n",
      "Epoch [734/1000], Train Loss: 4916.1459, Test Loss: 2097.0175\n",
      "Epoch [735/1000], Train Loss: 2001.9602, Test Loss: 1959.1694\n",
      "Epoch [736/1000], Train Loss: 5063.1041, Test Loss: 2069.9919\n",
      "Epoch [737/1000], Train Loss: 2409.2481, Test Loss: 1961.5557\n",
      "Epoch [738/1000], Train Loss: 2264.9377, Test Loss: 2316.0188\n",
      "Epoch [739/1000], Train Loss: 2822.7760, Test Loss: 2224.2476\n",
      "Epoch [740/1000], Train Loss: 15429.0558, Test Loss: 2094.5849\n",
      "Epoch [741/1000], Train Loss: 1803.0047, Test Loss: 1913.9770\n",
      "Epoch [742/1000], Train Loss: 1566.2120, Test Loss: 1870.9587\n",
      "Epoch [743/1000], Train Loss: 1812.4956, Test Loss: 1954.1166\n",
      "Epoch [744/1000], Train Loss: 1654.0431, Test Loss: 2007.7884\n",
      "Epoch [745/1000], Train Loss: 1707.2044, Test Loss: 1953.7485\n",
      "Epoch [746/1000], Train Loss: 2241.1107, Test Loss: 2052.4406\n",
      "Epoch [747/1000], Train Loss: 2467.6108, Test Loss: 2097.0865\n",
      "Epoch [748/1000], Train Loss: 3429.5391, Test Loss: 2250.8905\n",
      "Epoch [749/1000], Train Loss: 6234.8000, Test Loss: 2030.7431\n",
      "Epoch [750/1000], Train Loss: 2085.5141, Test Loss: 2101.7773\n",
      "Epoch [751/1000], Train Loss: 2046.9510, Test Loss: 1999.8722\n",
      "Epoch [752/1000], Train Loss: 6046.4323, Test Loss: 2181.1579\n",
      "Epoch [753/1000], Train Loss: 2423.4448, Test Loss: 2073.4643\n",
      "Epoch [754/1000], Train Loss: 2013.3067, Test Loss: 1975.1009\n",
      "Epoch [755/1000], Train Loss: 67087.1224, Test Loss: 2492.5261\n",
      "Epoch [756/1000], Train Loss: 3408.3347, Test Loss: 2084.7968\n",
      "Epoch [757/1000], Train Loss: 2323.4455, Test Loss: 1949.7290\n",
      "Epoch [758/1000], Train Loss: 2104.0926, Test Loss: 1965.9361\n",
      "Epoch [759/1000], Train Loss: 2188.2994, Test Loss: 2066.9744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m save_path \u001b[38;5;241m=\u001b[39m ModelName \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Params.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m train_losses, test_losses, is_model_trained \u001b[38;5;241m=\u001b[39m train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Finish timing cell run time\u001b[39;00m\n\u001b[1;32m     25\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[14], line 63\u001b[0m, in \u001b[0;36mtrain_or_load_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pretrained model found. Training from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#optimizer = optim.Adam(model.parameters())  \u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n\u001b[1;32m     64\u001b[0m is_model_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set flag to True after training\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Save losses per epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mtrain_and_save_best_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 18\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load('Dataset44_Val_Spec.npy')\n",
    "concVal = np.load('Dataset44_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "ValSpectra = np.load(\"Dataset44_RepresentativeExamples_Spectra.npy\")\n",
    "ValConc = np.load(\"Dataset44_RepresentativeExamples_Concentrations.npy\")\n",
    "ValSpecNames = np.load(\"Dataset44_RepresentativeExamples_VariableNames.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ValConc = torch.tensor(ValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=23552, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ValConc[i]\n",
    "    Prediction = model_aq(ValSpectra[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(21):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.73  -  AllAq1\n",
      "1.74  -  AllAq5\n",
      "0.34  -  AllAq25\n",
      "2.64  -  AllAq50\n",
      "0.71  -  ThreeAddedSinglets\n",
      "10.65  -  ThirtyAddedSinglets\n",
      "79.03  -  ShiftedSpec\n",
      "24.71  -  SineBase\n",
      "221.44  -  HighDynamicRange\n",
      "inf  -  HalfZeros\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - \",ValSpecNames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
