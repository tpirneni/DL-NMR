{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer Encoder on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"Transformer_44met_TrainingAndValidation_500bin_NoDropout_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('Dataset44_Spec.npy')\n",
    "conc1 = np.load('Dataset44_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "#spectraVal = np.load('Dataset44_Val_Spec.npy')\n",
    "#concVal = np.load('Dataset44_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "#ValSpectra = np.load(\"Dataset44_RepresentativeExamples_Spectra.npy\")\n",
    "#ValConc = np.load(\"Dataset44_RepresentativeExamples_Concentrations.npy\")\n",
    "#ValSpecNames = np.load(\"Dataset44_RepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 128, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.Linear(23552*2, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Binning\n",
    "        batch_size, seq_length = x.size()\n",
    "        num_bins = seq_length // self.input_dim\n",
    "        x = x.view(batch_size, num_bins, self.input_dim)  # (batch_size, num_bins, input_dim)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = x.permute(1, 0, 2)  # (num_bins, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)  # (num_bins, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Reconstruct original sequence\n",
    "        x = x.reshape(batch_size, num_bins * d_model)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder(x)  # (batch_size, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "input_dim = 500  # Size of each bin\n",
    "d_model = 512     # Embedding dimension\n",
    "nhead = 1         # Number of attention heads\n",
    "num_encoder_layers = 1  # Number of transformer encoder layers\n",
    "dim_feedforward = 2048  # Feedforward dimension\n",
    "dropout = 0.0     # Dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "       \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/1000], Train Loss: 4475241.1895, Test Loss: 920553.0190\n",
      "Epoch [2/1000], Train Loss: 2477672.8613, Test Loss: 421194.0432\n",
      "Epoch [3/1000], Train Loss: 1357106.8677, Test Loss: 264067.4453\n",
      "Epoch [4/1000], Train Loss: 949335.8193, Test Loss: 188826.7706\n",
      "Epoch [5/1000], Train Loss: 578456.8376, Test Loss: 119127.1824\n",
      "Epoch [6/1000], Train Loss: 406697.2683, Test Loss: 88391.9847\n",
      "Epoch [7/1000], Train Loss: 300763.4158, Test Loss: 66964.2117\n",
      "Epoch [8/1000], Train Loss: 221595.5941, Test Loss: 47469.9222\n",
      "Epoch [9/1000], Train Loss: 164437.1197, Test Loss: 37285.5182\n",
      "Epoch [10/1000], Train Loss: 132793.2350, Test Loss: 32241.0223\n",
      "Epoch [11/1000], Train Loss: 109723.3702, Test Loss: 25284.0925\n",
      "Epoch [12/1000], Train Loss: 94049.8086, Test Loss: 22635.9111\n",
      "Epoch [13/1000], Train Loss: 82865.8765, Test Loss: 19827.8382\n",
      "Epoch [14/1000], Train Loss: 72902.1622, Test Loss: 18140.5408\n",
      "Epoch [15/1000], Train Loss: 65196.7788, Test Loss: 16254.8895\n",
      "Epoch [16/1000], Train Loss: 59676.0089, Test Loss: 14832.7189\n",
      "Epoch [17/1000], Train Loss: 54190.1841, Test Loss: 13893.4640\n",
      "Epoch [18/1000], Train Loss: 50128.4563, Test Loss: 13010.9729\n",
      "Epoch [19/1000], Train Loss: 45622.8136, Test Loss: 11420.0949\n",
      "Epoch [20/1000], Train Loss: 43284.7157, Test Loss: 11138.1168\n",
      "Epoch [21/1000], Train Loss: 40359.9300, Test Loss: 10859.0845\n",
      "Epoch [22/1000], Train Loss: 38899.1882, Test Loss: 9872.9368\n",
      "Epoch [23/1000], Train Loss: 35862.0556, Test Loss: 9636.7567\n",
      "Epoch [24/1000], Train Loss: 33727.8113, Test Loss: 9269.5161\n",
      "Epoch [25/1000], Train Loss: 33431.8269, Test Loss: 9627.1910\n",
      "Epoch [26/1000], Train Loss: 31313.6660, Test Loss: 8437.9020\n",
      "Epoch [27/1000], Train Loss: 29709.5840, Test Loss: 8361.0693\n",
      "Epoch [28/1000], Train Loss: 27798.0625, Test Loss: 7917.1606\n",
      "Epoch [29/1000], Train Loss: 26725.1440, Test Loss: 7105.0717\n",
      "Epoch [30/1000], Train Loss: 25575.7231, Test Loss: 6944.1902\n",
      "Epoch [31/1000], Train Loss: 24929.8805, Test Loss: 7381.4222\n",
      "Epoch [32/1000], Train Loss: 23840.9791, Test Loss: 7043.5887\n",
      "Epoch [33/1000], Train Loss: 23419.8684, Test Loss: 6834.2653\n",
      "Epoch [34/1000], Train Loss: 22026.4801, Test Loss: 6595.1210\n",
      "Epoch [35/1000], Train Loss: 21411.2566, Test Loss: 6504.2313\n",
      "Epoch [36/1000], Train Loss: 20917.2929, Test Loss: 5484.3706\n",
      "Epoch [37/1000], Train Loss: 20388.5564, Test Loss: 5772.6588\n",
      "Epoch [38/1000], Train Loss: 19461.5677, Test Loss: 5487.3274\n",
      "Epoch [39/1000], Train Loss: 22702.0267, Test Loss: 8571.3690\n",
      "Epoch [40/1000], Train Loss: 19247.1937, Test Loss: 5103.8197\n",
      "Epoch [41/1000], Train Loss: 17255.0348, Test Loss: 4979.7045\n",
      "Epoch [42/1000], Train Loss: 16556.6663, Test Loss: 4993.3262\n",
      "Epoch [43/1000], Train Loss: 16012.3252, Test Loss: 4831.0917\n",
      "Epoch [44/1000], Train Loss: 16369.0127, Test Loss: 4590.4228\n",
      "Epoch [45/1000], Train Loss: 15666.8689, Test Loss: 4596.2277\n",
      "Epoch [46/1000], Train Loss: 14925.4418, Test Loss: 4311.0007\n",
      "Epoch [47/1000], Train Loss: 15698.1642, Test Loss: 4347.8644\n",
      "Epoch [48/1000], Train Loss: 14589.1662, Test Loss: 5222.5390\n",
      "Epoch [49/1000], Train Loss: 14161.5259, Test Loss: 4131.6447\n",
      "Epoch [50/1000], Train Loss: 4384817.5937, Test Loss: 1574069.0684\n",
      "Epoch [51/1000], Train Loss: 4599907.5625, Test Loss: 1109464.6025\n",
      "Epoch [52/1000], Train Loss: 4390918.9688, Test Loss: 1088068.0693\n",
      "Epoch [53/1000], Train Loss: 4082730.3652, Test Loss: 1004594.4858\n",
      "Epoch [54/1000], Train Loss: 3949173.0801, Test Loss: 1001135.3311\n",
      "Epoch [55/1000], Train Loss: 3928382.3633, Test Loss: 992226.4834\n",
      "Epoch [56/1000], Train Loss: 3940556.3730, Test Loss: 982276.7715\n",
      "Epoch [57/1000], Train Loss: 3929114.8555, Test Loss: 982332.5249\n",
      "Epoch [58/1000], Train Loss: 3934764.9180, Test Loss: 980698.0400\n",
      "Epoch [59/1000], Train Loss: 3928161.9453, Test Loss: 998476.8359\n",
      "Epoch [60/1000], Train Loss: 3915608.1641, Test Loss: 979292.9170\n",
      "Epoch [61/1000], Train Loss: 3889279.1426, Test Loss: 955302.8486\n",
      "Epoch [62/1000], Train Loss: 3127602.4980, Test Loss: 535806.3926\n",
      "Epoch [63/1000], Train Loss: 1898548.1611, Test Loss: 438963.0078\n",
      "Epoch [64/1000], Train Loss: 1654232.1943, Test Loss: 394188.0559\n",
      "Epoch [65/1000], Train Loss: 1485437.8838, Test Loss: 351523.2002\n",
      "Epoch [66/1000], Train Loss: 1348599.9092, Test Loss: 327937.3235\n",
      "Epoch [67/1000], Train Loss: 1278301.9980, Test Loss: 320260.2983\n",
      "Epoch [68/1000], Train Loss: 1248639.3760, Test Loss: 315260.2310\n",
      "Epoch [69/1000], Train Loss: 1199571.3955, Test Loss: 300121.8098\n",
      "Epoch [70/1000], Train Loss: 1155270.0518, Test Loss: 301719.8655\n",
      "Epoch [71/1000], Train Loss: 1131370.6836, Test Loss: 278738.8381\n",
      "Epoch [72/1000], Train Loss: 1095789.7319, Test Loss: 283073.4336\n",
      "Epoch [73/1000], Train Loss: 1059730.0835, Test Loss: 269741.8420\n",
      "Epoch [74/1000], Train Loss: 1036248.7085, Test Loss: 278177.8796\n",
      "Epoch [75/1000], Train Loss: 1004763.5205, Test Loss: 271429.0444\n",
      "Epoch [76/1000], Train Loss: 991431.1357, Test Loss: 245456.4020\n",
      "Epoch [77/1000], Train Loss: 956834.8257, Test Loss: 238249.1907\n",
      "Epoch [78/1000], Train Loss: 944908.1831, Test Loss: 234118.1786\n",
      "Epoch [79/1000], Train Loss: 937524.6177, Test Loss: 248980.6816\n",
      "Epoch [80/1000], Train Loss: 920663.1548, Test Loss: 228077.3511\n",
      "Epoch [81/1000], Train Loss: 889325.7944, Test Loss: 220685.3868\n",
      "Epoch [82/1000], Train Loss: 882062.5874, Test Loss: 213026.4001\n",
      "Epoch [83/1000], Train Loss: 840422.6479, Test Loss: 219182.7192\n",
      "Epoch [84/1000], Train Loss: 829864.5884, Test Loss: 197163.1700\n",
      "Epoch [85/1000], Train Loss: 710053.2734, Test Loss: 170517.1353\n",
      "Epoch [86/1000], Train Loss: 653575.0642, Test Loss: 135197.0811\n",
      "Epoch [87/1000], Train Loss: 491675.0723, Test Loss: 107781.8536\n",
      "Epoch [88/1000], Train Loss: 430490.1921, Test Loss: 112351.5521\n",
      "Epoch [89/1000], Train Loss: 1737390.4905, Test Loss: 301124.6943\n",
      "Epoch [90/1000], Train Loss: 1104534.7788, Test Loss: 255645.6761\n",
      "Epoch [91/1000], Train Loss: 942121.9268, Test Loss: 218665.2716\n",
      "Epoch [92/1000], Train Loss: 849409.1094, Test Loss: 312319.4348\n",
      "Epoch [93/1000], Train Loss: 791159.6802, Test Loss: 172770.4468\n",
      "Epoch [94/1000], Train Loss: 987819.6914, Test Loss: 219523.1691\n",
      "Epoch [95/1000], Train Loss: 630922.5283, Test Loss: 128527.6757\n",
      "Epoch [96/1000], Train Loss: 458524.4038, Test Loss: 126276.4041\n",
      "Epoch [97/1000], Train Loss: 411609.1135, Test Loss: 94552.3146\n",
      "Epoch [98/1000], Train Loss: 367030.2588, Test Loss: 89117.5195\n",
      "Epoch [99/1000], Train Loss: 331223.1362, Test Loss: 79552.7003\n",
      "Epoch [100/1000], Train Loss: 280974.9532, Test Loss: 65539.4835\n",
      "Epoch [101/1000], Train Loss: 244590.1324, Test Loss: 58786.5681\n",
      "Epoch [102/1000], Train Loss: 210244.4709, Test Loss: 50936.5613\n",
      "Epoch [103/1000], Train Loss: 187488.0154, Test Loss: 48110.1566\n",
      "Epoch [104/1000], Train Loss: 168075.2023, Test Loss: 39941.5202\n",
      "Epoch [105/1000], Train Loss: 147552.3584, Test Loss: 36447.9961\n",
      "Epoch [106/1000], Train Loss: 131426.1376, Test Loss: 33637.7205\n",
      "Epoch [107/1000], Train Loss: 116137.6568, Test Loss: 29635.4802\n",
      "Epoch [108/1000], Train Loss: 103778.1463, Test Loss: 25899.5682\n",
      "Epoch [109/1000], Train Loss: 95519.0842, Test Loss: 23714.0946\n",
      "Epoch [110/1000], Train Loss: 85726.1187, Test Loss: 22142.0704\n",
      "Epoch [111/1000], Train Loss: 80192.2710, Test Loss: 20003.0540\n",
      "Epoch [112/1000], Train Loss: 74097.2723, Test Loss: 18717.6921\n",
      "Epoch [113/1000], Train Loss: 67496.3920, Test Loss: 17886.1540\n",
      "Epoch [114/1000], Train Loss: 62101.8298, Test Loss: 16173.9335\n",
      "Epoch [115/1000], Train Loss: 58588.5458, Test Loss: 15214.8567\n",
      "Epoch [116/1000], Train Loss: 54739.4996, Test Loss: 14381.7994\n",
      "Epoch [117/1000], Train Loss: 50654.7660, Test Loss: 12877.5763\n",
      "Epoch [118/1000], Train Loss: 48182.5152, Test Loss: 12774.7573\n",
      "Epoch [119/1000], Train Loss: 44524.0404, Test Loss: 13131.4181\n",
      "Epoch [120/1000], Train Loss: 42989.2099, Test Loss: 11389.0187\n",
      "Epoch [121/1000], Train Loss: 40663.0633, Test Loss: 10912.7429\n",
      "Epoch [122/1000], Train Loss: 38477.8153, Test Loss: 10616.9270\n",
      "Epoch [123/1000], Train Loss: 36414.0300, Test Loss: 10219.6485\n",
      "Epoch [124/1000], Train Loss: 35755.3097, Test Loss: 9422.3554\n",
      "Epoch [125/1000], Train Loss: 33998.5133, Test Loss: 10159.1894\n",
      "Epoch [126/1000], Train Loss: 32492.3438, Test Loss: 8568.2057\n",
      "Epoch [127/1000], Train Loss: 31686.7719, Test Loss: 9831.1159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [128/1000], Train Loss: 29750.0097, Test Loss: 8283.5274\n",
      "Epoch [129/1000], Train Loss: 28359.0295, Test Loss: 9233.6477\n",
      "Epoch [130/1000], Train Loss: 28847.5586, Test Loss: 8740.2865\n",
      "Epoch [131/1000], Train Loss: 27253.2663, Test Loss: 7494.4864\n",
      "Epoch [132/1000], Train Loss: 26649.4653, Test Loss: 7439.0194\n",
      "Epoch [133/1000], Train Loss: 24295.2154, Test Loss: 6703.6724\n",
      "Epoch [134/1000], Train Loss: 23609.2332, Test Loss: 6519.1880\n",
      "Epoch [135/1000], Train Loss: 22386.7545, Test Loss: 6306.6674\n",
      "Epoch [136/1000], Train Loss: 22233.1600, Test Loss: 6175.5463\n",
      "Epoch [137/1000], Train Loss: 21045.7205, Test Loss: 5958.9722\n",
      "Epoch [138/1000], Train Loss: 20119.4028, Test Loss: 5909.4367\n",
      "Epoch [139/1000], Train Loss: 19434.2959, Test Loss: 5842.3461\n",
      "Epoch [140/1000], Train Loss: 19208.5080, Test Loss: 6838.7620\n",
      "Epoch [141/1000], Train Loss: 18266.7739, Test Loss: 5154.9584\n",
      "Epoch [142/1000], Train Loss: 18134.2704, Test Loss: 5141.7600\n",
      "Epoch [143/1000], Train Loss: 17441.5616, Test Loss: 5373.6404\n",
      "Epoch [144/1000], Train Loss: 15971.6757, Test Loss: 4723.9129\n",
      "Epoch [145/1000], Train Loss: 15752.8576, Test Loss: 4994.0914\n",
      "Epoch [146/1000], Train Loss: 15544.9172, Test Loss: 4733.0807\n",
      "Epoch [147/1000], Train Loss: 15750.7565, Test Loss: 4607.2560\n",
      "Epoch [148/1000], Train Loss: 14941.1531, Test Loss: 4869.4170\n",
      "Epoch [149/1000], Train Loss: 14389.2231, Test Loss: 4624.5509\n",
      "Epoch [150/1000], Train Loss: 13931.7838, Test Loss: 4630.5933\n",
      "Epoch [151/1000], Train Loss: 14254.6086, Test Loss: 4364.0824\n",
      "Epoch [152/1000], Train Loss: 13059.7965, Test Loss: 4774.2262\n",
      "Epoch [153/1000], Train Loss: 13105.5002, Test Loss: 4127.3021\n",
      "Epoch [154/1000], Train Loss: 12994.7775, Test Loss: 4111.8720\n",
      "Epoch [155/1000], Train Loss: 12874.3601, Test Loss: 4382.3501\n",
      "Epoch [156/1000], Train Loss: 12650.6134, Test Loss: 3835.9310\n",
      "Epoch [157/1000], Train Loss: 11576.6261, Test Loss: 3810.8902\n",
      "Epoch [158/1000], Train Loss: 12129.3581, Test Loss: 3650.4574\n",
      "Epoch [159/1000], Train Loss: 11334.8526, Test Loss: 3751.2114\n",
      "Epoch [160/1000], Train Loss: 11330.2142, Test Loss: 3825.2904\n",
      "Epoch [161/1000], Train Loss: 11877.1812, Test Loss: 3499.7461\n",
      "Epoch [162/1000], Train Loss: 10799.4005, Test Loss: 3956.3874\n",
      "Epoch [163/1000], Train Loss: 11043.8963, Test Loss: 3526.8058\n",
      "Epoch [164/1000], Train Loss: 10676.6798, Test Loss: 4018.9519\n",
      "Epoch [165/1000], Train Loss: 10820.5433, Test Loss: 3428.9985\n",
      "Epoch [166/1000], Train Loss: 10482.1825, Test Loss: 3428.9867\n",
      "Epoch [167/1000], Train Loss: 10152.4299, Test Loss: 3392.5375\n",
      "Epoch [168/1000], Train Loss: 10312.0776, Test Loss: 3279.8880\n",
      "Epoch [169/1000], Train Loss: 9800.4971, Test Loss: 3261.4152\n",
      "Epoch [170/1000], Train Loss: 10172.1277, Test Loss: 3080.3137\n",
      "Epoch [171/1000], Train Loss: 9172.5658, Test Loss: 3394.7925\n",
      "Epoch [172/1000], Train Loss: 9692.8334, Test Loss: 3591.3781\n",
      "Epoch [173/1000], Train Loss: 9096.9284, Test Loss: 3021.5982\n",
      "Epoch [174/1000], Train Loss: 5349279.1234, Test Loss: 1122450.9170\n",
      "Epoch [175/1000], Train Loss: 4111005.3008, Test Loss: 985904.5391\n",
      "Epoch [176/1000], Train Loss: 3906556.5566, Test Loss: 969065.7515\n",
      "Epoch [177/1000], Train Loss: 3624024.0605, Test Loss: 759073.9277\n",
      "Epoch [178/1000], Train Loss: 2689300.9531, Test Loss: 625278.8667\n",
      "Epoch [179/1000], Train Loss: 2429314.3008, Test Loss: 596695.7549\n",
      "Epoch [180/1000], Train Loss: 2101455.5166, Test Loss: 494917.0146\n",
      "Epoch [181/1000], Train Loss: 1849673.3379, Test Loss: 438934.7058\n",
      "Epoch [182/1000], Train Loss: 1678629.6982, Test Loss: 404862.7627\n",
      "Epoch [183/1000], Train Loss: 1554331.2520, Test Loss: 379175.1741\n",
      "Epoch [184/1000], Train Loss: 1482577.7432, Test Loss: 373992.9126\n",
      "Epoch [185/1000], Train Loss: 1419208.4492, Test Loss: 363728.2334\n",
      "Epoch [186/1000], Train Loss: 1358507.2842, Test Loss: 336239.9170\n",
      "Epoch [187/1000], Train Loss: 1332168.3164, Test Loss: 323163.5327\n",
      "Epoch [188/1000], Train Loss: 1266253.3145, Test Loss: 311682.6184\n",
      "Epoch [189/1000], Train Loss: 1208685.7383, Test Loss: 298504.6245\n",
      "Epoch [190/1000], Train Loss: 1178268.8291, Test Loss: 287118.8076\n",
      "Epoch [191/1000], Train Loss: 1108819.7437, Test Loss: 270387.7729\n",
      "Epoch [192/1000], Train Loss: 1024496.9707, Test Loss: 244036.4161\n",
      "Epoch [193/1000], Train Loss: 920280.7739, Test Loss: 214664.3807\n",
      "Epoch [194/1000], Train Loss: 814317.2544, Test Loss: 185009.0736\n",
      "Epoch [195/1000], Train Loss: 714990.3462, Test Loss: 163247.2345\n",
      "Epoch [196/1000], Train Loss: 615446.7290, Test Loss: 153700.5002\n",
      "Epoch [197/1000], Train Loss: 554370.1960, Test Loss: 118610.2601\n",
      "Epoch [198/1000], Train Loss: 1080275.5164, Test Loss: 465750.5984\n",
      "Epoch [199/1000], Train Loss: 1391075.8047, Test Loss: 293307.7532\n",
      "Epoch [200/1000], Train Loss: 977512.4731, Test Loss: 203716.5284\n",
      "Epoch [201/1000], Train Loss: 676440.5420, Test Loss: 140917.0007\n",
      "Epoch [202/1000], Train Loss: 467060.7412, Test Loss: 97608.3768\n",
      "Epoch [203/1000], Train Loss: 327764.0807, Test Loss: 64887.1703\n",
      "Epoch [204/1000], Train Loss: 234316.3547, Test Loss: 46512.0964\n",
      "Epoch [205/1000], Train Loss: 155991.2575, Test Loss: 33852.2561\n",
      "Epoch [206/1000], Train Loss: 123172.9443, Test Loss: 30587.3043\n",
      "Epoch [207/1000], Train Loss: 104139.0669, Test Loss: 24369.6914\n",
      "Epoch [208/1000], Train Loss: 90327.5348, Test Loss: 24096.7689\n",
      "Epoch [209/1000], Train Loss: 81434.3585, Test Loss: 20394.1078\n",
      "Epoch [210/1000], Train Loss: 73793.2042, Test Loss: 19227.1010\n",
      "Epoch [211/1000], Train Loss: 68737.3759, Test Loss: 16933.1316\n",
      "Epoch [212/1000], Train Loss: 62948.9901, Test Loss: 16101.4025\n",
      "Epoch [213/1000], Train Loss: 59221.4944, Test Loss: 14972.9567\n",
      "Epoch [214/1000], Train Loss: 54453.8571, Test Loss: 13792.2480\n",
      "Epoch [215/1000], Train Loss: 49803.5229, Test Loss: 12573.5126\n",
      "Epoch [216/1000], Train Loss: 49022.0736, Test Loss: 12743.0265\n",
      "Epoch [217/1000], Train Loss: 45921.7221, Test Loss: 12615.0884\n",
      "Epoch [218/1000], Train Loss: 41955.9379, Test Loss: 10748.1939\n",
      "Epoch [219/1000], Train Loss: 39997.6946, Test Loss: 10653.4036\n",
      "Epoch [220/1000], Train Loss: 37926.1765, Test Loss: 9760.4454\n",
      "Epoch [221/1000], Train Loss: 34630.7141, Test Loss: 9685.3663\n",
      "Epoch [222/1000], Train Loss: 33381.3614, Test Loss: 9264.4068\n",
      "Epoch [223/1000], Train Loss: 31817.8210, Test Loss: 8192.0076\n",
      "Epoch [224/1000], Train Loss: 31383.9892, Test Loss: 9067.3507\n",
      "Epoch [225/1000], Train Loss: 38941.1154, Test Loss: 8366.5721\n",
      "Epoch [226/1000], Train Loss: 29176.4825, Test Loss: 7940.1190\n",
      "Epoch [227/1000], Train Loss: 33925.8056, Test Loss: 7658.9746\n",
      "Epoch [228/1000], Train Loss: 26033.7959, Test Loss: 7070.0763\n",
      "Epoch [229/1000], Train Loss: 25049.3062, Test Loss: 7147.3695\n",
      "Epoch [230/1000], Train Loss: 24081.9683, Test Loss: 6703.4786\n",
      "Epoch [231/1000], Train Loss: 22524.8872, Test Loss: 6015.9648\n",
      "Epoch [232/1000], Train Loss: 21664.0331, Test Loss: 5928.0869\n",
      "Epoch [233/1000], Train Loss: 20164.3068, Test Loss: 5567.3378\n",
      "Epoch [234/1000], Train Loss: 19136.3916, Test Loss: 5612.0054\n",
      "Epoch [235/1000], Train Loss: 18867.9239, Test Loss: 5396.1286\n",
      "Epoch [236/1000], Train Loss: 18519.2099, Test Loss: 5814.4307\n",
      "Epoch [237/1000], Train Loss: 18012.6355, Test Loss: 5164.4471\n",
      "Epoch [238/1000], Train Loss: 17316.4162, Test Loss: 5390.5222\n",
      "Epoch [239/1000], Train Loss: 16956.4661, Test Loss: 5141.6060\n",
      "Epoch [240/1000], Train Loss: 16148.6618, Test Loss: 4952.2510\n",
      "Epoch [241/1000], Train Loss: 15593.0694, Test Loss: 4882.2610\n",
      "Epoch [242/1000], Train Loss: 15659.0808, Test Loss: 4747.1591\n",
      "Epoch [243/1000], Train Loss: 14988.3460, Test Loss: 4710.4288\n",
      "Epoch [244/1000], Train Loss: 15030.8507, Test Loss: 4832.6186\n",
      "Epoch [245/1000], Train Loss: 14975.5442, Test Loss: 4578.9721\n",
      "Epoch [246/1000], Train Loss: 15418.9236, Test Loss: 4640.3336\n",
      "Epoch [247/1000], Train Loss: 13631.9428, Test Loss: 4324.4626\n",
      "Epoch [248/1000], Train Loss: 13438.2190, Test Loss: 4252.4746\n",
      "Epoch [249/1000], Train Loss: 13451.5705, Test Loss: 4184.9957\n",
      "Epoch [250/1000], Train Loss: 13191.8992, Test Loss: 4047.4477\n",
      "Epoch [251/1000], Train Loss: 12875.4221, Test Loss: 4096.8749\n",
      "Epoch [252/1000], Train Loss: 12922.6800, Test Loss: 3923.2652\n",
      "Epoch [253/1000], Train Loss: 12384.7034, Test Loss: 3855.9400\n",
      "Epoch [254/1000], Train Loss: 11898.3167, Test Loss: 3713.2607\n",
      "Epoch [255/1000], Train Loss: 11724.5621, Test Loss: 3709.0636\n",
      "Epoch [256/1000], Train Loss: 14354.4136, Test Loss: 4116.9681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [257/1000], Train Loss: 13005.3386, Test Loss: 4049.2372\n",
      "Epoch [258/1000], Train Loss: 11554.3481, Test Loss: 3710.0396\n",
      "Epoch [259/1000], Train Loss: 11341.9497, Test Loss: 3573.3506\n",
      "Epoch [260/1000], Train Loss: 10704.0636, Test Loss: 3388.6580\n",
      "Epoch [261/1000], Train Loss: 10294.3476, Test Loss: 3464.4209\n",
      "Epoch [262/1000], Train Loss: 10858.3039, Test Loss: 3682.2910\n",
      "Epoch [263/1000], Train Loss: 10701.6629, Test Loss: 3457.8257\n",
      "Epoch [264/1000], Train Loss: 10026.4396, Test Loss: 3320.3838\n",
      "Epoch [265/1000], Train Loss: 9761.4997, Test Loss: 3352.3840\n",
      "Epoch [266/1000], Train Loss: 9949.5584, Test Loss: 3295.6630\n",
      "Epoch [267/1000], Train Loss: 3430932.8702, Test Loss: 905000.6929\n",
      "Epoch [268/1000], Train Loss: 1914491.0906, Test Loss: 86376.8217\n",
      "Epoch [269/1000], Train Loss: 200799.2020, Test Loss: 27895.7867\n",
      "Epoch [270/1000], Train Loss: 73877.2103, Test Loss: 12186.2963\n",
      "Epoch [271/1000], Train Loss: 39160.4335, Test Loss: 8638.7984\n",
      "Epoch [272/1000], Train Loss: 29028.7235, Test Loss: 7183.1797\n",
      "Epoch [273/1000], Train Loss: 25249.4987, Test Loss: 6321.1454\n",
      "Epoch [274/1000], Train Loss: 21631.4624, Test Loss: 6124.0741\n",
      "Epoch [275/1000], Train Loss: 20209.9542, Test Loss: 5448.2106\n",
      "Epoch [276/1000], Train Loss: 18534.8505, Test Loss: 4921.6761\n",
      "Epoch [277/1000], Train Loss: 16670.5630, Test Loss: 4891.9447\n",
      "Epoch [278/1000], Train Loss: 16303.1862, Test Loss: 5876.4176\n",
      "Epoch [279/1000], Train Loss: 16201.4124, Test Loss: 4245.8453\n",
      "Epoch [280/1000], Train Loss: 14754.9439, Test Loss: 4534.9690\n",
      "Epoch [281/1000], Train Loss: 14727.7423, Test Loss: 4196.6434\n",
      "Epoch [282/1000], Train Loss: 14722.1623, Test Loss: 4367.1964\n",
      "Epoch [283/1000], Train Loss: 13514.0176, Test Loss: 4420.5925\n",
      "Epoch [284/1000], Train Loss: 13523.9573, Test Loss: 4592.6305\n",
      "Epoch [285/1000], Train Loss: 14120.1310, Test Loss: 3900.2235\n",
      "Epoch [286/1000], Train Loss: 14142.7687, Test Loss: 3961.6847\n",
      "Epoch [287/1000], Train Loss: 12982.2257, Test Loss: 3862.7113\n",
      "Epoch [288/1000], Train Loss: 12522.6403, Test Loss: 3851.8008\n",
      "Epoch [289/1000], Train Loss: 12555.4423, Test Loss: 4109.7705\n",
      "Epoch [290/1000], Train Loss: 13683.5343, Test Loss: 3849.4066\n",
      "Epoch [291/1000], Train Loss: 11712.3876, Test Loss: 3676.9919\n",
      "Epoch [292/1000], Train Loss: 11881.4446, Test Loss: 10874.8361\n",
      "Epoch [293/1000], Train Loss: 1061334.9083, Test Loss: 12227.5622\n",
      "Epoch [294/1000], Train Loss: 26456.7579, Test Loss: 5474.1542\n",
      "Epoch [295/1000], Train Loss: 16679.6287, Test Loss: 4483.0442\n",
      "Epoch [296/1000], Train Loss: 14493.2119, Test Loss: 4142.0488\n",
      "Epoch [297/1000], Train Loss: 13533.2699, Test Loss: 4065.6946\n",
      "Epoch [298/1000], Train Loss: 12797.0118, Test Loss: 3912.8620\n",
      "Epoch [299/1000], Train Loss: 12242.0725, Test Loss: 3765.7614\n",
      "Epoch [300/1000], Train Loss: 11877.5985, Test Loss: 3775.0904\n",
      "Epoch [301/1000], Train Loss: 11458.0830, Test Loss: 3770.8236\n",
      "Epoch [302/1000], Train Loss: 11243.7113, Test Loss: 3601.9577\n",
      "Epoch [303/1000], Train Loss: 11075.8891, Test Loss: 3792.4032\n",
      "Epoch [304/1000], Train Loss: 10952.3125, Test Loss: 3540.6700\n",
      "Epoch [305/1000], Train Loss: 10732.8041, Test Loss: 3522.3677\n",
      "Epoch [306/1000], Train Loss: 10521.7112, Test Loss: 3457.9876\n",
      "Epoch [307/1000], Train Loss: 10550.4251, Test Loss: 3491.6248\n",
      "Epoch [308/1000], Train Loss: 10347.4774, Test Loss: 3418.0066\n",
      "Epoch [309/1000], Train Loss: 10306.1350, Test Loss: 3476.8888\n",
      "Epoch [310/1000], Train Loss: 10181.8140, Test Loss: 3497.2863\n",
      "Epoch [311/1000], Train Loss: 10060.1778, Test Loss: 3452.6249\n",
      "Epoch [312/1000], Train Loss: 10046.5375, Test Loss: 3331.1099\n",
      "Epoch [313/1000], Train Loss: 10081.2401, Test Loss: 3421.7746\n",
      "Epoch [314/1000], Train Loss: 10017.7439, Test Loss: 3602.0532\n",
      "Epoch [315/1000], Train Loss: 9797.4016, Test Loss: 3406.2400\n",
      "Epoch [316/1000], Train Loss: 9736.7393, Test Loss: 3318.8587\n",
      "Epoch [317/1000], Train Loss: 9391.6865, Test Loss: 3097.4576\n",
      "Epoch [318/1000], Train Loss: 9464.5564, Test Loss: 3100.1790\n",
      "Epoch [319/1000], Train Loss: 9329.3201, Test Loss: 3282.1848\n",
      "Epoch [320/1000], Train Loss: 9348.0361, Test Loss: 3022.2349\n",
      "Epoch [321/1000], Train Loss: 9542.0987, Test Loss: 3223.7976\n",
      "Epoch [322/1000], Train Loss: 9090.6268, Test Loss: 3075.3719\n",
      "Epoch [323/1000], Train Loss: 8903.4986, Test Loss: 3007.6331\n",
      "Epoch [324/1000], Train Loss: 8990.8523, Test Loss: 3335.8781\n",
      "Epoch [325/1000], Train Loss: 8875.7349, Test Loss: 2918.3328\n",
      "Epoch [326/1000], Train Loss: 8685.1288, Test Loss: 2937.4550\n",
      "Epoch [327/1000], Train Loss: 8767.1365, Test Loss: 2945.5347\n",
      "Epoch [328/1000], Train Loss: 8926.6637, Test Loss: 2989.8011\n",
      "Epoch [329/1000], Train Loss: 8394.7432, Test Loss: 2990.0454\n",
      "Epoch [330/1000], Train Loss: 8429.2411, Test Loss: 3071.7712\n",
      "Epoch [331/1000], Train Loss: 8471.1164, Test Loss: 3031.2137\n",
      "Epoch [332/1000], Train Loss: 8402.8053, Test Loss: 2915.8985\n",
      "Epoch [333/1000], Train Loss: 8073.3438, Test Loss: 2963.1144\n",
      "Epoch [334/1000], Train Loss: 7999.5716, Test Loss: 2846.3475\n",
      "Epoch [335/1000], Train Loss: 7867.2257, Test Loss: 2921.4532\n",
      "Epoch [336/1000], Train Loss: 8186.5885, Test Loss: 2817.7519\n",
      "Epoch [337/1000], Train Loss: 8197.6431, Test Loss: 2735.8801\n",
      "Epoch [338/1000], Train Loss: 7764.0994, Test Loss: 2951.9782\n",
      "Epoch [339/1000], Train Loss: 7748.6187, Test Loss: 2938.6924\n",
      "Epoch [340/1000], Train Loss: 7671.2128, Test Loss: 2799.1970\n",
      "Epoch [341/1000], Train Loss: 7652.7171, Test Loss: 2975.7828\n",
      "Epoch [342/1000], Train Loss: 8154.9830, Test Loss: 2831.5993\n",
      "Epoch [343/1000], Train Loss: 7484.6133, Test Loss: 2735.5407\n",
      "Epoch [344/1000], Train Loss: 7399.5615, Test Loss: 2658.9856\n",
      "Epoch [345/1000], Train Loss: 7346.7204, Test Loss: 2785.8456\n",
      "Epoch [346/1000], Train Loss: 7217.3935, Test Loss: 2663.6431\n",
      "Epoch [347/1000], Train Loss: 7405.1262, Test Loss: 2646.0910\n",
      "Epoch [348/1000], Train Loss: 7052.4118, Test Loss: 2663.9072\n",
      "Epoch [349/1000], Train Loss: 7176.2816, Test Loss: 2613.4204\n",
      "Epoch [350/1000], Train Loss: 6967.4031, Test Loss: 2831.3010\n",
      "Epoch [351/1000], Train Loss: 7277.8952, Test Loss: 2815.3914\n",
      "Epoch [352/1000], Train Loss: 7059.3902, Test Loss: 2790.2197\n",
      "Epoch [353/1000], Train Loss: 7087.2234, Test Loss: 2551.3925\n",
      "Epoch [354/1000], Train Loss: 6777.9547, Test Loss: 2559.9630\n",
      "Epoch [355/1000], Train Loss: 6739.9622, Test Loss: 2613.4985\n",
      "Epoch [356/1000], Train Loss: 6680.1669, Test Loss: 2867.9970\n",
      "Epoch [357/1000], Train Loss: 6765.1741, Test Loss: 2614.3763\n",
      "Epoch [358/1000], Train Loss: 7007.0695, Test Loss: 2837.8007\n",
      "Epoch [359/1000], Train Loss: 6719.3320, Test Loss: 2560.4953\n",
      "Epoch [360/1000], Train Loss: 6593.7823, Test Loss: 2622.2461\n",
      "Epoch [361/1000], Train Loss: 6673.4968, Test Loss: 2526.0628\n",
      "Epoch [362/1000], Train Loss: 6601.3684, Test Loss: 2555.0685\n",
      "Epoch [363/1000], Train Loss: 6491.0983, Test Loss: 2420.3995\n",
      "Epoch [364/1000], Train Loss: 6386.8135, Test Loss: 2429.1792\n",
      "Epoch [365/1000], Train Loss: 6228.6580, Test Loss: 2537.2200\n",
      "Epoch [366/1000], Train Loss: 6443.8971, Test Loss: 2609.6119\n",
      "Epoch [367/1000], Train Loss: 6412.6506, Test Loss: 2803.8724\n",
      "Epoch [368/1000], Train Loss: 6127.2990, Test Loss: 2426.9821\n",
      "Epoch [369/1000], Train Loss: 6167.4198, Test Loss: 2519.3867\n",
      "Epoch [370/1000], Train Loss: 6225.0459, Test Loss: 2486.9728\n",
      "Epoch [371/1000], Train Loss: 6208.6338, Test Loss: 2622.3878\n",
      "Epoch [372/1000], Train Loss: 6122.8787, Test Loss: 2391.5775\n",
      "Epoch [373/1000], Train Loss: 5977.5690, Test Loss: 2458.3426\n",
      "Epoch [374/1000], Train Loss: 6013.8554, Test Loss: 2683.0712\n",
      "Epoch [375/1000], Train Loss: 5920.5245, Test Loss: 2460.5059\n",
      "Epoch [376/1000], Train Loss: 5939.1071, Test Loss: 2427.1673\n",
      "Epoch [377/1000], Train Loss: 5867.8217, Test Loss: 2830.4138\n",
      "Epoch [378/1000], Train Loss: 5964.4708, Test Loss: 2437.2633\n",
      "Epoch [379/1000], Train Loss: 6059.4032, Test Loss: 2681.9645\n",
      "Epoch [380/1000], Train Loss: 5714.8374, Test Loss: 2597.5305\n",
      "Epoch [381/1000], Train Loss: 5697.0314, Test Loss: 2434.6208\n",
      "Epoch [382/1000], Train Loss: 5574.8792, Test Loss: 2364.7664\n",
      "Epoch [383/1000], Train Loss: 5557.8362, Test Loss: 2563.0258\n",
      "Epoch [384/1000], Train Loss: 5662.6003, Test Loss: 2558.0168\n",
      "Epoch [385/1000], Train Loss: 5670.1688, Test Loss: 2493.3204\n",
      "Epoch [386/1000], Train Loss: 5367.6569, Test Loss: 2479.8874\n",
      "Epoch [387/1000], Train Loss: 5742.5071, Test Loss: 2458.8697\n",
      "Epoch [388/1000], Train Loss: 5407.6923, Test Loss: 2376.3491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [389/1000], Train Loss: 5471.1686, Test Loss: 2331.0398\n",
      "Epoch [390/1000], Train Loss: 5229.2766, Test Loss: 2247.1127\n",
      "Epoch [391/1000], Train Loss: 5365.5845, Test Loss: 2562.8030\n",
      "Epoch [392/1000], Train Loss: 2012504.5980, Test Loss: 484422.0195\n",
      "Epoch [393/1000], Train Loss: 772166.0889, Test Loss: 53577.7085\n",
      "Epoch [394/1000], Train Loss: 124104.3860, Test Loss: 17052.8282\n",
      "Epoch [395/1000], Train Loss: 61262.9713, Test Loss: 11056.3151\n",
      "Epoch [396/1000], Train Loss: 42700.5671, Test Loss: 10313.9117\n",
      "Epoch [397/1000], Train Loss: 31617.6316, Test Loss: 7417.1688\n",
      "Epoch [398/1000], Train Loss: 29164.2625, Test Loss: 7430.9212\n",
      "Epoch [399/1000], Train Loss: 23400.4467, Test Loss: 5972.8927\n",
      "Epoch [400/1000], Train Loss: 23170.2643, Test Loss: 5373.7363\n",
      "Epoch [401/1000], Train Loss: 20557.0768, Test Loss: 5336.9127\n",
      "Epoch [402/1000], Train Loss: 19291.1346, Test Loss: 4654.4946\n",
      "Epoch [403/1000], Train Loss: 17425.8804, Test Loss: 4733.5226\n",
      "Epoch [404/1000], Train Loss: 16594.6667, Test Loss: 4419.4545\n",
      "Epoch [405/1000], Train Loss: 17410.1754, Test Loss: 6667.1095\n",
      "Epoch [406/1000], Train Loss: 14883.8301, Test Loss: 4118.1146\n",
      "Epoch [407/1000], Train Loss: 15069.1994, Test Loss: 5309.4732\n",
      "Epoch [408/1000], Train Loss: 13471.5735, Test Loss: 6232.1351\n",
      "Epoch [409/1000], Train Loss: 13510.9341, Test Loss: 3784.0909\n",
      "Epoch [410/1000], Train Loss: 13530.8017, Test Loss: 3652.4641\n",
      "Epoch [411/1000], Train Loss: 12703.2081, Test Loss: 3460.5490\n",
      "Epoch [412/1000], Train Loss: 12787.5833, Test Loss: 3982.5650\n",
      "Epoch [413/1000], Train Loss: 11301.7057, Test Loss: 3512.6747\n",
      "Epoch [414/1000], Train Loss: 11468.4092, Test Loss: 3712.8064\n",
      "Epoch [415/1000], Train Loss: 12273.8391, Test Loss: 3757.4039\n",
      "Epoch [416/1000], Train Loss: 11014.2577, Test Loss: 4555.7935\n",
      "Epoch [417/1000], Train Loss: 10542.0588, Test Loss: 3545.1947\n",
      "Epoch [418/1000], Train Loss: 11041.9616, Test Loss: 3984.6141\n",
      "Epoch [419/1000], Train Loss: 10195.6432, Test Loss: 3016.8723\n",
      "Epoch [420/1000], Train Loss: 9746.2761, Test Loss: 3840.5891\n",
      "Epoch [421/1000], Train Loss: 10996.6287, Test Loss: 3308.8721\n",
      "Epoch [422/1000], Train Loss: 10328.0005, Test Loss: 3132.8478\n",
      "Epoch [423/1000], Train Loss: 9558.0658, Test Loss: 2923.7024\n",
      "Epoch [424/1000], Train Loss: 9319.2271, Test Loss: 2845.5092\n",
      "Epoch [425/1000], Train Loss: 9317.3585, Test Loss: 2865.6127\n",
      "Epoch [426/1000], Train Loss: 8837.3513, Test Loss: 3024.2793\n",
      "Epoch [427/1000], Train Loss: 8477.7079, Test Loss: 3253.4402\n",
      "Epoch [428/1000], Train Loss: 9386.8822, Test Loss: 2758.1196\n",
      "Epoch [429/1000], Train Loss: 8578.2468, Test Loss: 4292.8723\n",
      "Epoch [430/1000], Train Loss: 8161.6686, Test Loss: 2712.8254\n",
      "Epoch [431/1000], Train Loss: 8801.6121, Test Loss: 2671.4059\n",
      "Epoch [432/1000], Train Loss: 7499.4993, Test Loss: 4474.4066\n",
      "Epoch [433/1000], Train Loss: 8218.1010, Test Loss: 3139.3492\n",
      "Epoch [434/1000], Train Loss: 8164.4099, Test Loss: 3370.8933\n",
      "Epoch [435/1000], Train Loss: 7731.2118, Test Loss: 3164.1475\n",
      "Epoch [436/1000], Train Loss: 7345.2222, Test Loss: 2586.7377\n",
      "Epoch [437/1000], Train Loss: 8432.0352, Test Loss: 2568.5439\n",
      "Epoch [438/1000], Train Loss: 7745.6856, Test Loss: 2727.6498\n",
      "Epoch [439/1000], Train Loss: 7335.6217, Test Loss: 2570.8298\n",
      "Epoch [440/1000], Train Loss: 7275.7263, Test Loss: 2503.3371\n",
      "Epoch [441/1000], Train Loss: 7470.1154, Test Loss: 2633.7794\n",
      "Epoch [442/1000], Train Loss: 6880.6175, Test Loss: 3130.3315\n",
      "Epoch [443/1000], Train Loss: 7035.5780, Test Loss: 2759.6080\n",
      "Epoch [444/1000], Train Loss: 7172.1121, Test Loss: 2688.9165\n",
      "Epoch [445/1000], Train Loss: 7198.7818, Test Loss: 2843.5884\n",
      "Epoch [446/1000], Train Loss: 7771.0033, Test Loss: 2536.8622\n",
      "Epoch [447/1000], Train Loss: 6326.7918, Test Loss: 2543.5655\n",
      "Epoch [448/1000], Train Loss: 6527.0582, Test Loss: 2624.8361\n",
      "Epoch [449/1000], Train Loss: 7298.3811, Test Loss: 2677.1778\n",
      "Epoch [450/1000], Train Loss: 6472.7115, Test Loss: 2655.6950\n",
      "Epoch [451/1000], Train Loss: 6543.3474, Test Loss: 2708.6372\n",
      "Epoch [452/1000], Train Loss: 6401.5804, Test Loss: 2757.2222\n",
      "Epoch [453/1000], Train Loss: 6405.6700, Test Loss: 2551.3194\n",
      "Epoch [454/1000], Train Loss: 6153.1327, Test Loss: 2645.6883\n",
      "Epoch [455/1000], Train Loss: 6354.5558, Test Loss: 2338.1271\n",
      "Epoch [456/1000], Train Loss: 6017.7921, Test Loss: 2828.6253\n",
      "Epoch [457/1000], Train Loss: 6084.6976, Test Loss: 2498.2389\n",
      "Epoch [458/1000], Train Loss: 6267.7337, Test Loss: 2530.0422\n",
      "Epoch [459/1000], Train Loss: 5745.9750, Test Loss: 2402.5934\n",
      "Epoch [460/1000], Train Loss: 6559.1971, Test Loss: 2388.8023\n",
      "Epoch [461/1000], Train Loss: 5629.8102, Test Loss: 2447.7942\n",
      "Epoch [462/1000], Train Loss: 5854.0288, Test Loss: 2432.2158\n",
      "Epoch [463/1000], Train Loss: 5795.9748, Test Loss: 2431.9900\n",
      "Epoch [464/1000], Train Loss: 5518.7770, Test Loss: 2385.4249\n",
      "Epoch [465/1000], Train Loss: 5694.5371, Test Loss: 2475.0662\n",
      "Epoch [466/1000], Train Loss: 5709.2482, Test Loss: 2715.7708\n",
      "Epoch [467/1000], Train Loss: 5666.5215, Test Loss: 2489.6348\n",
      "Epoch [468/1000], Train Loss: 5366.9453, Test Loss: 2374.7728\n",
      "Epoch [469/1000], Train Loss: 5485.9444, Test Loss: 2357.4201\n",
      "Epoch [470/1000], Train Loss: 5706.4532, Test Loss: 2412.7118\n",
      "Epoch [471/1000], Train Loss: 5364.1010, Test Loss: 2252.3603\n",
      "Epoch [472/1000], Train Loss: 5369.7322, Test Loss: 2301.1371\n",
      "Epoch [473/1000], Train Loss: 5229.8683, Test Loss: 2487.3360\n",
      "Epoch [474/1000], Train Loss: 5391.3650, Test Loss: 2280.4275\n",
      "Epoch [475/1000], Train Loss: 5563.2638, Test Loss: 2333.6269\n",
      "Epoch [476/1000], Train Loss: 5157.6246, Test Loss: 2327.0270\n",
      "Epoch [477/1000], Train Loss: 5153.9681, Test Loss: 2130.5094\n",
      "Epoch [478/1000], Train Loss: 5137.6896, Test Loss: 2401.8408\n",
      "Epoch [479/1000], Train Loss: 5090.1190, Test Loss: 2199.3456\n",
      "Epoch [480/1000], Train Loss: 5193.5811, Test Loss: 2792.8301\n",
      "Epoch [481/1000], Train Loss: 5035.7268, Test Loss: 2189.7842\n",
      "Epoch [482/1000], Train Loss: 4947.3344, Test Loss: 2263.0572\n",
      "Epoch [483/1000], Train Loss: 5334.2473, Test Loss: 2524.9118\n",
      "Epoch [484/1000], Train Loss: 4874.0664, Test Loss: 2170.6856\n",
      "Epoch [485/1000], Train Loss: 4700.9454, Test Loss: 2275.4393\n",
      "Epoch [486/1000], Train Loss: 4811.9736, Test Loss: 2104.3351\n",
      "Epoch [487/1000], Train Loss: 4784.5749, Test Loss: 2106.0196\n",
      "Epoch [488/1000], Train Loss: 4857.9929, Test Loss: 2209.4471\n",
      "Epoch [489/1000], Train Loss: 5217.6969, Test Loss: 2256.2899\n",
      "Epoch [490/1000], Train Loss: 4674.4510, Test Loss: 2260.8409\n",
      "Epoch [491/1000], Train Loss: 4677.9594, Test Loss: 2300.8018\n",
      "Epoch [492/1000], Train Loss: 4977.2192, Test Loss: 2176.3316\n",
      "Epoch [493/1000], Train Loss: 4492.2019, Test Loss: 2343.9005\n",
      "Epoch [494/1000], Train Loss: 4718.2986, Test Loss: 2291.9164\n",
      "Epoch [495/1000], Train Loss: 4696.4566, Test Loss: 2310.0916\n",
      "Epoch [496/1000], Train Loss: 4575.2688, Test Loss: 2140.2072\n",
      "Epoch [497/1000], Train Loss: 4761.6382, Test Loss: 2090.0817\n",
      "Epoch [498/1000], Train Loss: 4549.7445, Test Loss: 2224.2308\n",
      "Epoch [499/1000], Train Loss: 4546.3797, Test Loss: 2133.2477\n",
      "Epoch [500/1000], Train Loss: 4388.4265, Test Loss: 2305.7233\n",
      "Epoch [501/1000], Train Loss: 4582.7437, Test Loss: 2168.3369\n",
      "Epoch [502/1000], Train Loss: 4589.9269, Test Loss: 2098.2692\n",
      "Epoch [503/1000], Train Loss: 4537.8588, Test Loss: 2245.4116\n",
      "Epoch [504/1000], Train Loss: 4546.0596, Test Loss: 2398.7041\n",
      "Epoch [505/1000], Train Loss: 4454.7400, Test Loss: 2189.9537\n",
      "Epoch [506/1000], Train Loss: 4347.8843, Test Loss: 2226.7050\n",
      "Epoch [507/1000], Train Loss: 4177.5998, Test Loss: 2133.4625\n",
      "Epoch [508/1000], Train Loss: 4457.9307, Test Loss: 2169.0218\n",
      "Epoch [509/1000], Train Loss: 4567.4068, Test Loss: 2108.0879\n",
      "Epoch [510/1000], Train Loss: 4285.4566, Test Loss: 2201.0399\n",
      "Epoch [511/1000], Train Loss: 4444.8783, Test Loss: 2220.6752\n",
      "Epoch [512/1000], Train Loss: 4261.5712, Test Loss: 2225.9158\n",
      "Epoch [513/1000], Train Loss: 4198.7517, Test Loss: 2199.1534\n",
      "Epoch [514/1000], Train Loss: 4517.6120, Test Loss: 2064.2407\n",
      "Epoch [515/1000], Train Loss: 4308.9632, Test Loss: 2050.5603\n",
      "Epoch [516/1000], Train Loss: 4185.8729, Test Loss: 2086.2403\n",
      "Epoch [517/1000], Train Loss: 4132.9319, Test Loss: 2078.6022\n",
      "Epoch [518/1000], Train Loss: 4149.2685, Test Loss: 2097.7691\n",
      "Epoch [519/1000], Train Loss: 4051.5640, Test Loss: 2122.1872\n",
      "Epoch [520/1000], Train Loss: 4095.4380, Test Loss: 2087.2763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [521/1000], Train Loss: 4052.1523, Test Loss: 2495.7789\n",
      "Epoch [522/1000], Train Loss: 4086.9914, Test Loss: 2094.7396\n",
      "Epoch [523/1000], Train Loss: 4113.7699, Test Loss: 2083.2197\n",
      "Epoch [524/1000], Train Loss: 3918.8212, Test Loss: 2134.2687\n",
      "Epoch [525/1000], Train Loss: 4074.3573, Test Loss: 2114.4071\n",
      "Epoch [526/1000], Train Loss: 4082.7936, Test Loss: 2200.2889\n",
      "Epoch [527/1000], Train Loss: 3954.9440, Test Loss: 2025.2812\n",
      "Epoch [528/1000], Train Loss: 3975.4181, Test Loss: 2159.6285\n",
      "Epoch [529/1000], Train Loss: 3960.2374, Test Loss: 2148.2196\n",
      "Epoch [530/1000], Train Loss: 4022.6331, Test Loss: 2082.7180\n",
      "Epoch [531/1000], Train Loss: 3817.7151, Test Loss: 2066.3821\n",
      "Epoch [532/1000], Train Loss: 3860.3014, Test Loss: 2032.1837\n",
      "Epoch [533/1000], Train Loss: 3824.2570, Test Loss: 2525.5785\n",
      "Epoch [534/1000], Train Loss: 4016.0316, Test Loss: 2045.6716\n",
      "Epoch [535/1000], Train Loss: 3825.7984, Test Loss: 2117.7204\n",
      "Epoch [536/1000], Train Loss: 234095.2151, Test Loss: 1881630.0117\n",
      "Epoch [537/1000], Train Loss: 870982.7439, Test Loss: 14227.1714\n",
      "Epoch [538/1000], Train Loss: 36259.2012, Test Loss: 7088.6189\n",
      "Epoch [539/1000], Train Loss: 21032.4004, Test Loss: 5196.7577\n",
      "Epoch [540/1000], Train Loss: 15889.7960, Test Loss: 4215.4606\n",
      "Epoch [541/1000], Train Loss: 13290.7364, Test Loss: 3876.4791\n",
      "Epoch [542/1000], Train Loss: 11626.4452, Test Loss: 3514.1312\n",
      "Epoch [543/1000], Train Loss: 10504.4220, Test Loss: 3294.5801\n",
      "Epoch [544/1000], Train Loss: 9606.4290, Test Loss: 3122.8338\n",
      "Epoch [545/1000], Train Loss: 9015.2747, Test Loss: 2985.6196\n",
      "Epoch [546/1000], Train Loss: 8506.6419, Test Loss: 2889.3248\n",
      "Epoch [547/1000], Train Loss: 7974.1841, Test Loss: 2838.3111\n",
      "Epoch [548/1000], Train Loss: 7607.0116, Test Loss: 2752.9955\n",
      "Epoch [549/1000], Train Loss: 7223.7387, Test Loss: 2839.1921\n",
      "Epoch [550/1000], Train Loss: 7250.4269, Test Loss: 2789.1586\n",
      "Epoch [551/1000], Train Loss: 6819.0444, Test Loss: 2567.6217\n",
      "Epoch [552/1000], Train Loss: 6597.7375, Test Loss: 2662.4798\n",
      "Epoch [553/1000], Train Loss: 6403.6549, Test Loss: 2458.5643\n",
      "Epoch [554/1000], Train Loss: 6256.8176, Test Loss: 2472.1547\n",
      "Epoch [555/1000], Train Loss: 5957.8219, Test Loss: 2420.5613\n",
      "Epoch [556/1000], Train Loss: 6011.5333, Test Loss: 2418.2364\n",
      "Epoch [557/1000], Train Loss: 5907.5994, Test Loss: 2379.9050\n",
      "Epoch [558/1000], Train Loss: 5610.2790, Test Loss: 2479.5174\n",
      "Epoch [559/1000], Train Loss: 5773.7155, Test Loss: 2367.1754\n",
      "Epoch [560/1000], Train Loss: 5693.7171, Test Loss: 2367.6613\n",
      "Epoch [561/1000], Train Loss: 5465.3316, Test Loss: 2330.0353\n",
      "Epoch [562/1000], Train Loss: 5276.2791, Test Loss: 2248.3569\n",
      "Epoch [563/1000], Train Loss: 5324.5568, Test Loss: 2250.7329\n",
      "Epoch [564/1000], Train Loss: 5479.2384, Test Loss: 2327.1101\n",
      "Epoch [565/1000], Train Loss: 5247.5993, Test Loss: 2470.5455\n",
      "Epoch [566/1000], Train Loss: 5085.3441, Test Loss: 2388.9076\n",
      "Epoch [567/1000], Train Loss: 5037.3703, Test Loss: 2176.3779\n",
      "Epoch [568/1000], Train Loss: 4957.6378, Test Loss: 2234.8451\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m save_path \u001b[38;5;241m=\u001b[39m ModelName \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Params.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m train_losses, test_losses, is_model_trained \u001b[38;5;241m=\u001b[39m train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Finish timing cell run time\u001b[39;00m\n\u001b[1;32m     25\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[9], line 63\u001b[0m, in \u001b[0;36mtrain_or_load_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pretrained model found. Training from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#optimizer = optim.Adam(model.parameters())  \u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n\u001b[1;32m     64\u001b[0m is_model_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set flag to True after training\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Save losses per epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mtrain_and_save_best_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 18\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load('Dataset44_Val_Spec.npy')\n",
    "concVal = np.load('Dataset44_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "ValSpectra = np.load(\"Dataset44_RepresentativeExamples_Spectra.npy\")\n",
    "ValConc = np.load(\"Dataset44_RepresentativeExamples_Concentrations.npy\")\n",
    "ValSpecNames = np.load(\"Dataset44_RepresentativeExamples_VariableNames.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ValConc = torch.tensor(ValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=23552, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ValConc[i]\n",
    "    Prediction = model_aq(ValSpectra[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(21):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.83  -  AllAq1\n",
      "1.85  -  AllAq5\n",
      "0.37  -  AllAq25\n",
      "2.6  -  AllAq50\n",
      "0.57  -  ThreeAddedSinglets\n",
      "13.64  -  ThirtyAddedSinglets\n",
      "75.61  -  ShiftedSpec\n",
      "27.68  -  SineBase\n",
      "187.91  -  HighDynamicRange\n",
      "inf  -  HalfZeros\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - \",ValSpecNames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
