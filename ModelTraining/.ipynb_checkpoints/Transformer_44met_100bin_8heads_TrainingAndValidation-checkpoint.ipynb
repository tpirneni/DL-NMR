{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer Encoder on dataset of 44 metabolites, test with bin size of 100 (rather than 1000) and 4 attention heads and 4 encoder layers (rather than 1 each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f69180924d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"Transformer_44met_TrainingAndValidation_100bin_8heads_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('Dataset44_Spec.npy')\n",
    "conc1 = np.load('Dataset44_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "#spectraVal = np.load('Dataset44_Val_Spec.npy')\n",
    "#concVal = np.load('Dataset44_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "#ValSpectra = np.load(\"Dataset44_RepresentativeExamples_Spectra.npy\")\n",
    "#ValConc = np.load(\"Dataset44_RepresentativeExamples_Concentrations.npy\")\n",
    "#ValSpecNames = np.load(\"Dataset44_RepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 32, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.Linear(235520, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Binning\n",
    "        batch_size, seq_length = x.size()\n",
    "        num_bins = seq_length // self.input_dim\n",
    "        x = x.view(batch_size, num_bins, self.input_dim)  # (batch_size, num_bins, input_dim)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = x.permute(1, 0, 2)  # (num_bins, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)  # (num_bins, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Reconstruct original sequence\n",
    "        x = x.reshape(batch_size, num_bins * d_model)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder(x)  # (batch_size, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "input_dim = 100   # Size of each bin\n",
    "d_model = 512     # Embedding dimension\n",
    "nhead = 8         # Number of attention heads\n",
    "num_encoder_layers = 1  # Number of transformer encoder layers\n",
    "dim_feedforward = 2048  # Feedforward dimension\n",
    "dropout = 0.1     # Dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "       \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/1000], Train Loss: 6471504.4009, Test Loss: 1045438.1479\n",
      "Epoch [2/1000], Train Loss: 1130787.5048, Test Loss: 50747.4413\n",
      "Epoch [3/1000], Train Loss: 201794.8249, Test Loss: 37249.0394\n",
      "Epoch [4/1000], Train Loss: 150166.8682, Test Loss: 44012.7564\n",
      "Epoch [5/1000], Train Loss: 128198.6796, Test Loss: 42031.4422\n",
      "Epoch [6/1000], Train Loss: 113256.0994, Test Loss: 27147.0827\n",
      "Epoch [7/1000], Train Loss: 101955.3067, Test Loss: 23965.1582\n",
      "Epoch [8/1000], Train Loss: 92850.3442, Test Loss: 23461.1537\n",
      "Epoch [9/1000], Train Loss: 84659.1372, Test Loss: 26501.9825\n",
      "Epoch [10/1000], Train Loss: 76223.8913, Test Loss: 23926.4955\n",
      "Epoch [11/1000], Train Loss: 71432.6924, Test Loss: 21152.3494\n",
      "Epoch [12/1000], Train Loss: 67464.1652, Test Loss: 22281.9811\n",
      "Epoch [13/1000], Train Loss: 61748.8017, Test Loss: 19305.4788\n",
      "Epoch [14/1000], Train Loss: 56776.0628, Test Loss: 17074.2848\n",
      "Epoch [15/1000], Train Loss: 53125.1230, Test Loss: 19413.3281\n",
      "Epoch [16/1000], Train Loss: 50251.5829, Test Loss: 24045.1569\n",
      "Epoch [17/1000], Train Loss: 48092.6727, Test Loss: 20476.5833\n",
      "Epoch [18/1000], Train Loss: 42752.7421, Test Loss: 19523.7275\n",
      "Epoch [19/1000], Train Loss: 41404.1593, Test Loss: 19557.4888\n",
      "Epoch [20/1000], Train Loss: 40016.2415, Test Loss: 13853.5198\n",
      "Epoch [21/1000], Train Loss: 37425.5360, Test Loss: 16988.2545\n",
      "Epoch [22/1000], Train Loss: 35418.3449, Test Loss: 16375.8712\n",
      "Epoch [23/1000], Train Loss: 35084.4462, Test Loss: 11959.4848\n",
      "Epoch [24/1000], Train Loss: 34718.0574, Test Loss: 11015.9867\n",
      "Epoch [25/1000], Train Loss: 32398.2551, Test Loss: 15581.8718\n",
      "Epoch [26/1000], Train Loss: 1230121.1521, Test Loss: 128605.4025\n",
      "Epoch [27/1000], Train Loss: 503095.3011, Test Loss: 84221.8723\n",
      "Epoch [28/1000], Train Loss: 340618.9666, Test Loss: 63379.8749\n",
      "Epoch [29/1000], Train Loss: 248665.0963, Test Loss: 59448.1327\n",
      "Epoch [30/1000], Train Loss: 212181.1394, Test Loss: 45903.4962\n",
      "Epoch [31/1000], Train Loss: 178712.6573, Test Loss: 44229.3109\n",
      "Epoch [32/1000], Train Loss: 162756.6015, Test Loss: 44645.6746\n",
      "Epoch [33/1000], Train Loss: 147609.2290, Test Loss: 42502.4195\n",
      "Epoch [34/1000], Train Loss: 140834.4447, Test Loss: 41366.9120\n",
      "Epoch [35/1000], Train Loss: 136444.2207, Test Loss: 33137.3257\n",
      "Epoch [36/1000], Train Loss: 130485.5344, Test Loss: 34001.9586\n",
      "Epoch [37/1000], Train Loss: 125623.9364, Test Loss: 33130.1446\n",
      "Epoch [38/1000], Train Loss: 122734.9623, Test Loss: 32364.1698\n",
      "Epoch [39/1000], Train Loss: 114206.2847, Test Loss: 32568.2553\n",
      "Epoch [40/1000], Train Loss: 114914.4595, Test Loss: 37162.0969\n",
      "Epoch [41/1000], Train Loss: 108921.7471, Test Loss: 32854.0467\n",
      "Epoch [42/1000], Train Loss: 105175.2794, Test Loss: 36743.6453\n",
      "Epoch [43/1000], Train Loss: 102291.1187, Test Loss: 25926.6497\n",
      "Epoch [44/1000], Train Loss: 100675.1272, Test Loss: 30075.1621\n",
      "Epoch [45/1000], Train Loss: 100667.0301, Test Loss: 29545.6808\n",
      "Epoch [46/1000], Train Loss: 97545.2299, Test Loss: 30282.4963\n",
      "Epoch [47/1000], Train Loss: 94151.2970, Test Loss: 31908.5998\n",
      "Epoch [48/1000], Train Loss: 94478.8245, Test Loss: 31389.5957\n",
      "Epoch [49/1000], Train Loss: 90531.9277, Test Loss: 26748.1952\n",
      "Epoch [50/1000], Train Loss: 88862.4313, Test Loss: 29083.4117\n",
      "Epoch [51/1000], Train Loss: 85508.3991, Test Loss: 30317.0696\n",
      "Epoch [52/1000], Train Loss: 84461.9874, Test Loss: 28721.9713\n",
      "Epoch [53/1000], Train Loss: 83937.4733, Test Loss: 26325.8523\n",
      "Epoch [54/1000], Train Loss: 81996.2486, Test Loss: 26001.8269\n",
      "Epoch [55/1000], Train Loss: 81454.0275, Test Loss: 27951.4334\n",
      "Epoch [56/1000], Train Loss: 80904.0337, Test Loss: 26647.5586\n",
      "Epoch [57/1000], Train Loss: 84786.8004, Test Loss: 28783.4631\n",
      "Epoch [58/1000], Train Loss: 80158.8690, Test Loss: 27340.6050\n",
      "Epoch [59/1000], Train Loss: 78943.0071, Test Loss: 25455.4213\n",
      "Epoch [60/1000], Train Loss: 77242.8873, Test Loss: 28624.9736\n",
      "Epoch [61/1000], Train Loss: 75063.4306, Test Loss: 25026.8498\n",
      "Epoch [62/1000], Train Loss: 73558.1285, Test Loss: 23395.6782\n",
      "Epoch [63/1000], Train Loss: 71823.8506, Test Loss: 26556.1825\n",
      "Epoch [64/1000], Train Loss: 71497.8945, Test Loss: 22632.9787\n",
      "Epoch [65/1000], Train Loss: 71125.4167, Test Loss: 26503.0397\n",
      "Epoch [66/1000], Train Loss: 72193.0605, Test Loss: 25507.8454\n",
      "Epoch [67/1000], Train Loss: 73337.8274, Test Loss: 25588.1978\n",
      "Epoch [68/1000], Train Loss: 70493.3263, Test Loss: 27252.4207\n",
      "Epoch [69/1000], Train Loss: 73969.9869, Test Loss: 30143.6456\n",
      "Epoch [70/1000], Train Loss: 69681.5863, Test Loss: 27403.6861\n",
      "Epoch [71/1000], Train Loss: 67324.7301, Test Loss: 23854.9620\n",
      "Epoch [72/1000], Train Loss: 69174.4768, Test Loss: 23149.6871\n",
      "Epoch [73/1000], Train Loss: 69641.7980, Test Loss: 25384.9832\n",
      "Epoch [74/1000], Train Loss: 66272.2548, Test Loss: 25854.3837\n",
      "Epoch [75/1000], Train Loss: 66971.2434, Test Loss: 23873.4188\n",
      "Epoch [76/1000], Train Loss: 65812.9568, Test Loss: 23755.2232\n",
      "Epoch [77/1000], Train Loss: 65381.7541, Test Loss: 23092.9940\n",
      "Epoch [78/1000], Train Loss: 64527.6279, Test Loss: 25623.6838\n",
      "Epoch [79/1000], Train Loss: 63309.9096, Test Loss: 25319.0659\n",
      "Epoch [80/1000], Train Loss: 61436.8471, Test Loss: 23156.8472\n",
      "Epoch [81/1000], Train Loss: 60228.0277, Test Loss: 25198.6555\n",
      "Epoch [82/1000], Train Loss: 63170.4950, Test Loss: 24687.8154\n",
      "Epoch [83/1000], Train Loss: 61625.8150, Test Loss: 22951.4907\n",
      "Epoch [84/1000], Train Loss: 61139.4036, Test Loss: 23107.1853\n",
      "Epoch [85/1000], Train Loss: 60555.6132, Test Loss: 23734.0952\n",
      "Epoch [86/1000], Train Loss: 61088.7476, Test Loss: 25474.6556\n",
      "Epoch [87/1000], Train Loss: 58658.5161, Test Loss: 25400.7316\n",
      "Epoch [88/1000], Train Loss: 59829.5017, Test Loss: 25803.4342\n",
      "Epoch [89/1000], Train Loss: 58890.1432, Test Loss: 23837.1075\n",
      "Epoch [90/1000], Train Loss: 58574.5952, Test Loss: 24789.2151\n",
      "Epoch [91/1000], Train Loss: 57464.4449, Test Loss: 27357.4039\n",
      "Epoch [92/1000], Train Loss: 57957.9325, Test Loss: 23436.3983\n",
      "Epoch [93/1000], Train Loss: 58398.4378, Test Loss: 26781.1384\n",
      "Epoch [94/1000], Train Loss: 59402.8602, Test Loss: 27298.4698\n",
      "Epoch [95/1000], Train Loss: 58316.6582, Test Loss: 24803.0734\n",
      "Epoch [96/1000], Train Loss: 56268.6871, Test Loss: 22581.8138\n",
      "Epoch [97/1000], Train Loss: 56291.1595, Test Loss: 25196.6409\n",
      "Epoch [98/1000], Train Loss: 56905.5228, Test Loss: 28380.0125\n",
      "Epoch [99/1000], Train Loss: 55572.8913, Test Loss: 25701.2372\n",
      "Epoch [100/1000], Train Loss: 57691.1634, Test Loss: 25143.7248\n",
      "Epoch [101/1000], Train Loss: 55594.8356, Test Loss: 25004.8022\n",
      "Epoch [102/1000], Train Loss: 54048.5030, Test Loss: 25910.8336\n",
      "Epoch [103/1000], Train Loss: 53215.0408, Test Loss: 24218.5152\n",
      "Epoch [104/1000], Train Loss: 53141.2319, Test Loss: 24599.9286\n",
      "Epoch [105/1000], Train Loss: 53872.2735, Test Loss: 24179.7192\n",
      "Epoch [106/1000], Train Loss: 54375.2109, Test Loss: 26116.6391\n",
      "Epoch [107/1000], Train Loss: 55560.8966, Test Loss: 25682.3340\n",
      "Epoch [108/1000], Train Loss: 53613.5524, Test Loss: 23856.0379\n",
      "Epoch [109/1000], Train Loss: 52455.9704, Test Loss: 24978.2725\n",
      "Epoch [110/1000], Train Loss: 52698.3654, Test Loss: 27880.4975\n",
      "Epoch [111/1000], Train Loss: 53836.0865, Test Loss: 26247.0141\n",
      "Epoch [112/1000], Train Loss: 52190.0309, Test Loss: 25598.0718\n",
      "Epoch [113/1000], Train Loss: 51075.0835, Test Loss: 25940.0246\n",
      "Epoch [114/1000], Train Loss: 52456.3291, Test Loss: 25074.9016\n",
      "Epoch [115/1000], Train Loss: 51653.1202, Test Loss: 24310.8770\n",
      "Epoch [116/1000], Train Loss: 50310.6422, Test Loss: 24566.3675\n",
      "Epoch [117/1000], Train Loss: 49503.9365, Test Loss: 24593.2410\n",
      "Epoch [118/1000], Train Loss: 50148.7073, Test Loss: 25684.4549\n",
      "Epoch [119/1000], Train Loss: 51154.4844, Test Loss: 26359.9050\n",
      "Epoch [120/1000], Train Loss: 49301.0676, Test Loss: 25507.9285\n",
      "Epoch [121/1000], Train Loss: 50341.8075, Test Loss: 23721.9642\n",
      "Epoch [122/1000], Train Loss: 53663.7042, Test Loss: 23496.4093\n",
      "Epoch [123/1000], Train Loss: 52089.7497, Test Loss: 24880.7484\n",
      "Epoch [124/1000], Train Loss: 49328.6523, Test Loss: 25520.3201\n",
      "Epoch [125/1000], Train Loss: 49554.6000, Test Loss: 25964.3160\n",
      "Epoch [126/1000], Train Loss: 51784.0450, Test Loss: 25563.7853\n",
      "Epoch [127/1000], Train Loss: 49891.6746, Test Loss: 25856.4911\n",
      "Epoch [128/1000], Train Loss: 53124.1997, Test Loss: 26714.3697\n",
      "Epoch [129/1000], Train Loss: 48745.1957, Test Loss: 27674.7976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000], Train Loss: 48457.3824, Test Loss: 26426.3143\n",
      "Epoch [131/1000], Train Loss: 49605.9786, Test Loss: 24529.2953\n",
      "Epoch [132/1000], Train Loss: 48353.7628, Test Loss: 25558.8305\n",
      "Epoch [133/1000], Train Loss: 47155.0230, Test Loss: 24530.2215\n",
      "Epoch [134/1000], Train Loss: 46398.6418, Test Loss: 24003.2215\n",
      "Epoch [135/1000], Train Loss: 46064.2903, Test Loss: 25254.7756\n",
      "Epoch [136/1000], Train Loss: 47598.2830, Test Loss: 25667.9202\n",
      "Epoch [137/1000], Train Loss: 46532.1085, Test Loss: 26071.2375\n",
      "Epoch [138/1000], Train Loss: 46167.7355, Test Loss: 24066.0430\n",
      "Epoch [139/1000], Train Loss: 45930.7014, Test Loss: 26390.1797\n",
      "Epoch [140/1000], Train Loss: 45255.3972, Test Loss: 25307.4010\n",
      "Epoch [141/1000], Train Loss: 44782.9937, Test Loss: 25585.9902\n",
      "Epoch [142/1000], Train Loss: 45381.7541, Test Loss: 27658.8855\n",
      "Epoch [143/1000], Train Loss: 48288.5819, Test Loss: 26384.5676\n",
      "Epoch [144/1000], Train Loss: 44396.0145, Test Loss: 26201.4354\n",
      "Epoch [145/1000], Train Loss: 44923.0215, Test Loss: 27063.2369\n",
      "Epoch [146/1000], Train Loss: 44839.8876, Test Loss: 28089.7329\n",
      "Epoch [147/1000], Train Loss: 43932.2779, Test Loss: 24849.8600\n",
      "Epoch [148/1000], Train Loss: 42932.2279, Test Loss: 27171.9442\n",
      "Epoch [149/1000], Train Loss: 43484.1786, Test Loss: 24448.7746\n",
      "Epoch [150/1000], Train Loss: 44228.4198, Test Loss: 23493.2858\n",
      "Epoch [151/1000], Train Loss: 44761.8514, Test Loss: 28052.3298\n",
      "Epoch [152/1000], Train Loss: 42306.5091, Test Loss: 22994.1091\n",
      "Epoch [153/1000], Train Loss: 42012.7059, Test Loss: 24216.3042\n",
      "Epoch [154/1000], Train Loss: 42446.0187, Test Loss: 27275.7457\n",
      "Epoch [155/1000], Train Loss: 42223.3228, Test Loss: 28361.5519\n",
      "Epoch [156/1000], Train Loss: 42780.0799, Test Loss: 26106.1362\n",
      "Epoch [157/1000], Train Loss: 41903.7333, Test Loss: 27456.5679\n",
      "Epoch [158/1000], Train Loss: 42603.4755, Test Loss: 25329.9357\n",
      "Epoch [159/1000], Train Loss: 41641.4794, Test Loss: 27059.4610\n",
      "Epoch [160/1000], Train Loss: 41859.6207, Test Loss: 25451.2415\n",
      "Epoch [161/1000], Train Loss: 42520.8762, Test Loss: 24443.0126\n",
      "Epoch [162/1000], Train Loss: 40124.5842, Test Loss: 23835.8458\n",
      "Epoch [163/1000], Train Loss: 41472.9894, Test Loss: 23955.2395\n",
      "Epoch [164/1000], Train Loss: 42575.4985, Test Loss: 24769.2493\n",
      "Epoch [165/1000], Train Loss: 40511.7995, Test Loss: 25730.8367\n",
      "Epoch [166/1000], Train Loss: 40182.6886, Test Loss: 24236.7190\n",
      "Epoch [167/1000], Train Loss: 40649.8554, Test Loss: 26185.5700\n",
      "Epoch [168/1000], Train Loss: 41462.8360, Test Loss: 24404.6194\n",
      "Epoch [169/1000], Train Loss: 46454.9238, Test Loss: 28096.8320\n",
      "Epoch [170/1000], Train Loss: 43304.1828, Test Loss: 27477.4408\n",
      "Epoch [171/1000], Train Loss: 39722.0296, Test Loss: 24769.1368\n",
      "Epoch [172/1000], Train Loss: 38899.1173, Test Loss: 24986.6659\n",
      "Epoch [173/1000], Train Loss: 38934.1429, Test Loss: 27087.0656\n",
      "Epoch [174/1000], Train Loss: 38828.9525, Test Loss: 25358.2928\n",
      "Epoch [175/1000], Train Loss: 43450.1405, Test Loss: 26087.6097\n",
      "Epoch [176/1000], Train Loss: 38656.7763, Test Loss: 24028.1273\n",
      "Epoch [177/1000], Train Loss: 38550.8314, Test Loss: 26172.9532\n",
      "Epoch [178/1000], Train Loss: 39146.8716, Test Loss: 25121.9815\n",
      "Epoch [179/1000], Train Loss: 40650.7259, Test Loss: 26223.0376\n",
      "Epoch [180/1000], Train Loss: 39698.3153, Test Loss: 27643.3905\n",
      "Epoch [181/1000], Train Loss: 38151.5040, Test Loss: 27503.6249\n",
      "Epoch [182/1000], Train Loss: 37832.1494, Test Loss: 25017.9126\n",
      "Epoch [183/1000], Train Loss: 37233.6057, Test Loss: 25208.7289\n",
      "Epoch [184/1000], Train Loss: 37553.2779, Test Loss: 25436.7216\n",
      "Epoch [185/1000], Train Loss: 37735.0436, Test Loss: 24539.0567\n",
      "Epoch [186/1000], Train Loss: 36947.9584, Test Loss: 27629.7824\n",
      "Epoch [187/1000], Train Loss: 39798.9483, Test Loss: 26005.5243\n",
      "Epoch [188/1000], Train Loss: 36680.4127, Test Loss: 26163.6042\n",
      "Epoch [189/1000], Train Loss: 37993.7702, Test Loss: 27307.3268\n",
      "Epoch [190/1000], Train Loss: 36140.1984, Test Loss: 25929.2082\n",
      "Epoch [191/1000], Train Loss: 37752.2114, Test Loss: 29720.2553\n",
      "Epoch [192/1000], Train Loss: 40363.8320, Test Loss: 26656.9156\n",
      "Epoch [193/1000], Train Loss: 37243.3788, Test Loss: 24873.0463\n",
      "Epoch [194/1000], Train Loss: 35763.3113, Test Loss: 25910.3051\n",
      "Epoch [195/1000], Train Loss: 37200.4008, Test Loss: 23468.2672\n",
      "Epoch [196/1000], Train Loss: 37271.3752, Test Loss: 26825.6123\n",
      "Epoch [197/1000], Train Loss: 35800.8206, Test Loss: 25226.5346\n",
      "Epoch [198/1000], Train Loss: 36245.8686, Test Loss: 25704.9415\n",
      "Epoch [199/1000], Train Loss: 35505.0412, Test Loss: 24555.7604\n",
      "Epoch [200/1000], Train Loss: 36386.6694, Test Loss: 25890.9241\n",
      "Epoch [201/1000], Train Loss: 35448.8695, Test Loss: 26293.4443\n",
      "Epoch [202/1000], Train Loss: 35719.9395, Test Loss: 26187.8449\n",
      "Epoch [203/1000], Train Loss: 36605.9290, Test Loss: 29107.9065\n",
      "Epoch [204/1000], Train Loss: 36091.6251, Test Loss: 27219.2432\n",
      "Epoch [205/1000], Train Loss: 37918.3433, Test Loss: 27197.7812\n",
      "Epoch [206/1000], Train Loss: 36073.7942, Test Loss: 24436.1874\n",
      "Epoch [207/1000], Train Loss: 34932.5217, Test Loss: 23737.0273\n",
      "Epoch [208/1000], Train Loss: 34986.7071, Test Loss: 24544.6346\n",
      "Epoch [209/1000], Train Loss: 34099.9588, Test Loss: 25052.9323\n",
      "Epoch [210/1000], Train Loss: 34631.3756, Test Loss: 25714.3030\n",
      "Epoch [211/1000], Train Loss: 34887.7091, Test Loss: 24036.1681\n",
      "Epoch [212/1000], Train Loss: 34666.1990, Test Loss: 23742.0461\n",
      "Epoch [213/1000], Train Loss: 36315.0556, Test Loss: 25182.1757\n",
      "Epoch [214/1000], Train Loss: 37568.3248, Test Loss: 25924.4278\n",
      "Epoch [215/1000], Train Loss: 35422.3393, Test Loss: 24971.0522\n",
      "Epoch [216/1000], Train Loss: 34263.3895, Test Loss: 24424.8015\n",
      "Epoch [217/1000], Train Loss: 35232.8494, Test Loss: 27415.3414\n",
      "Epoch [218/1000], Train Loss: 40545.9201, Test Loss: 27044.3811\n",
      "Epoch [219/1000], Train Loss: 34720.7037, Test Loss: 25479.9600\n",
      "Epoch [220/1000], Train Loss: 34539.2366, Test Loss: 25188.9017\n",
      "Epoch [221/1000], Train Loss: 34515.8732, Test Loss: 28578.6051\n",
      "Epoch [222/1000], Train Loss: 34807.3998, Test Loss: 26578.8108\n",
      "Epoch [223/1000], Train Loss: 34078.0556, Test Loss: 26961.6448\n",
      "Epoch [224/1000], Train Loss: 34567.3889, Test Loss: 26737.4904\n",
      "Epoch [225/1000], Train Loss: 34803.2982, Test Loss: 26456.2884\n",
      "Epoch [226/1000], Train Loss: 33439.3999, Test Loss: 22276.7884\n",
      "Epoch [227/1000], Train Loss: 33240.3218, Test Loss: 25800.2842\n",
      "Epoch [228/1000], Train Loss: 32199.2737, Test Loss: 25711.3616\n",
      "Epoch [229/1000], Train Loss: 32580.2374, Test Loss: 26552.6397\n",
      "Epoch [230/1000], Train Loss: 35116.7992, Test Loss: 27275.5696\n",
      "Epoch [231/1000], Train Loss: 34049.3683, Test Loss: 25128.4920\n",
      "Epoch [232/1000], Train Loss: 33169.6324, Test Loss: 26206.8877\n",
      "Epoch [233/1000], Train Loss: 33081.5395, Test Loss: 23714.6073\n",
      "Epoch [234/1000], Train Loss: 32637.6897, Test Loss: 24790.3450\n",
      "Epoch [235/1000], Train Loss: 32750.7404, Test Loss: 24812.7631\n",
      "Epoch [236/1000], Train Loss: 32969.1026, Test Loss: 24781.2031\n",
      "Epoch [237/1000], Train Loss: 32443.2772, Test Loss: 26164.5468\n",
      "Epoch [238/1000], Train Loss: 32523.3181, Test Loss: 25927.3165\n",
      "Epoch [239/1000], Train Loss: 33338.8003, Test Loss: 24759.3252\n",
      "Epoch [240/1000], Train Loss: 32565.1984, Test Loss: 25868.1010\n",
      "Epoch [241/1000], Train Loss: 33186.4784, Test Loss: 28023.9807\n",
      "Epoch [242/1000], Train Loss: 32121.8306, Test Loss: 23863.3083\n",
      "Epoch [243/1000], Train Loss: 31322.3340, Test Loss: 24740.6502\n",
      "Epoch [244/1000], Train Loss: 31487.4730, Test Loss: 25322.0944\n",
      "Epoch [245/1000], Train Loss: 35280.3156, Test Loss: 26803.6613\n",
      "Epoch [246/1000], Train Loss: 32680.4804, Test Loss: 26784.1836\n",
      "Epoch [247/1000], Train Loss: 33250.3707, Test Loss: 24934.7272\n",
      "Epoch [248/1000], Train Loss: 31639.9700, Test Loss: 25928.1760\n",
      "Epoch [249/1000], Train Loss: 32121.7152, Test Loss: 26485.3103\n",
      "Epoch [250/1000], Train Loss: 34431.0438, Test Loss: 24473.5285\n",
      "Epoch [251/1000], Train Loss: 31224.9520, Test Loss: 28184.1902\n",
      "Epoch [252/1000], Train Loss: 30664.7077, Test Loss: 25826.0138\n",
      "Epoch [253/1000], Train Loss: 31143.8906, Test Loss: 26194.9787\n",
      "Epoch [254/1000], Train Loss: 31797.3391, Test Loss: 25770.6376\n",
      "Epoch [255/1000], Train Loss: 34772.9079, Test Loss: 25268.3454\n",
      "Epoch [256/1000], Train Loss: 32423.6335, Test Loss: 26847.9642\n",
      "Epoch [257/1000], Train Loss: 30778.9646, Test Loss: 25386.4572\n",
      "Epoch [258/1000], Train Loss: 32246.4701, Test Loss: 24509.8356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [259/1000], Train Loss: 32565.8465, Test Loss: 26282.4735\n",
      "Epoch [260/1000], Train Loss: 30932.1760, Test Loss: 24765.5404\n",
      "Epoch [261/1000], Train Loss: 31872.1624, Test Loss: 26969.0582\n",
      "Epoch [262/1000], Train Loss: 30474.2945, Test Loss: 24498.6760\n",
      "Epoch [263/1000], Train Loss: 31603.4303, Test Loss: 27685.9811\n",
      "Epoch [264/1000], Train Loss: 30935.7200, Test Loss: 27993.1984\n",
      "Epoch [265/1000], Train Loss: 30599.5765, Test Loss: 25183.5627\n",
      "Epoch [266/1000], Train Loss: 30703.1562, Test Loss: 26915.3775\n",
      "Epoch [267/1000], Train Loss: 29907.7640, Test Loss: 26254.5184\n",
      "Epoch [268/1000], Train Loss: 29773.6716, Test Loss: 24940.4028\n",
      "Epoch [269/1000], Train Loss: 29621.4986, Test Loss: 25162.7956\n",
      "Epoch [270/1000], Train Loss: 29858.7317, Test Loss: 25105.4347\n",
      "Epoch [271/1000], Train Loss: 31372.6743, Test Loss: 25707.9946\n",
      "Epoch [272/1000], Train Loss: 29734.4327, Test Loss: 23528.3218\n",
      "Epoch [273/1000], Train Loss: 33200.0561, Test Loss: 26431.2644\n",
      "Epoch [274/1000], Train Loss: 29488.3948, Test Loss: 25620.7224\n",
      "Epoch [275/1000], Train Loss: 29978.6005, Test Loss: 25535.9386\n",
      "Epoch [276/1000], Train Loss: 29799.0908, Test Loss: 27689.8948\n",
      "Epoch [277/1000], Train Loss: 29825.2694, Test Loss: 24969.1124\n",
      "Epoch [278/1000], Train Loss: 28636.9307, Test Loss: 25246.6695\n",
      "Epoch [279/1000], Train Loss: 29486.3908, Test Loss: 24084.9182\n",
      "Epoch [280/1000], Train Loss: 29589.2680, Test Loss: 24031.6222\n",
      "Epoch [281/1000], Train Loss: 30740.2995, Test Loss: 24752.3577\n",
      "Epoch [282/1000], Train Loss: 29234.4783, Test Loss: 26137.6470\n",
      "Epoch [283/1000], Train Loss: 32093.8017, Test Loss: 23196.5329\n",
      "Epoch [284/1000], Train Loss: 29051.4744, Test Loss: 26486.7665\n",
      "Epoch [285/1000], Train Loss: 29530.0286, Test Loss: 26801.4279\n",
      "Epoch [286/1000], Train Loss: 28413.9343, Test Loss: 25149.5132\n",
      "Epoch [287/1000], Train Loss: 29471.9928, Test Loss: 26606.7013\n",
      "Epoch [288/1000], Train Loss: 29368.9487, Test Loss: 25154.4958\n",
      "Epoch [289/1000], Train Loss: 28566.8144, Test Loss: 27408.4682\n",
      "Epoch [290/1000], Train Loss: 30138.2038, Test Loss: 24958.7638\n",
      "Epoch [291/1000], Train Loss: 30437.9550, Test Loss: 26863.3502\n",
      "Epoch [292/1000], Train Loss: 29584.3995, Test Loss: 25222.2874\n",
      "Epoch [293/1000], Train Loss: 29402.0522, Test Loss: 28644.4612\n",
      "Epoch [294/1000], Train Loss: 28230.9511, Test Loss: 25499.7150\n",
      "Epoch [295/1000], Train Loss: 27940.4431, Test Loss: 27629.7125\n",
      "Epoch [296/1000], Train Loss: 27776.6866, Test Loss: 24764.4501\n",
      "Epoch [297/1000], Train Loss: 28198.8629, Test Loss: 23509.3343\n",
      "Epoch [298/1000], Train Loss: 28596.3686, Test Loss: 26045.4629\n",
      "Epoch [299/1000], Train Loss: 30162.7631, Test Loss: 24387.9216\n",
      "Epoch [300/1000], Train Loss: 30485.0944, Test Loss: 26044.9533\n",
      "Epoch [301/1000], Train Loss: 27759.0486, Test Loss: 25787.6114\n",
      "Epoch [302/1000], Train Loss: 27713.7566, Test Loss: 24936.6276\n",
      "Epoch [303/1000], Train Loss: 27838.7841, Test Loss: 27360.3053\n",
      "Epoch [304/1000], Train Loss: 30529.3940, Test Loss: 24772.9488\n",
      "Epoch [305/1000], Train Loss: 28528.2863, Test Loss: 26034.0798\n",
      "Epoch [306/1000], Train Loss: 28699.6004, Test Loss: 25623.4070\n",
      "Epoch [307/1000], Train Loss: 29129.7750, Test Loss: 25883.4605\n",
      "Epoch [308/1000], Train Loss: 28653.4212, Test Loss: 23953.9871\n",
      "Epoch [309/1000], Train Loss: 28884.3711, Test Loss: 23611.2075\n",
      "Epoch [310/1000], Train Loss: 28997.1947, Test Loss: 25571.9283\n",
      "Epoch [311/1000], Train Loss: 28519.3013, Test Loss: 26083.8210\n",
      "Epoch [312/1000], Train Loss: 28694.1511, Test Loss: 25381.9281\n",
      "Epoch [313/1000], Train Loss: 29756.4647, Test Loss: 26420.7307\n",
      "Epoch [314/1000], Train Loss: 28639.1060, Test Loss: 24752.7788\n",
      "Epoch [315/1000], Train Loss: 27832.0674, Test Loss: 25165.9683\n",
      "Epoch [316/1000], Train Loss: 28214.3376, Test Loss: 27187.0387\n",
      "Epoch [317/1000], Train Loss: 28590.9519, Test Loss: 27085.7281\n",
      "Epoch [318/1000], Train Loss: 28560.7391, Test Loss: 25278.4969\n",
      "Epoch [319/1000], Train Loss: 26662.9099, Test Loss: 25563.1521\n",
      "Epoch [320/1000], Train Loss: 29382.6134, Test Loss: 27382.7950\n",
      "Epoch [321/1000], Train Loss: 28075.8705, Test Loss: 25062.4602\n",
      "Epoch [322/1000], Train Loss: 27846.2650, Test Loss: 24918.8158\n",
      "Epoch [323/1000], Train Loss: 27108.7101, Test Loss: 26078.6216\n",
      "Epoch [324/1000], Train Loss: 27826.5173, Test Loss: 26870.0408\n",
      "Epoch [325/1000], Train Loss: 28822.3982, Test Loss: 28246.2333\n",
      "Epoch [326/1000], Train Loss: 27898.1348, Test Loss: 23973.2474\n",
      "Epoch [327/1000], Train Loss: 27566.6085, Test Loss: 25827.5648\n",
      "Epoch [328/1000], Train Loss: 27477.4789, Test Loss: 27560.5806\n",
      "Epoch [329/1000], Train Loss: 27572.6713, Test Loss: 24960.9366\n",
      "Epoch [330/1000], Train Loss: 28090.2343, Test Loss: 25015.3121\n",
      "Epoch [331/1000], Train Loss: 29164.7379, Test Loss: 23816.8038\n",
      "Epoch [332/1000], Train Loss: 28100.1118, Test Loss: 25346.7872\n",
      "Epoch [333/1000], Train Loss: 33762.5799, Test Loss: 27320.4675\n",
      "Epoch [334/1000], Train Loss: 31315.5129, Test Loss: 26198.9701\n",
      "Epoch [335/1000], Train Loss: 28525.2755, Test Loss: 25534.1019\n",
      "Epoch [336/1000], Train Loss: 27507.0926, Test Loss: 24127.0691\n",
      "Epoch [337/1000], Train Loss: 26794.6673, Test Loss: 25467.0272\n",
      "Epoch [338/1000], Train Loss: 26405.2226, Test Loss: 26117.7412\n",
      "Epoch [339/1000], Train Loss: 28368.5240, Test Loss: 25347.0000\n",
      "Epoch [340/1000], Train Loss: 27227.5376, Test Loss: 23552.5616\n",
      "Epoch [341/1000], Train Loss: 27499.1688, Test Loss: 27473.1820\n",
      "Epoch [342/1000], Train Loss: 28350.6014, Test Loss: 24056.4186\n",
      "Epoch [343/1000], Train Loss: 26716.7949, Test Loss: 25768.7725\n",
      "Epoch [344/1000], Train Loss: 26872.5880, Test Loss: 26180.3512\n",
      "Epoch [345/1000], Train Loss: 29266.7389, Test Loss: 23844.9179\n",
      "Epoch [346/1000], Train Loss: 29551.7322, Test Loss: 25594.7175\n",
      "Epoch [347/1000], Train Loss: 28974.5576, Test Loss: 24524.5958\n",
      "Epoch [348/1000], Train Loss: 27858.6178, Test Loss: 25792.2240\n",
      "Epoch [349/1000], Train Loss: 27601.0281, Test Loss: 24516.8988\n",
      "Epoch [350/1000], Train Loss: 25949.3414, Test Loss: 26794.9203\n",
      "Epoch [351/1000], Train Loss: 28459.5835, Test Loss: 26541.7463\n",
      "Epoch [352/1000], Train Loss: 27843.3343, Test Loss: 26610.2288\n",
      "Epoch [353/1000], Train Loss: 27125.9964, Test Loss: 26833.1013\n",
      "Epoch [354/1000], Train Loss: 29760.2093, Test Loss: 27973.8610\n",
      "Epoch [355/1000], Train Loss: 30071.7986, Test Loss: 24897.2471\n",
      "Epoch [356/1000], Train Loss: 26805.4832, Test Loss: 26125.0559\n",
      "Epoch [357/1000], Train Loss: 25925.0752, Test Loss: 26810.6328\n",
      "Epoch [358/1000], Train Loss: 26764.7847, Test Loss: 25043.8259\n",
      "Epoch [359/1000], Train Loss: 30217.9519, Test Loss: 26669.0772\n",
      "Epoch [360/1000], Train Loss: 27015.2149, Test Loss: 27272.6439\n",
      "Epoch [361/1000], Train Loss: 27993.0709, Test Loss: 24707.1175\n",
      "Epoch [362/1000], Train Loss: 26445.6596, Test Loss: 27294.8741\n",
      "Epoch [363/1000], Train Loss: 25712.4731, Test Loss: 24846.4471\n",
      "Epoch [364/1000], Train Loss: 25435.6074, Test Loss: 24282.8977\n",
      "Epoch [365/1000], Train Loss: 27397.9522, Test Loss: 24580.4344\n",
      "Epoch [366/1000], Train Loss: 27433.6578, Test Loss: 25858.9202\n",
      "Epoch [367/1000], Train Loss: 28214.6184, Test Loss: 25668.8735\n",
      "Epoch [368/1000], Train Loss: 26072.7119, Test Loss: 25686.5914\n",
      "Epoch [369/1000], Train Loss: 25665.8250, Test Loss: 24869.1186\n",
      "Epoch [370/1000], Train Loss: 25213.9193, Test Loss: 26234.2210\n",
      "Epoch [371/1000], Train Loss: 24548.1618, Test Loss: 26017.6425\n",
      "Epoch [372/1000], Train Loss: 26757.2152, Test Loss: 25584.6627\n",
      "Epoch [373/1000], Train Loss: 26133.7283, Test Loss: 27235.1507\n",
      "Epoch [374/1000], Train Loss: 26275.1534, Test Loss: 24423.9331\n",
      "Epoch [375/1000], Train Loss: 28188.4075, Test Loss: 25355.6443\n",
      "Epoch [376/1000], Train Loss: 24893.1461, Test Loss: 25883.4392\n",
      "Epoch [377/1000], Train Loss: 25605.3743, Test Loss: 26417.0419\n",
      "Epoch [378/1000], Train Loss: 25385.2308, Test Loss: 25011.1069\n",
      "Epoch [379/1000], Train Loss: 25504.6435, Test Loss: 27452.2176\n",
      "Epoch [380/1000], Train Loss: 25251.3638, Test Loss: 27356.6322\n",
      "Epoch [381/1000], Train Loss: 24492.9852, Test Loss: 25436.0662\n",
      "Epoch [382/1000], Train Loss: 25093.3598, Test Loss: 25714.9202\n",
      "Epoch [383/1000], Train Loss: 25004.0507, Test Loss: 24169.4566\n",
      "Epoch [384/1000], Train Loss: 26026.9347, Test Loss: 26429.7767\n",
      "Epoch [385/1000], Train Loss: 28181.1794, Test Loss: 25700.5241\n",
      "Epoch [386/1000], Train Loss: 25034.2249, Test Loss: 24674.8114\n",
      "Epoch [387/1000], Train Loss: 26896.5468, Test Loss: 27034.5264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [388/1000], Train Loss: 25714.2524, Test Loss: 24436.8986\n",
      "Epoch [389/1000], Train Loss: 24405.3175, Test Loss: 27559.7167\n",
      "Epoch [390/1000], Train Loss: 25666.5161, Test Loss: 26853.8151\n",
      "Epoch [391/1000], Train Loss: 25733.6577, Test Loss: 25423.8202\n",
      "Epoch [392/1000], Train Loss: 28327.9870, Test Loss: 27224.7444\n",
      "Epoch [393/1000], Train Loss: 25008.5312, Test Loss: 26020.7690\n",
      "Epoch [394/1000], Train Loss: 24760.8592, Test Loss: 25505.6151\n",
      "Epoch [395/1000], Train Loss: 25746.1592, Test Loss: 26501.6617\n",
      "Epoch [396/1000], Train Loss: 25291.1972, Test Loss: 23267.9217\n",
      "Epoch [397/1000], Train Loss: 25287.3901, Test Loss: 24241.1724\n",
      "Epoch [398/1000], Train Loss: 25735.1622, Test Loss: 24822.1376\n",
      "Epoch [399/1000], Train Loss: 25541.4939, Test Loss: 27030.9661\n",
      "Epoch [400/1000], Train Loss: 27795.1708, Test Loss: 26423.1758\n",
      "Epoch [401/1000], Train Loss: 25183.9809, Test Loss: 24710.3867\n",
      "Epoch [402/1000], Train Loss: 25058.5157, Test Loss: 26031.3412\n",
      "Epoch [403/1000], Train Loss: 24972.8879, Test Loss: 26159.9702\n",
      "Epoch [404/1000], Train Loss: 24492.3017, Test Loss: 28402.8297\n",
      "Epoch [405/1000], Train Loss: 24190.8646, Test Loss: 25282.0908\n",
      "Epoch [406/1000], Train Loss: 23938.7789, Test Loss: 26161.4728\n",
      "Epoch [407/1000], Train Loss: 24825.4851, Test Loss: 24375.0554\n",
      "Epoch [408/1000], Train Loss: 24800.5996, Test Loss: 24802.6083\n",
      "Epoch [409/1000], Train Loss: 25384.7294, Test Loss: 26702.1602\n",
      "Epoch [410/1000], Train Loss: 25142.2146, Test Loss: 24300.3725\n",
      "Epoch [411/1000], Train Loss: 25317.7289, Test Loss: 25848.6858\n",
      "Epoch [412/1000], Train Loss: 33011.1314, Test Loss: 26505.5600\n",
      "Epoch [413/1000], Train Loss: 27154.6222, Test Loss: 25079.4693\n",
      "Epoch [414/1000], Train Loss: 24532.0258, Test Loss: 26508.7234\n",
      "Epoch [415/1000], Train Loss: 23949.8719, Test Loss: 25705.6170\n",
      "Epoch [416/1000], Train Loss: 24323.8994, Test Loss: 25334.1756\n",
      "Epoch [417/1000], Train Loss: 26314.5814, Test Loss: 26380.5527\n",
      "Epoch [418/1000], Train Loss: 24828.8094, Test Loss: 24510.0611\n",
      "Epoch [419/1000], Train Loss: 25591.8342, Test Loss: 25713.2503\n",
      "Epoch [420/1000], Train Loss: 25467.1538, Test Loss: 24833.3777\n",
      "Epoch [421/1000], Train Loss: 23710.4159, Test Loss: 25765.2605\n",
      "Epoch [422/1000], Train Loss: 26680.6060, Test Loss: 26565.3933\n",
      "Epoch [423/1000], Train Loss: 25453.1228, Test Loss: 25459.4701\n",
      "Epoch [424/1000], Train Loss: 23521.7843, Test Loss: 24499.1188\n",
      "Epoch [425/1000], Train Loss: 23213.4447, Test Loss: 26297.7488\n",
      "Epoch [426/1000], Train Loss: 24287.4598, Test Loss: 25474.0009\n",
      "Epoch [427/1000], Train Loss: 25138.2689, Test Loss: 23678.7026\n",
      "Epoch [428/1000], Train Loss: 24440.8403, Test Loss: 25864.6085\n",
      "Epoch [429/1000], Train Loss: 31031.1023, Test Loss: 26742.3101\n",
      "Epoch [430/1000], Train Loss: 23601.5627, Test Loss: 25960.5988\n",
      "Epoch [431/1000], Train Loss: 26000.6237, Test Loss: 24516.1242\n",
      "Epoch [432/1000], Train Loss: 24976.1949, Test Loss: 26925.0689\n",
      "Epoch [433/1000], Train Loss: 24170.0779, Test Loss: 26926.0763\n",
      "Epoch [434/1000], Train Loss: 24012.6571, Test Loss: 25363.7004\n",
      "Epoch [435/1000], Train Loss: 25074.7293, Test Loss: 25346.7929\n",
      "Epoch [436/1000], Train Loss: 23377.4474, Test Loss: 25741.4647\n",
      "Epoch [437/1000], Train Loss: 24749.9329, Test Loss: 25367.6211\n",
      "Epoch [438/1000], Train Loss: 24978.5559, Test Loss: 25269.2767\n",
      "Epoch [439/1000], Train Loss: 25312.2103, Test Loss: 25652.8214\n",
      "Epoch [440/1000], Train Loss: 23402.9211, Test Loss: 25210.3699\n",
      "Epoch [441/1000], Train Loss: 24565.9658, Test Loss: 26353.7464\n",
      "Epoch [442/1000], Train Loss: 25630.6898, Test Loss: 25364.2479\n",
      "Epoch [443/1000], Train Loss: 23151.1120, Test Loss: 25570.3885\n",
      "Epoch [444/1000], Train Loss: 23526.0713, Test Loss: 25293.5751\n",
      "Epoch [445/1000], Train Loss: 23746.5819, Test Loss: 24964.0315\n",
      "Epoch [446/1000], Train Loss: 23343.5383, Test Loss: 26022.3093\n",
      "Epoch [447/1000], Train Loss: 24958.8382, Test Loss: 25211.5244\n",
      "Epoch [448/1000], Train Loss: 23921.0471, Test Loss: 24193.2408\n",
      "Epoch [449/1000], Train Loss: 23687.1306, Test Loss: 24380.5962\n",
      "Epoch [450/1000], Train Loss: 23833.3041, Test Loss: 25912.7689\n",
      "Epoch [451/1000], Train Loss: 23706.8793, Test Loss: 23862.0079\n",
      "Epoch [452/1000], Train Loss: 24271.6815, Test Loss: 24848.3947\n",
      "Epoch [453/1000], Train Loss: 23879.4094, Test Loss: 23709.0857\n",
      "Epoch [454/1000], Train Loss: 23140.9360, Test Loss: 24615.5365\n",
      "Epoch [455/1000], Train Loss: 22945.5351, Test Loss: 25461.2273\n",
      "Epoch [456/1000], Train Loss: 23183.8678, Test Loss: 24844.3048\n",
      "Epoch [457/1000], Train Loss: 23558.3025, Test Loss: 25472.3362\n",
      "Epoch [458/1000], Train Loss: 22131.8793, Test Loss: 24375.2254\n",
      "Epoch [459/1000], Train Loss: 32776.5444, Test Loss: 25183.7232\n",
      "Epoch [460/1000], Train Loss: 22663.1678, Test Loss: 24637.3823\n",
      "Epoch [461/1000], Train Loss: 22109.6607, Test Loss: 25630.0690\n",
      "Epoch [462/1000], Train Loss: 23280.1712, Test Loss: 24333.4774\n",
      "Epoch [463/1000], Train Loss: 22806.9191, Test Loss: 24009.5155\n",
      "Epoch [464/1000], Train Loss: 23578.6118, Test Loss: 24537.4193\n",
      "Epoch [465/1000], Train Loss: 23002.7375, Test Loss: 26620.7314\n",
      "Epoch [466/1000], Train Loss: 22755.7785, Test Loss: 24203.5860\n",
      "Epoch [467/1000], Train Loss: 26257.9308, Test Loss: 24942.6261\n",
      "Epoch [468/1000], Train Loss: 23520.7691, Test Loss: 23898.5441\n",
      "Epoch [469/1000], Train Loss: 27769.4522, Test Loss: 24971.6914\n",
      "Epoch [470/1000], Train Loss: 22920.6880, Test Loss: 23837.1876\n",
      "Epoch [471/1000], Train Loss: 22208.6093, Test Loss: 24488.3868\n",
      "Epoch [472/1000], Train Loss: 24663.1506, Test Loss: 23718.7531\n",
      "Epoch [473/1000], Train Loss: 23652.3085, Test Loss: 26066.3143\n",
      "Epoch [474/1000], Train Loss: 22907.5804, Test Loss: 25401.9019\n",
      "Epoch [475/1000], Train Loss: 22642.7564, Test Loss: 24444.4896\n",
      "Epoch [476/1000], Train Loss: 24131.6030, Test Loss: 24926.8008\n",
      "Epoch [477/1000], Train Loss: 23501.9421, Test Loss: 27066.0077\n",
      "Epoch [478/1000], Train Loss: 22437.3852, Test Loss: 24494.3787\n",
      "Epoch [479/1000], Train Loss: 22106.6852, Test Loss: 23068.3730\n",
      "Epoch [480/1000], Train Loss: 22482.5262, Test Loss: 26091.9529\n",
      "Epoch [481/1000], Train Loss: 22987.6877, Test Loss: 25040.0354\n",
      "Epoch [482/1000], Train Loss: 22490.6445, Test Loss: 25423.1841\n",
      "Epoch [483/1000], Train Loss: 21664.4600, Test Loss: 25322.7725\n",
      "Epoch [484/1000], Train Loss: 21948.1261, Test Loss: 25667.1450\n",
      "Epoch [485/1000], Train Loss: 21397.1002, Test Loss: 25325.6529\n",
      "Epoch [486/1000], Train Loss: 21855.9365, Test Loss: 24000.6307\n",
      "Epoch [487/1000], Train Loss: 22829.6277, Test Loss: 23466.0474\n",
      "Epoch [488/1000], Train Loss: 22696.4166, Test Loss: 23066.8291\n",
      "Epoch [489/1000], Train Loss: 22709.0438, Test Loss: 27576.8558\n",
      "Epoch [490/1000], Train Loss: 22548.8174, Test Loss: 27770.1752\n",
      "Epoch [491/1000], Train Loss: 29531.7202, Test Loss: 28237.9206\n",
      "Epoch [492/1000], Train Loss: 23506.3286, Test Loss: 25122.2916\n",
      "Epoch [493/1000], Train Loss: 21825.9477, Test Loss: 24721.7476\n",
      "Epoch [494/1000], Train Loss: 21477.5063, Test Loss: 23937.4888\n",
      "Epoch [495/1000], Train Loss: 21975.0551, Test Loss: 25194.7158\n",
      "Epoch [496/1000], Train Loss: 21831.1989, Test Loss: 24916.6813\n",
      "Epoch [497/1000], Train Loss: 21349.4810, Test Loss: 24738.3350\n",
      "Epoch [498/1000], Train Loss: 21582.8905, Test Loss: 24743.3221\n",
      "Epoch [499/1000], Train Loss: 20869.2788, Test Loss: 24901.6749\n",
      "Epoch [500/1000], Train Loss: 21509.6628, Test Loss: 24643.5522\n",
      "Epoch [501/1000], Train Loss: 22444.3159, Test Loss: 24318.3437\n",
      "Epoch [502/1000], Train Loss: 21655.7957, Test Loss: 25395.2659\n",
      "Epoch [503/1000], Train Loss: 22416.4240, Test Loss: 25955.0889\n",
      "Epoch [504/1000], Train Loss: 21479.9646, Test Loss: 25541.9982\n",
      "Epoch [505/1000], Train Loss: 21767.2531, Test Loss: 27608.1473\n",
      "Epoch [506/1000], Train Loss: 20818.7417, Test Loss: 24964.8648\n",
      "Epoch [507/1000], Train Loss: 22738.2120, Test Loss: 24957.2924\n",
      "Epoch [508/1000], Train Loss: 21154.5893, Test Loss: 24387.0807\n",
      "Epoch [509/1000], Train Loss: 21256.3434, Test Loss: 26096.1617\n",
      "Epoch [510/1000], Train Loss: 21011.9261, Test Loss: 24063.8367\n",
      "Epoch [511/1000], Train Loss: 22560.9973, Test Loss: 25706.6398\n",
      "Epoch [512/1000], Train Loss: 26409.1432, Test Loss: 26026.8183\n",
      "Epoch [513/1000], Train Loss: 22720.6710, Test Loss: 24637.6713\n",
      "Epoch [514/1000], Train Loss: 22331.8458, Test Loss: 25538.3742\n",
      "Epoch [515/1000], Train Loss: 20335.0246, Test Loss: 25031.8789\n",
      "Epoch [516/1000], Train Loss: 20847.2890, Test Loss: 26630.5183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [517/1000], Train Loss: 21980.3275, Test Loss: 25906.0091\n",
      "Epoch [518/1000], Train Loss: 21784.3831, Test Loss: 26685.1008\n",
      "Epoch [519/1000], Train Loss: 22082.5539, Test Loss: 25716.3421\n",
      "Epoch [520/1000], Train Loss: 24124.1364, Test Loss: 25120.8876\n",
      "Epoch [521/1000], Train Loss: 20834.6228, Test Loss: 23020.6340\n",
      "Epoch [522/1000], Train Loss: 21310.0788, Test Loss: 25959.0830\n",
      "Epoch [523/1000], Train Loss: 22873.3673, Test Loss: 24162.6738\n",
      "Epoch [524/1000], Train Loss: 21523.4321, Test Loss: 25294.7373\n",
      "Epoch [525/1000], Train Loss: 20885.7011, Test Loss: 24836.5376\n",
      "Epoch [526/1000], Train Loss: 20732.9401, Test Loss: 25201.7940\n",
      "Epoch [527/1000], Train Loss: 21146.1985, Test Loss: 25914.0296\n",
      "Epoch [528/1000], Train Loss: 22519.9685, Test Loss: 23904.2839\n",
      "Epoch [529/1000], Train Loss: 23902.6708, Test Loss: 25836.4244\n",
      "Epoch [530/1000], Train Loss: 21149.4006, Test Loss: 25522.0437\n",
      "Epoch [531/1000], Train Loss: 20719.2594, Test Loss: 25237.5003\n",
      "Epoch [532/1000], Train Loss: 21911.8745, Test Loss: 23877.6249\n",
      "Epoch [533/1000], Train Loss: 20949.8067, Test Loss: 24450.0782\n",
      "Epoch [534/1000], Train Loss: 20452.2047, Test Loss: 24811.4313\n",
      "Epoch [535/1000], Train Loss: 20489.5430, Test Loss: 25724.6799\n",
      "Epoch [536/1000], Train Loss: 21516.2676, Test Loss: 25473.5668\n",
      "Epoch [537/1000], Train Loss: 21676.6516, Test Loss: 25084.3510\n",
      "Epoch [538/1000], Train Loss: 20657.5037, Test Loss: 24801.6526\n",
      "Epoch [539/1000], Train Loss: 21183.3363, Test Loss: 24628.4465\n",
      "Epoch [540/1000], Train Loss: 20838.4730, Test Loss: 25835.2095\n",
      "Epoch [541/1000], Train Loss: 21477.0847, Test Loss: 24808.9602\n",
      "Epoch [542/1000], Train Loss: 20625.3876, Test Loss: 24986.8045\n",
      "Epoch [543/1000], Train Loss: 21735.4197, Test Loss: 25295.2675\n",
      "Epoch [544/1000], Train Loss: 22028.0942, Test Loss: 24500.2414\n",
      "Epoch [545/1000], Train Loss: 23143.5264, Test Loss: 25980.5371\n",
      "Epoch [546/1000], Train Loss: 27940.0471, Test Loss: 24035.4337\n",
      "Epoch [547/1000], Train Loss: 23389.1909, Test Loss: 25715.0302\n",
      "Epoch [548/1000], Train Loss: 21982.4443, Test Loss: 24379.6496\n",
      "Epoch [549/1000], Train Loss: 21345.8420, Test Loss: 25022.6922\n",
      "Epoch [550/1000], Train Loss: 21177.6958, Test Loss: 22846.3447\n",
      "Epoch [551/1000], Train Loss: 21377.3640, Test Loss: 24719.2347\n",
      "Epoch [552/1000], Train Loss: 20712.0992, Test Loss: 23963.9411\n",
      "Epoch [553/1000], Train Loss: 20931.1872, Test Loss: 24451.7467\n",
      "Epoch [554/1000], Train Loss: 21737.0151, Test Loss: 23549.2141\n",
      "Epoch [555/1000], Train Loss: 20890.8839, Test Loss: 24946.6235\n",
      "Epoch [556/1000], Train Loss: 20242.6075, Test Loss: 25203.3767\n",
      "Epoch [557/1000], Train Loss: 19971.8690, Test Loss: 26048.3466\n",
      "Epoch [558/1000], Train Loss: 20440.3313, Test Loss: 24111.4956\n",
      "Epoch [559/1000], Train Loss: 21694.7082, Test Loss: 29686.2353\n",
      "Epoch [560/1000], Train Loss: 20887.4905, Test Loss: 25624.4700\n",
      "Epoch [561/1000], Train Loss: 21104.3929, Test Loss: 24377.6702\n",
      "Epoch [562/1000], Train Loss: 20513.3440, Test Loss: 26575.6640\n",
      "Epoch [563/1000], Train Loss: 20239.9665, Test Loss: 24661.0557\n",
      "Epoch [564/1000], Train Loss: 21495.9003, Test Loss: 24015.2867\n",
      "Epoch [565/1000], Train Loss: 20908.1774, Test Loss: 25278.0149\n",
      "Epoch [566/1000], Train Loss: 19680.9488, Test Loss: 23818.2246\n",
      "Epoch [567/1000], Train Loss: 20250.8418, Test Loss: 24444.1349\n",
      "Epoch [568/1000], Train Loss: 21281.4589, Test Loss: 24953.9361\n",
      "Epoch [569/1000], Train Loss: 20233.5119, Test Loss: 24835.1707\n",
      "Epoch [570/1000], Train Loss: 20462.5162, Test Loss: 25490.2035\n",
      "Epoch [571/1000], Train Loss: 20091.7034, Test Loss: 23030.2662\n",
      "Epoch [572/1000], Train Loss: 21104.8005, Test Loss: 24330.0390\n",
      "Epoch [573/1000], Train Loss: 21442.9612, Test Loss: 24192.7528\n",
      "Epoch [574/1000], Train Loss: 20077.1776, Test Loss: 24124.7757\n",
      "Epoch [575/1000], Train Loss: 19726.7374, Test Loss: 25689.1103\n",
      "Epoch [576/1000], Train Loss: 20365.6252, Test Loss: 24994.9178\n",
      "Epoch [577/1000], Train Loss: 22738.1767, Test Loss: 25316.4031\n",
      "Epoch [578/1000], Train Loss: 20939.9952, Test Loss: 24241.2569\n",
      "Epoch [579/1000], Train Loss: 20291.9992, Test Loss: 24665.0514\n",
      "Epoch [580/1000], Train Loss: 20021.8966, Test Loss: 26413.4104\n",
      "Epoch [581/1000], Train Loss: 20824.4117, Test Loss: 24338.0437\n",
      "Epoch [582/1000], Train Loss: 20467.1919, Test Loss: 24860.8506\n",
      "Epoch [583/1000], Train Loss: 20443.8738, Test Loss: 26467.1091\n",
      "Epoch [584/1000], Train Loss: 19793.7315, Test Loss: 24373.6429\n",
      "Epoch [585/1000], Train Loss: 20245.3857, Test Loss: 24584.4746\n",
      "Epoch [586/1000], Train Loss: 20782.7503, Test Loss: 25532.4617\n",
      "Epoch [587/1000], Train Loss: 20455.9119, Test Loss: 23891.8339\n",
      "Epoch [588/1000], Train Loss: 20894.7515, Test Loss: 24699.4951\n",
      "Epoch [589/1000], Train Loss: 20537.9090, Test Loss: 26214.2666\n",
      "Epoch [590/1000], Train Loss: 21071.9456, Test Loss: 24600.9683\n",
      "Epoch [591/1000], Train Loss: 21237.0750, Test Loss: 26400.5196\n",
      "Epoch [592/1000], Train Loss: 19876.1583, Test Loss: 24148.8687\n",
      "Epoch [593/1000], Train Loss: 19820.7413, Test Loss: 24471.5482\n",
      "Epoch [594/1000], Train Loss: 20049.3894, Test Loss: 24549.0385\n",
      "Epoch [595/1000], Train Loss: 20966.6610, Test Loss: 22484.1658\n",
      "Epoch [596/1000], Train Loss: 20839.6553, Test Loss: 26611.3691\n",
      "Epoch [597/1000], Train Loss: 19892.3347, Test Loss: 24936.1868\n",
      "Epoch [598/1000], Train Loss: 21491.6997, Test Loss: 23688.8532\n",
      "Epoch [599/1000], Train Loss: 20075.9938, Test Loss: 24641.7505\n",
      "Epoch [600/1000], Train Loss: 19359.2009, Test Loss: 24537.4981\n",
      "Epoch [601/1000], Train Loss: 19112.9975, Test Loss: 24891.2250\n",
      "Epoch [602/1000], Train Loss: 19404.3157, Test Loss: 24752.6000\n",
      "Epoch [603/1000], Train Loss: 20060.5760, Test Loss: 22453.0442\n",
      "Epoch [604/1000], Train Loss: 20837.1161, Test Loss: 23477.0740\n",
      "Epoch [605/1000], Train Loss: 19023.2733, Test Loss: 26161.2937\n",
      "Epoch [606/1000], Train Loss: 18784.2700, Test Loss: 22053.8900\n",
      "Epoch [607/1000], Train Loss: 19917.5705, Test Loss: 23209.2896\n",
      "Epoch [608/1000], Train Loss: 20135.6622, Test Loss: 25329.4524\n",
      "Epoch [609/1000], Train Loss: 20023.0737, Test Loss: 25866.5130\n",
      "Epoch [610/1000], Train Loss: 19830.2956, Test Loss: 25624.2337\n",
      "Epoch [611/1000], Train Loss: 19968.2612, Test Loss: 24065.1985\n",
      "Epoch [612/1000], Train Loss: 20248.1142, Test Loss: 23431.8558\n",
      "Epoch [613/1000], Train Loss: 21539.6549, Test Loss: 23801.5725\n",
      "Epoch [614/1000], Train Loss: 20782.0876, Test Loss: 24832.2038\n",
      "Epoch [615/1000], Train Loss: 19665.7776, Test Loss: 27024.0971\n",
      "Epoch [616/1000], Train Loss: 20169.9929, Test Loss: 26664.1841\n",
      "Epoch [617/1000], Train Loss: 21057.7234, Test Loss: 24624.6082\n",
      "Epoch [618/1000], Train Loss: 20296.7961, Test Loss: 26237.9093\n",
      "Epoch [619/1000], Train Loss: 19781.6384, Test Loss: 24484.7907\n",
      "Epoch [620/1000], Train Loss: 19647.3210, Test Loss: 27097.7817\n",
      "Epoch [621/1000], Train Loss: 19577.1370, Test Loss: 24633.9882\n",
      "Epoch [622/1000], Train Loss: 19416.3989, Test Loss: 24265.2121\n",
      "Epoch [623/1000], Train Loss: 19640.7626, Test Loss: 23878.0404\n",
      "Epoch [624/1000], Train Loss: 22613.2530, Test Loss: 24267.1060\n",
      "Epoch [625/1000], Train Loss: 19975.8738, Test Loss: 25219.0409\n",
      "Epoch [626/1000], Train Loss: 22606.7870, Test Loss: 23114.9658\n",
      "Epoch [627/1000], Train Loss: 19052.7616, Test Loss: 23509.5505\n",
      "Epoch [628/1000], Train Loss: 18486.4031, Test Loss: 23799.5560\n",
      "Epoch [629/1000], Train Loss: 19045.0121, Test Loss: 23181.7353\n",
      "Epoch [630/1000], Train Loss: 19838.4870, Test Loss: 24701.0752\n",
      "Epoch [631/1000], Train Loss: 18501.2244, Test Loss: 23798.8513\n",
      "Epoch [632/1000], Train Loss: 19957.7378, Test Loss: 25169.8831\n",
      "Epoch [633/1000], Train Loss: 19314.0048, Test Loss: 25284.8416\n",
      "Epoch [634/1000], Train Loss: 19236.5315, Test Loss: 24828.2877\n",
      "Epoch [635/1000], Train Loss: 19572.3372, Test Loss: 25299.0974\n",
      "Epoch [636/1000], Train Loss: 19211.5837, Test Loss: 24655.9769\n",
      "Epoch [637/1000], Train Loss: 20032.8556, Test Loss: 25487.3769\n",
      "Epoch [638/1000], Train Loss: 19735.9644, Test Loss: 24456.2936\n",
      "Epoch [639/1000], Train Loss: 19462.1357, Test Loss: 24877.7382\n",
      "Epoch [640/1000], Train Loss: 18732.2210, Test Loss: 25058.6676\n",
      "Epoch [641/1000], Train Loss: 19251.5157, Test Loss: 25748.5454\n",
      "Epoch [642/1000], Train Loss: 19243.8916, Test Loss: 25873.8534\n",
      "Epoch [643/1000], Train Loss: 21011.3459, Test Loss: 25433.5206\n",
      "Epoch [644/1000], Train Loss: 21960.4621, Test Loss: 24293.6711\n",
      "Epoch [645/1000], Train Loss: 19842.1599, Test Loss: 24094.8656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [646/1000], Train Loss: 20801.3974, Test Loss: 23929.4273\n",
      "Epoch [647/1000], Train Loss: 19320.3324, Test Loss: 26963.4021\n",
      "Epoch [648/1000], Train Loss: 19504.7592, Test Loss: 25007.1138\n",
      "Epoch [649/1000], Train Loss: 18956.3884, Test Loss: 24143.2983\n",
      "Epoch [650/1000], Train Loss: 18916.9632, Test Loss: 22724.2258\n",
      "Epoch [651/1000], Train Loss: 18432.4905, Test Loss: 26101.9257\n",
      "Epoch [652/1000], Train Loss: 18859.0941, Test Loss: 26029.6556\n",
      "Epoch [653/1000], Train Loss: 19302.1396, Test Loss: 25367.6961\n",
      "Epoch [654/1000], Train Loss: 19996.4522, Test Loss: 23133.2846\n",
      "Epoch [655/1000], Train Loss: 20343.6308, Test Loss: 23965.1908\n",
      "Epoch [656/1000], Train Loss: 19448.3524, Test Loss: 24232.0074\n",
      "Epoch [657/1000], Train Loss: 18793.5906, Test Loss: 24928.7264\n",
      "Epoch [658/1000], Train Loss: 18968.4904, Test Loss: 26714.5269\n",
      "Epoch [659/1000], Train Loss: 19978.0070, Test Loss: 24628.6869\n",
      "Epoch [660/1000], Train Loss: 19909.7919, Test Loss: 23073.7869\n",
      "Epoch [661/1000], Train Loss: 20766.8247, Test Loss: 23663.2463\n",
      "Epoch [662/1000], Train Loss: 19683.0168, Test Loss: 25146.8660\n",
      "Epoch [663/1000], Train Loss: 19243.0837, Test Loss: 23016.1802\n",
      "Epoch [664/1000], Train Loss: 19315.3979, Test Loss: 23458.1458\n",
      "Epoch [665/1000], Train Loss: 18745.0073, Test Loss: 24490.8869\n",
      "Epoch [666/1000], Train Loss: 18971.0481, Test Loss: 24636.6068\n",
      "Epoch [667/1000], Train Loss: 18673.7682, Test Loss: 25043.1254\n",
      "Epoch [668/1000], Train Loss: 20021.1683, Test Loss: 24753.9433\n",
      "Epoch [669/1000], Train Loss: 19145.2243, Test Loss: 25577.5679\n",
      "Epoch [670/1000], Train Loss: 18873.0462, Test Loss: 25424.8939\n",
      "Epoch [671/1000], Train Loss: 19397.5813, Test Loss: 23827.5506\n",
      "Epoch [672/1000], Train Loss: 18226.8186, Test Loss: 24736.5519\n",
      "Epoch [673/1000], Train Loss: 18560.4018, Test Loss: 24735.2829\n",
      "Epoch [674/1000], Train Loss: 18511.0910, Test Loss: 24393.6429\n",
      "Epoch [675/1000], Train Loss: 19527.6040, Test Loss: 23607.9284\n",
      "Epoch [676/1000], Train Loss: 18516.8305, Test Loss: 25110.9371\n",
      "Epoch [677/1000], Train Loss: 19255.6414, Test Loss: 25607.6465\n",
      "Epoch [678/1000], Train Loss: 18626.2256, Test Loss: 25711.5118\n",
      "Epoch [679/1000], Train Loss: 18756.9517, Test Loss: 25070.2586\n",
      "Epoch [680/1000], Train Loss: 18262.6811, Test Loss: 26633.1746\n",
      "Epoch [681/1000], Train Loss: 18371.3727, Test Loss: 26574.5594\n",
      "Epoch [682/1000], Train Loss: 19898.7704, Test Loss: 22567.3383\n",
      "Epoch [683/1000], Train Loss: 19145.3779, Test Loss: 25185.9390\n",
      "Epoch [684/1000], Train Loss: 19159.3748, Test Loss: 24846.4696\n",
      "Epoch [685/1000], Train Loss: 19209.1193, Test Loss: 23545.9408\n",
      "Epoch [686/1000], Train Loss: 18458.0213, Test Loss: 24256.3080\n",
      "Epoch [687/1000], Train Loss: 19152.7384, Test Loss: 24446.8414\n",
      "Epoch [688/1000], Train Loss: 18589.0264, Test Loss: 24970.8534\n",
      "Epoch [689/1000], Train Loss: 18193.0394, Test Loss: 23402.7950\n",
      "Epoch [690/1000], Train Loss: 18330.9317, Test Loss: 25861.8131\n",
      "Epoch [691/1000], Train Loss: 18824.0340, Test Loss: 25082.4372\n",
      "Epoch [692/1000], Train Loss: 18856.0180, Test Loss: 24878.1345\n",
      "Epoch [693/1000], Train Loss: 18058.6839, Test Loss: 24524.2604\n",
      "Epoch [694/1000], Train Loss: 18484.4272, Test Loss: 24765.1877\n",
      "Epoch [695/1000], Train Loss: 18268.9053, Test Loss: 24249.4791\n",
      "Epoch [696/1000], Train Loss: 18970.7586, Test Loss: 22583.2176\n",
      "Epoch [697/1000], Train Loss: 19791.3786, Test Loss: 24028.1588\n",
      "Epoch [698/1000], Train Loss: 18324.5338, Test Loss: 24875.8765\n",
      "Epoch [699/1000], Train Loss: 17926.0102, Test Loss: 25855.1425\n",
      "Epoch [700/1000], Train Loss: 18292.4246, Test Loss: 25704.3809\n",
      "Epoch [701/1000], Train Loss: 18348.5658, Test Loss: 24497.9753\n",
      "Epoch [702/1000], Train Loss: 19407.6295, Test Loss: 24857.9371\n",
      "Epoch [703/1000], Train Loss: 19197.3491, Test Loss: 25686.2338\n",
      "Epoch [704/1000], Train Loss: 17871.3620, Test Loss: 25070.1382\n",
      "Epoch [705/1000], Train Loss: 18170.5772, Test Loss: 24170.7260\n",
      "Epoch [706/1000], Train Loss: 18692.8809, Test Loss: 24543.3307\n",
      "Epoch [707/1000], Train Loss: 18127.7558, Test Loss: 24308.6570\n",
      "Epoch [708/1000], Train Loss: 17317.7760, Test Loss: 24076.5598\n",
      "Epoch [709/1000], Train Loss: 17561.4288, Test Loss: 25241.8958\n",
      "Epoch [710/1000], Train Loss: 17926.6312, Test Loss: 24756.1270\n",
      "Epoch [711/1000], Train Loss: 17901.7642, Test Loss: 23877.6747\n",
      "Epoch [712/1000], Train Loss: 17656.7652, Test Loss: 23658.1918\n",
      "Epoch [713/1000], Train Loss: 18133.3132, Test Loss: 24869.5349\n",
      "Epoch [714/1000], Train Loss: 18347.8759, Test Loss: 25392.9570\n",
      "Epoch [715/1000], Train Loss: 18956.2759, Test Loss: 22483.0221\n",
      "Epoch [716/1000], Train Loss: 19038.9740, Test Loss: 24996.3058\n",
      "Epoch [717/1000], Train Loss: 17472.6375, Test Loss: 23829.0465\n",
      "Epoch [718/1000], Train Loss: 18347.7324, Test Loss: 25642.6102\n",
      "Epoch [719/1000], Train Loss: 19285.2882, Test Loss: 25002.9196\n",
      "Epoch [720/1000], Train Loss: 18106.9129, Test Loss: 24504.1325\n",
      "Epoch [721/1000], Train Loss: 18233.7459, Test Loss: 24395.6955\n",
      "Epoch [722/1000], Train Loss: 18357.1396, Test Loss: 24527.7246\n",
      "Epoch [723/1000], Train Loss: 18419.1915, Test Loss: 26457.5883\n",
      "Epoch [724/1000], Train Loss: 17826.3147, Test Loss: 25083.7061\n",
      "Epoch [725/1000], Train Loss: 18750.2221, Test Loss: 23627.4391\n",
      "Epoch [726/1000], Train Loss: 18391.6547, Test Loss: 22870.6462\n",
      "Epoch [727/1000], Train Loss: 18045.1460, Test Loss: 24011.6247\n",
      "Epoch [728/1000], Train Loss: 17950.4725, Test Loss: 24229.2702\n",
      "Epoch [729/1000], Train Loss: 17440.4134, Test Loss: 23679.0984\n",
      "Epoch [730/1000], Train Loss: 18259.7152, Test Loss: 23837.7126\n",
      "Epoch [731/1000], Train Loss: 17756.3693, Test Loss: 24270.3801\n",
      "Epoch [732/1000], Train Loss: 20553.0749, Test Loss: 23495.9829\n",
      "Epoch [733/1000], Train Loss: 18338.5231, Test Loss: 24405.6379\n",
      "Epoch [734/1000], Train Loss: 18098.7198, Test Loss: 24641.1874\n",
      "Epoch [735/1000], Train Loss: 17630.2073, Test Loss: 25856.3589\n",
      "Epoch [736/1000], Train Loss: 18893.1944, Test Loss: 24521.9184\n",
      "Epoch [737/1000], Train Loss: 18307.4807, Test Loss: 25188.0300\n",
      "Epoch [738/1000], Train Loss: 17818.1277, Test Loss: 23915.0834\n",
      "Epoch [739/1000], Train Loss: 18853.2786, Test Loss: 23959.1561\n",
      "Epoch [740/1000], Train Loss: 18284.3371, Test Loss: 23768.6726\n",
      "Epoch [741/1000], Train Loss: 18478.0082, Test Loss: 25767.2987\n",
      "Epoch [742/1000], Train Loss: 19669.1904, Test Loss: 23385.1429\n",
      "Epoch [743/1000], Train Loss: 18299.5983, Test Loss: 25413.5006\n",
      "Epoch [744/1000], Train Loss: 17709.1624, Test Loss: 24017.4210\n",
      "Epoch [745/1000], Train Loss: 17973.1929, Test Loss: 25752.6592\n",
      "Epoch [746/1000], Train Loss: 17545.5343, Test Loss: 23325.5855\n",
      "Epoch [747/1000], Train Loss: 17652.5898, Test Loss: 26185.4332\n",
      "Epoch [748/1000], Train Loss: 17457.1435, Test Loss: 23685.2406\n",
      "Epoch [749/1000], Train Loss: 17128.4237, Test Loss: 23602.8250\n",
      "Epoch [750/1000], Train Loss: 18955.5944, Test Loss: 25607.1316\n",
      "Epoch [751/1000], Train Loss: 18304.8448, Test Loss: 25312.3818\n",
      "Epoch [752/1000], Train Loss: 17400.7148, Test Loss: 24586.8012\n",
      "Epoch [753/1000], Train Loss: 18448.8528, Test Loss: 24397.2183\n",
      "Epoch [754/1000], Train Loss: 18208.2403, Test Loss: 23620.8562\n",
      "Epoch [755/1000], Train Loss: 17455.5763, Test Loss: 23213.3591\n",
      "Epoch [756/1000], Train Loss: 17238.7793, Test Loss: 25687.2125\n",
      "Epoch [757/1000], Train Loss: 18252.9421, Test Loss: 24509.2445\n",
      "Epoch [758/1000], Train Loss: 17779.5179, Test Loss: 25400.6010\n",
      "Epoch [759/1000], Train Loss: 17201.5651, Test Loss: 24323.4182\n",
      "Epoch [760/1000], Train Loss: 17022.9172, Test Loss: 25831.5667\n",
      "Epoch [761/1000], Train Loss: 17806.3796, Test Loss: 23232.2961\n",
      "Epoch [762/1000], Train Loss: 17976.9216, Test Loss: 23872.5665\n",
      "Epoch [763/1000], Train Loss: 17408.9143, Test Loss: 24619.0134\n",
      "Epoch [764/1000], Train Loss: 18266.5563, Test Loss: 25520.1374\n",
      "Epoch [765/1000], Train Loss: 17748.0276, Test Loss: 24075.5971\n",
      "Epoch [766/1000], Train Loss: 17598.8776, Test Loss: 23996.3483\n",
      "Epoch [767/1000], Train Loss: 21393.6714, Test Loss: 24445.9501\n",
      "Epoch [768/1000], Train Loss: 18150.0422, Test Loss: 25133.5757\n",
      "Epoch [769/1000], Train Loss: 17526.7927, Test Loss: 24511.6415\n",
      "Epoch [770/1000], Train Loss: 16950.5814, Test Loss: 23413.8182\n",
      "Epoch [771/1000], Train Loss: 17247.6617, Test Loss: 24792.9172\n",
      "Epoch [772/1000], Train Loss: 17476.1355, Test Loss: 25074.9256\n",
      "Epoch [773/1000], Train Loss: 17427.5926, Test Loss: 24560.5525\n",
      "Epoch [774/1000], Train Loss: 17519.6451, Test Loss: 23105.0818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [775/1000], Train Loss: 17636.4259, Test Loss: 24364.4066\n",
      "Epoch [776/1000], Train Loss: 17096.6231, Test Loss: 23606.3631\n",
      "Epoch [777/1000], Train Loss: 17727.5310, Test Loss: 23771.0949\n",
      "Epoch [778/1000], Train Loss: 17246.0171, Test Loss: 24496.3643\n",
      "Epoch [779/1000], Train Loss: 16930.1427, Test Loss: 24047.6887\n",
      "Epoch [780/1000], Train Loss: 17548.5055, Test Loss: 23300.1323\n",
      "Epoch [781/1000], Train Loss: 16831.4441, Test Loss: 22481.5231\n",
      "Epoch [782/1000], Train Loss: 17026.7958, Test Loss: 24922.2533\n",
      "Epoch [783/1000], Train Loss: 17322.1427, Test Loss: 23563.6552\n",
      "Epoch [784/1000], Train Loss: 18109.5893, Test Loss: 24844.5146\n",
      "Epoch [785/1000], Train Loss: 19617.9676, Test Loss: 23937.5436\n",
      "Epoch [786/1000], Train Loss: 17406.1247, Test Loss: 25261.6828\n",
      "Epoch [787/1000], Train Loss: 17537.5458, Test Loss: 23761.5061\n",
      "Epoch [788/1000], Train Loss: 17209.1436, Test Loss: 24117.5347\n",
      "Epoch [789/1000], Train Loss: 17214.5566, Test Loss: 23808.9569\n",
      "Epoch [790/1000], Train Loss: 17277.6044, Test Loss: 24398.6682\n",
      "Epoch [791/1000], Train Loss: 17128.3397, Test Loss: 24690.9792\n",
      "Epoch [792/1000], Train Loss: 17175.3804, Test Loss: 23515.3596\n",
      "Epoch [793/1000], Train Loss: 17596.3277, Test Loss: 24775.2252\n",
      "Epoch [794/1000], Train Loss: 16811.6940, Test Loss: 24652.3917\n",
      "Epoch [795/1000], Train Loss: 16773.7458, Test Loss: 24334.4624\n",
      "Epoch [796/1000], Train Loss: 17328.4486, Test Loss: 24688.9032\n",
      "Epoch [797/1000], Train Loss: 16539.1023, Test Loss: 22403.5543\n",
      "Epoch [798/1000], Train Loss: 17589.9729, Test Loss: 24182.4461\n",
      "Epoch [799/1000], Train Loss: 16991.8297, Test Loss: 25577.8359\n",
      "Epoch [800/1000], Train Loss: 17858.7544, Test Loss: 23488.0953\n",
      "Epoch [801/1000], Train Loss: 18091.9187, Test Loss: 23340.3083\n",
      "Epoch [802/1000], Train Loss: 17319.5098, Test Loss: 23364.2307\n",
      "Epoch [803/1000], Train Loss: 17572.3765, Test Loss: 21242.8552\n",
      "Epoch [804/1000], Train Loss: 17455.3022, Test Loss: 23628.3143\n",
      "Epoch [805/1000], Train Loss: 17792.4111, Test Loss: 23225.9345\n",
      "Epoch [806/1000], Train Loss: 18407.2129, Test Loss: 24336.3785\n",
      "Epoch [807/1000], Train Loss: 17449.3137, Test Loss: 24892.0963\n",
      "Epoch [808/1000], Train Loss: 17342.0257, Test Loss: 24839.1564\n",
      "Epoch [809/1000], Train Loss: 17500.8559, Test Loss: 24792.8158\n",
      "Epoch [810/1000], Train Loss: 18395.7703, Test Loss: 22677.8747\n",
      "Epoch [811/1000], Train Loss: 17439.6718, Test Loss: 24568.2445\n",
      "Epoch [812/1000], Train Loss: 17606.6747, Test Loss: 25537.9935\n",
      "Epoch [813/1000], Train Loss: 18372.2820, Test Loss: 26613.6523\n",
      "Epoch [814/1000], Train Loss: 17533.5274, Test Loss: 23791.5826\n",
      "Epoch [815/1000], Train Loss: 18474.8002, Test Loss: 26133.2894\n",
      "Epoch [816/1000], Train Loss: 18597.3670, Test Loss: 25739.2343\n",
      "Epoch [817/1000], Train Loss: 17523.9478, Test Loss: 24082.9724\n",
      "Epoch [818/1000], Train Loss: 19495.2953, Test Loss: 23294.2407\n",
      "Epoch [819/1000], Train Loss: 17315.7559, Test Loss: 24884.8054\n",
      "Epoch [820/1000], Train Loss: 17075.7659, Test Loss: 24741.8428\n",
      "Epoch [821/1000], Train Loss: 17383.6054, Test Loss: 23474.4721\n",
      "Epoch [822/1000], Train Loss: 16639.4193, Test Loss: 22692.4101\n",
      "Epoch [823/1000], Train Loss: 16530.2411, Test Loss: 26317.8843\n",
      "Epoch [824/1000], Train Loss: 17066.2424, Test Loss: 23388.0699\n",
      "Epoch [825/1000], Train Loss: 16680.8307, Test Loss: 25915.0764\n",
      "Epoch [826/1000], Train Loss: 16838.3868, Test Loss: 26358.2795\n",
      "Epoch [827/1000], Train Loss: 17126.6970, Test Loss: 25287.8996\n",
      "Epoch [828/1000], Train Loss: 16687.5606, Test Loss: 24570.3008\n",
      "Epoch [829/1000], Train Loss: 18219.1057, Test Loss: 23728.4098\n",
      "Epoch [830/1000], Train Loss: 17359.5954, Test Loss: 25367.7981\n",
      "Epoch [831/1000], Train Loss: 17286.5733, Test Loss: 24195.7443\n",
      "Epoch [832/1000], Train Loss: 16697.5241, Test Loss: 24403.6852\n",
      "Epoch [833/1000], Train Loss: 16216.9209, Test Loss: 24618.2397\n",
      "Epoch [834/1000], Train Loss: 16090.9565, Test Loss: 24868.7106\n",
      "Epoch [835/1000], Train Loss: 17702.7231, Test Loss: 22672.9102\n",
      "Epoch [836/1000], Train Loss: 16944.3411, Test Loss: 24528.3078\n",
      "Epoch [837/1000], Train Loss: 16776.4722, Test Loss: 25428.9628\n",
      "Epoch [838/1000], Train Loss: 38238.3336, Test Loss: 25123.9870\n",
      "Epoch [839/1000], Train Loss: 18552.6798, Test Loss: 23692.0236\n",
      "Epoch [840/1000], Train Loss: 16681.2794, Test Loss: 25427.3183\n",
      "Epoch [841/1000], Train Loss: 16567.3483, Test Loss: 23814.1540\n",
      "Epoch [842/1000], Train Loss: 17845.3920, Test Loss: 24410.4751\n",
      "Epoch [843/1000], Train Loss: 17721.0711, Test Loss: 24467.7136\n",
      "Epoch [844/1000], Train Loss: 16468.7201, Test Loss: 23415.6494\n",
      "Epoch [845/1000], Train Loss: 16597.6371, Test Loss: 24264.3061\n",
      "Epoch [846/1000], Train Loss: 16108.6152, Test Loss: 23721.8467\n",
      "Epoch [847/1000], Train Loss: 17175.2651, Test Loss: 23606.4728\n",
      "Epoch [848/1000], Train Loss: 16392.1301, Test Loss: 23332.6865\n",
      "Epoch [849/1000], Train Loss: 17096.5985, Test Loss: 23300.5225\n",
      "Epoch [850/1000], Train Loss: 16853.0573, Test Loss: 24129.8638\n",
      "Epoch [851/1000], Train Loss: 17087.1647, Test Loss: 23585.9280\n",
      "Epoch [852/1000], Train Loss: 16307.1439, Test Loss: 24734.7265\n",
      "Epoch [853/1000], Train Loss: 16512.1443, Test Loss: 24796.9138\n",
      "Epoch [854/1000], Train Loss: 16385.9064, Test Loss: 23525.6418\n",
      "Epoch [855/1000], Train Loss: 17415.5878, Test Loss: 25371.7346\n",
      "Epoch [856/1000], Train Loss: 17400.8317, Test Loss: 24402.1762\n",
      "Epoch [857/1000], Train Loss: 17885.5294, Test Loss: 24874.1527\n",
      "Epoch [858/1000], Train Loss: 16009.6397, Test Loss: 24089.0576\n",
      "Epoch [859/1000], Train Loss: 16867.8208, Test Loss: 25487.7255\n",
      "Epoch [860/1000], Train Loss: 16298.0777, Test Loss: 25005.8833\n",
      "Epoch [861/1000], Train Loss: 16145.2224, Test Loss: 23686.5441\n",
      "Epoch [862/1000], Train Loss: 17556.3027, Test Loss: 23746.7453\n",
      "Epoch [863/1000], Train Loss: 18175.4532, Test Loss: 23611.8246\n",
      "Epoch [864/1000], Train Loss: 17489.5837, Test Loss: 25333.7503\n",
      "Epoch [865/1000], Train Loss: 17631.1649, Test Loss: 24257.3020\n",
      "Epoch [866/1000], Train Loss: 18679.1278, Test Loss: 23938.2112\n",
      "Epoch [867/1000], Train Loss: 16864.0819, Test Loss: 26132.2529\n",
      "Epoch [868/1000], Train Loss: 16161.1461, Test Loss: 23619.0284\n",
      "Epoch [869/1000], Train Loss: 16290.7156, Test Loss: 24589.1681\n",
      "Epoch [870/1000], Train Loss: 16811.8662, Test Loss: 25089.8400\n",
      "Epoch [871/1000], Train Loss: 16896.4972, Test Loss: 25394.8734\n",
      "Epoch [872/1000], Train Loss: 17020.5021, Test Loss: 23767.2154\n",
      "Epoch [873/1000], Train Loss: 17479.7450, Test Loss: 23574.5497\n",
      "Epoch [874/1000], Train Loss: 17864.4926, Test Loss: 24272.1626\n",
      "Epoch [875/1000], Train Loss: 16682.4176, Test Loss: 23303.3586\n",
      "Epoch [876/1000], Train Loss: 16309.2827, Test Loss: 22978.2682\n",
      "Epoch [877/1000], Train Loss: 17018.0638, Test Loss: 24166.3338\n",
      "Epoch [878/1000], Train Loss: 16597.1122, Test Loss: 23279.2412\n",
      "Epoch [879/1000], Train Loss: 16460.4093, Test Loss: 24254.0239\n",
      "Epoch [880/1000], Train Loss: 17080.0403, Test Loss: 24816.9109\n",
      "Epoch [881/1000], Train Loss: 16236.2497, Test Loss: 23404.2642\n",
      "Epoch [882/1000], Train Loss: 16918.5130, Test Loss: 23521.8657\n",
      "Epoch [883/1000], Train Loss: 16367.6077, Test Loss: 23621.3139\n",
      "Epoch [884/1000], Train Loss: 16283.0533, Test Loss: 23449.3152\n",
      "Epoch [885/1000], Train Loss: 16018.6527, Test Loss: 24172.9867\n",
      "Epoch [886/1000], Train Loss: 16649.5694, Test Loss: 25589.5155\n",
      "Epoch [887/1000], Train Loss: 16912.2478, Test Loss: 23625.5686\n",
      "Epoch [888/1000], Train Loss: 16292.0711, Test Loss: 24225.6740\n",
      "Epoch [889/1000], Train Loss: 16220.4839, Test Loss: 25190.5730\n",
      "Epoch [890/1000], Train Loss: 15703.3569, Test Loss: 25060.5983\n",
      "Epoch [891/1000], Train Loss: 15844.9677, Test Loss: 24292.0667\n",
      "Epoch [892/1000], Train Loss: 16225.5590, Test Loss: 24139.8782\n",
      "Epoch [893/1000], Train Loss: 15982.8714, Test Loss: 24690.0208\n",
      "Epoch [894/1000], Train Loss: 18052.5003, Test Loss: 24804.8502\n",
      "Epoch [895/1000], Train Loss: 19269.8560, Test Loss: 24013.2897\n",
      "Epoch [896/1000], Train Loss: 16058.7438, Test Loss: 24303.4417\n",
      "Epoch [897/1000], Train Loss: 16391.4258, Test Loss: 25980.2715\n",
      "Epoch [898/1000], Train Loss: 16382.1834, Test Loss: 23524.4881\n",
      "Epoch [899/1000], Train Loss: 16561.1074, Test Loss: 23252.3718\n",
      "Epoch [900/1000], Train Loss: 16382.4280, Test Loss: 22896.5543\n",
      "Epoch [901/1000], Train Loss: 16867.2169, Test Loss: 25837.1202\n",
      "Epoch [902/1000], Train Loss: 16728.6663, Test Loss: 23144.3617\n",
      "Epoch [903/1000], Train Loss: 17635.1720, Test Loss: 24168.5988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [904/1000], Train Loss: 18176.9685, Test Loss: 25999.2720\n",
      "Epoch [905/1000], Train Loss: 17970.5814, Test Loss: 25004.2337\n",
      "Epoch [906/1000], Train Loss: 16639.5102, Test Loss: 23809.3818\n",
      "Epoch [907/1000], Train Loss: 16179.3844, Test Loss: 23629.4998\n",
      "Epoch [908/1000], Train Loss: 18324.9732, Test Loss: 25343.1358\n",
      "Epoch [909/1000], Train Loss: 17488.0021, Test Loss: 23403.1822\n",
      "Epoch [910/1000], Train Loss: 16190.1619, Test Loss: 24060.3336\n",
      "Epoch [911/1000], Train Loss: 16033.2987, Test Loss: 24073.2914\n",
      "Epoch [912/1000], Train Loss: 16414.8301, Test Loss: 24816.4917\n",
      "Epoch [913/1000], Train Loss: 15998.6090, Test Loss: 24755.7442\n",
      "Epoch [914/1000], Train Loss: 16903.6508, Test Loss: 23072.3863\n",
      "Epoch [915/1000], Train Loss: 15867.1497, Test Loss: 23931.7006\n",
      "Epoch [916/1000], Train Loss: 15858.3110, Test Loss: 23360.8041\n",
      "Epoch [917/1000], Train Loss: 16680.0919, Test Loss: 23940.2455\n",
      "Epoch [918/1000], Train Loss: 18207.6199, Test Loss: 24975.1195\n",
      "Epoch [919/1000], Train Loss: 18486.0036, Test Loss: 22732.0792\n",
      "Epoch [920/1000], Train Loss: 16495.0802, Test Loss: 22676.2029\n",
      "Epoch [921/1000], Train Loss: 16052.2902, Test Loss: 23873.5650\n",
      "Epoch [922/1000], Train Loss: 15840.3954, Test Loss: 24627.9470\n",
      "Epoch [923/1000], Train Loss: 15668.6478, Test Loss: 23978.1313\n",
      "Epoch [924/1000], Train Loss: 15438.5785, Test Loss: 23597.8391\n",
      "Epoch [925/1000], Train Loss: 16197.4126, Test Loss: 23918.2711\n",
      "Epoch [926/1000], Train Loss: 15760.8850, Test Loss: 22966.7855\n",
      "Epoch [927/1000], Train Loss: 16021.0117, Test Loss: 23250.6615\n",
      "Epoch [928/1000], Train Loss: 16031.2611, Test Loss: 24293.7273\n",
      "Epoch [929/1000], Train Loss: 17945.3117, Test Loss: 23286.8673\n",
      "Epoch [930/1000], Train Loss: 16010.1273, Test Loss: 24125.7374\n",
      "Epoch [931/1000], Train Loss: 15950.2260, Test Loss: 23575.2013\n",
      "Epoch [932/1000], Train Loss: 16184.5424, Test Loss: 24602.5380\n",
      "Epoch [933/1000], Train Loss: 16532.9376, Test Loss: 23818.2803\n",
      "Epoch [934/1000], Train Loss: 15661.2859, Test Loss: 23551.3990\n",
      "Epoch [935/1000], Train Loss: 16573.5947, Test Loss: 24422.4370\n",
      "Epoch [936/1000], Train Loss: 15960.8138, Test Loss: 23783.1709\n",
      "Epoch [937/1000], Train Loss: 15994.3711, Test Loss: 24653.8155\n",
      "Epoch [938/1000], Train Loss: 15769.7282, Test Loss: 23963.3981\n",
      "Epoch [939/1000], Train Loss: 16341.2599, Test Loss: 22895.6673\n",
      "Epoch [940/1000], Train Loss: 15868.1699, Test Loss: 24058.8330\n",
      "Epoch [941/1000], Train Loss: 16561.7497, Test Loss: 24304.1075\n",
      "Epoch [942/1000], Train Loss: 16570.9464, Test Loss: 23703.0535\n",
      "Epoch [943/1000], Train Loss: 36187.0515, Test Loss: 24611.0283\n",
      "Epoch [944/1000], Train Loss: 16270.9120, Test Loss: 24893.6499\n",
      "Epoch [945/1000], Train Loss: 16230.7514, Test Loss: 23750.1073\n",
      "Epoch [946/1000], Train Loss: 17463.9591, Test Loss: 25123.1579\n",
      "Epoch [947/1000], Train Loss: 15969.1581, Test Loss: 24122.3355\n",
      "Epoch [948/1000], Train Loss: 15958.7036, Test Loss: 24389.5582\n",
      "Epoch [949/1000], Train Loss: 16027.5893, Test Loss: 24152.0452\n",
      "Epoch [950/1000], Train Loss: 15653.8104, Test Loss: 24433.2147\n",
      "Epoch [951/1000], Train Loss: 15802.1175, Test Loss: 23465.2492\n",
      "Epoch [952/1000], Train Loss: 15924.8413, Test Loss: 25429.6453\n",
      "Epoch [953/1000], Train Loss: 16397.9410, Test Loss: 23353.6326\n",
      "Epoch [954/1000], Train Loss: 15634.5786, Test Loss: 23779.1415\n",
      "Epoch [955/1000], Train Loss: 15931.1058, Test Loss: 23434.5671\n",
      "Epoch [956/1000], Train Loss: 16236.5107, Test Loss: 25652.9330\n",
      "Epoch [957/1000], Train Loss: 16575.5378, Test Loss: 23936.2713\n",
      "Epoch [958/1000], Train Loss: 15512.9242, Test Loss: 24274.1098\n",
      "Epoch [959/1000], Train Loss: 17589.9460, Test Loss: 24429.0912\n",
      "Epoch [960/1000], Train Loss: 15994.1803, Test Loss: 24225.9028\n",
      "Epoch [961/1000], Train Loss: 16253.2048, Test Loss: 24589.9044\n",
      "Epoch [962/1000], Train Loss: 15563.7309, Test Loss: 23543.5569\n",
      "Epoch [963/1000], Train Loss: 16162.2875, Test Loss: 23831.4618\n",
      "Epoch [964/1000], Train Loss: 15827.1475, Test Loss: 23640.9952\n",
      "Epoch [965/1000], Train Loss: 19979.1645, Test Loss: 24496.0418\n",
      "Epoch [966/1000], Train Loss: 18305.2923, Test Loss: 24082.7721\n",
      "Epoch [967/1000], Train Loss: 15909.2991, Test Loss: 23715.3625\n",
      "Epoch [968/1000], Train Loss: 15607.6925, Test Loss: 24066.8815\n",
      "Epoch [969/1000], Train Loss: 15058.5734, Test Loss: 22222.8000\n",
      "Epoch [970/1000], Train Loss: 15882.3805, Test Loss: 25192.8397\n",
      "Epoch [971/1000], Train Loss: 15492.3954, Test Loss: 25041.3568\n",
      "Epoch [972/1000], Train Loss: 15672.4318, Test Loss: 22531.3144\n",
      "Epoch [973/1000], Train Loss: 15331.2283, Test Loss: 23851.0021\n",
      "Epoch [974/1000], Train Loss: 16097.5714, Test Loss: 23985.5490\n",
      "Epoch [975/1000], Train Loss: 15895.9011, Test Loss: 23995.4322\n",
      "Epoch [976/1000], Train Loss: 15687.3060, Test Loss: 23641.1583\n",
      "Epoch [977/1000], Train Loss: 15461.3639, Test Loss: 26797.2259\n",
      "Epoch [978/1000], Train Loss: 16033.4299, Test Loss: 22917.7213\n",
      "Epoch [979/1000], Train Loss: 15717.6541, Test Loss: 22697.0056\n",
      "Epoch [980/1000], Train Loss: 16385.8868, Test Loss: 24541.9059\n",
      "Epoch [981/1000], Train Loss: 15676.3109, Test Loss: 22824.0216\n",
      "Epoch [982/1000], Train Loss: 16208.5327, Test Loss: 23013.0347\n",
      "Epoch [983/1000], Train Loss: 15494.2656, Test Loss: 23069.7050\n",
      "Epoch [984/1000], Train Loss: 15580.6222, Test Loss: 23817.9769\n",
      "Epoch [985/1000], Train Loss: 16417.1262, Test Loss: 24852.5731\n",
      "Epoch [986/1000], Train Loss: 15505.8860, Test Loss: 24382.0450\n",
      "Epoch [987/1000], Train Loss: 15231.4666, Test Loss: 23252.5004\n",
      "Epoch [988/1000], Train Loss: 16158.1071, Test Loss: 21927.5378\n",
      "Epoch [989/1000], Train Loss: 15859.5304, Test Loss: 23257.9167\n",
      "Epoch [990/1000], Train Loss: 15878.7398, Test Loss: 23828.8331\n",
      "Epoch [991/1000], Train Loss: 15960.7441, Test Loss: 23931.7644\n",
      "Epoch [992/1000], Train Loss: 15951.8204, Test Loss: 23228.3359\n",
      "Epoch [993/1000], Train Loss: 15395.9571, Test Loss: 23614.5740\n",
      "Epoch [994/1000], Train Loss: 16001.5014, Test Loss: 25438.8471\n",
      "Epoch [995/1000], Train Loss: 15551.6124, Test Loss: 23696.6236\n",
      "Epoch [996/1000], Train Loss: 15705.8417, Test Loss: 23413.6722\n",
      "Epoch [997/1000], Train Loss: 15526.0078, Test Loss: 24098.3598\n",
      "Epoch [998/1000], Train Loss: 15832.5791, Test Loss: 24145.8569\n",
      "Epoch [999/1000], Train Loss: 15318.0061, Test Loss: 23436.2123\n",
      "Epoch [1000/1000], Train Loss: 15883.8833, Test Loss: 23442.0202\n",
      "Execution time: 23751.439964056015 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
