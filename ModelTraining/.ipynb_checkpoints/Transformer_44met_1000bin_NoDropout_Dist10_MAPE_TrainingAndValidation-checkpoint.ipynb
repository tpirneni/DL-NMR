{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer Encoder on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"Transformer_44met_TrainingAndValidation_1000bin_NoDropout_Dist9_MAPE_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('Dataset44_Dist9_Spec.npy')\n",
    "conc1 = np.load('Dataset44_Dist9_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "#spectraVal = np.load('Dataset44_Dist9_Val_Spec.npy')\n",
    "#concVal = np.load('Dataset44_Dist9_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "#ValSpectra = np.load(\"Dataset44_Dist9_RepresentativeExamples_Spectra.npy\")\n",
    "#ValConc = np.load(\"Dataset44_Dist9_RepresentativeExamples_Concentrations.npy\")\n",
    "#ValSpecNames = np.load(\"Dataset44_Dist9_RepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 32, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.Linear(23552, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Binning\n",
    "        batch_size, seq_length = x.size()\n",
    "        num_bins = seq_length // self.input_dim\n",
    "        x = x.view(batch_size, num_bins, self.input_dim)  # (batch_size, num_bins, input_dim)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = x.permute(1, 0, 2)  # (num_bins, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)  # (num_bins, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Reconstruct original sequence\n",
    "        x = x.reshape(batch_size, num_bins * d_model)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder(x)  # (batch_size, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "input_dim = 1000  # Size of each bin\n",
    "d_model = 512     # Embedding dimension\n",
    "nhead = 1         # Number of attention heads\n",
    "num_encoder_layers = 1  # Number of transformer encoder layers\n",
    "dim_feedforward = 2048  # Feedforward dimension\n",
    "dropout = 0.0     # Dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.mean(torch.abs((y_true - y_pred) / y_true))\n",
    "        return loss * 100  # To get percentage\n",
    "\n",
    "# Example usage:\n",
    "mape_loss = MAPELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = MAPELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "       \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/1000], Train Loss: 2152006.1743, Test Loss: 384925.9602\n",
      "Epoch [2/1000], Train Loss: 1590417.0676, Test Loss: 396643.8804\n",
      "Epoch [3/1000], Train Loss: 1480632.6843, Test Loss: 355120.5120\n",
      "Epoch [4/1000], Train Loss: 1437964.5688, Test Loss: 362474.2195\n",
      "Epoch [5/1000], Train Loss: 1446349.1055, Test Loss: 354910.1243\n",
      "Epoch [6/1000], Train Loss: 1459493.1184, Test Loss: 357656.9907\n",
      "Epoch [7/1000], Train Loss: 1438594.6804, Test Loss: 355422.2170\n",
      "Epoch [8/1000], Train Loss: 1423235.9082, Test Loss: 361700.0598\n",
      "Epoch [9/1000], Train Loss: 1424547.8743, Test Loss: 355965.3860\n",
      "Epoch [10/1000], Train Loss: 1428602.4749, Test Loss: 352828.2722\n",
      "Epoch [11/1000], Train Loss: 1421148.3235, Test Loss: 352769.0525\n",
      "Epoch [12/1000], Train Loss: 1422341.7366, Test Loss: 354432.8462\n",
      "Epoch [13/1000], Train Loss: 1423091.6267, Test Loss: 353113.2734\n",
      "Epoch [14/1000], Train Loss: 1419044.0247, Test Loss: 357649.5762\n",
      "Epoch [15/1000], Train Loss: 1421491.6260, Test Loss: 351961.6499\n",
      "Epoch [16/1000], Train Loss: 1419673.1948, Test Loss: 352642.8428\n",
      "Epoch [17/1000], Train Loss: 1419539.1450, Test Loss: 353223.9502\n",
      "Epoch [18/1000], Train Loss: 1418313.1780, Test Loss: 352055.7422\n",
      "Epoch [19/1000], Train Loss: 1419925.5959, Test Loss: 353043.5867\n",
      "Epoch [20/1000], Train Loss: 1418718.8406, Test Loss: 360412.7356\n",
      "Epoch [21/1000], Train Loss: 1421651.7537, Test Loss: 353234.2258\n",
      "Epoch [22/1000], Train Loss: 1419787.0964, Test Loss: 353008.1733\n",
      "Epoch [23/1000], Train Loss: 1420679.4590, Test Loss: 355233.0579\n",
      "Epoch [24/1000], Train Loss: 1416153.0493, Test Loss: 355566.3176\n",
      "Epoch [25/1000], Train Loss: 1416756.3276, Test Loss: 363086.3049\n",
      "Epoch [26/1000], Train Loss: 1421144.9111, Test Loss: 353509.1184\n",
      "Epoch [27/1000], Train Loss: 1416618.6260, Test Loss: 353448.6165\n",
      "Epoch [28/1000], Train Loss: 1416789.7527, Test Loss: 356689.0474\n",
      "Epoch [29/1000], Train Loss: 1415246.0864, Test Loss: 352012.4368\n",
      "Epoch [30/1000], Train Loss: 1416447.5808, Test Loss: 353511.5186\n",
      "Epoch [31/1000], Train Loss: 1416382.4722, Test Loss: 353778.5234\n",
      "Epoch [32/1000], Train Loss: 1413774.1040, Test Loss: 352749.9314\n",
      "Epoch [33/1000], Train Loss: 1415683.7417, Test Loss: 354107.7117\n",
      "Epoch [34/1000], Train Loss: 1413047.6440, Test Loss: 355286.0359\n",
      "Epoch [35/1000], Train Loss: 1413368.3125, Test Loss: 351857.9368\n",
      "Epoch [36/1000], Train Loss: 1410229.6838, Test Loss: 352616.0732\n",
      "Epoch [37/1000], Train Loss: 1411790.3147, Test Loss: 352325.1057\n",
      "Epoch [38/1000], Train Loss: 1408753.5820, Test Loss: 353024.9795\n",
      "Epoch [39/1000], Train Loss: 1410770.9138, Test Loss: 351818.9250\n",
      "Epoch [40/1000], Train Loss: 1413174.9111, Test Loss: 350899.8665\n",
      "Epoch [41/1000], Train Loss: 1408638.6045, Test Loss: 351586.6448\n",
      "Epoch [42/1000], Train Loss: 1405774.4224, Test Loss: 351477.7883\n",
      "Epoch [43/1000], Train Loss: 1399825.5544, Test Loss: 347416.3823\n",
      "Epoch [44/1000], Train Loss: 1396093.7263, Test Loss: 349429.5522\n",
      "Epoch [45/1000], Train Loss: 1392153.2910, Test Loss: 345424.5164\n",
      "Epoch [46/1000], Train Loss: 1385271.5720, Test Loss: 346639.5989\n",
      "Epoch [47/1000], Train Loss: 1377804.4602, Test Loss: 345950.8943\n",
      "Epoch [48/1000], Train Loss: 1376474.5505, Test Loss: 345460.8457\n",
      "Epoch [49/1000], Train Loss: 1372557.1609, Test Loss: 342788.1260\n",
      "Epoch [50/1000], Train Loss: 1374554.4573, Test Loss: 344866.5505\n",
      "Epoch [51/1000], Train Loss: 1367608.1475, Test Loss: 343432.7090\n",
      "Epoch [52/1000], Train Loss: 1367247.9160, Test Loss: 342088.9429\n",
      "Epoch [53/1000], Train Loss: 1361591.4587, Test Loss: 340693.2859\n",
      "Epoch [54/1000], Train Loss: 1356668.4189, Test Loss: 343761.4080\n",
      "Epoch [55/1000], Train Loss: 1360931.4995, Test Loss: 342767.3191\n",
      "Epoch [56/1000], Train Loss: 1362429.8511, Test Loss: 345198.9268\n",
      "Epoch [57/1000], Train Loss: 1355252.0605, Test Loss: 343808.6614\n",
      "Epoch [58/1000], Train Loss: 1350585.7793, Test Loss: 343570.0132\n",
      "Epoch [59/1000], Train Loss: 1350121.7148, Test Loss: 339712.1150\n",
      "Epoch [60/1000], Train Loss: 1348423.1060, Test Loss: 346914.2219\n",
      "Epoch [61/1000], Train Loss: 1350160.1072, Test Loss: 339850.0068\n",
      "Epoch [62/1000], Train Loss: 1344484.4626, Test Loss: 339489.4988\n",
      "Epoch [63/1000], Train Loss: 1351164.0015, Test Loss: 338982.5991\n",
      "Epoch [64/1000], Train Loss: 1345465.7336, Test Loss: 338777.1758\n",
      "Epoch [65/1000], Train Loss: 1346801.2258, Test Loss: 339719.4177\n",
      "Epoch [66/1000], Train Loss: 1349534.6526, Test Loss: 342309.9463\n",
      "Epoch [67/1000], Train Loss: 1348531.5352, Test Loss: 341878.5417\n",
      "Epoch [68/1000], Train Loss: 1347760.4312, Test Loss: 340997.5366\n",
      "Epoch [69/1000], Train Loss: 1345799.5378, Test Loss: 339133.7119\n",
      "Epoch [70/1000], Train Loss: 1341436.9058, Test Loss: 345154.2944\n",
      "Epoch [71/1000], Train Loss: 1342810.7014, Test Loss: 338574.9319\n",
      "Epoch [72/1000], Train Loss: 1342232.7073, Test Loss: 338395.4172\n",
      "Epoch [73/1000], Train Loss: 1351359.1099, Test Loss: 339865.3167\n",
      "Epoch [74/1000], Train Loss: 1352467.6484, Test Loss: 342422.7402\n",
      "Epoch [75/1000], Train Loss: 1364611.3633, Test Loss: 343935.7761\n",
      "Epoch [76/1000], Train Loss: 1351699.7673, Test Loss: 339983.8347\n",
      "Epoch [77/1000], Train Loss: 1357800.4312, Test Loss: 344693.0356\n",
      "Epoch [78/1000], Train Loss: 1350576.5989, Test Loss: 339857.1331\n",
      "Epoch [79/1000], Train Loss: 1339785.6875, Test Loss: 342016.7903\n",
      "Epoch [80/1000], Train Loss: 1341432.9397, Test Loss: 338720.2815\n",
      "Epoch [81/1000], Train Loss: 1337886.6455, Test Loss: 338994.3960\n",
      "Epoch [82/1000], Train Loss: 1335106.4082, Test Loss: 339025.9214\n",
      "Epoch [83/1000], Train Loss: 1341210.2314, Test Loss: 340657.6682\n",
      "Epoch [84/1000], Train Loss: 1337069.5107, Test Loss: 337712.0083\n",
      "Epoch [85/1000], Train Loss: 1334491.8247, Test Loss: 344300.7354\n",
      "Epoch [86/1000], Train Loss: 1331935.3679, Test Loss: 337806.4529\n",
      "Epoch [87/1000], Train Loss: 1327029.6687, Test Loss: 341860.3071\n",
      "Epoch [88/1000], Train Loss: 1330889.5728, Test Loss: 338242.9077\n",
      "Epoch [89/1000], Train Loss: 1328200.8425, Test Loss: 339195.5984\n",
      "Epoch [90/1000], Train Loss: 1323843.5825, Test Loss: 343887.2393\n",
      "Epoch [91/1000], Train Loss: 1322984.2249, Test Loss: 345514.2930\n",
      "Epoch [92/1000], Train Loss: 1324366.5312, Test Loss: 338585.0886\n",
      "Epoch [93/1000], Train Loss: 1323918.4111, Test Loss: 337075.2693\n",
      "Epoch [94/1000], Train Loss: 1322183.5198, Test Loss: 337084.8384\n",
      "Epoch [95/1000], Train Loss: 1322896.7668, Test Loss: 337847.0845\n",
      "Epoch [96/1000], Train Loss: 1324085.0247, Test Loss: 342749.7754\n",
      "Epoch [97/1000], Train Loss: 1348916.1663, Test Loss: 351695.4734\n",
      "Epoch [98/1000], Train Loss: 1372557.8518, Test Loss: 348706.3899\n",
      "Epoch [99/1000], Train Loss: 1373203.4919, Test Loss: 346867.3975\n",
      "Epoch [100/1000], Train Loss: 1386543.7783, Test Loss: 351283.9788\n",
      "Epoch [101/1000], Train Loss: 1355576.3364, Test Loss: 343730.8447\n",
      "Epoch [102/1000], Train Loss: 1364084.3140, Test Loss: 348981.1821\n",
      "Epoch [103/1000], Train Loss: 1355448.7874, Test Loss: 343462.1287\n",
      "Epoch [104/1000], Train Loss: 1343355.8308, Test Loss: 339950.6692\n",
      "Epoch [105/1000], Train Loss: 1329354.4788, Test Loss: 346611.9048\n",
      "Epoch [106/1000], Train Loss: 1327002.7771, Test Loss: 339677.5149\n",
      "Epoch [107/1000], Train Loss: 1325308.5862, Test Loss: 340201.6755\n",
      "Epoch [108/1000], Train Loss: 1319826.3215, Test Loss: 338702.1169\n",
      "Epoch [109/1000], Train Loss: 1316922.9548, Test Loss: 337168.2966\n",
      "Epoch [110/1000], Train Loss: 1316775.7312, Test Loss: 336132.6743\n",
      "Epoch [111/1000], Train Loss: 1322512.4473, Test Loss: 338258.4080\n",
      "Epoch [112/1000], Train Loss: 1318052.7527, Test Loss: 343063.2678\n",
      "Epoch [113/1000], Train Loss: 1316503.8806, Test Loss: 336686.5688\n",
      "Epoch [114/1000], Train Loss: 1309238.0784, Test Loss: 339318.7920\n",
      "Epoch [115/1000], Train Loss: 1315590.8755, Test Loss: 338003.0081\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m save_path \u001b[38;5;241m=\u001b[39m ModelName \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__Params.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m train_losses, test_losses, is_model_trained \u001b[38;5;241m=\u001b[39m train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Finish timing cell run time\u001b[39;00m\n\u001b[1;32m     25\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[9], line 63\u001b[0m, in \u001b[0;36mtrain_or_load_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pretrained model found. Training from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#optimizer = optim.Adam(model.parameters())  \u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n\u001b[1;32m     64\u001b[0m is_model_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set flag to True after training\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Save losses per epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m, in \u001b[0;36mtrain_and_save_best_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameter weights\n",
    "save_path = ModelName + '__Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39marray(test_losses)\u001b[38;5;241m.\u001b[39mmin()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_losses' is not defined"
     ]
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load('Dataset44_Dist9_Val_Spec.npy')\n",
    "concVal = np.load('Dataset44_Dist9_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "ValSpectra = np.load(\"Dataset44_Dist9_RepresentativeExamples_Spectra.npy\")\n",
    "ValConc = np.load(\"Dataset44_Dist9_RepresentativeExamples_Concentrations.npy\")\n",
    "ValSpecNames = np.load(\"Dataset44_Dist9_RepresentativeExamples_VariableNames.npy\")\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ValConc = torch.tensor(ValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=23552, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ValConc[i]\n",
    "    Prediction = model_aq(ValSpectra[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "    Prediction_cpu[0][Prediction_cpu[0] < 0] = 0\n",
    "    \n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.3  -  AllAq1\n",
      "87.09  -  AllAq5\n",
      "96.02  -  AllAq25\n",
      "98.0  -  AllAq50\n",
      "96.03  -  ThreeAddedSinglets\n",
      "96.09  -  ThirtyAddedSinglets\n",
      "96.79  -  ShiftedSpec\n",
      "94.76  -  SineBase\n",
      "58.3  -  HighDynamicRange\n",
      "inf  -  HalfZeros\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - \",ValSpecNames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist9 - HD-Range w/ 1's\n",
      "tensor([0.8149, 0.8944, 0.9175, 1.0043, 0.7298, 0.8750, 1.3530, 0.6859, 0.9632,\n",
      "        1.2207, 0.5444, 0.8407, 0.7106, 0.7134, 1.0106, 0.7951, 0.9120, 0.8198,\n",
      "        0.8566, 0.9513, 1.1113, 0.7353, 0.6171, 1.0384, 0.8487, 0.7623, 0.8810,\n",
      "        0.8684, 0.9537, 1.3547, 1.0797, 1.2304, 0.9661, 1.2335, 0.9144, 1.0013,\n",
      "        0.5737, 0.7035, 0.9683, 1.1587, 0.6345, 0.6668, 0.6916, 0.7531],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Dist9 - HD-Range w/ 0's\n",
      "tensor([0.7807, 0.8870, 0.9073, 0.9994, 0.7319, 0.8752, 1.3456, 0.6787, 0.9565,\n",
      "        1.2064, 0.5390, 0.8287, 0.6996, 0.7054, 0.9993, 0.7842, 0.9043, 0.8087,\n",
      "        0.8443, 0.9428, 1.1002, 0.7218, 0.6151, 1.0319, 0.8404, 0.7524, 0.8745,\n",
      "        0.8619, 0.9509, 1.3490, 1.0688, 1.2213, 0.9589, 1.2369, 0.9055, 0.9926,\n",
      "        0.5754, 0.6980, 0.9623, 1.1501, 0.6251, 0.6628, 0.6789, 0.7458],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Dist9 - Blank\n",
      "tensor([0.6334, 0.4941, 0.5780, 0.5006, 0.6378, 0.5635, 0.7591, 0.6802, 0.6177,\n",
      "        0.4851, 0.4731, 0.6518, 0.5334, 0.6114, 0.3427, 0.5690, 0.6691, 0.3189,\n",
      "        0.4519, 0.4833, 0.7793, 0.5260, 0.5946, 0.6250, 0.5161, 0.4641, 0.5770,\n",
      "        0.5058, 0.6554, 0.7462, 0.7198, 0.6694, 0.5797, 0.6959, 0.6008, 0.5360,\n",
      "        0.5613, 0.5361, 0.6827, 0.5887, 0.6937, 0.5112, 0.5261, 0.5197],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(ValSpectra[8])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist9 - HD-Range w/ 1's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[9])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist9 - HD-Range w/ 0's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[10])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist9 - Blank\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
