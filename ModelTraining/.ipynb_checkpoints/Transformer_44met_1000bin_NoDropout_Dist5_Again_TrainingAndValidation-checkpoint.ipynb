{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer Encoder on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"Transformer_44met_TrainingAndValidation_1000bin_NoDropout_Dist5_Again_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('Dataset44_Dist5_Spec.npy')\n",
    "conc1 = np.load('Dataset44_Dist5_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "#spectraVal = np.load('Dataset44_Dist5_Val_Spec.npy')\n",
    "#concVal = np.load('Dataset44_Dist5_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "#ValSpectra = np.load(\"Dataset44_Dist5_RepresentativeExamples_Spectra.npy\")\n",
    "#ValConc = np.load(\"Dataset44_Dist5_RepresentativeExamples_Concentrations.npy\")\n",
    "#ValSpecNames = np.load(\"Dataset44_Dist5_RepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 32, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.Linear(23552, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Binning\n",
    "        batch_size, seq_length = x.size()\n",
    "        num_bins = seq_length // self.input_dim\n",
    "        x = x.view(batch_size, num_bins, self.input_dim)  # (batch_size, num_bins, input_dim)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = x.permute(1, 0, 2)  # (num_bins, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)  # (num_bins, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Reconstruct original sequence\n",
    "        x = x.reshape(batch_size, num_bins * d_model)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder(x)  # (batch_size, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "input_dim = 1000  # Size of each bin\n",
    "d_model = 512     # Embedding dimension\n",
    "nhead = 1         # Number of attention heads\n",
    "num_encoder_layers = 1  # Number of transformer encoder layers\n",
    "dim_feedforward = 2048  # Feedforward dimension\n",
    "dropout = 0.0     # Dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "       \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/214], Train Loss: 4369360.0166, Test Loss: 1006840.3535\n",
      "Epoch [2/214], Train Loss: 2813543.1860, Test Loss: 555389.5039\n",
      "Epoch [3/214], Train Loss: 1528278.0654, Test Loss: 221569.2198\n",
      "Epoch [4/214], Train Loss: 571413.1769, Test Loss: 83165.9757\n",
      "Epoch [5/214], Train Loss: 245189.0149, Test Loss: 47893.3774\n",
      "Epoch [6/214], Train Loss: 157966.8657, Test Loss: 32852.3861\n",
      "Epoch [7/214], Train Loss: 114242.8048, Test Loss: 24819.5542\n",
      "Epoch [8/214], Train Loss: 90514.0927, Test Loss: 22484.4232\n",
      "Epoch [9/214], Train Loss: 76298.1618, Test Loss: 19374.6333\n",
      "Epoch [10/214], Train Loss: 64886.5840, Test Loss: 16240.5978\n",
      "Epoch [11/214], Train Loss: 56127.2765, Test Loss: 13726.7287\n",
      "Epoch [12/214], Train Loss: 48910.5915, Test Loss: 12304.2846\n",
      "Epoch [13/214], Train Loss: 43190.7371, Test Loss: 11177.8027\n",
      "Epoch [14/214], Train Loss: 39805.4860, Test Loss: 10367.3500\n",
      "Epoch [15/214], Train Loss: 35937.8120, Test Loss: 9592.3829\n",
      "Epoch [16/214], Train Loss: 32223.8781, Test Loss: 7952.7220\n",
      "Epoch [17/214], Train Loss: 30031.1231, Test Loss: 8255.7936\n",
      "Epoch [18/214], Train Loss: 27323.9379, Test Loss: 8422.2666\n",
      "Epoch [19/214], Train Loss: 28203.1893, Test Loss: 11427.5163\n",
      "Epoch [20/214], Train Loss: 25877.6951, Test Loss: 6920.5093\n",
      "Epoch [21/214], Train Loss: 23853.8052, Test Loss: 6520.5959\n",
      "Epoch [22/214], Train Loss: 21711.5221, Test Loss: 7315.8226\n",
      "Epoch [23/214], Train Loss: 20595.6941, Test Loss: 6905.4508\n",
      "Epoch [24/214], Train Loss: 273395.8669, Test Loss: 65086.7147\n",
      "Epoch [25/214], Train Loss: 62585.1359, Test Loss: 7859.9584\n",
      "Epoch [26/214], Train Loss: 26100.2515, Test Loss: 6278.3427\n",
      "Epoch [27/214], Train Loss: 19736.7289, Test Loss: 5705.1424\n",
      "Epoch [28/214], Train Loss: 18078.3826, Test Loss: 5184.9675\n",
      "Epoch [29/214], Train Loss: 16810.4692, Test Loss: 5245.7167\n",
      "Epoch [30/214], Train Loss: 16805.0438, Test Loss: 5016.3070\n",
      "Epoch [31/214], Train Loss: 15443.7330, Test Loss: 4615.4556\n",
      "Epoch [32/214], Train Loss: 15387.5528, Test Loss: 4783.4538\n",
      "Epoch [33/214], Train Loss: 15430.5989, Test Loss: 4673.6485\n",
      "Epoch [34/214], Train Loss: 61290.0995, Test Loss: 7674.7913\n",
      "Epoch [35/214], Train Loss: 16685.1553, Test Loss: 4416.4626\n",
      "Epoch [36/214], Train Loss: 13451.2486, Test Loss: 4255.5050\n",
      "Epoch [37/214], Train Loss: 13048.6449, Test Loss: 4252.7784\n",
      "Epoch [38/214], Train Loss: 12882.4143, Test Loss: 4319.1792\n",
      "Epoch [39/214], Train Loss: 12796.1118, Test Loss: 4593.3015\n",
      "Epoch [40/214], Train Loss: 13153.1759, Test Loss: 4260.2768\n",
      "Epoch [41/214], Train Loss: 12925.9342, Test Loss: 4006.5904\n",
      "Epoch [42/214], Train Loss: 13374.1187, Test Loss: 4363.9696\n",
      "Epoch [43/214], Train Loss: 13269.6981, Test Loss: 4703.4954\n",
      "Epoch [44/214], Train Loss: 13022.9125, Test Loss: 4483.4479\n",
      "Epoch [45/214], Train Loss: 13152.8382, Test Loss: 4735.3668\n",
      "Epoch [46/214], Train Loss: 44435.3156, Test Loss: 4105.9203\n",
      "Epoch [47/214], Train Loss: 11671.8664, Test Loss: 3608.4235\n",
      "Epoch [48/214], Train Loss: 10085.8421, Test Loss: 3362.2677\n",
      "Epoch [49/214], Train Loss: 9818.7631, Test Loss: 3328.6247\n",
      "Epoch [50/214], Train Loss: 9700.5620, Test Loss: 3400.4173\n",
      "Epoch [51/214], Train Loss: 10440.2378, Test Loss: 3523.4657\n",
      "Epoch [52/214], Train Loss: 10170.1881, Test Loss: 3623.5679\n",
      "Epoch [53/214], Train Loss: 10708.0927, Test Loss: 4034.3627\n",
      "Epoch [54/214], Train Loss: 11028.0941, Test Loss: 4704.9559\n",
      "Epoch [55/214], Train Loss: 19749.0880, Test Loss: 6412.1626\n",
      "Epoch [56/214], Train Loss: 19610.8145, Test Loss: 3236.6909\n",
      "Epoch [57/214], Train Loss: 9194.7753, Test Loss: 4688.8982\n",
      "Epoch [58/214], Train Loss: 9893.7322, Test Loss: 4422.0226\n",
      "Epoch [59/214], Train Loss: 9675.1569, Test Loss: 3421.9529\n",
      "Epoch [60/214], Train Loss: 9111.8136, Test Loss: 3282.5710\n",
      "Epoch [61/214], Train Loss: 10065.9736, Test Loss: 5751.8597\n",
      "Epoch [62/214], Train Loss: 11121.9036, Test Loss: 7865.9833\n",
      "Epoch [63/214], Train Loss: 25393.4573, Test Loss: 3721.9175\n",
      "Epoch [64/214], Train Loss: 12015.8957, Test Loss: 4770.6313\n",
      "Epoch [65/214], Train Loss: 20567.7628, Test Loss: 3249.5305\n",
      "Epoch [66/214], Train Loss: 11056.4768, Test Loss: 3990.1374\n",
      "Epoch [67/214], Train Loss: 9461.9186, Test Loss: 3643.0614\n",
      "Epoch [68/214], Train Loss: 7928.8387, Test Loss: 2903.6450\n",
      "Epoch [69/214], Train Loss: 7598.2230, Test Loss: 2982.2091\n",
      "Epoch [70/214], Train Loss: 8594.8922, Test Loss: 3065.1563\n",
      "Epoch [71/214], Train Loss: 8039.4822, Test Loss: 3134.7175\n",
      "Epoch [72/214], Train Loss: 8902.5668, Test Loss: 3453.9662\n",
      "Epoch [73/214], Train Loss: 9599.8570, Test Loss: 3190.1790\n",
      "Epoch [74/214], Train Loss: 30863.8973, Test Loss: 61687.1045\n",
      "Epoch [75/214], Train Loss: 16267.0225, Test Loss: 2967.7272\n",
      "Epoch [76/214], Train Loss: 6903.5675, Test Loss: 2605.7074\n",
      "Epoch [77/214], Train Loss: 6502.7938, Test Loss: 2786.5544\n",
      "Epoch [78/214], Train Loss: 6541.8554, Test Loss: 2791.4864\n",
      "Epoch [79/214], Train Loss: 6813.4793, Test Loss: 3069.2811\n",
      "Epoch [80/214], Train Loss: 6899.0877, Test Loss: 2762.9950\n",
      "Epoch [81/214], Train Loss: 7353.7076, Test Loss: 3346.3661\n",
      "Epoch [82/214], Train Loss: 639809.7170, Test Loss: 16867.8111\n",
      "Epoch [83/214], Train Loss: 46390.0063, Test Loss: 7720.6983\n",
      "Epoch [84/214], Train Loss: 24633.2056, Test Loss: 6482.0154\n",
      "Epoch [85/214], Train Loss: 24778.1294, Test Loss: 5705.8096\n",
      "Epoch [86/214], Train Loss: 15989.5839, Test Loss: 4756.1253\n",
      "Epoch [87/214], Train Loss: 14437.8194, Test Loss: 4290.1702\n",
      "Epoch [88/214], Train Loss: 13788.6106, Test Loss: 3494.2624\n",
      "Epoch [89/214], Train Loss: 22185.1522, Test Loss: 3489.2486\n",
      "Epoch [90/214], Train Loss: 10328.8518, Test Loss: 3874.2945\n",
      "Epoch [91/214], Train Loss: 22173.3688, Test Loss: 3670.9818\n",
      "Epoch [92/214], Train Loss: 14895.4104, Test Loss: 3929.2858\n",
      "Epoch [93/214], Train Loss: 8762.2412, Test Loss: 3061.4447\n",
      "Epoch [94/214], Train Loss: 8444.0434, Test Loss: 3113.8252\n",
      "Epoch [95/214], Train Loss: 8350.3946, Test Loss: 2978.6709\n",
      "Epoch [96/214], Train Loss: 8225.1951, Test Loss: 3233.3729\n",
      "Epoch [97/214], Train Loss: 8059.0085, Test Loss: 2823.8915\n",
      "Epoch [98/214], Train Loss: 8099.3433, Test Loss: 3046.0671\n",
      "Epoch [99/214], Train Loss: 8097.3432, Test Loss: 2889.4878\n",
      "Epoch [100/214], Train Loss: 21324.3482, Test Loss: 2770.8755\n",
      "Epoch [101/214], Train Loss: 7079.4169, Test Loss: 2768.0433\n",
      "Epoch [102/214], Train Loss: 6779.9669, Test Loss: 2589.7194\n",
      "Epoch [103/214], Train Loss: 6796.3177, Test Loss: 2722.7609\n",
      "Epoch [104/214], Train Loss: 7136.2963, Test Loss: 2881.7429\n",
      "Epoch [105/214], Train Loss: 7379.6136, Test Loss: 2963.5376\n",
      "Epoch [106/214], Train Loss: 7446.0996, Test Loss: 2937.5166\n",
      "Epoch [107/214], Train Loss: 116114.7306, Test Loss: 14045.8148\n",
      "Epoch [108/214], Train Loss: 17229.8242, Test Loss: 3574.5888\n",
      "Epoch [109/214], Train Loss: 9080.2400, Test Loss: 3148.8921\n",
      "Epoch [110/214], Train Loss: 7621.6770, Test Loss: 2779.5943\n",
      "Epoch [111/214], Train Loss: 7216.2143, Test Loss: 3064.0881\n",
      "Epoch [112/214], Train Loss: 7017.1420, Test Loss: 2763.7786\n",
      "Epoch [113/214], Train Loss: 6804.7660, Test Loss: 2803.5584\n",
      "Epoch [114/214], Train Loss: 6836.9823, Test Loss: 2728.0634\n",
      "Epoch [115/214], Train Loss: 6826.5123, Test Loss: 2743.0912\n",
      "Epoch [116/214], Train Loss: 6796.7286, Test Loss: 2886.2187\n",
      "Epoch [117/214], Train Loss: 6980.9871, Test Loss: 2774.0114\n",
      "Epoch [118/214], Train Loss: 7177.1908, Test Loss: 2680.3095\n",
      "Epoch [119/214], Train Loss: 9739.1274, Test Loss: 3360.1912\n",
      "Epoch [120/214], Train Loss: 7056.0879, Test Loss: 2635.1015\n",
      "Epoch [121/214], Train Loss: 6542.0578, Test Loss: 2657.7731\n",
      "Epoch [122/214], Train Loss: 6613.3384, Test Loss: 2573.6808\n",
      "Epoch [123/214], Train Loss: 6664.6995, Test Loss: 2791.8965\n",
      "Epoch [124/214], Train Loss: 7330.4220, Test Loss: 3173.4117\n",
      "Epoch [125/214], Train Loss: 6906.1005, Test Loss: 2775.8824\n",
      "Epoch [126/214], Train Loss: 6928.7083, Test Loss: 2756.4131\n",
      "Epoch [127/214], Train Loss: 6934.7744, Test Loss: 2868.2520\n",
      "Epoch [128/214], Train Loss: 6889.2214, Test Loss: 2823.9135\n",
      "Epoch [129/214], Train Loss: 8911.2534, Test Loss: 3342.4676\n",
      "Epoch [130/214], Train Loss: 6359.0151, Test Loss: 2392.1615\n",
      "Epoch [131/214], Train Loss: 5860.3800, Test Loss: 2540.4815\n",
      "Epoch [132/214], Train Loss: 6100.7065, Test Loss: 2536.6633\n",
      "Epoch [133/214], Train Loss: 6131.4861, Test Loss: 2745.5052\n",
      "Epoch [134/214], Train Loss: 6583.1772, Test Loss: 2807.2458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [135/214], Train Loss: 6283.2372, Test Loss: 2562.5606\n",
      "Epoch [136/214], Train Loss: 7095.4123, Test Loss: 2745.8648\n",
      "Epoch [137/214], Train Loss: 6352.4601, Test Loss: 3578.8279\n",
      "Epoch [138/214], Train Loss: 6412.4513, Test Loss: 2361.5124\n",
      "Epoch [139/214], Train Loss: 6161.8786, Test Loss: 2769.3006\n",
      "Epoch [140/214], Train Loss: 6019.8007, Test Loss: 2400.0540\n",
      "Epoch [141/214], Train Loss: 11451.7848, Test Loss: 2425.8925\n",
      "Epoch [142/214], Train Loss: 5446.7123, Test Loss: 2416.5074\n",
      "Epoch [143/214], Train Loss: 5044.4783, Test Loss: 2485.8923\n",
      "Epoch [144/214], Train Loss: 5260.8592, Test Loss: 2637.1394\n",
      "Epoch [145/214], Train Loss: 5266.4874, Test Loss: 2609.2795\n",
      "Epoch [146/214], Train Loss: 7904.9995, Test Loss: 2556.0675\n",
      "Epoch [147/214], Train Loss: 5073.6626, Test Loss: 2451.2199\n",
      "Epoch [148/214], Train Loss: 5142.1595, Test Loss: 2479.2234\n",
      "Epoch [149/214], Train Loss: 5314.5373, Test Loss: 2575.7549\n",
      "Epoch [150/214], Train Loss: 5397.6833, Test Loss: 2621.8579\n",
      "Epoch [151/214], Train Loss: 5721.3526, Test Loss: 2607.4278\n",
      "Epoch [152/214], Train Loss: 6391.8107, Test Loss: 2475.8843\n",
      "Epoch [153/214], Train Loss: 5408.5063, Test Loss: 2364.5697\n",
      "Epoch [154/214], Train Loss: 6283.0433, Test Loss: 2523.4744\n",
      "Epoch [155/214], Train Loss: 5184.5679, Test Loss: 2574.0932\n",
      "Epoch [156/214], Train Loss: 6421.3014, Test Loss: 2668.3422\n",
      "Epoch [157/214], Train Loss: 5402.5135, Test Loss: 2416.1006\n",
      "Epoch [158/214], Train Loss: 5286.5674, Test Loss: 2383.0492\n",
      "Epoch [159/214], Train Loss: 5268.4886, Test Loss: 2407.8690\n",
      "Epoch [160/214], Train Loss: 5690.8068, Test Loss: 2565.2787\n",
      "Epoch [161/214], Train Loss: 5315.1980, Test Loss: 2565.9405\n",
      "Epoch [162/214], Train Loss: 5254.2261, Test Loss: 2314.5164\n",
      "Epoch [163/214], Train Loss: 5381.1685, Test Loss: 2399.3851\n",
      "Epoch [164/214], Train Loss: 4873.1446, Test Loss: 2453.5295\n",
      "Epoch [165/214], Train Loss: 5146.6025, Test Loss: 2490.2595\n",
      "Epoch [166/214], Train Loss: 5066.8975, Test Loss: 2383.3975\n",
      "Epoch [167/214], Train Loss: 5191.5335, Test Loss: 2391.6290\n",
      "Epoch [168/214], Train Loss: 5274.7126, Test Loss: 2471.3577\n",
      "Epoch [169/214], Train Loss: 4940.4296, Test Loss: 2932.7242\n",
      "Epoch [170/214], Train Loss: 4922.0658, Test Loss: 2287.6806\n",
      "Epoch [171/214], Train Loss: 5151.2159, Test Loss: 2306.7534\n",
      "Epoch [172/214], Train Loss: 4890.1788, Test Loss: 2296.0741\n",
      "Epoch [173/214], Train Loss: 5008.2412, Test Loss: 2397.5356\n",
      "Epoch [174/214], Train Loss: 5677.0227, Test Loss: 2418.0881\n",
      "Epoch [175/214], Train Loss: 4518.6515, Test Loss: 2396.1349\n",
      "Epoch [176/214], Train Loss: 4773.4121, Test Loss: 2468.3781\n",
      "Epoch [177/214], Train Loss: 4612.4341, Test Loss: 2294.0104\n",
      "Epoch [178/214], Train Loss: 5900.9487, Test Loss: 2659.4776\n",
      "Epoch [179/214], Train Loss: 4642.2187, Test Loss: 2248.8741\n",
      "Epoch [180/214], Train Loss: 4491.5613, Test Loss: 2282.6513\n",
      "Epoch [181/214], Train Loss: 4519.3725, Test Loss: 2275.3698\n",
      "Epoch [182/214], Train Loss: 4545.4177, Test Loss: 2351.4531\n",
      "Epoch [183/214], Train Loss: 4730.8599, Test Loss: 2410.3160\n",
      "Epoch [184/214], Train Loss: 4513.4276, Test Loss: 2232.2545\n",
      "Epoch [185/214], Train Loss: 4384.3698, Test Loss: 2137.0233\n",
      "Epoch [186/214], Train Loss: 7402.7386, Test Loss: 2950.7254\n",
      "Epoch [187/214], Train Loss: 4357.3382, Test Loss: 2314.6719\n",
      "Epoch [188/214], Train Loss: 4063.7856, Test Loss: 2330.5526\n",
      "Epoch [189/214], Train Loss: 3971.4961, Test Loss: 2201.6641\n",
      "Epoch [190/214], Train Loss: 4006.3187, Test Loss: 2624.4324\n",
      "Epoch [191/214], Train Loss: 4276.2930, Test Loss: 2916.1486\n",
      "Epoch [192/214], Train Loss: 4503.0906, Test Loss: 2296.6566\n",
      "Epoch [193/214], Train Loss: 4247.4588, Test Loss: 2283.4293\n",
      "Epoch [194/214], Train Loss: 5845.0219, Test Loss: 2343.4883\n",
      "Epoch [195/214], Train Loss: 4400.2139, Test Loss: 2463.6833\n",
      "Epoch [196/214], Train Loss: 4079.2433, Test Loss: 2212.0684\n",
      "Epoch [197/214], Train Loss: 3953.0287, Test Loss: 2360.1511\n",
      "Epoch [198/214], Train Loss: 4374.8559, Test Loss: 2240.2110\n",
      "Epoch [199/214], Train Loss: 4747.8236, Test Loss: 2719.8384\n",
      "Epoch [200/214], Train Loss: 4517.9866, Test Loss: 2488.9595\n",
      "Epoch [201/214], Train Loss: 4038.4661, Test Loss: 2182.8568\n",
      "Epoch [202/214], Train Loss: 4002.7450, Test Loss: 2240.7611\n",
      "Epoch [203/214], Train Loss: 4154.1785, Test Loss: 2151.1795\n",
      "Epoch [204/214], Train Loss: 4005.6499, Test Loss: 2319.1144\n",
      "Epoch [205/214], Train Loss: 4220.2784, Test Loss: 2292.5232\n",
      "Epoch [206/214], Train Loss: 4394.1649, Test Loss: 2179.9990\n",
      "Epoch [207/214], Train Loss: 4196.3121, Test Loss: 2357.5736\n",
      "Epoch [208/214], Train Loss: 4384.7324, Test Loss: 2223.2681\n",
      "Epoch [209/214], Train Loss: 4020.5143, Test Loss: 2188.6124\n",
      "Epoch [210/214], Train Loss: 4060.2058, Test Loss: 2702.9599\n",
      "Epoch [211/214], Train Loss: 4301.4513, Test Loss: 2186.7220\n",
      "Epoch [212/214], Train Loss: 3907.5374, Test Loss: 2170.5694\n",
      "Epoch [213/214], Train Loss: 4394.6508, Test Loss: 2285.5416\n",
      "Epoch [214/214], Train Loss: 3894.4780, Test Loss: 2035.4799\n",
      "Execution time: 579.63987159729 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2035.4798583984375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load('Dataset44_Dist5_Val_Spec.npy')\n",
    "concVal = np.load('Dataset44_Dist5_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "ValSpectra = np.load(\"Dataset44_Dist5_RepresentativeExamples_Spectra.npy\")\n",
    "ValConc = np.load(\"Dataset44_Dist5_RepresentativeExamples_Concentrations.npy\")\n",
    "ValSpecNames = np.load(\"Dataset44_Dist5_RepresentativeExamples_VariableNames.npy\")\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ValConc = torch.tensor(ValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=23552, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ValConc[i]\n",
    "    Prediction = model_aq(ValSpectra[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.03  -  AllAq1\n",
      "1.59  -  AllAq5\n",
      "0.72  -  AllAq25\n",
      "1.49  -  AllAq50\n",
      "1.06  -  ThreeAddedSinglets\n",
      "4.72  -  ThirtyAddedSinglets\n",
      "74.78  -  ShiftedSpec\n",
      "32.28  -  SineBase\n",
      "80.73  -  HighDynamicRange\n",
      "inf  -  HalfZeros\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - \",ValSpecNames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist5 - HD-Range w/ 1's\n",
      "tensor([ 2.0345, 49.8143,  0.2665, 50.1297,  1.6480, 51.0745,  2.9037, 49.3319,\n",
      "         0.8775, 49.3895,  0.0000, 52.8247,  0.9186, 47.8408,  0.0918, 50.3880,\n",
      "         0.6889, 50.3005,  0.2150, 47.7139,  0.0000, 49.6068,  3.6918, 48.8991,\n",
      "         1.7999, 51.9566, 11.5035, 47.8026,  1.6543, 48.5136,  2.8510, 50.6913,\n",
      "         2.2599, 48.8167,  0.6834, 50.7618,  0.7868, 48.8140,  0.1450, 48.7735,\n",
      "         1.1544, 49.4561,  0.0000, 49.4941], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Dist5 - HD-Range w/ 0's\n",
      "tensor([ 0.9509, 49.8107,  0.0000, 50.1177,  0.6697, 51.0897,  1.9043, 49.3387,\n",
      "         0.0000, 49.3408,  0.0000, 52.8262,  0.0000, 47.8357,  0.0000, 50.4380,\n",
      "         0.0000, 50.2545,  0.0000, 47.7468,  0.0000, 49.5971,  2.6753, 48.8435,\n",
      "         0.7925, 51.9237, 10.5168, 47.8447,  0.6381, 48.4676,  1.9229, 50.4518,\n",
      "         1.2626, 48.7928,  0.0000, 50.7786,  0.0000, 48.7955,  0.0000, 48.7627,\n",
      "         0.1667, 49.4410,  0.0000, 49.5259], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Dist5 - Blank\n",
      "tensor([0.0311, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0194, 0.0000,\n",
      "        0.0000, 0.0000, 0.1499, 0.0000, 0.1381, 0.1178, 0.0000, 0.0240, 0.0000,\n",
      "        0.0000, 0.0000, 0.0266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0220, 0.0000, 0.0970, 0.0223, 0.0000, 0.0000, 0.1266,\n",
      "        0.0650, 0.0149, 0.0000, 0.0000, 0.0000, 0.0222, 0.0820, 0.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(ValSpectra[8])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist5 - HD-Range w/ 1's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[9])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist5 - HD-Range w/ 0's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[10])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist5 - Blank\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
