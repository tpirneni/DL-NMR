{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer Encoder on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"Transformer_44met_TrainingAndValidation_1000bin_NoDropout_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('Dataset44_Spec.npy')\n",
    "conc1 = np.load('Dataset44_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "#spectraVal = np.load('Dataset44_Val_Spec.npy')\n",
    "#concVal = np.load('Dataset44_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "#ValSpectra = np.load(\"Dataset44_RepresentativeExamples_Spectra.npy\")\n",
    "#ValConc = np.load(\"Dataset44_RepresentativeExamples_Concentrations.npy\")\n",
    "#ValSpecNames = np.load(\"Dataset44_RepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 32, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.Linear(23552, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Binning\n",
    "        batch_size, seq_length = x.size()\n",
    "        num_bins = seq_length // self.input_dim\n",
    "        x = x.view(batch_size, num_bins, self.input_dim)  # (batch_size, num_bins, input_dim)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = x.permute(1, 0, 2)  # (num_bins, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)  # (num_bins, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Reconstruct original sequence\n",
    "        x = x.reshape(batch_size, num_bins * d_model)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder(x)  # (batch_size, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "input_dim = 1000  # Size of each bin\n",
    "d_model = 512     # Embedding dimension\n",
    "nhead = 1         # Number of attention heads\n",
    "num_encoder_layers = 1  # Number of transformer encoder layers\n",
    "dim_feedforward = 2048  # Feedforward dimension\n",
    "dropout = 0.0     # Dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "       \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/1000], Train Loss: 3838602.7583, Test Loss: 734511.0522\n",
      "Epoch [2/1000], Train Loss: 1735770.1323, Test Loss: 173169.0690\n",
      "Epoch [3/1000], Train Loss: 415445.2115, Test Loss: 68892.4673\n",
      "Epoch [4/1000], Train Loss: 196021.7499, Test Loss: 36653.2902\n",
      "Epoch [5/1000], Train Loss: 118566.5149, Test Loss: 25023.1151\n",
      "Epoch [6/1000], Train Loss: 85947.5968, Test Loss: 18777.2193\n",
      "Epoch [7/1000], Train Loss: 67893.8178, Test Loss: 15857.8397\n",
      "Epoch [8/1000], Train Loss: 55481.5807, Test Loss: 14768.6482\n",
      "Epoch [9/1000], Train Loss: 48004.0894, Test Loss: 12483.7555\n",
      "Epoch [10/1000], Train Loss: 43077.7700, Test Loss: 10674.5310\n",
      "Epoch [11/1000], Train Loss: 38440.8471, Test Loss: 9758.7366\n",
      "Epoch [12/1000], Train Loss: 35028.3788, Test Loss: 9982.0055\n",
      "Epoch [13/1000], Train Loss: 31709.0295, Test Loss: 8271.8263\n",
      "Epoch [14/1000], Train Loss: 29657.3546, Test Loss: 7364.2374\n",
      "Epoch [15/1000], Train Loss: 26894.8267, Test Loss: 8148.7481\n",
      "Epoch [16/1000], Train Loss: 25302.4500, Test Loss: 7259.2790\n",
      "Epoch [17/1000], Train Loss: 24560.2546, Test Loss: 6929.1345\n",
      "Epoch [18/1000], Train Loss: 22429.2531, Test Loss: 5964.3900\n",
      "Epoch [19/1000], Train Loss: 20861.8473, Test Loss: 6808.4865\n",
      "Epoch [20/1000], Train Loss: 20261.6268, Test Loss: 7239.4499\n",
      "Epoch [21/1000], Train Loss: 19562.0634, Test Loss: 5412.1049\n",
      "Epoch [22/1000], Train Loss: 592752.1494, Test Loss: 246966.1013\n",
      "Epoch [23/1000], Train Loss: 99869.1863, Test Loss: 8245.5117\n",
      "Epoch [24/1000], Train Loss: 25120.3513, Test Loss: 5879.7296\n",
      "Epoch [25/1000], Train Loss: 19460.2570, Test Loss: 5177.4997\n",
      "Epoch [26/1000], Train Loss: 17076.8317, Test Loss: 4847.4907\n",
      "Epoch [27/1000], Train Loss: 15369.6572, Test Loss: 4426.1978\n",
      "Epoch [28/1000], Train Loss: 14644.4455, Test Loss: 4574.0797\n",
      "Epoch [29/1000], Train Loss: 14158.5864, Test Loss: 4238.4231\n",
      "Epoch [30/1000], Train Loss: 13733.7493, Test Loss: 4510.7470\n",
      "Epoch [31/1000], Train Loss: 13441.3163, Test Loss: 4521.1059\n",
      "Epoch [32/1000], Train Loss: 13423.4533, Test Loss: 4558.7822\n",
      "Epoch [33/1000], Train Loss: 13371.2768, Test Loss: 4149.0840\n",
      "Epoch [34/1000], Train Loss: 13830.8135, Test Loss: 4295.6410\n",
      "Epoch [35/1000], Train Loss: 12982.5348, Test Loss: 3883.9054\n",
      "Epoch [36/1000], Train Loss: 12676.1990, Test Loss: 4244.0654\n",
      "Epoch [37/1000], Train Loss: 15792.1046, Test Loss: 4475.8086\n",
      "Epoch [38/1000], Train Loss: 24223.0894, Test Loss: 12752.4723\n",
      "Epoch [39/1000], Train Loss: 12582.7309, Test Loss: 3448.4258\n",
      "Epoch [40/1000], Train Loss: 9688.9584, Test Loss: 3253.3085\n",
      "Epoch [41/1000], Train Loss: 10202.5106, Test Loss: 3845.8288\n",
      "Epoch [42/1000], Train Loss: 10149.5634, Test Loss: 3874.9680\n",
      "Epoch [43/1000], Train Loss: 11025.3929, Test Loss: 3487.3793\n",
      "Epoch [44/1000], Train Loss: 10843.1225, Test Loss: 4927.8075\n",
      "Epoch [45/1000], Train Loss: 83029.9501, Test Loss: 4101.1018\n",
      "Epoch [46/1000], Train Loss: 10679.5801, Test Loss: 3318.4522\n",
      "Epoch [47/1000], Train Loss: 8841.4147, Test Loss: 3245.6094\n",
      "Epoch [48/1000], Train Loss: 8369.2481, Test Loss: 3188.0966\n",
      "Epoch [49/1000], Train Loss: 8304.6890, Test Loss: 2992.2745\n",
      "Epoch [50/1000], Train Loss: 8355.6786, Test Loss: 3150.6034\n",
      "Epoch [51/1000], Train Loss: 8253.3811, Test Loss: 3696.3301\n",
      "Epoch [52/1000], Train Loss: 8647.2019, Test Loss: 3249.7149\n",
      "Epoch [53/1000], Train Loss: 8901.0182, Test Loss: 3061.6682\n",
      "Epoch [54/1000], Train Loss: 9002.6512, Test Loss: 3508.7248\n",
      "Epoch [55/1000], Train Loss: 9498.6132, Test Loss: 6498.7179\n",
      "Epoch [56/1000], Train Loss: 204982.0677, Test Loss: 4792.9728\n",
      "Epoch [57/1000], Train Loss: 12921.8190, Test Loss: 3659.1013\n",
      "Epoch [58/1000], Train Loss: 9946.9165, Test Loss: 3193.5331\n",
      "Epoch [59/1000], Train Loss: 8468.9359, Test Loss: 3152.6940\n",
      "Epoch [60/1000], Train Loss: 8211.1803, Test Loss: 3119.4987\n",
      "Epoch [61/1000], Train Loss: 7998.0304, Test Loss: 3270.6483\n",
      "Epoch [62/1000], Train Loss: 7874.3619, Test Loss: 3123.9719\n",
      "Epoch [63/1000], Train Loss: 7755.8077, Test Loss: 3021.2398\n",
      "Epoch [64/1000], Train Loss: 7596.9108, Test Loss: 3044.1720\n",
      "Epoch [65/1000], Train Loss: 7969.2666, Test Loss: 3178.3538\n",
      "Epoch [66/1000], Train Loss: 8126.7322, Test Loss: 3486.0341\n",
      "Epoch [67/1000], Train Loss: 13149.6253, Test Loss: 3706.3425\n",
      "Epoch [68/1000], Train Loss: 10366.2532, Test Loss: 3251.7737\n",
      "Epoch [69/1000], Train Loss: 8500.8567, Test Loss: 30563.7356\n",
      "Epoch [70/1000], Train Loss: 30792.9843, Test Loss: 3194.8251\n",
      "Epoch [71/1000], Train Loss: 7292.9346, Test Loss: 2840.7451\n",
      "Epoch [72/1000], Train Loss: 7030.9111, Test Loss: 2973.0209\n",
      "Epoch [73/1000], Train Loss: 6897.4363, Test Loss: 2773.6302\n",
      "Epoch [74/1000], Train Loss: 7592.5239, Test Loss: 3152.4512\n",
      "Epoch [75/1000], Train Loss: 7069.6952, Test Loss: 2964.8134\n",
      "Epoch [76/1000], Train Loss: 7263.3872, Test Loss: 2973.0900\n",
      "Epoch [77/1000], Train Loss: 7820.6615, Test Loss: 3201.5862\n",
      "Epoch [78/1000], Train Loss: 239680.9649, Test Loss: 24649.8122\n",
      "Epoch [79/1000], Train Loss: 29262.1891, Test Loss: 4139.4463\n",
      "Epoch [80/1000], Train Loss: 10926.2585, Test Loss: 3178.1855\n",
      "Epoch [81/1000], Train Loss: 9256.4202, Test Loss: 3068.3096\n",
      "Epoch [82/1000], Train Loss: 8002.0159, Test Loss: 3031.0084\n",
      "Epoch [83/1000], Train Loss: 7368.2878, Test Loss: 2802.7334\n",
      "Epoch [84/1000], Train Loss: 8343.2976, Test Loss: 2738.1004\n",
      "Epoch [85/1000], Train Loss: 6348.5995, Test Loss: 3094.4190\n",
      "Epoch [86/1000], Train Loss: 6251.6998, Test Loss: 2994.6371\n",
      "Epoch [87/1000], Train Loss: 6339.9147, Test Loss: 2715.2281\n",
      "Epoch [88/1000], Train Loss: 8673.5366, Test Loss: 2653.0473\n",
      "Epoch [89/1000], Train Loss: 7731.6783, Test Loss: 4254.7018\n",
      "Epoch [90/1000], Train Loss: 9222.0754, Test Loss: 2597.3302\n",
      "Epoch [91/1000], Train Loss: 6060.7090, Test Loss: 2682.0431\n",
      "Epoch [92/1000], Train Loss: 49258.2257, Test Loss: 4300.3535\n",
      "Epoch [93/1000], Train Loss: 9506.2529, Test Loss: 2751.9924\n",
      "Epoch [94/1000], Train Loss: 6327.5338, Test Loss: 2602.3445\n",
      "Epoch [95/1000], Train Loss: 7623.4997, Test Loss: 2788.2905\n",
      "Epoch [96/1000], Train Loss: 6661.1956, Test Loss: 2419.1024\n",
      "Epoch [97/1000], Train Loss: 6326.2245, Test Loss: 2527.9032\n",
      "Epoch [98/1000], Train Loss: 5984.3603, Test Loss: 2621.2319\n",
      "Epoch [99/1000], Train Loss: 5911.6976, Test Loss: 2676.0017\n",
      "Epoch [100/1000], Train Loss: 6354.8945, Test Loss: 2937.4633\n",
      "Epoch [101/1000], Train Loss: 6348.8098, Test Loss: 3179.4384\n",
      "Epoch [102/1000], Train Loss: 8590.5441, Test Loss: 3259.5304\n",
      "Epoch [103/1000], Train Loss: 12662.9556, Test Loss: 2722.5723\n",
      "Epoch [104/1000], Train Loss: 10472.1789, Test Loss: 2913.3387\n",
      "Epoch [105/1000], Train Loss: 6484.2664, Test Loss: 2560.2513\n",
      "Epoch [106/1000], Train Loss: 42572.2502, Test Loss: 5461.1797\n",
      "Epoch [107/1000], Train Loss: 8222.0236, Test Loss: 2634.2361\n",
      "Epoch [108/1000], Train Loss: 5586.7214, Test Loss: 2450.1120\n",
      "Epoch [109/1000], Train Loss: 5479.5783, Test Loss: 2413.6133\n",
      "Epoch [110/1000], Train Loss: 5165.1931, Test Loss: 2492.0095\n",
      "Epoch [111/1000], Train Loss: 5160.0650, Test Loss: 2421.7530\n",
      "Epoch [112/1000], Train Loss: 5244.4242, Test Loss: 2516.9583\n",
      "Epoch [113/1000], Train Loss: 5361.7429, Test Loss: 2583.9849\n",
      "Epoch [114/1000], Train Loss: 5760.3288, Test Loss: 2540.9938\n",
      "Epoch [115/1000], Train Loss: 6219.3452, Test Loss: 2596.0185\n",
      "Epoch [116/1000], Train Loss: 6668.3408, Test Loss: 2895.3843\n",
      "Epoch [117/1000], Train Loss: 5705.9574, Test Loss: 3109.2323\n",
      "Epoch [118/1000], Train Loss: 12961.1907, Test Loss: 16472.1276\n",
      "Epoch [119/1000], Train Loss: 36287.7668, Test Loss: 7287.1455\n",
      "Epoch [120/1000], Train Loss: 8852.9562, Test Loss: 2547.8447\n",
      "Epoch [121/1000], Train Loss: 5645.5874, Test Loss: 2399.2997\n",
      "Epoch [122/1000], Train Loss: 4961.1538, Test Loss: 2300.5293\n",
      "Epoch [123/1000], Train Loss: 4786.0990, Test Loss: 2296.5644\n",
      "Epoch [124/1000], Train Loss: 4994.0522, Test Loss: 2385.6207\n",
      "Epoch [125/1000], Train Loss: 5510.5945, Test Loss: 2515.2127\n",
      "Epoch [126/1000], Train Loss: 5793.0733, Test Loss: 2768.5195\n",
      "Epoch [127/1000], Train Loss: 6140.1594, Test Loss: 2676.0453\n",
      "Epoch [128/1000], Train Loss: 17799.8693, Test Loss: 6812.8682\n",
      "Epoch [129/1000], Train Loss: 7956.5724, Test Loss: 2552.6943\n",
      "Epoch [130/1000], Train Loss: 6029.8269, Test Loss: 2579.3202\n",
      "Epoch [131/1000], Train Loss: 5037.8930, Test Loss: 2505.6427\n",
      "Epoch [132/1000], Train Loss: 4794.1954, Test Loss: 2539.5558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [133/1000], Train Loss: 5014.8535, Test Loss: 2321.1012\n",
      "Epoch [134/1000], Train Loss: 5317.0190, Test Loss: 2486.9333\n",
      "Epoch [135/1000], Train Loss: 5757.4004, Test Loss: 2627.4896\n",
      "Epoch [136/1000], Train Loss: 5283.1786, Test Loss: 2586.0700\n",
      "Epoch [137/1000], Train Loss: 20735.7451, Test Loss: 5052.0820\n",
      "Epoch [138/1000], Train Loss: 9015.3980, Test Loss: 2385.9498\n",
      "Epoch [139/1000], Train Loss: 4367.1063, Test Loss: 2208.7507\n",
      "Epoch [140/1000], Train Loss: 5355.2770, Test Loss: 2188.8632\n",
      "Epoch [141/1000], Train Loss: 4402.7366, Test Loss: 2311.4315\n",
      "Epoch [142/1000], Train Loss: 4327.0969, Test Loss: 2245.3807\n",
      "Epoch [143/1000], Train Loss: 4616.0857, Test Loss: 2408.0996\n",
      "Epoch [144/1000], Train Loss: 4766.7225, Test Loss: 2288.6656\n",
      "Epoch [145/1000], Train Loss: 5735.0671, Test Loss: 2576.3637\n",
      "Epoch [146/1000], Train Loss: 11970.6225, Test Loss: 11727.8198\n",
      "Epoch [147/1000], Train Loss: 34734.5087, Test Loss: 3452.3885\n",
      "Epoch [148/1000], Train Loss: 6238.6504, Test Loss: 2300.0622\n",
      "Epoch [149/1000], Train Loss: 4799.1053, Test Loss: 2246.5140\n",
      "Epoch [150/1000], Train Loss: 4363.0969, Test Loss: 2237.1084\n",
      "Epoch [151/1000], Train Loss: 10189.2604, Test Loss: 2325.5965\n",
      "Epoch [152/1000], Train Loss: 4626.7465, Test Loss: 2223.7778\n",
      "Epoch [153/1000], Train Loss: 4284.5534, Test Loss: 2198.0994\n",
      "Epoch [154/1000], Train Loss: 4315.5917, Test Loss: 2211.0192\n",
      "Epoch [155/1000], Train Loss: 4375.1126, Test Loss: 2275.4683\n",
      "Epoch [156/1000], Train Loss: 4633.2338, Test Loss: 2768.9980\n",
      "Epoch [157/1000], Train Loss: 15647.3821, Test Loss: 2501.5325\n",
      "Epoch [158/1000], Train Loss: 4564.2358, Test Loss: 2134.7492\n",
      "Epoch [159/1000], Train Loss: 3977.0272, Test Loss: 2153.5067\n",
      "Epoch [160/1000], Train Loss: 4208.8918, Test Loss: 2137.4484\n",
      "Epoch [161/1000], Train Loss: 47183.4837, Test Loss: 2583.5457\n",
      "Epoch [162/1000], Train Loss: 5251.3844, Test Loss: 2284.9194\n",
      "Epoch [163/1000], Train Loss: 4390.9512, Test Loss: 2248.6143\n",
      "Epoch [164/1000], Train Loss: 4280.8254, Test Loss: 2138.0767\n",
      "Epoch [165/1000], Train Loss: 4196.9203, Test Loss: 2228.9583\n",
      "Epoch [166/1000], Train Loss: 4405.5087, Test Loss: 2182.4440\n",
      "Epoch [167/1000], Train Loss: 4523.0445, Test Loss: 2142.6356\n",
      "Epoch [168/1000], Train Loss: 4301.5961, Test Loss: 2417.9552\n",
      "Epoch [169/1000], Train Loss: 4460.1167, Test Loss: 2350.5150\n",
      "Epoch [170/1000], Train Loss: 4903.0474, Test Loss: 2348.3702\n",
      "Epoch [171/1000], Train Loss: 66733.5755, Test Loss: 2841.5583\n",
      "Epoch [172/1000], Train Loss: 5801.6614, Test Loss: 2265.9471\n",
      "Epoch [173/1000], Train Loss: 4471.4871, Test Loss: 2311.9911\n",
      "Epoch [174/1000], Train Loss: 4340.1141, Test Loss: 2061.7020\n",
      "Epoch [175/1000], Train Loss: 3808.6214, Test Loss: 2066.1136\n",
      "Epoch [176/1000], Train Loss: 3887.9265, Test Loss: 2146.6266\n",
      "Epoch [177/1000], Train Loss: 3924.9762, Test Loss: 2389.9309\n",
      "Epoch [178/1000], Train Loss: 4102.3235, Test Loss: 2255.1611\n",
      "Epoch [179/1000], Train Loss: 4005.0697, Test Loss: 2310.5559\n",
      "Epoch [180/1000], Train Loss: 4100.8182, Test Loss: 2268.9890\n",
      "Epoch [181/1000], Train Loss: 134843.4603, Test Loss: 4673.1556\n",
      "Epoch [182/1000], Train Loss: 9193.5957, Test Loss: 3333.4857\n",
      "Epoch [183/1000], Train Loss: 6394.7180, Test Loss: 3241.5529\n",
      "Epoch [184/1000], Train Loss: 6376.8020, Test Loss: 2392.5044\n",
      "Epoch [185/1000], Train Loss: 4805.7282, Test Loss: 2408.1156\n",
      "Epoch [186/1000], Train Loss: 4853.6143, Test Loss: 2904.1265\n",
      "Epoch [187/1000], Train Loss: 4843.8652, Test Loss: 2331.4772\n",
      "Epoch [188/1000], Train Loss: 4145.6837, Test Loss: 2281.5185\n",
      "Epoch [189/1000], Train Loss: 4258.7702, Test Loss: 2256.6413\n",
      "Epoch [190/1000], Train Loss: 5028.4838, Test Loss: 2557.4345\n",
      "Epoch [191/1000], Train Loss: 4515.6408, Test Loss: 2392.9150\n",
      "Epoch [192/1000], Train Loss: 4249.3433, Test Loss: 2244.1496\n",
      "Epoch [193/1000], Train Loss: 4591.1125, Test Loss: 2375.4477\n",
      "Epoch [194/1000], Train Loss: 5901.5117, Test Loss: 10647.8097\n",
      "Epoch [195/1000], Train Loss: 9956.9458, Test Loss: 2331.5295\n",
      "Epoch [196/1000], Train Loss: 4244.2572, Test Loss: 2137.6411\n",
      "Epoch [197/1000], Train Loss: 4452.4292, Test Loss: 2456.4779\n",
      "Epoch [198/1000], Train Loss: 5475.8215, Test Loss: 2338.3849\n",
      "Epoch [199/1000], Train Loss: 7246.5127, Test Loss: 3311.5442\n",
      "Epoch [200/1000], Train Loss: 5762.1856, Test Loss: 2493.4569\n",
      "Epoch [201/1000], Train Loss: 9202.1564, Test Loss: 2334.3826\n",
      "Epoch [202/1000], Train Loss: 6130.9249, Test Loss: 2204.4393\n",
      "Epoch [203/1000], Train Loss: 4602.2335, Test Loss: 2944.1736\n",
      "Epoch [204/1000], Train Loss: 19903.6510, Test Loss: 2913.2049\n",
      "Epoch [205/1000], Train Loss: 5123.5251, Test Loss: 2367.6142\n",
      "Epoch [206/1000], Train Loss: 4181.0427, Test Loss: 2334.1105\n",
      "Epoch [207/1000], Train Loss: 4236.1384, Test Loss: 2452.9565\n",
      "Epoch [208/1000], Train Loss: 4918.4362, Test Loss: 2422.2064\n",
      "Epoch [209/1000], Train Loss: 8059.5410, Test Loss: 3927.2395\n",
      "Epoch [210/1000], Train Loss: 127292.1680, Test Loss: 3753.2423\n",
      "Epoch [211/1000], Train Loss: 8846.7038, Test Loss: 2785.5382\n",
      "Epoch [212/1000], Train Loss: 5570.2595, Test Loss: 2812.8744\n",
      "Epoch [213/1000], Train Loss: 4959.7266, Test Loss: 2339.6792\n",
      "Epoch [214/1000], Train Loss: 4694.5929, Test Loss: 2439.2452\n",
      "Epoch [215/1000], Train Loss: 4760.4389, Test Loss: 2270.9605\n",
      "Epoch [216/1000], Train Loss: 4084.3110, Test Loss: 2307.0657\n",
      "Epoch [217/1000], Train Loss: 4592.5192, Test Loss: 2241.0391\n",
      "Epoch [218/1000], Train Loss: 4519.7625, Test Loss: 2284.9112\n",
      "Epoch [219/1000], Train Loss: 4273.7160, Test Loss: 2323.6122\n",
      "Epoch [220/1000], Train Loss: 4450.6716, Test Loss: 2288.8556\n",
      "Epoch [221/1000], Train Loss: 6402.0601, Test Loss: 2293.0417\n",
      "Epoch [222/1000], Train Loss: 4501.0693, Test Loss: 2412.0325\n",
      "Epoch [223/1000], Train Loss: 4726.3583, Test Loss: 2708.3387\n",
      "Epoch [224/1000], Train Loss: 101968.3329, Test Loss: 3744.6177\n",
      "Epoch [225/1000], Train Loss: 6717.5091, Test Loss: 2438.0412\n",
      "Epoch [226/1000], Train Loss: 4569.1811, Test Loss: 2257.2190\n",
      "Epoch [227/1000], Train Loss: 4075.2350, Test Loss: 2399.8469\n",
      "Epoch [228/1000], Train Loss: 3922.0115, Test Loss: 2110.6302\n",
      "Epoch [229/1000], Train Loss: 3762.4012, Test Loss: 2129.6199\n",
      "Epoch [230/1000], Train Loss: 3716.4562, Test Loss: 2054.5618\n",
      "Epoch [231/1000], Train Loss: 3668.2990, Test Loss: 2236.8875\n",
      "Epoch [232/1000], Train Loss: 3873.9879, Test Loss: 2123.5157\n",
      "Epoch [233/1000], Train Loss: 3951.2370, Test Loss: 2218.6708\n",
      "Epoch [234/1000], Train Loss: 20884.9994, Test Loss: 9732.7686\n",
      "Epoch [235/1000], Train Loss: 7442.0284, Test Loss: 2367.0211\n",
      "Epoch [236/1000], Train Loss: 3963.0132, Test Loss: 2337.0087\n",
      "Epoch [237/1000], Train Loss: 4776.5078, Test Loss: 2221.7750\n",
      "Epoch [238/1000], Train Loss: 3884.6274, Test Loss: 2229.6490\n",
      "Epoch [239/1000], Train Loss: 4998.1434, Test Loss: 2413.4664\n",
      "Epoch [240/1000], Train Loss: 10984.7573, Test Loss: 17448.6978\n",
      "Epoch [241/1000], Train Loss: 6992.0774, Test Loss: 2119.0487\n",
      "Epoch [242/1000], Train Loss: 3413.6456, Test Loss: 2057.2181\n",
      "Epoch [243/1000], Train Loss: 3340.5758, Test Loss: 2024.3858\n",
      "Epoch [244/1000], Train Loss: 3715.7627, Test Loss: 2152.2530\n",
      "Epoch [245/1000], Train Loss: 3844.4027, Test Loss: 2155.2949\n",
      "Epoch [246/1000], Train Loss: 4759.6934, Test Loss: 2519.8206\n",
      "Epoch [247/1000], Train Loss: 26098.1839, Test Loss: 2636.4624\n",
      "Epoch [248/1000], Train Loss: 4353.3618, Test Loss: 2141.2632\n",
      "Epoch [249/1000], Train Loss: 3458.1665, Test Loss: 2153.4204\n",
      "Epoch [250/1000], Train Loss: 3300.6459, Test Loss: 2033.2240\n",
      "Epoch [251/1000], Train Loss: 3338.7221, Test Loss: 2090.3087\n",
      "Epoch [252/1000], Train Loss: 6456.9022, Test Loss: 2602.5702\n",
      "Epoch [253/1000], Train Loss: 5531.1460, Test Loss: 2329.8216\n",
      "Epoch [254/1000], Train Loss: 3837.9172, Test Loss: 2103.4063\n",
      "Epoch [255/1000], Train Loss: 3618.5393, Test Loss: 2412.4218\n",
      "Epoch [256/1000], Train Loss: 4036.3114, Test Loss: 2225.3125\n",
      "Epoch [257/1000], Train Loss: 19088.7820, Test Loss: 3162.3254\n",
      "Epoch [258/1000], Train Loss: 4483.4562, Test Loss: 2097.0066\n",
      "Epoch [259/1000], Train Loss: 3296.9849, Test Loss: 2062.2639\n",
      "Epoch [260/1000], Train Loss: 3324.0870, Test Loss: 2334.9814\n",
      "Epoch [261/1000], Train Loss: 3871.2346, Test Loss: 2243.6402\n",
      "Epoch [262/1000], Train Loss: 4258.8429, Test Loss: 2337.3905\n",
      "Epoch [263/1000], Train Loss: 4296.5415, Test Loss: 2840.2643\n",
      "Epoch [264/1000], Train Loss: 5150.8500, Test Loss: 3476.6739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [265/1000], Train Loss: 13957.9134, Test Loss: 2228.9597\n",
      "Epoch [266/1000], Train Loss: 3960.9687, Test Loss: 2123.7397\n",
      "Epoch [267/1000], Train Loss: 3781.1326, Test Loss: 2302.1239\n",
      "Epoch [268/1000], Train Loss: 4106.1280, Test Loss: 2602.1981\n",
      "Epoch [269/1000], Train Loss: 114078.7487, Test Loss: 2714.3558\n",
      "Epoch [270/1000], Train Loss: 5204.4469, Test Loss: 2216.5481\n",
      "Epoch [271/1000], Train Loss: 3951.2298, Test Loss: 2247.7456\n",
      "Epoch [272/1000], Train Loss: 3587.7993, Test Loss: 2117.2646\n",
      "Epoch [273/1000], Train Loss: 3393.8749, Test Loss: 2042.5431\n",
      "Epoch [274/1000], Train Loss: 3643.5597, Test Loss: 2090.1556\n",
      "Epoch [275/1000], Train Loss: 3418.2001, Test Loss: 2085.6459\n",
      "Epoch [276/1000], Train Loss: 3468.7488, Test Loss: 2333.6040\n",
      "Epoch [277/1000], Train Loss: 3408.0886, Test Loss: 2058.5637\n",
      "Epoch [278/1000], Train Loss: 3351.5267, Test Loss: 2084.1371\n",
      "Epoch [279/1000], Train Loss: 3411.7434, Test Loss: 2045.5466\n",
      "Epoch [280/1000], Train Loss: 4133.5840, Test Loss: 2610.1103\n",
      "Epoch [281/1000], Train Loss: 24272.0174, Test Loss: 2175.4858\n",
      "Epoch [282/1000], Train Loss: 3970.6178, Test Loss: 2040.3173\n",
      "Epoch [283/1000], Train Loss: 3332.3309, Test Loss: 2133.8117\n",
      "Epoch [284/1000], Train Loss: 3088.8219, Test Loss: 2079.8580\n",
      "Epoch [285/1000], Train Loss: 3229.2485, Test Loss: 2076.5245\n",
      "Epoch [286/1000], Train Loss: 6124.6959, Test Loss: 2385.3392\n",
      "Epoch [287/1000], Train Loss: 4036.4139, Test Loss: 2215.2954\n",
      "Epoch [288/1000], Train Loss: 3562.3218, Test Loss: 2110.6121\n",
      "Epoch [289/1000], Train Loss: 3883.8793, Test Loss: 2606.3137\n",
      "Epoch [290/1000], Train Loss: 15791.6794, Test Loss: 2855.6993\n",
      "Epoch [291/1000], Train Loss: 4724.9429, Test Loss: 2105.9930\n",
      "Epoch [292/1000], Train Loss: 3790.4160, Test Loss: 5211.2364\n",
      "Epoch [293/1000], Train Loss: 12268.5491, Test Loss: 2472.3130\n",
      "Epoch [294/1000], Train Loss: 8275.6981, Test Loss: 2403.9900\n",
      "Epoch [295/1000], Train Loss: 3849.1823, Test Loss: 2052.0849\n",
      "Epoch [296/1000], Train Loss: 3061.7006, Test Loss: 2141.2015\n",
      "Epoch [297/1000], Train Loss: 4306.0128, Test Loss: 3642.1236\n",
      "Epoch [298/1000], Train Loss: 9043.0284, Test Loss: 2643.3950\n",
      "Epoch [299/1000], Train Loss: 5295.9599, Test Loss: 2323.0072\n",
      "Epoch [300/1000], Train Loss: 3639.2913, Test Loss: 2115.0854\n",
      "Epoch [301/1000], Train Loss: 3636.6675, Test Loss: 2270.3239\n",
      "Epoch [302/1000], Train Loss: 5628.0596, Test Loss: 2240.5937\n",
      "Epoch [303/1000], Train Loss: 4126.0010, Test Loss: 3096.4863\n",
      "Epoch [304/1000], Train Loss: 6229.6632, Test Loss: 2146.9889\n",
      "Epoch [305/1000], Train Loss: 11972.0198, Test Loss: 2286.1033\n",
      "Epoch [306/1000], Train Loss: 3584.5966, Test Loss: 2189.1645\n",
      "Epoch [307/1000], Train Loss: 3286.8556, Test Loss: 2050.3143\n",
      "Epoch [308/1000], Train Loss: 5511.3606, Test Loss: 2350.5965\n",
      "Epoch [309/1000], Train Loss: 5025.8442, Test Loss: 2130.8982\n",
      "Epoch [310/1000], Train Loss: 31434.2373, Test Loss: 2336.2497\n",
      "Epoch [311/1000], Train Loss: 4100.8279, Test Loss: 2099.8767\n",
      "Epoch [312/1000], Train Loss: 3294.8190, Test Loss: 1975.8952\n",
      "Epoch [313/1000], Train Loss: 3229.2045, Test Loss: 2006.1097\n",
      "Epoch [314/1000], Train Loss: 3176.4029, Test Loss: 2060.2965\n",
      "Epoch [315/1000], Train Loss: 3181.8978, Test Loss: 2085.9739\n",
      "Epoch [316/1000], Train Loss: 3208.6460, Test Loss: 2109.2050\n",
      "Epoch [317/1000], Train Loss: 140005.6099, Test Loss: 9484.9174\n",
      "Epoch [318/1000], Train Loss: 12170.4207, Test Loss: 2874.7762\n",
      "Epoch [319/1000], Train Loss: 7734.7819, Test Loss: 2587.7458\n",
      "Epoch [320/1000], Train Loss: 5588.4036, Test Loss: 2270.0586\n",
      "Epoch [321/1000], Train Loss: 4085.3848, Test Loss: 2330.7615\n",
      "Epoch [322/1000], Train Loss: 4472.5922, Test Loss: 2133.4106\n",
      "Epoch [323/1000], Train Loss: 4507.7612, Test Loss: 2108.5391\n",
      "Epoch [324/1000], Train Loss: 3558.9820, Test Loss: 1987.9868\n",
      "Epoch [325/1000], Train Loss: 3183.9061, Test Loss: 2536.2074\n",
      "Epoch [326/1000], Train Loss: 3275.2857, Test Loss: 1989.3354\n",
      "Epoch [327/1000], Train Loss: 3519.3173, Test Loss: 2211.8079\n",
      "Epoch [328/1000], Train Loss: 3902.9106, Test Loss: 2417.4746\n",
      "Epoch [329/1000], Train Loss: 7532.0826, Test Loss: 3699.7445\n",
      "Epoch [330/1000], Train Loss: 6242.7705, Test Loss: 2136.4761\n",
      "Epoch [331/1000], Train Loss: 4379.8943, Test Loss: 2706.8814\n",
      "Epoch [332/1000], Train Loss: 6202.9569, Test Loss: 2424.6480\n",
      "Epoch [333/1000], Train Loss: 3914.2392, Test Loss: 2239.8894\n",
      "Epoch [334/1000], Train Loss: 4086.7147, Test Loss: 3080.3479\n",
      "Epoch [335/1000], Train Loss: 83309.5081, Test Loss: 3378.3609\n",
      "Epoch [336/1000], Train Loss: 5538.7069, Test Loss: 2245.8801\n",
      "Epoch [337/1000], Train Loss: 3697.6061, Test Loss: 2077.5086\n",
      "Epoch [338/1000], Train Loss: 3207.1281, Test Loss: 2152.3254\n",
      "Epoch [339/1000], Train Loss: 3217.7110, Test Loss: 2112.1190\n",
      "Epoch [340/1000], Train Loss: 3354.9396, Test Loss: 2151.3293\n",
      "Epoch [341/1000], Train Loss: 3157.0170, Test Loss: 2239.7080\n",
      "Epoch [342/1000], Train Loss: 3424.2695, Test Loss: 2448.8152\n",
      "Epoch [343/1000], Train Loss: 3622.6249, Test Loss: 2227.0079\n",
      "Epoch [344/1000], Train Loss: 5274.2580, Test Loss: 5274.2558\n",
      "Epoch [345/1000], Train Loss: 5557.9439, Test Loss: 2319.9949\n",
      "Epoch [346/1000], Train Loss: 4042.7684, Test Loss: 2303.0700\n",
      "Epoch [347/1000], Train Loss: 3411.9117, Test Loss: 2400.7126\n",
      "Epoch [348/1000], Train Loss: 10780.7192, Test Loss: 5231.9781\n",
      "Epoch [349/1000], Train Loss: 10947.5470, Test Loss: 2146.9404\n",
      "Epoch [350/1000], Train Loss: 3379.3323, Test Loss: 2077.5015\n",
      "Epoch [351/1000], Train Loss: 2999.1024, Test Loss: 2126.6686\n",
      "Epoch [352/1000], Train Loss: 3939.6516, Test Loss: 2319.5507\n",
      "Epoch [353/1000], Train Loss: 4677.7590, Test Loss: 4132.5927\n",
      "Epoch [354/1000], Train Loss: 7215.3366, Test Loss: 2482.8469\n",
      "Epoch [355/1000], Train Loss: 137665.1882, Test Loss: 3078.2583\n",
      "Epoch [356/1000], Train Loss: 6635.5846, Test Loss: 2664.2780\n",
      "Epoch [357/1000], Train Loss: 4980.8315, Test Loss: 2457.4884\n",
      "Epoch [358/1000], Train Loss: 4525.4916, Test Loss: 2143.6411\n",
      "Epoch [359/1000], Train Loss: 4499.8515, Test Loss: 2180.9193\n",
      "Epoch [360/1000], Train Loss: 3803.5401, Test Loss: 2155.3256\n",
      "Epoch [361/1000], Train Loss: 4692.2423, Test Loss: 2055.0011\n",
      "Epoch [362/1000], Train Loss: 3296.4332, Test Loss: 2053.8115\n",
      "Epoch [363/1000], Train Loss: 3117.5455, Test Loss: 1961.4476\n",
      "Epoch [364/1000], Train Loss: 3435.2644, Test Loss: 2122.2490\n",
      "Epoch [365/1000], Train Loss: 6704.2691, Test Loss: 2988.9614\n",
      "Epoch [366/1000], Train Loss: 4461.9425, Test Loss: 2394.5209\n",
      "Epoch [367/1000], Train Loss: 3930.5955, Test Loss: 2099.3233\n",
      "Epoch [368/1000], Train Loss: 4479.4166, Test Loss: 2460.4123\n",
      "Epoch [369/1000], Train Loss: 4322.4510, Test Loss: 2217.0721\n",
      "Epoch [370/1000], Train Loss: 102148.5205, Test Loss: 7560.4592\n",
      "Epoch [371/1000], Train Loss: 13534.8605, Test Loss: 3313.1316\n",
      "Epoch [372/1000], Train Loss: 5581.5962, Test Loss: 2271.2743\n",
      "Epoch [373/1000], Train Loss: 3928.6688, Test Loss: 2229.0297\n",
      "Epoch [374/1000], Train Loss: 4008.8622, Test Loss: 1967.1799\n",
      "Epoch [375/1000], Train Loss: 3225.3134, Test Loss: 2009.3527\n",
      "Epoch [376/1000], Train Loss: 3028.7873, Test Loss: 2072.5660\n",
      "Epoch [377/1000], Train Loss: 3129.7527, Test Loss: 2017.6086\n",
      "Epoch [378/1000], Train Loss: 3098.1483, Test Loss: 1928.9520\n",
      "Epoch [379/1000], Train Loss: 3150.8373, Test Loss: 2143.2093\n",
      "Epoch [380/1000], Train Loss: 7782.3626, Test Loss: 6619.6475\n",
      "Epoch [381/1000], Train Loss: 8308.2346, Test Loss: 3217.2739\n",
      "Epoch [382/1000], Train Loss: 5924.5520, Test Loss: 1983.9709\n",
      "Epoch [383/1000], Train Loss: 3448.0938, Test Loss: 2146.8384\n",
      "Epoch [384/1000], Train Loss: 3259.7829, Test Loss: 2405.4511\n",
      "Epoch [385/1000], Train Loss: 3296.0662, Test Loss: 2068.8341\n",
      "Epoch [386/1000], Train Loss: 35558.7165, Test Loss: 91836.9897\n",
      "Epoch [387/1000], Train Loss: 17941.1582, Test Loss: 2417.0880\n",
      "Epoch [388/1000], Train Loss: 3967.9997, Test Loss: 2489.3256\n",
      "Epoch [389/1000], Train Loss: 3212.1530, Test Loss: 2068.7323\n",
      "Epoch [390/1000], Train Loss: 3081.2875, Test Loss: 1973.0359\n",
      "Epoch [391/1000], Train Loss: 2914.5901, Test Loss: 2130.4837\n",
      "Epoch [392/1000], Train Loss: 3195.2522, Test Loss: 2162.6181\n",
      "Epoch [393/1000], Train Loss: 3251.5635, Test Loss: 2085.0170\n",
      "Epoch [394/1000], Train Loss: 3662.4253, Test Loss: 3691.9365\n",
      "Epoch [395/1000], Train Loss: 26015.0444, Test Loss: 2620.0445\n",
      "Epoch [396/1000], Train Loss: 3991.3502, Test Loss: 2083.9288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [397/1000], Train Loss: 3155.0469, Test Loss: 2021.6680\n",
      "Epoch [398/1000], Train Loss: 3047.7859, Test Loss: 2100.1979\n",
      "Epoch [399/1000], Train Loss: 3019.8650, Test Loss: 2064.5170\n",
      "Epoch [400/1000], Train Loss: 3188.2522, Test Loss: 2046.7771\n",
      "Epoch [401/1000], Train Loss: 3182.3662, Test Loss: 2059.4718\n",
      "Epoch [402/1000], Train Loss: 8096.8333, Test Loss: 16438.4137\n",
      "Epoch [403/1000], Train Loss: 14105.8528, Test Loss: 2112.8622\n",
      "Epoch [404/1000], Train Loss: 3113.2282, Test Loss: 1978.1851\n",
      "Epoch [405/1000], Train Loss: 2964.3758, Test Loss: 1983.5688\n",
      "Epoch [406/1000], Train Loss: 3101.1634, Test Loss: 2026.8217\n",
      "Epoch [407/1000], Train Loss: 3247.6192, Test Loss: 1978.6409\n",
      "Epoch [408/1000], Train Loss: 3426.7636, Test Loss: 2030.4922\n",
      "Epoch [409/1000], Train Loss: 4433.5684, Test Loss: 2995.1250\n",
      "Epoch [410/1000], Train Loss: 10007.1110, Test Loss: 2028.7007\n",
      "Epoch [411/1000], Train Loss: 12215.7627, Test Loss: 2283.6990\n",
      "Epoch [412/1000], Train Loss: 3332.7901, Test Loss: 1957.1949\n",
      "Epoch [413/1000], Train Loss: 3151.3336, Test Loss: 2076.3655\n",
      "Epoch [414/1000], Train Loss: 2875.5060, Test Loss: 1968.9500\n",
      "Epoch [415/1000], Train Loss: 2827.8508, Test Loss: 2054.3704\n",
      "Epoch [416/1000], Train Loss: 3104.4898, Test Loss: 2009.4039\n",
      "Epoch [417/1000], Train Loss: 3313.8616, Test Loss: 2103.2370\n",
      "Epoch [418/1000], Train Loss: 3518.0453, Test Loss: 2009.9275\n",
      "Epoch [419/1000], Train Loss: 3603.7127, Test Loss: 2187.4920\n",
      "Epoch [420/1000], Train Loss: 3933.8116, Test Loss: 2257.0306\n",
      "Epoch [421/1000], Train Loss: 3794.2926, Test Loss: 2530.2734\n",
      "Epoch [422/1000], Train Loss: 4572.0407, Test Loss: 2400.3521\n",
      "Epoch [423/1000], Train Loss: 4758.9785, Test Loss: 2426.0743\n",
      "Epoch [424/1000], Train Loss: 3383.4835, Test Loss: 2013.4844\n",
      "Epoch [425/1000], Train Loss: 3244.8895, Test Loss: 2131.4507\n",
      "Epoch [426/1000], Train Loss: 3184.1344, Test Loss: 2050.6562\n",
      "Epoch [427/1000], Train Loss: 3406.5459, Test Loss: 2054.1915\n",
      "Epoch [428/1000], Train Loss: 3501.3859, Test Loss: 2263.6391\n",
      "Epoch [429/1000], Train Loss: 3875.4912, Test Loss: 2027.8026\n",
      "Epoch [430/1000], Train Loss: 3204.3860, Test Loss: 2049.8228\n",
      "Epoch [431/1000], Train Loss: 8182.3702, Test Loss: 2140.1909\n",
      "Epoch [432/1000], Train Loss: 3147.4985, Test Loss: 1936.2485\n",
      "Epoch [433/1000], Train Loss: 2795.0215, Test Loss: 1990.5053\n",
      "Epoch [434/1000], Train Loss: 3146.1344, Test Loss: 3217.4303\n",
      "Epoch [435/1000], Train Loss: 4717.4913, Test Loss: 1957.0872\n",
      "Epoch [436/1000], Train Loss: 2775.7084, Test Loss: 1998.4900\n",
      "Epoch [437/1000], Train Loss: 2863.0188, Test Loss: 2043.0991\n",
      "Epoch [438/1000], Train Loss: 3173.8758, Test Loss: 2576.0116\n",
      "Epoch [439/1000], Train Loss: 4338.6266, Test Loss: 2331.5717\n",
      "Epoch [440/1000], Train Loss: 4395.9491, Test Loss: 2139.7715\n",
      "Epoch [441/1000], Train Loss: 3320.5905, Test Loss: 1958.0793\n",
      "Epoch [442/1000], Train Loss: 4078.2782, Test Loss: 2461.0572\n",
      "Epoch [443/1000], Train Loss: 3123.2575, Test Loss: 2103.9344\n",
      "Epoch [444/1000], Train Loss: 3419.3763, Test Loss: 2041.5176\n",
      "Epoch [445/1000], Train Loss: 5394.0596, Test Loss: 2029.8730\n",
      "Epoch [446/1000], Train Loss: 2967.9410, Test Loss: 1949.3893\n",
      "Epoch [447/1000], Train Loss: 2961.8800, Test Loss: 2026.5935\n",
      "Epoch [448/1000], Train Loss: 2762.5644, Test Loss: 1991.1702\n",
      "Epoch [449/1000], Train Loss: 2877.4813, Test Loss: 1962.8493\n",
      "Epoch [450/1000], Train Loss: 3084.3059, Test Loss: 2027.3033\n",
      "Epoch [451/1000], Train Loss: 3777.3117, Test Loss: 2125.3690\n",
      "Epoch [452/1000], Train Loss: 3152.0743, Test Loss: 2118.1314\n",
      "Epoch [453/1000], Train Loss: 3129.9611, Test Loss: 1978.6439\n",
      "Epoch [454/1000], Train Loss: 4192.1328, Test Loss: 1986.2842\n",
      "Epoch [455/1000], Train Loss: 2966.1620, Test Loss: 1939.2499\n",
      "Epoch [456/1000], Train Loss: 2744.9196, Test Loss: 2089.9939\n",
      "Epoch [457/1000], Train Loss: 3076.0068, Test Loss: 2008.6789\n",
      "Epoch [458/1000], Train Loss: 3549.3689, Test Loss: 2679.0111\n",
      "Epoch [459/1000], Train Loss: 8627.6434, Test Loss: 2815.4584\n",
      "Epoch [460/1000], Train Loss: 3725.3025, Test Loss: 1973.9314\n",
      "Epoch [461/1000], Train Loss: 2781.6934, Test Loss: 1827.0752\n",
      "Epoch [462/1000], Train Loss: 2453.5980, Test Loss: 1872.5526\n",
      "Epoch [463/1000], Train Loss: 2357.3116, Test Loss: 1908.1702\n",
      "Epoch [464/1000], Train Loss: 3554.0484, Test Loss: 1876.9137\n",
      "Epoch [465/1000], Train Loss: 3701.0733, Test Loss: 3005.9333\n",
      "Epoch [466/1000], Train Loss: 3087.9735, Test Loss: 2022.5584\n",
      "Epoch [467/1000], Train Loss: 2459.5010, Test Loss: 1965.1729\n",
      "Epoch [468/1000], Train Loss: 2502.3996, Test Loss: 1867.8976\n",
      "Epoch [469/1000], Train Loss: 3739.5094, Test Loss: 9059.7881\n",
      "Epoch [470/1000], Train Loss: 3725.3270, Test Loss: 2055.1180\n",
      "Epoch [471/1000], Train Loss: 2427.6591, Test Loss: 2066.5390\n",
      "Epoch [472/1000], Train Loss: 2671.0361, Test Loss: 2010.1136\n",
      "Epoch [473/1000], Train Loss: 2546.6952, Test Loss: 2088.4424\n",
      "Epoch [474/1000], Train Loss: 2693.2578, Test Loss: 1893.3162\n",
      "Epoch [475/1000], Train Loss: 3175.3040, Test Loss: 2063.9808\n",
      "Epoch [476/1000], Train Loss: 4380.0375, Test Loss: 2421.7956\n",
      "Epoch [477/1000], Train Loss: 2668.6695, Test Loss: 1976.6489\n",
      "Epoch [478/1000], Train Loss: 2404.2524, Test Loss: 1997.6669\n",
      "Epoch [479/1000], Train Loss: 2378.1218, Test Loss: 2069.2210\n",
      "Epoch [480/1000], Train Loss: 3175.1881, Test Loss: 2029.9234\n",
      "Epoch [481/1000], Train Loss: 2819.6474, Test Loss: 2046.7711\n",
      "Epoch [482/1000], Train Loss: 3108.6754, Test Loss: 1938.4891\n",
      "Epoch [483/1000], Train Loss: 2747.8018, Test Loss: 2023.6211\n",
      "Epoch [484/1000], Train Loss: 2764.1942, Test Loss: 1964.4544\n",
      "Epoch [485/1000], Train Loss: 3539.9479, Test Loss: 1905.9806\n",
      "Epoch [486/1000], Train Loss: 2347.7089, Test Loss: 2024.9524\n",
      "Epoch [487/1000], Train Loss: 3143.8542, Test Loss: 2230.1033\n",
      "Epoch [488/1000], Train Loss: 3812.0952, Test Loss: 4486.3402\n",
      "Epoch [489/1000], Train Loss: 3393.7114, Test Loss: 1910.2938\n",
      "Epoch [490/1000], Train Loss: 2449.2938, Test Loss: 1922.6024\n",
      "Epoch [491/1000], Train Loss: 2704.0368, Test Loss: 2120.3205\n",
      "Epoch [492/1000], Train Loss: 2843.9156, Test Loss: 1969.4337\n",
      "Epoch [493/1000], Train Loss: 2298.4788, Test Loss: 1964.4035\n",
      "Epoch [494/1000], Train Loss: 3802.7551, Test Loss: 2152.8060\n",
      "Epoch [495/1000], Train Loss: 3273.2658, Test Loss: 1970.2150\n",
      "Epoch [496/1000], Train Loss: 3664.9531, Test Loss: 1960.7944\n",
      "Epoch [497/1000], Train Loss: 2139.0828, Test Loss: 1855.3389\n",
      "Epoch [498/1000], Train Loss: 2294.9745, Test Loss: 2072.4596\n",
      "Epoch [499/1000], Train Loss: 3088.9292, Test Loss: 2012.5246\n",
      "Epoch [500/1000], Train Loss: 3541.8785, Test Loss: 10138.1756\n",
      "Epoch [501/1000], Train Loss: 4055.4120, Test Loss: 1839.4620\n",
      "Epoch [502/1000], Train Loss: 2229.0709, Test Loss: 1935.3428\n",
      "Epoch [503/1000], Train Loss: 2319.7210, Test Loss: 1895.7350\n",
      "Epoch [504/1000], Train Loss: 2381.2498, Test Loss: 2008.8485\n",
      "Epoch [505/1000], Train Loss: 2508.8431, Test Loss: 1966.3694\n",
      "Epoch [506/1000], Train Loss: 2661.3888, Test Loss: 1954.2316\n",
      "Epoch [507/1000], Train Loss: 2395.0213, Test Loss: 2134.6125\n",
      "Epoch [508/1000], Train Loss: 2921.6462, Test Loss: 1991.3470\n",
      "Epoch [509/1000], Train Loss: 2824.3229, Test Loss: 2027.7474\n",
      "Epoch [510/1000], Train Loss: 2551.1140, Test Loss: 1945.2951\n",
      "Epoch [511/1000], Train Loss: 2329.3520, Test Loss: 1925.0854\n",
      "Epoch [512/1000], Train Loss: 2700.7120, Test Loss: 2463.8264\n",
      "Epoch [513/1000], Train Loss: 12363.8922, Test Loss: 1986.3846\n",
      "Epoch [514/1000], Train Loss: 2241.3896, Test Loss: 1841.4575\n",
      "Epoch [515/1000], Train Loss: 1948.0912, Test Loss: 1843.4590\n",
      "Epoch [516/1000], Train Loss: 1965.8680, Test Loss: 1851.9492\n",
      "Epoch [517/1000], Train Loss: 2063.3192, Test Loss: 1869.7297\n",
      "Epoch [518/1000], Train Loss: 2071.9675, Test Loss: 1923.6386\n",
      "Epoch [519/1000], Train Loss: 2237.4871, Test Loss: 1850.6919\n",
      "Epoch [520/1000], Train Loss: 2824.8083, Test Loss: 1910.1191\n",
      "Epoch [521/1000], Train Loss: 2392.8158, Test Loss: 2084.5237\n",
      "Epoch [522/1000], Train Loss: 4741.3568, Test Loss: 6788.9190\n",
      "Epoch [523/1000], Train Loss: 6489.0349, Test Loss: 1882.9477\n",
      "Epoch [524/1000], Train Loss: 1994.2080, Test Loss: 1832.6075\n",
      "Epoch [525/1000], Train Loss: 1898.7816, Test Loss: 1853.1745\n",
      "Epoch [526/1000], Train Loss: 1915.1690, Test Loss: 1864.5712\n",
      "Epoch [527/1000], Train Loss: 1987.9062, Test Loss: 1858.6142\n",
      "Epoch [528/1000], Train Loss: 2127.6963, Test Loss: 1887.2795\n",
      "Epoch [529/1000], Train Loss: 3276.1859, Test Loss: 3964.3569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [530/1000], Train Loss: 3160.3199, Test Loss: 1841.5455\n",
      "Epoch [531/1000], Train Loss: 2055.6676, Test Loss: 1866.6480\n",
      "Epoch [532/1000], Train Loss: 2034.2193, Test Loss: 1926.6294\n",
      "Epoch [533/1000], Train Loss: 2188.4065, Test Loss: 1901.5699\n",
      "Epoch [534/1000], Train Loss: 2632.9009, Test Loss: 1943.4056\n",
      "Epoch [535/1000], Train Loss: 2722.9995, Test Loss: 2179.1490\n",
      "Epoch [536/1000], Train Loss: 2389.8367, Test Loss: 2035.5596\n",
      "Epoch [537/1000], Train Loss: 2424.2435, Test Loss: 1876.2159\n",
      "Epoch [538/1000], Train Loss: 2276.6812, Test Loss: 1972.8596\n",
      "Epoch [539/1000], Train Loss: 2772.9538, Test Loss: 2383.5889\n",
      "Epoch [540/1000], Train Loss: 2477.6957, Test Loss: 2124.5995\n",
      "Epoch [541/1000], Train Loss: 2382.6643, Test Loss: 2168.4496\n",
      "Epoch [542/1000], Train Loss: 2597.3073, Test Loss: 1989.6655\n",
      "Epoch [543/1000], Train Loss: 2664.8036, Test Loss: 2136.6624\n",
      "Epoch [544/1000], Train Loss: 2343.5732, Test Loss: 1964.0594\n",
      "Epoch [545/1000], Train Loss: 3127.2517, Test Loss: 2273.2982\n",
      "Epoch [546/1000], Train Loss: 2430.0959, Test Loss: 1879.9234\n",
      "Epoch [547/1000], Train Loss: 2706.0671, Test Loss: 1929.0749\n",
      "Epoch [548/1000], Train Loss: 1946.3917, Test Loss: 1888.0612\n",
      "Epoch [549/1000], Train Loss: 2048.1246, Test Loss: 1862.2573\n",
      "Epoch [550/1000], Train Loss: 2243.3077, Test Loss: 1930.2972\n",
      "Epoch [551/1000], Train Loss: 9880.2679, Test Loss: 2438.9395\n",
      "Epoch [552/1000], Train Loss: 2502.1213, Test Loss: 1904.5599\n",
      "Epoch [553/1000], Train Loss: 2452.8836, Test Loss: 1874.3251\n",
      "Epoch [554/1000], Train Loss: 2064.2220, Test Loss: 1904.0368\n",
      "Epoch [555/1000], Train Loss: 1917.9610, Test Loss: 1823.3875\n",
      "Epoch [556/1000], Train Loss: 1985.6251, Test Loss: 1833.5122\n",
      "Epoch [557/1000], Train Loss: 2334.7685, Test Loss: 1886.8209\n",
      "Epoch [558/1000], Train Loss: 2288.4194, Test Loss: 1843.4256\n",
      "Epoch [559/1000], Train Loss: 2288.6328, Test Loss: 2340.8268\n",
      "Epoch [560/1000], Train Loss: 9876.2595, Test Loss: 1852.9600\n",
      "Epoch [561/1000], Train Loss: 1860.7105, Test Loss: 1784.3693\n",
      "Epoch [562/1000], Train Loss: 1755.1626, Test Loss: 1818.9241\n",
      "Epoch [563/1000], Train Loss: 1777.5878, Test Loss: 1845.4417\n",
      "Epoch [564/1000], Train Loss: 1835.4339, Test Loss: 1909.5399\n",
      "Epoch [565/1000], Train Loss: 1902.7902, Test Loss: 1877.6366\n",
      "Epoch [566/1000], Train Loss: 1945.2820, Test Loss: 1918.6864\n",
      "Epoch [567/1000], Train Loss: 2007.4478, Test Loss: 2013.3917\n",
      "Epoch [568/1000], Train Loss: 6376.6496, Test Loss: 3753.6080\n",
      "Epoch [569/1000], Train Loss: 6480.6910, Test Loss: 1964.1286\n",
      "Epoch [570/1000], Train Loss: 1920.0620, Test Loss: 1805.3476\n",
      "Epoch [571/1000], Train Loss: 1805.7101, Test Loss: 1832.3815\n",
      "Epoch [572/1000], Train Loss: 1802.6934, Test Loss: 1855.3808\n",
      "Epoch [573/1000], Train Loss: 1821.8094, Test Loss: 1850.5283\n",
      "Epoch [574/1000], Train Loss: 1909.4598, Test Loss: 1836.5503\n",
      "Epoch [575/1000], Train Loss: 1950.4256, Test Loss: 1920.6895\n",
      "Epoch [576/1000], Train Loss: 2085.8147, Test Loss: 1876.0577\n",
      "Epoch [577/1000], Train Loss: 2141.2879, Test Loss: 1987.4118\n",
      "Epoch [578/1000], Train Loss: 2283.5585, Test Loss: 1900.3041\n",
      "Epoch [579/1000], Train Loss: 2473.2774, Test Loss: 1895.4236\n",
      "Epoch [580/1000], Train Loss: 2131.6930, Test Loss: 2001.3770\n",
      "Epoch [581/1000], Train Loss: 2263.9997, Test Loss: 2002.5487\n",
      "Epoch [582/1000], Train Loss: 3752.5067, Test Loss: 2030.8097\n",
      "Epoch [583/1000], Train Loss: 2164.1319, Test Loss: 1982.1284\n",
      "Epoch [584/1000], Train Loss: 1779.8757, Test Loss: 1854.0359\n",
      "Epoch [585/1000], Train Loss: 1729.3714, Test Loss: 1846.3432\n",
      "Epoch [586/1000], Train Loss: 1834.7944, Test Loss: 1965.1758\n",
      "Epoch [587/1000], Train Loss: 2240.6887, Test Loss: 2070.7467\n",
      "Epoch [588/1000], Train Loss: 2007.1569, Test Loss: 2110.9664\n",
      "Epoch [589/1000], Train Loss: 2136.8539, Test Loss: 2016.5531\n",
      "Epoch [590/1000], Train Loss: 2090.2007, Test Loss: 1980.7378\n",
      "Epoch [591/1000], Train Loss: 2032.0709, Test Loss: 1834.7727\n",
      "Epoch [592/1000], Train Loss: 2358.8066, Test Loss: 2142.5236\n",
      "Epoch [593/1000], Train Loss: 2202.4468, Test Loss: 1947.0294\n",
      "Epoch [594/1000], Train Loss: 1925.6109, Test Loss: 1988.5099\n",
      "Epoch [595/1000], Train Loss: 2876.0604, Test Loss: 1895.8446\n",
      "Epoch [596/1000], Train Loss: 1840.8510, Test Loss: 1932.5622\n",
      "Epoch [597/1000], Train Loss: 1801.8424, Test Loss: 2243.2957\n",
      "Epoch [598/1000], Train Loss: 2203.4500, Test Loss: 1887.8980\n",
      "Epoch [599/1000], Train Loss: 1899.6173, Test Loss: 1972.2553\n",
      "Epoch [600/1000], Train Loss: 2208.4050, Test Loss: 1990.5916\n",
      "Epoch [601/1000], Train Loss: 3663.0296, Test Loss: 1993.0582\n",
      "Epoch [602/1000], Train Loss: 1827.2637, Test Loss: 1908.1888\n",
      "Epoch [603/1000], Train Loss: 1755.3252, Test Loss: 1878.9247\n",
      "Epoch [604/1000], Train Loss: 1722.9476, Test Loss: 1881.5266\n",
      "Epoch [605/1000], Train Loss: 2102.0740, Test Loss: 1890.8891\n",
      "Epoch [606/1000], Train Loss: 1833.0340, Test Loss: 1972.6123\n",
      "Epoch [607/1000], Train Loss: 1940.3777, Test Loss: 1974.7547\n",
      "Epoch [608/1000], Train Loss: 1965.0730, Test Loss: 1879.2647\n",
      "Epoch [609/1000], Train Loss: 2086.7019, Test Loss: 1929.3605\n",
      "Epoch [610/1000], Train Loss: 2180.5738, Test Loss: 1879.6152\n",
      "Epoch [611/1000], Train Loss: 2835.3856, Test Loss: 2161.7918\n",
      "Epoch [612/1000], Train Loss: 2363.0167, Test Loss: 1830.3121\n",
      "Epoch [613/1000], Train Loss: 2484.8213, Test Loss: 2586.8519\n",
      "Epoch [614/1000], Train Loss: 7101.2132, Test Loss: 1836.7596\n",
      "Epoch [615/1000], Train Loss: 1594.4379, Test Loss: 1836.7397\n",
      "Epoch [616/1000], Train Loss: 1616.2145, Test Loss: 1886.9268\n",
      "Epoch [617/1000], Train Loss: 1723.1986, Test Loss: 1875.2921\n",
      "Epoch [618/1000], Train Loss: 1720.9028, Test Loss: 1877.9078\n",
      "Epoch [619/1000], Train Loss: 1945.4550, Test Loss: 1938.3805\n",
      "Epoch [620/1000], Train Loss: 2117.7750, Test Loss: 1938.7929\n",
      "Epoch [621/1000], Train Loss: 1798.5483, Test Loss: 2302.3012\n",
      "Epoch [622/1000], Train Loss: 2094.3583, Test Loss: 1881.5961\n",
      "Epoch [623/1000], Train Loss: 1904.3821, Test Loss: 1904.1241\n",
      "Epoch [624/1000], Train Loss: 1972.7411, Test Loss: 1940.9242\n",
      "Epoch [625/1000], Train Loss: 1847.3013, Test Loss: 2002.9351\n",
      "Epoch [626/1000], Train Loss: 1732.7043, Test Loss: 1978.8056\n",
      "Epoch [627/1000], Train Loss: 2421.8396, Test Loss: 2046.5812\n",
      "Epoch [628/1000], Train Loss: 2356.9410, Test Loss: 1889.7230\n",
      "Epoch [629/1000], Train Loss: 1704.4415, Test Loss: 1943.2888\n",
      "Epoch [630/1000], Train Loss: 1642.6848, Test Loss: 1950.0783\n",
      "Epoch [631/1000], Train Loss: 1829.2368, Test Loss: 1842.9627\n",
      "Epoch [632/1000], Train Loss: 1725.6578, Test Loss: 1951.1555\n",
      "Epoch [633/1000], Train Loss: 2341.7897, Test Loss: 2199.5061\n",
      "Epoch [634/1000], Train Loss: 2103.4841, Test Loss: 1908.2342\n",
      "Epoch [635/1000], Train Loss: 1683.7918, Test Loss: 2016.0365\n",
      "Epoch [636/1000], Train Loss: 1682.4447, Test Loss: 1874.5804\n",
      "Epoch [637/1000], Train Loss: 1655.3152, Test Loss: 1970.4511\n",
      "Epoch [638/1000], Train Loss: 2030.9702, Test Loss: 1872.3289\n",
      "Epoch [639/1000], Train Loss: 2062.3107, Test Loss: 1953.4986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m save_path \u001b[38;5;241m=\u001b[39m ModelName \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Params.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m train_losses, test_losses, is_model_trained \u001b[38;5;241m=\u001b[39m train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Finish timing cell run time\u001b[39;00m\n\u001b[1;32m     25\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[11], line 63\u001b[0m, in \u001b[0;36mtrain_or_load_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pretrained model found. Training from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#optimizer = optim.Adam(model.parameters())  \u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n\u001b[1;32m     64\u001b[0m is_model_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set flag to True after training\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Save losses per epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m, in \u001b[0;36mtrain_and_save_best_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     26\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 27\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m     test_losses\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# The last number here denotes how often to print loss metrics in terms of epochs\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load('Dataset44_Val_Spec.npy')\n",
    "concVal = np.load('Dataset44_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "ValSpectra = np.load(\"Dataset44_RepresentativeExamples_Spectra.npy\")\n",
    "ValConc = np.load(\"Dataset44_RepresentativeExamples_Concentrations.npy\")\n",
    "ValSpecNames = np.load(\"Dataset44_RepresentativeExamples_VariableNames.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ValConc = torch.tensor(ValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=23552, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ValConc[i]\n",
    "    Prediction = model_aq(ValSpectra[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.39  -  AllAq1\n",
      "1.56  -  AllAq5\n",
      "0.33  -  AllAq25\n",
      "2.56  -  AllAq50\n",
      "0.64  -  ThreeAddedSinglets\n",
      "8.66  -  ThirtyAddedSinglets\n",
      "75.98  -  ShiftedSpec\n",
      "25.87  -  SineBase\n",
      "191.48  -  HighDynamicRange\n",
      "inf  -  HalfZeros\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - \",ValSpecNames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard - HD-Range w/ 1's\n",
      "tensor([ 0.0000, 46.1581,  0.0000, 46.5479,  0.0000, 48.1040,  0.0000, 47.0457,\n",
      "         0.0000, 47.4051,  0.0000, 50.6954,  0.0000, 47.7626,  0.0000, 47.0406,\n",
      "         0.0000, 47.0140,  0.0000, 44.9664,  0.0000, 43.2984,  0.0000, 45.9196,\n",
      "         0.0000, 47.4171,  0.0000, 47.3669,  0.0000, 47.5498,  0.0000, 48.9012,\n",
      "         0.0000, 45.5062,  0.0000, 48.2964,  0.0000, 47.7710,  0.0000, 45.3902,\n",
      "         0.0000, 46.3859,  0.0000, 47.3673], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Standard - HD-Range w/ 0's\n",
      "tensor([ 0.0000, 46.1016,  0.0000, 46.4799,  0.0000, 48.1260,  0.0000, 46.9538,\n",
      "         0.0000, 47.4789,  0.0000, 50.5423,  0.0000, 47.7795,  0.0000, 47.0045,\n",
      "         0.0000, 46.9940,  0.0000, 44.9754,  0.0000, 43.2951,  0.0000, 45.9109,\n",
      "         0.0000, 47.4381,  0.0000, 47.3589,  0.0000, 47.6115,  0.0000, 48.9226,\n",
      "         0.0000, 45.4166,  0.0000, 48.3467,  0.0000, 47.6803,  0.0000, 45.3109,\n",
      "         0.0000, 46.3721,  0.0000, 47.4170], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Standard - Blank\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.3754, 0.0000, 0.1964, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0027, 0.0000, 0.0000, 0.0281, 0.0000, 0.0000, 0.0000, 0.2446,\n",
      "        0.0000, 0.2325, 0.1468, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(ValSpectra[8])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Standard - HD-Range w/ 1's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[9])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Standard - HD-Range w/ 0's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[10])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Standard - Blank\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = concVal[i]\n",
    "    Prediction = model_aq(spectraVal[i].unsqueeze(0))\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Random Testing Examples [MAPE]:\n",
      "__________________________________\n",
      "0.95\n",
      "1.83\n",
      "1.62\n",
      "1.08\n",
      "3.76\n",
      "1.4\n",
      "2.09\n",
      "2.14\n",
      "2.18\n",
      "2.23\n"
     ]
    }
   ],
   "source": [
    "print('10 Random Testing Examples [MAPE]:')\n",
    "print('__________________________________')\n",
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted ___________ Ground Truth\n",
      "46.29092788696289 ___ 46.645503997802734\n",
      "21.357200622558594 ___ 21.5980224609375\n",
      "35.54960250854492 ___ 35.24427032470703\n",
      "13.600829124450684 ___ 14.074536323547363\n",
      "34.66399002075195 ___ 34.4682731628418\n",
      "9.93795394897461 ___ 10.037553787231445\n",
      "27.701231002807617 ___ 27.764158248901367\n",
      "4.92189884185791 ___ 4.902628421783447\n",
      "15.125494003295898 ___ 15.105571746826172\n",
      "6.8646087646484375 ___ 7.230838775634766\n",
      "11.866439819335938 ___ 11.86992359161377\n",
      "17.602149963378906 ___ 18.14148712158203\n",
      "23.816287994384766 ___ 23.96500587463379\n",
      "25.86268424987793 ___ 25.75659942626953\n",
      "49.64263916015625 ___ 49.5517463684082\n",
      "35.624794006347656 ___ 35.33686447143555\n",
      "11.541516304016113 ___ 11.646225929260254\n",
      "28.80061149597168 ___ 28.790061950683594\n",
      "18.36214256286621 ___ 18.330093383789062\n",
      "47.701629638671875 ___ 47.733367919921875\n",
      "47.31493377685547 ___ 47.4564323425293\n",
      "20.27424430847168 ___ 20.9074764251709\n",
      "22.004154205322266 ___ 22.14535140991211\n",
      "48.93192672729492 ___ 48.558433532714844\n",
      "35.61684036254883 ___ 35.62055969238281\n",
      "14.99241828918457 ___ 15.358138084411621\n",
      "38.116416931152344 ___ 38.33456802368164\n",
      "22.555879592895508 ___ 22.62639045715332\n",
      "28.222978591918945 ___ 28.1280574798584\n",
      "41.73350524902344 ___ 41.768829345703125\n",
      "36.507476806640625 ___ 36.69642639160156\n",
      "14.002884864807129 ___ 14.200051307678223\n",
      "18.522991180419922 ___ 18.802968978881836\n",
      "25.016942977905273 ___ 25.356616973876953\n",
      "21.3751163482666 ___ 21.175575256347656\n",
      "6.30458927154541 ___ 6.526034355163574\n",
      "32.99372482299805 ___ 32.703697204589844\n",
      "11.614424705505371 ___ 11.517151832580566\n",
      "30.406461715698242 ___ 30.03232192993164\n",
      "44.20779037475586 ___ 44.200958251953125\n",
      "24.5177001953125 ___ 24.365327835083008\n",
      "40.128936767578125 ___ 40.13069534301758\n",
      "11.491950035095215 ___ 11.57451343536377\n",
      "18.038049697875977 ___ 17.96565818786621\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(spectraVal[0].unsqueeze(0))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Predicted\", \"___________\", \"Ground Truth\")\n",
    "for i in np.arange(44):\n",
    "    print(Pred[0][i].item(), '___', concVal[0][i].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted ___________ Ground Truth\n",
      "7.632160186767578 ___ 7.902399063110352\n",
      "32.47126388549805 ___ 32.42935562133789\n",
      "20.391075134277344 ___ 20.6121883392334\n",
      "13.351274490356445 ___ 13.753351211547852\n",
      "22.264142990112305 ___ 22.573623657226562\n",
      "32.03847885131836 ___ 31.770023345947266\n",
      "16.975793838500977 ___ 17.640289306640625\n",
      "33.84067153930664 ___ 33.91206741333008\n",
      "25.845338821411133 ___ 25.637516021728516\n",
      "29.895679473876953 ___ 29.872900009155273\n",
      "39.03532028198242 ___ 38.9141960144043\n",
      "3.7593507766723633 ___ 3.8552157878875732\n",
      "4.420292377471924 ___ 4.705685138702393\n",
      "1.6816678047180176 ___ 1.958586573600769\n",
      "7.994858741760254 ___ 7.700331211090088\n",
      "41.65766906738281 ___ 41.23240661621094\n",
      "15.21188735961914 ___ 15.577610969543457\n",
      "24.93393898010254 ___ 24.83091163635254\n",
      "16.931970596313477 ___ 16.82731819152832\n",
      "8.973280906677246 ___ 9.095264434814453\n",
      "46.85173034667969 ___ 46.73149871826172\n",
      "4.216188430786133 ___ 4.405956745147705\n",
      "3.287295341491699 ___ 3.5351171493530273\n",
      "43.23063278198242 ___ 42.706207275390625\n",
      "25.680253982543945 ___ 25.63140106201172\n",
      "42.090782165527344 ___ 42.30694580078125\n",
      "37.63337326049805 ___ 37.44513702392578\n",
      "22.681589126586914 ___ 22.729110717773438\n",
      "4.674045562744141 ___ 4.731714725494385\n",
      "12.89704704284668 ___ 13.287884712219238\n",
      "26.30945587158203 ___ 26.24576759338379\n",
      "6.0148024559021 ___ 6.026565074920654\n",
      "44.69783401489258 ___ 43.59522247314453\n",
      "44.73716354370117 ___ 44.461734771728516\n",
      "45.09949493408203 ___ 44.74156951904297\n",
      "16.445655822753906 ___ 16.877317428588867\n",
      "9.752296447753906 ___ 9.701047897338867\n",
      "13.854392051696777 ___ 13.666128158569336\n",
      "12.39079761505127 ___ 12.785639762878418\n",
      "14.272857666015625 ___ 14.350715637207031\n",
      "12.676602363586426 ___ 12.741924285888672\n",
      "34.9221305847168 ___ 34.8403205871582\n",
      "49.28139114379883 ___ 48.88707733154297\n",
      "14.668769836425781 ___ 14.55567741394043\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(spectraVal[1].unsqueeze(0))\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Predicted\", \"___________\", \"Ground Truth\")\n",
    "for i in np.arange(44):\n",
    "    print(Pred[0][i].item(), '___', concVal[1][i].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
