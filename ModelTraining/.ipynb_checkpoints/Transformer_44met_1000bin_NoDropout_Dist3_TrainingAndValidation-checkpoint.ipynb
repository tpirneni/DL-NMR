{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer Encoder on dataset of 44 metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"Transformer_44met_TrainingAndValidation_1000bin_NoDropout_Dist3_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/ModelPerformanceMetrics/') \n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('Dataset44_Dist3_Spec.npy')\n",
    "conc1 = np.load('Dataset44_Dist3_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "#spectraVal = np.load('Dataset44_Dist3_Val_Spec.npy')\n",
    "#concVal = np.load('Dataset44_Dist3_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "#ValSpectra = np.load(\"Dataset44_Dist3_RepresentativeExamples_Spectra.npy\")\n",
    "#ValConc = np.load(\"Dataset44_Dist3_RepresentativeExamples_Concentrations.npy\")\n",
    "#ValSpecNames = np.load(\"Dataset44_Dist3_RepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectra, conc1, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "#spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "#ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "#concVal = torch.tensor(concVal).float().to(device)\n",
    "#ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 32, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del spectra\n",
    "del conc1\n",
    "del datasets\n",
    "del Test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.Linear(23552, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Binning\n",
    "        batch_size, seq_length = x.size()\n",
    "        num_bins = seq_length // self.input_dim\n",
    "        x = x.view(batch_size, num_bins, self.input_dim)  # (batch_size, num_bins, input_dim)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        x = x.permute(1, 0, 2)  # (num_bins, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)  # (num_bins, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, num_bins, d_model)\n",
    "        \n",
    "        # Reconstruct original sequence\n",
    "        x = x.reshape(batch_size, num_bins * d_model)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder(x)  # (batch_size, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "input_dim = 1000  # Size of each bin\n",
    "d_model = 512     # Embedding dimension\n",
    "nhead = 1         # Number of attention heads\n",
    "num_encoder_layers = 1  # Number of transformer encoder layers\n",
    "dim_feedforward = 2048  # Feedforward dimension\n",
    "dropout = 0.0     # Dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "       \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [1/1000], Train Loss: 5269809.8635, Test Loss: 841871.9944\n",
      "Epoch [2/1000], Train Loss: 2902269.3110, Test Loss: 678621.8845\n",
      "Epoch [3/1000], Train Loss: 2186339.6958, Test Loss: 458955.5332\n",
      "Epoch [4/1000], Train Loss: 1259687.7277, Test Loss: 150806.3643\n",
      "Epoch [5/1000], Train Loss: 413861.8139, Test Loss: 75318.7871\n",
      "Epoch [6/1000], Train Loss: 232640.4701, Test Loss: 47072.6897\n",
      "Epoch [7/1000], Train Loss: 157894.7530, Test Loss: 35054.7700\n",
      "Epoch [8/1000], Train Loss: 117930.2078, Test Loss: 27749.1391\n",
      "Epoch [9/1000], Train Loss: 92164.2263, Test Loss: 21552.8924\n",
      "Epoch [10/1000], Train Loss: 74073.9682, Test Loss: 18211.5559\n",
      "Epoch [11/1000], Train Loss: 63560.2953, Test Loss: 15173.7333\n",
      "Epoch [12/1000], Train Loss: 56266.8468, Test Loss: 14649.6596\n",
      "Epoch [13/1000], Train Loss: 50996.9228, Test Loss: 13201.1676\n",
      "Epoch [14/1000], Train Loss: 45876.1004, Test Loss: 11193.8745\n",
      "Epoch [15/1000], Train Loss: 42400.2640, Test Loss: 11871.4396\n",
      "Epoch [16/1000], Train Loss: 37414.4009, Test Loss: 9270.8487\n",
      "Epoch [17/1000], Train Loss: 35144.0647, Test Loss: 9032.4732\n",
      "Epoch [18/1000], Train Loss: 31759.1668, Test Loss: 8610.5507\n",
      "Epoch [19/1000], Train Loss: 29902.2392, Test Loss: 7867.7462\n",
      "Epoch [20/1000], Train Loss: 27475.5629, Test Loss: 7596.2218\n",
      "Epoch [21/1000], Train Loss: 26446.4549, Test Loss: 6745.9281\n",
      "Epoch [22/1000], Train Loss: 24271.3677, Test Loss: 6173.5747\n",
      "Epoch [23/1000], Train Loss: 24098.8452, Test Loss: 7761.8146\n",
      "Epoch [24/1000], Train Loss: 1500044.4326, Test Loss: 1267825.2659\n",
      "Epoch [25/1000], Train Loss: 1909998.5091, Test Loss: 39782.5622\n",
      "Epoch [26/1000], Train Loss: 100876.9865, Test Loss: 17496.9668\n",
      "Epoch [27/1000], Train Loss: 53649.6241, Test Loss: 11265.7635\n",
      "Epoch [28/1000], Train Loss: 39330.5130, Test Loss: 9556.1834\n",
      "Epoch [29/1000], Train Loss: 31774.7997, Test Loss: 8011.9068\n",
      "Epoch [30/1000], Train Loss: 27516.8089, Test Loss: 6974.6290\n",
      "Epoch [31/1000], Train Loss: 30637.0553, Test Loss: 6953.0443\n",
      "Epoch [32/1000], Train Loss: 22176.6016, Test Loss: 6373.7987\n",
      "Epoch [33/1000], Train Loss: 21366.1448, Test Loss: 6639.5271\n",
      "Epoch [34/1000], Train Loss: 21392.9571, Test Loss: 5517.7772\n",
      "Epoch [35/1000], Train Loss: 21219.8812, Test Loss: 6838.6993\n",
      "Epoch [36/1000], Train Loss: 20685.5801, Test Loss: 5358.6079\n",
      "Epoch [37/1000], Train Loss: 20161.5948, Test Loss: 4929.5308\n",
      "Epoch [38/1000], Train Loss: 19116.2150, Test Loss: 4893.8760\n",
      "Epoch [39/1000], Train Loss: 239280.3003, Test Loss: 8655.1381\n",
      "Epoch [40/1000], Train Loss: 23325.1046, Test Loss: 5493.3440\n",
      "Epoch [41/1000], Train Loss: 19343.2843, Test Loss: 10991.8204\n",
      "Epoch [42/1000], Train Loss: 17590.9519, Test Loss: 4679.5805\n",
      "Epoch [43/1000], Train Loss: 16114.1161, Test Loss: 7004.6692\n",
      "Epoch [44/1000], Train Loss: 15287.5121, Test Loss: 4456.6588\n",
      "Epoch [45/1000], Train Loss: 14415.1826, Test Loss: 4747.2266\n",
      "Epoch [46/1000], Train Loss: 14859.0621, Test Loss: 4352.8839\n",
      "Epoch [47/1000], Train Loss: 15317.4894, Test Loss: 4574.4351\n",
      "Epoch [48/1000], Train Loss: 15319.8999, Test Loss: 4410.9571\n",
      "Epoch [49/1000], Train Loss: 16011.6900, Test Loss: 5621.5152\n",
      "Epoch [50/1000], Train Loss: 17809.3752, Test Loss: 5150.9105\n",
      "Epoch [51/1000], Train Loss: 15615.4241, Test Loss: 4930.4403\n",
      "Epoch [52/1000], Train Loss: 15637.1533, Test Loss: 4299.3383\n",
      "Epoch [53/1000], Train Loss: 14828.8633, Test Loss: 4361.5573\n",
      "Epoch [54/1000], Train Loss: 14699.4185, Test Loss: 5426.5729\n",
      "Epoch [55/1000], Train Loss: 22148.2728, Test Loss: 3925.9866\n",
      "Epoch [56/1000], Train Loss: 21609.2292, Test Loss: 4454.8098\n",
      "Epoch [57/1000], Train Loss: 11953.7432, Test Loss: 3783.9993\n",
      "Epoch [58/1000], Train Loss: 11485.7288, Test Loss: 3949.2512\n",
      "Epoch [59/1000], Train Loss: 12568.0181, Test Loss: 4705.9412\n",
      "Epoch [60/1000], Train Loss: 13068.7173, Test Loss: 3891.5287\n",
      "Epoch [61/1000], Train Loss: 12820.6852, Test Loss: 3841.8387\n",
      "Epoch [62/1000], Train Loss: 13329.2714, Test Loss: 4826.8471\n",
      "Epoch [63/1000], Train Loss: 15334.4431, Test Loss: 6705.0228\n",
      "Epoch [64/1000], Train Loss: 14579.2153, Test Loss: 3697.6438\n",
      "Epoch [65/1000], Train Loss: 14590.1617, Test Loss: 51942.4223\n",
      "Epoch [66/1000], Train Loss: 75852.7424, Test Loss: 3805.4290\n",
      "Epoch [67/1000], Train Loss: 10636.0229, Test Loss: 3652.6385\n",
      "Epoch [68/1000], Train Loss: 9812.6050, Test Loss: 3552.6944\n",
      "Epoch [69/1000], Train Loss: 10011.6224, Test Loss: 3728.5871\n",
      "Epoch [70/1000], Train Loss: 9998.9627, Test Loss: 3851.7403\n",
      "Epoch [71/1000], Train Loss: 10294.8198, Test Loss: 3625.2319\n",
      "Epoch [72/1000], Train Loss: 106589.3443, Test Loss: 38327.0975\n",
      "Epoch [73/1000], Train Loss: 23567.6704, Test Loss: 4025.6135\n",
      "Epoch [74/1000], Train Loss: 10986.2300, Test Loss: 3241.2802\n",
      "Epoch [75/1000], Train Loss: 9756.0445, Test Loss: 3301.7809\n",
      "Epoch [76/1000], Train Loss: 9831.3794, Test Loss: 3176.7831\n",
      "Epoch [77/1000], Train Loss: 9440.7030, Test Loss: 3509.6308\n",
      "Epoch [78/1000], Train Loss: 14280.1622, Test Loss: 3702.7170\n",
      "Epoch [79/1000], Train Loss: 10946.1336, Test Loss: 3534.4583\n",
      "Epoch [80/1000], Train Loss: 9719.5244, Test Loss: 3671.1054\n",
      "Epoch [81/1000], Train Loss: 10252.2591, Test Loss: 3482.4151\n",
      "Epoch [82/1000], Train Loss: 11264.1908, Test Loss: 3519.2352\n",
      "Epoch [83/1000], Train Loss: 11815.8263, Test Loss: 4078.3712\n",
      "Epoch [84/1000], Train Loss: 11282.9605, Test Loss: 3569.6534\n",
      "Epoch [85/1000], Train Loss: 11447.8855, Test Loss: 21411.5124\n",
      "Epoch [86/1000], Train Loss: 18796.8879, Test Loss: 3304.1706\n",
      "Epoch [87/1000], Train Loss: 8547.7917, Test Loss: 3094.0041\n",
      "Epoch [88/1000], Train Loss: 8996.9303, Test Loss: 3300.5335\n",
      "Epoch [89/1000], Train Loss: 9876.6011, Test Loss: 3618.8621\n",
      "Epoch [90/1000], Train Loss: 33936.3135, Test Loss: 5308.2303\n",
      "Epoch [91/1000], Train Loss: 9806.4614, Test Loss: 3199.4238\n",
      "Epoch [92/1000], Train Loss: 8439.7106, Test Loss: 3136.9282\n",
      "Epoch [93/1000], Train Loss: 8104.4852, Test Loss: 3070.1540\n",
      "Epoch [94/1000], Train Loss: 8489.6195, Test Loss: 3462.2688\n",
      "Epoch [95/1000], Train Loss: 9076.9689, Test Loss: 3474.8037\n",
      "Epoch [96/1000], Train Loss: 11887.2068, Test Loss: 4311.1630\n",
      "Epoch [97/1000], Train Loss: 12274.2420, Test Loss: 3542.8692\n",
      "Epoch [98/1000], Train Loss: 8425.2597, Test Loss: 3190.9126\n",
      "Epoch [99/1000], Train Loss: 8771.7686, Test Loss: 3612.0350\n",
      "Epoch [100/1000], Train Loss: 40477.6227, Test Loss: 3025.0461\n",
      "Epoch [101/1000], Train Loss: 7572.6741, Test Loss: 2936.2957\n",
      "Epoch [102/1000], Train Loss: 7148.3244, Test Loss: 2776.0685\n",
      "Epoch [103/1000], Train Loss: 7369.7029, Test Loss: 3082.9994\n",
      "Epoch [104/1000], Train Loss: 7619.7397, Test Loss: 3184.3017\n",
      "Epoch [105/1000], Train Loss: 8376.9796, Test Loss: 3159.2941\n",
      "Epoch [106/1000], Train Loss: 9222.3924, Test Loss: 3122.9525\n",
      "Epoch [107/1000], Train Loss: 8675.1982, Test Loss: 3170.6709\n",
      "Epoch [108/1000], Train Loss: 24787.3695, Test Loss: 7474.5824\n",
      "Epoch [109/1000], Train Loss: 32252.6761, Test Loss: 3120.2695\n",
      "Epoch [110/1000], Train Loss: 8282.5575, Test Loss: 3023.6457\n",
      "Epoch [111/1000], Train Loss: 8338.4717, Test Loss: 3224.6250\n",
      "Epoch [112/1000], Train Loss: 9117.2658, Test Loss: 3087.7100\n",
      "Epoch [113/1000], Train Loss: 404098.9864, Test Loss: 9027.5777\n",
      "Epoch [114/1000], Train Loss: 23167.8974, Test Loss: 4337.9444\n",
      "Epoch [115/1000], Train Loss: 13173.1186, Test Loss: 3650.8982\n",
      "Epoch [116/1000], Train Loss: 11309.3714, Test Loss: 4229.2972\n",
      "Epoch [117/1000], Train Loss: 9121.5998, Test Loss: 3055.1088\n",
      "Epoch [118/1000], Train Loss: 8251.2396, Test Loss: 2924.3401\n",
      "Epoch [119/1000], Train Loss: 8230.0655, Test Loss: 2849.8000\n",
      "Epoch [120/1000], Train Loss: 8693.0678, Test Loss: 2947.6378\n",
      "Epoch [121/1000], Train Loss: 8968.1039, Test Loss: 3280.7969\n",
      "Epoch [122/1000], Train Loss: 60830.1005, Test Loss: 8004.0187\n",
      "Epoch [123/1000], Train Loss: 11141.8700, Test Loss: 3112.3232\n",
      "Epoch [124/1000], Train Loss: 7527.2905, Test Loss: 3105.1469\n",
      "Epoch [125/1000], Train Loss: 7139.1279, Test Loss: 2843.1250\n",
      "Epoch [126/1000], Train Loss: 7280.3609, Test Loss: 2890.8065\n",
      "Epoch [127/1000], Train Loss: 7457.0595, Test Loss: 3012.0141\n",
      "Epoch [128/1000], Train Loss: 8171.6130, Test Loss: 3074.2495\n",
      "Epoch [129/1000], Train Loss: 8350.2019, Test Loss: 3106.1492\n",
      "Epoch [130/1000], Train Loss: 43893.7425, Test Loss: 3909.8616\n",
      "Epoch [131/1000], Train Loss: 8878.4942, Test Loss: 2901.4449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [132/1000], Train Loss: 7334.2577, Test Loss: 2800.4898\n",
      "Epoch [133/1000], Train Loss: 7381.3616, Test Loss: 3117.0587\n",
      "Epoch [134/1000], Train Loss: 15240.6422, Test Loss: 3116.6670\n",
      "Epoch [135/1000], Train Loss: 8413.0085, Test Loss: 3114.1769\n",
      "Epoch [136/1000], Train Loss: 7497.3195, Test Loss: 3033.8364\n",
      "Epoch [137/1000], Train Loss: 30750.6347, Test Loss: 6367.5994\n",
      "Epoch [138/1000], Train Loss: 9559.3558, Test Loss: 3002.7217\n",
      "Epoch [139/1000], Train Loss: 6582.1254, Test Loss: 2689.9135\n",
      "Epoch [140/1000], Train Loss: 6781.7081, Test Loss: 3028.5047\n",
      "Epoch [141/1000], Train Loss: 6860.1274, Test Loss: 2940.5921\n",
      "Epoch [142/1000], Train Loss: 7598.0079, Test Loss: 3048.5231\n",
      "Epoch [143/1000], Train Loss: 9301.4859, Test Loss: 3208.1683\n",
      "Epoch [144/1000], Train Loss: 8094.6528, Test Loss: 3325.1137\n",
      "Epoch [145/1000], Train Loss: 9310.6987, Test Loss: 3765.6297\n",
      "Epoch [146/1000], Train Loss: 16660.2626, Test Loss: 8034.7002\n",
      "Epoch [147/1000], Train Loss: 8734.2794, Test Loss: 2711.9348\n",
      "Epoch [148/1000], Train Loss: 6953.4944, Test Loss: 2859.5462\n",
      "Epoch [149/1000], Train Loss: 8208.8071, Test Loss: 3573.6967\n",
      "Epoch [150/1000], Train Loss: 8515.1533, Test Loss: 2771.0112\n",
      "Epoch [151/1000], Train Loss: 9769.8052, Test Loss: 3181.8061\n",
      "Epoch [152/1000], Train Loss: 7414.5018, Test Loss: 3148.4151\n",
      "Epoch [153/1000], Train Loss: 9865.5242, Test Loss: 2947.2522\n",
      "Epoch [154/1000], Train Loss: 8322.1349, Test Loss: 4137.1270\n",
      "Epoch [155/1000], Train Loss: 8315.5573, Test Loss: 3322.5357\n",
      "Epoch [156/1000], Train Loss: 14836.0882, Test Loss: 75814.9230\n",
      "Epoch [157/1000], Train Loss: 13616.6841, Test Loss: 2990.6350\n",
      "Epoch [158/1000], Train Loss: 6118.0269, Test Loss: 2674.8287\n",
      "Epoch [159/1000], Train Loss: 6224.9538, Test Loss: 2772.7885\n",
      "Epoch [160/1000], Train Loss: 6650.2469, Test Loss: 3255.9417\n",
      "Epoch [161/1000], Train Loss: 11554.1323, Test Loss: 2758.9892\n",
      "Epoch [162/1000], Train Loss: 8763.6821, Test Loss: 2799.7602\n",
      "Epoch [163/1000], Train Loss: 6885.9149, Test Loss: 2806.2484\n",
      "Epoch [164/1000], Train Loss: 18086.3558, Test Loss: 3785.0923\n",
      "Epoch [165/1000], Train Loss: 6983.2210, Test Loss: 2634.0926\n",
      "Epoch [166/1000], Train Loss: 5933.3301, Test Loss: 2779.0305\n",
      "Epoch [167/1000], Train Loss: 6192.2679, Test Loss: 2779.2646\n",
      "Epoch [168/1000], Train Loss: 6759.8228, Test Loss: 2821.7179\n",
      "Epoch [169/1000], Train Loss: 8009.4582, Test Loss: 2989.9606\n",
      "Epoch [170/1000], Train Loss: 7307.7949, Test Loss: 3047.4761\n",
      "Epoch [171/1000], Train Loss: 11842.1991, Test Loss: 4369.6870\n",
      "Epoch [172/1000], Train Loss: 7298.0776, Test Loss: 2594.3206\n",
      "Epoch [173/1000], Train Loss: 5928.0225, Test Loss: 2853.4107\n",
      "Epoch [174/1000], Train Loss: 6866.3070, Test Loss: 3167.9829\n",
      "Epoch [175/1000], Train Loss: 6998.2083, Test Loss: 2978.4901\n",
      "Epoch [176/1000], Train Loss: 44854.6180, Test Loss: 3704.5937\n",
      "Epoch [177/1000], Train Loss: 8408.3227, Test Loss: 2603.8957\n",
      "Epoch [178/1000], Train Loss: 6441.2554, Test Loss: 2720.7373\n",
      "Epoch [179/1000], Train Loss: 12979.3578, Test Loss: 4162.9616\n",
      "Epoch [180/1000], Train Loss: 10898.9129, Test Loss: 2764.1623\n",
      "Epoch [181/1000], Train Loss: 5664.6585, Test Loss: 2633.8711\n",
      "Epoch [182/1000], Train Loss: 5685.1338, Test Loss: 2716.9034\n",
      "Epoch [183/1000], Train Loss: 21673.2218, Test Loss: 2755.9286\n",
      "Epoch [184/1000], Train Loss: 6375.2645, Test Loss: 2526.9020\n",
      "Epoch [185/1000], Train Loss: 5492.1069, Test Loss: 2825.8648\n",
      "Epoch [186/1000], Train Loss: 6284.9266, Test Loss: 2965.4299\n",
      "Epoch [187/1000], Train Loss: 6353.7026, Test Loss: 4717.4348\n",
      "Epoch [188/1000], Train Loss: 21237.0435, Test Loss: 4921.7787\n",
      "Epoch [189/1000], Train Loss: 7343.8605, Test Loss: 3024.0799\n",
      "Epoch [190/1000], Train Loss: 15251.3917, Test Loss: 6967.5210\n",
      "Epoch [191/1000], Train Loss: 7604.7744, Test Loss: 2746.0525\n",
      "Epoch [192/1000], Train Loss: 6328.6149, Test Loss: 2924.4828\n",
      "Epoch [193/1000], Train Loss: 7195.8978, Test Loss: 8572.8661\n",
      "Epoch [194/1000], Train Loss: 194065.1776, Test Loss: 8355.2709\n",
      "Epoch [195/1000], Train Loss: 11964.9301, Test Loss: 2837.9891\n",
      "Epoch [196/1000], Train Loss: 10714.8893, Test Loss: 4423.3406\n",
      "Epoch [197/1000], Train Loss: 8156.4800, Test Loss: 4358.8622\n",
      "Epoch [198/1000], Train Loss: 7721.2657, Test Loss: 6105.7415\n",
      "Epoch [199/1000], Train Loss: 8276.4008, Test Loss: 2559.0285\n",
      "Epoch [200/1000], Train Loss: 6379.0059, Test Loss: 2826.4755\n",
      "Epoch [201/1000], Train Loss: 40692.9817, Test Loss: 18910.5894\n",
      "Epoch [202/1000], Train Loss: 11235.2917, Test Loss: 2764.7935\n",
      "Epoch [203/1000], Train Loss: 6280.9160, Test Loss: 2683.3475\n",
      "Epoch [204/1000], Train Loss: 5938.9917, Test Loss: 2551.9137\n",
      "Epoch [205/1000], Train Loss: 5956.4523, Test Loss: 2837.0889\n",
      "Epoch [206/1000], Train Loss: 6573.5031, Test Loss: 3085.4218\n",
      "Epoch [207/1000], Train Loss: 7939.2181, Test Loss: 5866.3224\n",
      "Epoch [208/1000], Train Loss: 12304.5126, Test Loss: 2597.4032\n",
      "Epoch [209/1000], Train Loss: 6623.5603, Test Loss: 11372.6418\n",
      "Epoch [210/1000], Train Loss: 20923.3940, Test Loss: 2645.8582\n",
      "Epoch [211/1000], Train Loss: 5654.3533, Test Loss: 2704.0939\n",
      "Epoch [212/1000], Train Loss: 5512.1315, Test Loss: 2513.1975\n",
      "Epoch [213/1000], Train Loss: 5873.2913, Test Loss: 2867.3184\n",
      "Epoch [214/1000], Train Loss: 17031.9029, Test Loss: 2609.0558\n",
      "Epoch [215/1000], Train Loss: 5763.5549, Test Loss: 2655.9452\n",
      "Epoch [216/1000], Train Loss: 5335.3292, Test Loss: 2628.1409\n",
      "Epoch [217/1000], Train Loss: 6360.4909, Test Loss: 2842.0480\n",
      "Epoch [218/1000], Train Loss: 10360.8088, Test Loss: 3268.3531\n",
      "Epoch [219/1000], Train Loss: 11324.1875, Test Loss: 61256.5564\n",
      "Epoch [220/1000], Train Loss: 10363.5042, Test Loss: 2499.1291\n",
      "Epoch [221/1000], Train Loss: 5024.9507, Test Loss: 2424.3774\n",
      "Epoch [222/1000], Train Loss: 5416.8223, Test Loss: 2607.3183\n",
      "Epoch [223/1000], Train Loss: 6718.5484, Test Loss: 3041.0827\n",
      "Epoch [224/1000], Train Loss: 7241.1107, Test Loss: 4373.7234\n",
      "Epoch [225/1000], Train Loss: 8707.3855, Test Loss: 2939.9263\n",
      "Epoch [226/1000], Train Loss: 12819.4801, Test Loss: 2473.0494\n",
      "Epoch [227/1000], Train Loss: 5422.2285, Test Loss: 2658.3703\n",
      "Epoch [228/1000], Train Loss: 12555.6853, Test Loss: 3040.8503\n",
      "Epoch [229/1000], Train Loss: 5790.3063, Test Loss: 2526.4948\n",
      "Epoch [230/1000], Train Loss: 6018.1566, Test Loss: 2625.7802\n",
      "Epoch [231/1000], Train Loss: 5565.4650, Test Loss: 3423.7542\n",
      "Epoch [232/1000], Train Loss: 6919.6364, Test Loss: 2535.0002\n",
      "Epoch [233/1000], Train Loss: 8949.5059, Test Loss: 2566.3719\n",
      "Epoch [234/1000], Train Loss: 7475.7838, Test Loss: 2816.0737\n",
      "Epoch [235/1000], Train Loss: 23661.3753, Test Loss: 2433.4469\n",
      "Epoch [236/1000], Train Loss: 4753.6355, Test Loss: 2573.9834\n",
      "Epoch [237/1000], Train Loss: 5327.6739, Test Loss: 2480.9450\n",
      "Epoch [238/1000], Train Loss: 12488.3518, Test Loss: 2569.9444\n",
      "Epoch [239/1000], Train Loss: 13959.4775, Test Loss: 5131.3024\n",
      "Epoch [240/1000], Train Loss: 7427.9463, Test Loss: 2657.7474\n",
      "Epoch [241/1000], Train Loss: 5022.7709, Test Loss: 2556.4779\n",
      "Epoch [242/1000], Train Loss: 5133.4919, Test Loss: 2772.9900\n",
      "Epoch [243/1000], Train Loss: 5642.9906, Test Loss: 2895.7365\n",
      "Epoch [244/1000], Train Loss: 10492.8447, Test Loss: 13306.9307\n",
      "Epoch [245/1000], Train Loss: 10749.4808, Test Loss: 2703.1241\n",
      "Epoch [246/1000], Train Loss: 5213.3195, Test Loss: 2652.8295\n",
      "Epoch [247/1000], Train Loss: 5693.2506, Test Loss: 2596.1544\n",
      "Epoch [248/1000], Train Loss: 14917.7943, Test Loss: 3595.6489\n",
      "Epoch [249/1000], Train Loss: 6636.9705, Test Loss: 2483.2548\n",
      "Epoch [250/1000], Train Loss: 5879.9713, Test Loss: 2723.3335\n",
      "Epoch [251/1000], Train Loss: 27520.8394, Test Loss: 2641.0292\n",
      "Epoch [252/1000], Train Loss: 5235.8296, Test Loss: 2409.6396\n",
      "Epoch [253/1000], Train Loss: 5100.8765, Test Loss: 2385.4401\n",
      "Epoch [254/1000], Train Loss: 4853.4022, Test Loss: 2447.4971\n",
      "Epoch [255/1000], Train Loss: 5522.1580, Test Loss: 2503.3416\n",
      "Epoch [256/1000], Train Loss: 5944.1956, Test Loss: 3640.8654\n",
      "Epoch [257/1000], Train Loss: 41420.1047, Test Loss: 60404.6951\n",
      "Epoch [258/1000], Train Loss: 15643.7624, Test Loss: 3036.8435\n",
      "Epoch [259/1000], Train Loss: 5304.9826, Test Loss: 2738.9494\n",
      "Epoch [260/1000], Train Loss: 4957.5096, Test Loss: 2427.8786\n",
      "Epoch [261/1000], Train Loss: 4990.0933, Test Loss: 2508.2388\n",
      "Epoch [262/1000], Train Loss: 5036.9626, Test Loss: 2608.6602\n",
      "Epoch [263/1000], Train Loss: 5076.1413, Test Loss: 2471.1193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [264/1000], Train Loss: 6413.8481, Test Loss: 3483.3714\n",
      "Epoch [265/1000], Train Loss: 6670.4778, Test Loss: 2596.2825\n",
      "Epoch [266/1000], Train Loss: 10343.7499, Test Loss: 3608.6220\n",
      "Epoch [267/1000], Train Loss: 27341.0938, Test Loss: 2915.9297\n",
      "Epoch [268/1000], Train Loss: 5914.2274, Test Loss: 2476.0336\n",
      "Epoch [269/1000], Train Loss: 4665.8332, Test Loss: 2776.1559\n",
      "Epoch [270/1000], Train Loss: 15809.1169, Test Loss: 2495.5700\n",
      "Epoch [271/1000], Train Loss: 4769.7227, Test Loss: 2427.8423\n",
      "Epoch [272/1000], Train Loss: 4768.5490, Test Loss: 2621.6782\n",
      "Epoch [273/1000], Train Loss: 5636.6487, Test Loss: 2600.9135\n",
      "Epoch [274/1000], Train Loss: 8071.4503, Test Loss: 2669.7372\n",
      "Epoch [275/1000], Train Loss: 6794.7202, Test Loss: 2778.9056\n",
      "Epoch [276/1000], Train Loss: 25617.4778, Test Loss: 2876.4476\n",
      "Epoch [277/1000], Train Loss: 5101.6083, Test Loss: 2332.1383\n",
      "Epoch [278/1000], Train Loss: 4375.4806, Test Loss: 2342.0177\n",
      "Epoch [279/1000], Train Loss: 4699.8200, Test Loss: 2441.2025\n",
      "Epoch [280/1000], Train Loss: 5181.7693, Test Loss: 2350.7059\n",
      "Epoch [281/1000], Train Loss: 65490.7657, Test Loss: 4516.7538\n",
      "Epoch [282/1000], Train Loss: 8044.6930, Test Loss: 2619.9138\n",
      "Epoch [283/1000], Train Loss: 4930.5843, Test Loss: 2533.0822\n",
      "Epoch [284/1000], Train Loss: 4744.4997, Test Loss: 2375.8620\n",
      "Epoch [285/1000], Train Loss: 4837.5803, Test Loss: 2421.6883\n",
      "Epoch [286/1000], Train Loss: 4906.1086, Test Loss: 2468.7999\n",
      "Epoch [287/1000], Train Loss: 4877.6144, Test Loss: 2428.4655\n",
      "Epoch [288/1000], Train Loss: 25964.9435, Test Loss: 12830.8889\n",
      "Epoch [289/1000], Train Loss: 7684.5464, Test Loss: 2605.6513\n",
      "Epoch [290/1000], Train Loss: 4510.5805, Test Loss: 2400.7226\n",
      "Epoch [291/1000], Train Loss: 4658.5719, Test Loss: 2437.3532\n",
      "Epoch [292/1000], Train Loss: 4566.0262, Test Loss: 2538.2884\n",
      "Epoch [293/1000], Train Loss: 5020.9647, Test Loss: 2529.5162\n",
      "Epoch [294/1000], Train Loss: 5240.4841, Test Loss: 2807.6696\n",
      "Epoch [295/1000], Train Loss: 18349.0444, Test Loss: 2916.4471\n",
      "Epoch [296/1000], Train Loss: 4882.2618, Test Loss: 2501.0492\n",
      "Epoch [297/1000], Train Loss: 4406.8769, Test Loss: 2418.6879\n",
      "Epoch [298/1000], Train Loss: 4679.7464, Test Loss: 2458.0786\n",
      "Epoch [299/1000], Train Loss: 6409.5358, Test Loss: 2921.2895\n",
      "Epoch [300/1000], Train Loss: 5267.5704, Test Loss: 2510.7428\n",
      "Epoch [301/1000], Train Loss: 30869.1325, Test Loss: 2504.5271\n",
      "Epoch [302/1000], Train Loss: 4524.1785, Test Loss: 2318.0446\n",
      "Epoch [303/1000], Train Loss: 4283.0235, Test Loss: 2538.9284\n",
      "Epoch [304/1000], Train Loss: 4282.1884, Test Loss: 2505.6425\n",
      "Epoch [305/1000], Train Loss: 4822.2582, Test Loss: 2343.3490\n",
      "Epoch [306/1000], Train Loss: 4899.7484, Test Loss: 2449.8704\n",
      "Epoch [307/1000], Train Loss: 5362.4220, Test Loss: 2873.6761\n",
      "Epoch [308/1000], Train Loss: 9154.7849, Test Loss: 2517.6130\n",
      "Epoch [309/1000], Train Loss: 4657.1330, Test Loss: 3029.9605\n",
      "Epoch [310/1000], Train Loss: 7689.0272, Test Loss: 7214.6087\n",
      "Epoch [311/1000], Train Loss: 7142.4209, Test Loss: 2566.6955\n",
      "Epoch [312/1000], Train Loss: 4670.2977, Test Loss: 2467.9117\n",
      "Epoch [313/1000], Train Loss: 4936.0961, Test Loss: 2793.8348\n",
      "Epoch [314/1000], Train Loss: 20875.8528, Test Loss: 2891.5125\n",
      "Epoch [315/1000], Train Loss: 4851.6895, Test Loss: 2444.0890\n",
      "Epoch [316/1000], Train Loss: 4171.9355, Test Loss: 2305.0266\n",
      "Epoch [317/1000], Train Loss: 4507.8575, Test Loss: 2479.0447\n",
      "Epoch [318/1000], Train Loss: 5988.4666, Test Loss: 3362.2881\n",
      "Epoch [319/1000], Train Loss: 12528.5100, Test Loss: 2752.6368\n",
      "Epoch [320/1000], Train Loss: 4438.0450, Test Loss: 2342.3007\n",
      "Epoch [321/1000], Train Loss: 4216.8072, Test Loss: 2544.6619\n",
      "Epoch [322/1000], Train Loss: 4528.7018, Test Loss: 2368.9703\n",
      "Epoch [323/1000], Train Loss: 8347.3000, Test Loss: 3604.9086\n",
      "Epoch [324/1000], Train Loss: 6035.4037, Test Loss: 2424.7515\n",
      "Epoch [325/1000], Train Loss: 4634.7115, Test Loss: 3241.3456\n",
      "Epoch [326/1000], Train Loss: 5567.1182, Test Loss: 2643.4389\n",
      "Epoch [327/1000], Train Loss: 10143.8000, Test Loss: 3232.7685\n",
      "Epoch [328/1000], Train Loss: 15519.6150, Test Loss: 3023.7653\n",
      "Epoch [329/1000], Train Loss: 4972.2909, Test Loss: 2476.6938\n",
      "Epoch [330/1000], Train Loss: 4189.3545, Test Loss: 2443.4476\n",
      "Epoch [331/1000], Train Loss: 4897.7649, Test Loss: 2861.0428\n",
      "Epoch [332/1000], Train Loss: 5451.2066, Test Loss: 2460.7980\n",
      "Epoch [333/1000], Train Loss: 7449.6513, Test Loss: 2492.6417\n",
      "Epoch [334/1000], Train Loss: 18019.3499, Test Loss: 2614.5699\n",
      "Epoch [335/1000], Train Loss: 4261.3289, Test Loss: 2332.1881\n",
      "Epoch [336/1000], Train Loss: 4290.0156, Test Loss: 2355.7389\n",
      "Epoch [337/1000], Train Loss: 4557.3535, Test Loss: 4012.1344\n",
      "Epoch [338/1000], Train Loss: 5436.8264, Test Loss: 2359.3511\n",
      "Epoch [339/1000], Train Loss: 7568.9238, Test Loss: 2416.9739\n",
      "Epoch [340/1000], Train Loss: 4600.9549, Test Loss: 2751.6105\n",
      "Epoch [341/1000], Train Loss: 9909.7893, Test Loss: 3218.6827\n",
      "Epoch [342/1000], Train Loss: 7456.9019, Test Loss: 3543.1677\n",
      "Epoch [343/1000], Train Loss: 4774.5450, Test Loss: 2419.7988\n",
      "Epoch [344/1000], Train Loss: 4006.9965, Test Loss: 2386.7154\n",
      "Epoch [345/1000], Train Loss: 7533.0438, Test Loss: 4994.1118\n",
      "Epoch [346/1000], Train Loss: 8553.3629, Test Loss: 2425.4599\n",
      "Epoch [347/1000], Train Loss: 4286.7947, Test Loss: 2742.6498\n",
      "Epoch [348/1000], Train Loss: 4165.6910, Test Loss: 2487.4053\n",
      "Epoch [349/1000], Train Loss: 6382.1923, Test Loss: 2734.8512\n",
      "Epoch [350/1000], Train Loss: 17463.4685, Test Loss: 2916.9676\n",
      "Epoch [351/1000], Train Loss: 4981.8496, Test Loss: 2277.3355\n",
      "Epoch [352/1000], Train Loss: 4923.5996, Test Loss: 2584.8086\n",
      "Epoch [353/1000], Train Loss: 4200.2131, Test Loss: 2440.5539\n",
      "Epoch [354/1000], Train Loss: 4053.7107, Test Loss: 2435.1533\n",
      "Epoch [355/1000], Train Loss: 5304.1932, Test Loss: 3940.3600\n",
      "Epoch [356/1000], Train Loss: 18679.3925, Test Loss: 6323.7104\n",
      "Epoch [357/1000], Train Loss: 8338.5319, Test Loss: 2409.6931\n",
      "Epoch [358/1000], Train Loss: 4194.1587, Test Loss: 2407.2448\n",
      "Epoch [359/1000], Train Loss: 4558.3767, Test Loss: 2395.9864\n",
      "Epoch [360/1000], Train Loss: 3974.7626, Test Loss: 2817.1744\n",
      "Epoch [361/1000], Train Loss: 11593.2212, Test Loss: 2544.7424\n",
      "Epoch [362/1000], Train Loss: 4283.0165, Test Loss: 2312.7800\n",
      "Epoch [363/1000], Train Loss: 6905.4421, Test Loss: 2346.7652\n",
      "Epoch [364/1000], Train Loss: 4123.3231, Test Loss: 2367.2343\n",
      "Epoch [365/1000], Train Loss: 4902.3945, Test Loss: 2408.5154\n",
      "Epoch [366/1000], Train Loss: 4118.2263, Test Loss: 2392.4404\n",
      "Epoch [367/1000], Train Loss: 9273.5002, Test Loss: 2515.9380\n",
      "Epoch [368/1000], Train Loss: 4036.9896, Test Loss: 2229.4492\n",
      "Epoch [369/1000], Train Loss: 3877.8471, Test Loss: 2500.6837\n",
      "Epoch [370/1000], Train Loss: 7461.6093, Test Loss: 8245.0213\n",
      "Epoch [371/1000], Train Loss: 6089.9167, Test Loss: 3273.4230\n",
      "Epoch [372/1000], Train Loss: 4206.2649, Test Loss: 2544.1556\n",
      "Epoch [373/1000], Train Loss: 5982.4677, Test Loss: 2642.9041\n",
      "Epoch [374/1000], Train Loss: 7119.3775, Test Loss: 5370.5395\n",
      "Epoch [375/1000], Train Loss: 6585.0850, Test Loss: 2353.6799\n",
      "Epoch [376/1000], Train Loss: 4421.9942, Test Loss: 2480.8987\n",
      "Epoch [377/1000], Train Loss: 4957.8378, Test Loss: 2554.3153\n",
      "Epoch [378/1000], Train Loss: 5670.3056, Test Loss: 2423.0222\n",
      "Epoch [379/1000], Train Loss: 4864.8984, Test Loss: 2366.9940\n",
      "Epoch [380/1000], Train Loss: 5042.3187, Test Loss: 2769.6884\n",
      "Epoch [381/1000], Train Loss: 4916.7489, Test Loss: 2955.3396\n",
      "Epoch [382/1000], Train Loss: 6781.1719, Test Loss: 2444.9900\n",
      "Epoch [383/1000], Train Loss: 4284.0944, Test Loss: 2613.5174\n",
      "Epoch [384/1000], Train Loss: 5907.4731, Test Loss: 3248.8976\n",
      "Epoch [385/1000], Train Loss: 5369.5243, Test Loss: 2304.2145\n",
      "Epoch [386/1000], Train Loss: 3850.4663, Test Loss: 2568.2052\n",
      "Epoch [387/1000], Train Loss: 5520.0308, Test Loss: 3598.5431\n",
      "Epoch [388/1000], Train Loss: 5492.6968, Test Loss: 3441.0916\n",
      "Epoch [389/1000], Train Loss: 9247.7634, Test Loss: 2258.6089\n",
      "Epoch [390/1000], Train Loss: 4091.8994, Test Loss: 2575.9858\n",
      "Epoch [391/1000], Train Loss: 9751.8789, Test Loss: 2382.3255\n",
      "Epoch [392/1000], Train Loss: 3860.5431, Test Loss: 2448.4450\n",
      "Epoch [393/1000], Train Loss: 5198.9826, Test Loss: 2507.2287\n",
      "Epoch [394/1000], Train Loss: 4259.1739, Test Loss: 2390.3265\n",
      "Epoch [395/1000], Train Loss: 4582.5964, Test Loss: 3318.3264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [396/1000], Train Loss: 10682.9466, Test Loss: 2275.1864\n",
      "Epoch [397/1000], Train Loss: 11161.3148, Test Loss: 2754.1769\n",
      "Epoch [398/1000], Train Loss: 4389.4828, Test Loss: 2312.7283\n",
      "Epoch [399/1000], Train Loss: 3758.1794, Test Loss: 2265.5577\n",
      "Epoch [400/1000], Train Loss: 3960.4681, Test Loss: 2650.1184\n",
      "Epoch [401/1000], Train Loss: 4489.3913, Test Loss: 2734.2452\n",
      "Epoch [402/1000], Train Loss: 5039.6960, Test Loss: 2467.9693\n",
      "Epoch [403/1000], Train Loss: 5053.5433, Test Loss: 2356.7846\n",
      "Epoch [404/1000], Train Loss: 5431.3452, Test Loss: 2454.3704\n",
      "Epoch [405/1000], Train Loss: 5190.2237, Test Loss: 2457.6234\n",
      "Epoch [406/1000], Train Loss: 7519.6344, Test Loss: 3044.1316\n",
      "Epoch [407/1000], Train Loss: 5532.4228, Test Loss: 3921.0945\n",
      "Epoch [408/1000], Train Loss: 4636.6112, Test Loss: 2303.5259\n",
      "Epoch [409/1000], Train Loss: 4065.1976, Test Loss: 2671.1098\n",
      "Epoch [410/1000], Train Loss: 4823.2772, Test Loss: 3008.1215\n",
      "Epoch [411/1000], Train Loss: 7227.4937, Test Loss: 2440.2675\n",
      "Epoch [412/1000], Train Loss: 4869.9452, Test Loss: 2425.5684\n",
      "Epoch [413/1000], Train Loss: 12079.9555, Test Loss: 2250.3039\n",
      "Epoch [414/1000], Train Loss: 3538.5612, Test Loss: 2249.1404\n",
      "Epoch [415/1000], Train Loss: 3507.0166, Test Loss: 2423.5675\n",
      "Epoch [416/1000], Train Loss: 3832.7613, Test Loss: 2504.3014\n",
      "Epoch [417/1000], Train Loss: 5606.3252, Test Loss: 2447.3011\n",
      "Epoch [418/1000], Train Loss: 4340.1858, Test Loss: 2556.0573\n",
      "Epoch [419/1000], Train Loss: 4668.1864, Test Loss: 2597.3988\n",
      "Epoch [420/1000], Train Loss: 5290.2453, Test Loss: 2354.8443\n",
      "Epoch [421/1000], Train Loss: 9558.2799, Test Loss: 4275.2667\n",
      "Epoch [422/1000], Train Loss: 4285.8373, Test Loss: 2355.2188\n",
      "Epoch [423/1000], Train Loss: 3303.8021, Test Loss: 2331.9477\n",
      "Epoch [424/1000], Train Loss: 6069.0168, Test Loss: 2397.2765\n",
      "Epoch [425/1000], Train Loss: 3604.4013, Test Loss: 2304.1392\n",
      "Epoch [426/1000], Train Loss: 3760.7765, Test Loss: 2460.8454\n",
      "Epoch [427/1000], Train Loss: 15707.5461, Test Loss: 2973.4870\n",
      "Epoch [428/1000], Train Loss: 4134.2236, Test Loss: 2255.6969\n",
      "Epoch [429/1000], Train Loss: 3090.8416, Test Loss: 2241.7750\n",
      "Epoch [430/1000], Train Loss: 3417.1270, Test Loss: 2341.5797\n",
      "Epoch [431/1000], Train Loss: 3269.7751, Test Loss: 2318.2740\n",
      "Epoch [432/1000], Train Loss: 3727.9360, Test Loss: 2673.9770\n",
      "Epoch [433/1000], Train Loss: 4763.4268, Test Loss: 3433.2982\n",
      "Epoch [434/1000], Train Loss: 9129.2539, Test Loss: 4517.7289\n",
      "Epoch [435/1000], Train Loss: 9242.8408, Test Loss: 3327.2377\n",
      "Epoch [436/1000], Train Loss: 3992.9242, Test Loss: 2253.1084\n",
      "Epoch [437/1000], Train Loss: 3363.3540, Test Loss: 2306.0928\n",
      "Epoch [438/1000], Train Loss: 3456.4355, Test Loss: 2256.1654\n",
      "Epoch [439/1000], Train Loss: 3642.0612, Test Loss: 2316.5218\n",
      "Epoch [440/1000], Train Loss: 8506.9171, Test Loss: 2348.0460\n",
      "Epoch [441/1000], Train Loss: 3632.0461, Test Loss: 2503.1205\n",
      "Epoch [442/1000], Train Loss: 3840.0653, Test Loss: 2807.1478\n",
      "Epoch [443/1000], Train Loss: 3654.2078, Test Loss: 2402.7481\n",
      "Epoch [444/1000], Train Loss: 8670.3545, Test Loss: 2367.7438\n",
      "Epoch [445/1000], Train Loss: 3347.3203, Test Loss: 2325.7912\n",
      "Epoch [446/1000], Train Loss: 3370.1373, Test Loss: 2459.0350\n",
      "Epoch [447/1000], Train Loss: 3678.0936, Test Loss: 2359.3273\n",
      "Epoch [448/1000], Train Loss: 3969.3684, Test Loss: 2314.6253\n",
      "Epoch [449/1000], Train Loss: 8025.6849, Test Loss: 2268.3931\n",
      "Epoch [450/1000], Train Loss: 3267.1133, Test Loss: 3733.8319\n",
      "Epoch [451/1000], Train Loss: 4142.3987, Test Loss: 2327.2091\n",
      "Epoch [452/1000], Train Loss: 3358.9818, Test Loss: 2285.1917\n",
      "Epoch [453/1000], Train Loss: 3981.5249, Test Loss: 2391.3839\n",
      "Epoch [454/1000], Train Loss: 9480.2633, Test Loss: 3802.8832\n",
      "Epoch [455/1000], Train Loss: 9760.9670, Test Loss: 2388.5583\n",
      "Epoch [456/1000], Train Loss: 3266.4446, Test Loss: 2188.3166\n",
      "Epoch [457/1000], Train Loss: 3101.4976, Test Loss: 2273.3639\n",
      "Epoch [458/1000], Train Loss: 3960.3881, Test Loss: 2578.6210\n",
      "Epoch [459/1000], Train Loss: 7245.1132, Test Loss: 3058.3221\n",
      "Epoch [460/1000], Train Loss: 3676.8265, Test Loss: 2478.0905\n",
      "Epoch [461/1000], Train Loss: 3259.9730, Test Loss: 2227.3782\n",
      "Epoch [462/1000], Train Loss: 3358.4353, Test Loss: 2523.4700\n",
      "Epoch [463/1000], Train Loss: 4862.4591, Test Loss: 3136.8716\n",
      "Epoch [464/1000], Train Loss: 3672.7975, Test Loss: 2620.8165\n",
      "Epoch [465/1000], Train Loss: 3798.5873, Test Loss: 2192.9430\n",
      "Epoch [466/1000], Train Loss: 10550.5994, Test Loss: 2257.3836\n",
      "Epoch [467/1000], Train Loss: 3118.9595, Test Loss: 2079.9285\n",
      "Epoch [468/1000], Train Loss: 2867.4946, Test Loss: 2177.2885\n",
      "Epoch [469/1000], Train Loss: 3387.3505, Test Loss: 2260.6957\n",
      "Epoch [470/1000], Train Loss: 4701.4263, Test Loss: 2777.9487\n",
      "Epoch [471/1000], Train Loss: 4872.9708, Test Loss: 2445.9912\n",
      "Epoch [472/1000], Train Loss: 5332.3015, Test Loss: 2563.5604\n",
      "Epoch [473/1000], Train Loss: 4155.4944, Test Loss: 2160.2072\n",
      "Epoch [474/1000], Train Loss: 3362.1242, Test Loss: 2488.4991\n",
      "Epoch [475/1000], Train Loss: 4758.3866, Test Loss: 2522.4561\n",
      "Epoch [476/1000], Train Loss: 5392.6359, Test Loss: 2483.7931\n",
      "Epoch [477/1000], Train Loss: 4652.7263, Test Loss: 2333.3265\n",
      "Epoch [478/1000], Train Loss: 4467.6648, Test Loss: 2734.9248\n",
      "Epoch [479/1000], Train Loss: 3707.5639, Test Loss: 2722.9642\n",
      "Epoch [480/1000], Train Loss: 4249.6832, Test Loss: 2338.5191\n",
      "Epoch [481/1000], Train Loss: 4175.0901, Test Loss: 2450.1924\n",
      "Epoch [482/1000], Train Loss: 5487.2766, Test Loss: 3385.6831\n",
      "Epoch [483/1000], Train Loss: 6065.8939, Test Loss: 2261.7684\n",
      "Epoch [484/1000], Train Loss: 3324.2909, Test Loss: 2391.9640\n",
      "Epoch [485/1000], Train Loss: 3496.4678, Test Loss: 2853.5374\n",
      "Epoch [486/1000], Train Loss: 5454.0680, Test Loss: 2287.7799\n",
      "Epoch [487/1000], Train Loss: 3278.5817, Test Loss: 2558.1291\n",
      "Epoch [488/1000], Train Loss: 3741.5726, Test Loss: 2490.6506\n",
      "Epoch [489/1000], Train Loss: 6496.3330, Test Loss: 2487.4946\n",
      "Epoch [490/1000], Train Loss: 3369.3113, Test Loss: 2462.9362\n",
      "Epoch [491/1000], Train Loss: 5318.0820, Test Loss: 2461.1237\n",
      "Epoch [492/1000], Train Loss: 4266.0665, Test Loss: 2256.1820\n",
      "Epoch [493/1000], Train Loss: 3075.2672, Test Loss: 2414.1428\n",
      "Epoch [494/1000], Train Loss: 8890.3346, Test Loss: 8091.2170\n",
      "Epoch [495/1000], Train Loss: 4667.9892, Test Loss: 2317.6720\n",
      "Epoch [496/1000], Train Loss: 3464.3874, Test Loss: 2677.7939\n",
      "Epoch [497/1000], Train Loss: 3720.9931, Test Loss: 2296.0166\n",
      "Epoch [498/1000], Train Loss: 5800.2615, Test Loss: 2450.8219\n",
      "Epoch [499/1000], Train Loss: 3363.9147, Test Loss: 2413.0061\n",
      "Epoch [500/1000], Train Loss: 3183.2843, Test Loss: 2257.9229\n",
      "Epoch [501/1000], Train Loss: 7326.8436, Test Loss: 2239.1787\n",
      "Epoch [502/1000], Train Loss: 2891.1815, Test Loss: 2228.6800\n",
      "Epoch [503/1000], Train Loss: 3289.2289, Test Loss: 2243.8681\n",
      "Epoch [504/1000], Train Loss: 3089.5483, Test Loss: 2269.8807\n",
      "Epoch [505/1000], Train Loss: 10108.3306, Test Loss: 5864.7252\n",
      "Epoch [506/1000], Train Loss: 9785.1192, Test Loss: 2856.1536\n",
      "Epoch [507/1000], Train Loss: 3043.0829, Test Loss: 2356.2998\n",
      "Epoch [508/1000], Train Loss: 4150.7976, Test Loss: 2210.3906\n",
      "Epoch [509/1000], Train Loss: 3034.5464, Test Loss: 2230.1617\n",
      "Epoch [510/1000], Train Loss: 3385.1318, Test Loss: 2466.1584\n",
      "Epoch [511/1000], Train Loss: 9732.7537, Test Loss: 2251.1143\n",
      "Epoch [512/1000], Train Loss: 2724.5439, Test Loss: 2231.4656\n",
      "Epoch [513/1000], Train Loss: 2728.2246, Test Loss: 2395.8353\n",
      "Epoch [514/1000], Train Loss: 2967.5713, Test Loss: 2457.6499\n",
      "Epoch [515/1000], Train Loss: 4992.9481, Test Loss: 2439.2413\n",
      "Epoch [516/1000], Train Loss: 3379.9161, Test Loss: 2399.3695\n",
      "Epoch [517/1000], Train Loss: 3607.2181, Test Loss: 2423.1347\n",
      "Epoch [518/1000], Train Loss: 5927.9752, Test Loss: 2304.2977\n",
      "Epoch [519/1000], Train Loss: 3624.1377, Test Loss: 2254.9514\n",
      "Epoch [520/1000], Train Loss: 3681.6284, Test Loss: 3204.2172\n",
      "Epoch [521/1000], Train Loss: 5703.8222, Test Loss: 2602.3483\n",
      "Epoch [522/1000], Train Loss: 7580.9916, Test Loss: 2354.1030\n",
      "Epoch [523/1000], Train Loss: 3681.3467, Test Loss: 2342.0208\n",
      "Epoch [524/1000], Train Loss: 3038.3619, Test Loss: 2223.0876\n",
      "Epoch [525/1000], Train Loss: 3060.1206, Test Loss: 2427.3254\n",
      "Epoch [526/1000], Train Loss: 3320.8910, Test Loss: 2414.3861\n",
      "Epoch [527/1000], Train Loss: 6851.8246, Test Loss: 8801.8096\n",
      "Epoch [528/1000], Train Loss: 5294.7388, Test Loss: 2582.4816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [529/1000], Train Loss: 3122.4118, Test Loss: 2379.4646\n",
      "Epoch [530/1000], Train Loss: 3147.4297, Test Loss: 2431.8998\n",
      "Epoch [531/1000], Train Loss: 4011.2741, Test Loss: 2380.8016\n",
      "Epoch [532/1000], Train Loss: 3500.8337, Test Loss: 2529.1981\n",
      "Epoch [533/1000], Train Loss: 3968.5261, Test Loss: 2296.4755\n",
      "Epoch [534/1000], Train Loss: 5797.2487, Test Loss: 2311.6760\n",
      "Epoch [535/1000], Train Loss: 4215.8684, Test Loss: 2274.3221\n",
      "Epoch [536/1000], Train Loss: 3035.8848, Test Loss: 2327.0447\n",
      "Epoch [537/1000], Train Loss: 4564.7441, Test Loss: 2270.8849\n",
      "Epoch [538/1000], Train Loss: 4683.5961, Test Loss: 2248.6411\n",
      "Epoch [539/1000], Train Loss: 3077.4272, Test Loss: 2329.7010\n",
      "Epoch [540/1000], Train Loss: 4187.5928, Test Loss: 2262.9420\n",
      "Epoch [541/1000], Train Loss: 6036.4075, Test Loss: 2426.3124\n",
      "Epoch [542/1000], Train Loss: 6819.7862, Test Loss: 4217.5755\n",
      "Epoch [543/1000], Train Loss: 9676.0347, Test Loss: 6338.1693\n",
      "Epoch [544/1000], Train Loss: 4983.3359, Test Loss: 2519.7710\n",
      "Epoch [545/1000], Train Loss: 2992.2159, Test Loss: 2230.6465\n",
      "Epoch [546/1000], Train Loss: 2970.9006, Test Loss: 2217.9089\n",
      "Epoch [547/1000], Train Loss: 3131.7070, Test Loss: 2245.4868\n",
      "Epoch [548/1000], Train Loss: 3303.0831, Test Loss: 2421.0198\n",
      "Epoch [549/1000], Train Loss: 4307.8985, Test Loss: 2462.4253\n",
      "Epoch [550/1000], Train Loss: 4798.1566, Test Loss: 2406.1764\n",
      "Epoch [551/1000], Train Loss: 9840.2097, Test Loss: 2396.3391\n",
      "Epoch [552/1000], Train Loss: 3344.0401, Test Loss: 2259.0176\n",
      "Epoch [553/1000], Train Loss: 3011.1185, Test Loss: 2665.9146\n",
      "Epoch [554/1000], Train Loss: 4039.8177, Test Loss: 2906.2498\n",
      "Epoch [555/1000], Train Loss: 3621.7784, Test Loss: 2392.3509\n",
      "Epoch [556/1000], Train Loss: 3735.5018, Test Loss: 2385.5362\n",
      "Epoch [557/1000], Train Loss: 3556.8032, Test Loss: 2338.1803\n",
      "Epoch [558/1000], Train Loss: 5585.4757, Test Loss: 2439.4772\n",
      "Epoch [559/1000], Train Loss: 3470.5346, Test Loss: 2224.2660\n",
      "Epoch [560/1000], Train Loss: 7695.7470, Test Loss: 2291.6876\n",
      "Epoch [561/1000], Train Loss: 2910.9041, Test Loss: 2282.7253\n",
      "Epoch [562/1000], Train Loss: 2896.4923, Test Loss: 2353.5103\n",
      "Epoch [563/1000], Train Loss: 5413.7047, Test Loss: 2352.4168\n",
      "Epoch [564/1000], Train Loss: 2896.3062, Test Loss: 2371.6164\n",
      "Epoch [565/1000], Train Loss: 3010.7063, Test Loss: 2443.1535\n",
      "Epoch [566/1000], Train Loss: 8297.1800, Test Loss: 2381.6440\n",
      "Epoch [567/1000], Train Loss: 2964.5696, Test Loss: 2251.9541\n",
      "Epoch [568/1000], Train Loss: 2913.4417, Test Loss: 2438.4965\n",
      "Epoch [569/1000], Train Loss: 3223.7886, Test Loss: 2265.1128\n",
      "Epoch [570/1000], Train Loss: 5819.6171, Test Loss: 2688.9553\n",
      "Epoch [571/1000], Train Loss: 11939.8794, Test Loss: 2601.5749\n",
      "Epoch [572/1000], Train Loss: 4119.2931, Test Loss: 2268.2161\n",
      "Epoch [573/1000], Train Loss: 7778.7148, Test Loss: 2353.9017\n",
      "Epoch [574/1000], Train Loss: 3449.0399, Test Loss: 2194.3136\n",
      "Epoch [575/1000], Train Loss: 3215.4196, Test Loss: 2194.1867\n",
      "Epoch [576/1000], Train Loss: 3020.6933, Test Loss: 2355.3760\n",
      "Epoch [577/1000], Train Loss: 3642.5170, Test Loss: 3187.4770\n",
      "Epoch [578/1000], Train Loss: 13590.5485, Test Loss: 2682.1327\n",
      "Epoch [579/1000], Train Loss: 3726.3835, Test Loss: 2226.8530\n",
      "Epoch [580/1000], Train Loss: 2716.5179, Test Loss: 2410.1013\n",
      "Epoch [581/1000], Train Loss: 3004.8691, Test Loss: 2274.8510\n",
      "Epoch [582/1000], Train Loss: 2922.7117, Test Loss: 2330.1688\n",
      "Epoch [583/1000], Train Loss: 4882.8215, Test Loss: 3056.7348\n",
      "Epoch [584/1000], Train Loss: 13925.4929, Test Loss: 4230.9105\n",
      "Epoch [585/1000], Train Loss: 8879.6110, Test Loss: 2487.3077\n",
      "Epoch [586/1000], Train Loss: 3393.1785, Test Loss: 2661.6860\n",
      "Epoch [587/1000], Train Loss: 9105.4359, Test Loss: 2365.5371\n",
      "Epoch [588/1000], Train Loss: 2894.7208, Test Loss: 2204.4671\n",
      "Epoch [589/1000], Train Loss: 2799.9510, Test Loss: 2354.4169\n",
      "Epoch [590/1000], Train Loss: 3281.6991, Test Loss: 2494.0676\n",
      "Epoch [591/1000], Train Loss: 33405.5559, Test Loss: 2837.0291\n",
      "Epoch [592/1000], Train Loss: 4713.5905, Test Loss: 2500.2999\n",
      "Epoch [593/1000], Train Loss: 3966.4409, Test Loss: 2663.6369\n",
      "Epoch [594/1000], Train Loss: 2846.9972, Test Loss: 2237.2077\n",
      "Epoch [595/1000], Train Loss: 2834.3068, Test Loss: 2243.1966\n",
      "Epoch [596/1000], Train Loss: 3050.3969, Test Loss: 2374.3236\n",
      "Epoch [597/1000], Train Loss: 3132.3866, Test Loss: 2548.4796\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m save_path \u001b[38;5;241m=\u001b[39m ModelName \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Params.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m train_losses, test_losses, is_model_trained \u001b[38;5;241m=\u001b[39m train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Finish timing cell run time\u001b[39;00m\n\u001b[1;32m     25\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[8], line 63\u001b[0m, in \u001b[0;36mtrain_or_load_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pretrained model found. Training from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#optimizer = optim.Adam(model.parameters())  \u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n\u001b[1;32m     64\u001b[0m is_model_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set flag to True after training\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Save losses per epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36mtrain_and_save_best_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, save_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 18\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/GeneratedDataAndVariables')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load('Dataset44_Dist3_Val_Spec.npy')\n",
    "concVal = np.load('Dataset44_Dist3_Val_Conc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "ValSpectra = np.load(\"Dataset44_Dist3_RepresentativeExamples_Spectra.npy\")\n",
    "ValConc = np.load(\"Dataset44_Dist3_RepresentativeExamples_Concentrations.npy\")\n",
    "ValSpecNames = np.load(\"Dataset44_Dist3_RepresentativeExamples_VariableNames.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ValConc = torch.tensor(ValConc).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htjhnson/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=23552, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/home/htjhnson/Desktop/DL-NMR-Optimization/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = Transformer(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ValConc[i]\n",
    "    Prediction = model_aq(ValSpectra[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(44):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8  -  AllAq1\n",
      "0.73  -  AllAq5\n",
      "0.58  -  AllAq25\n",
      "1.76  -  AllAq50\n",
      "0.92  -  ThreeAddedSinglets\n",
      "5.24  -  ThirtyAddedSinglets\n",
      "76.3  -  ShiftedSpec\n",
      "31.1  -  SineBase\n",
      "130.53  -  HighDynamicRange\n",
      "inf  -  HalfZeros\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    print(round(MAPEs[i].item(), 2), \" - \",ValSpecNames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist3 - HD-Range w/ 1's\n",
      "tensor([0.0000e+00, 4.6364e+01, 0.0000e+00, 4.6336e+01, 0.0000e+00, 4.7998e+01,\n",
      "        0.0000e+00, 4.7317e+01, 0.0000e+00, 4.8626e+01, 0.0000e+00, 4.9315e+01,\n",
      "        0.0000e+00, 5.1226e+01, 4.0140e-01, 4.8811e+01, 0.0000e+00, 4.7909e+01,\n",
      "        0.0000e+00, 4.7874e+01, 7.7485e-03, 4.5293e+01, 0.0000e+00, 4.9109e+01,\n",
      "        0.0000e+00, 4.5934e+01, 0.0000e+00, 4.6292e+01, 0.0000e+00, 4.7603e+01,\n",
      "        0.0000e+00, 4.9562e+01, 0.0000e+00, 4.6434e+01, 0.0000e+00, 4.7589e+01,\n",
      "        0.0000e+00, 4.8285e+01, 0.0000e+00, 4.6399e+01, 0.0000e+00, 4.6534e+01,\n",
      "        0.0000e+00, 4.8113e+01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Dist3 - HD-Range w/ 0's\n",
      "tensor([ 0.0000, 46.3436,  0.0000, 46.3809,  0.0000, 48.1082,  0.0000, 47.3491,\n",
      "         0.0000, 48.5945,  0.0000, 49.2881,  0.0000, 51.2026,  0.0000, 48.7722,\n",
      "         0.0000, 47.9417,  0.0000, 47.8865,  0.0000, 45.2982,  0.0000, 49.0940,\n",
      "         0.0000, 45.9500,  0.0000, 46.3138,  0.0000, 47.5874,  0.0000, 49.4940,\n",
      "         0.0000, 46.4088,  0.0000, 47.5304,  0.0000, 48.2139,  0.0000, 46.3838,\n",
      "         0.0000, 46.5293,  0.0000, 48.1574], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "___________\n",
      "___________\n",
      "Dist3 - Blank\n",
      "tensor([0.0468, 0.0000, 0.0000, 0.0000, 0.0000, 0.0241, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.1066, 0.0000, 0.0000, 0.0055, 0.0000,\n",
      "        0.0000, 0.0000, 0.1122, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0415, 0.0000, 0.0000, 0.0095, 0.0000, 0.0000, 0.0539, 0.0691,\n",
      "        0.1041, 0.0000, 0.0430, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Pred = model_aq(ValSpectra[8])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist3 - HD-Range w/ 1's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[9])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist3 - HD-Range w/ 0's\")\n",
    "print(Pred[0])\n",
    "print(\"___________\")\n",
    "print(\"___________\")\n",
    "\n",
    "Pred = model_aq(ValSpectra[10])\n",
    "Pred[0][Pred[0] < 0] = 0\n",
    "print(\"Dist3 - Blank\")\n",
    "print(Pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
